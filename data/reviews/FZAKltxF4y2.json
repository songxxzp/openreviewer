[
    {
        "id": "h8PTMWjE7A7",
        "original": null,
        "number": 1,
        "cdate": 1666281565929,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666281565929,
        "tmdate": 1666281565929,
        "tddate": null,
        "forum": "FZAKltxF4y2",
        "replyto": "FZAKltxF4y2",
        "invitation": "ICLR.cc/2023/Conference/Paper2626/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper builds on existing work showing that artificial neural networks often require only a small fraction of their parameters to perform well. The paper proposes the 'Multiple Subnetwork Hypothesis', which extends this observation to conclude that the weights of a single network could be separated into disjoint subnetworks that could each be trained for separate tasks. This allows a single network to be trained on many different tasks without using increased memory or suffering from catastrohpic forgetting. The authors propose an algorithm called RSN2 to train a single network in this disjoint fashion. This algorithm works by copying the model weights across an additional 'task' dimension, then ensuring that only one weight can be active at a time across the task dimension by masking the weights based on an initial pruning performed at the beginning of training. They test their hypothesis in a number of multitask settings by comparing the performance of a control model on a single task to their multiple subnetwork model across many tasks, and show comparable performance to the control with significant memory savings.",
            "strength_and_weaknesses": "This paper makes an interesting observation - if a network requires only a fraction of its weights to perform well, can the others be put to use for other tasks? The idea of performing multi-task learning this way is certainly interesting, and in a few cases they even show slightly improved performance with it over the control. I like the idea behind the approach, and I think a thorough evaluation of using ensembles of models trained in this way to compare performance against the base model could lead in interesting directions. I do have a number of reservations, however.\n\nWhile this method is interesting, I am not sure what concrete benefit it actually offers. The ability to store multiple trained networks within one network without using additional memory is certainly interesting, but all of those networks are constrained to using the same architecture, and since there is no interaction between the separate tasks it is hard to see how there could be any type of transfer learning or improved performance. The memory savings of this approach are certainly interesting, but it is unclear to me whether how this memory/energy savings compares to the memory/energy savings of existing methods for pruning neural networks. Perhaps the benefit is that existing weight pruning methods require dedicated hardware/software for dealing with sparse matrices to be fully realized? If so however, this is not made fully clear in the paper, nor is it clear that the proposed architecture would avoid having the same limitations. \nAs such, I do not fully understand what the actual contribution here is meant to be. Since there is no comparison done against any other similar approaches for memory/energy savings, it is hard to see if this is a significant contribution. And since each subnetwork is entirely disjoint, I do not see how this method would be likely to lead to improved performance. The results shown are largely comparable to the control in accuracy, save for a few examples where the proposed approach leads to slight improvements - but there is no statistical analysis done to show whether these small improvements are significant or not. There is also no comparison done to the performance of other weight pruning approaches on the same tasks and architectures, either in terms of memory use or model accuracy.\n\nThe most compelling example given is experiment 6, using a transformer architecture. The setting of this experiment is highly unclear to me, however. What is the actual task being performed here - simply text classification? If so, how is the multi-task approach being used, as there is only one task under consideration? Is the multi-task model using an ensemble of models each learning to perform the same task? Are they split by class somehow? How many models are being used? Is the final result shown in table 6 an ensemble across multiple models, or an average, or something else? The table and description make it appear as though only one model is being used, but the proposed approach is explicitly a method for training a single model's weights on multiple tasks.\n\nAs an additional note, the table given for experiment 3 seems highly incomplete, as there is no way to directly compare the authors' model to the baseline. The table should include either the results of the baseline model split across the two task settings, or some sort of ensemble of the experimental model evaluated on the whole dataset. As is, the values in the table are meaningless as there is no way to compare them.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea behind the work - the concept that a neural network requires only a fraction of its weights to perform well - is one that has already been explored at length in the literature. The observation that this could allow a single model to be trained multiple times using disjoint subnetworks is wholly novel, however (at least as far as I am aware).\nWhile I think the idea is interesting, the actual value of the contribution is not clear to me (see above for my specific concerns). I also have a number of concerns about their methodology - specifically the lack of any kind of comparison to other similar approaches, as well as the fact that each experiment was only a single trial rather than an average of multiple trials (or ideally an actual statistical significance test). Some of the results were also not reported clearly (see the previous discussion about table 3), which made them entirely inconclusive.\nThe clarity of the work was also highly lacking in parts, with the exact experimental settings for some of the experiments (notably experiment 6) being highly unclear. It would also have been helpful if the authors more clearly stated what their proposed contribution was (is it just model compression?), as well as a clearer case for why this model would actually lead to improved performance, if that is being claimed as a benefit of the approach.",
            "summary_of_the_review": "Overall I think this paper proposed an interesting idea, but failed to make a convincing case for why it would actually be useful. There were no empirical comparisons done to other existing approaches, the results were not statistically significant, and the authors were not clear about what the intended benefit of their approach was and how it compared to other similar methods. Many of the experiment settings and tables were unclear, and I think it is difficult to conclude much about their approach from the results they presented. I think the paper would need significant rewriting including at a minimum:\na) direct comparisons of the memory/energy savings of their method to other existing approaches\nb) justification for why they expect a performance increase from their method (if this is a claimed benefit)\nc) experiments with multiple trials, including comparisons to other weight pruning approaches and proper comparisons against the baseline\nd) a clearer description of the experimental settings",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2626/Reviewer_aGRW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2626/Reviewer_aGRW"
        ]
    },
    {
        "id": "cSLtY7htOA",
        "original": null,
        "number": 2,
        "cdate": 1666282290649,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666282290649,
        "tmdate": 1666282290649,
        "tddate": null,
        "forum": "FZAKltxF4y2",
        "replyto": "FZAKltxF4y2",
        "invitation": "ICLR.cc/2023/Conference/Paper2626/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed a hypothesis and network structure that allows a pruned network to employ previously unused weights to learn subsequent tasks. Specifically, the authors develop a customized ANN representational structure to isolate individual sub-networks and devise a training procedure named Reduction of Sub-Network Neuroplasticity (RSN2) to train the proposed model. Experimental results show that the proposed method can train an ANN without sacrificing performance or catastrophic forgetting.\n",
            "strength_and_weaknesses": "Strengths:\n1. The multiple subnetwork hypothesis is very interesting. The proposed multi-task training method based on this hypothesis is reasonable.\n2. This paper provides sufficient and detailed implementation details. The pseudocode in Algorithm 1 is clear.\n3. The limitations of the proposed method and future work are clearly discussed.\n\nWeaknesses:\n1. This paper evaluates the proposed method on both convolutional networks and transformer architectures. However, all the considered architectures do not belong to any popular architecture family. It would be better to conduct experiments on some popular architectures, e.g., ResNet [A] and Swin Transformer [B].\n\n2. It is unclear what the essential differences are between the proposed RSN2 and existing pruning methods. Moreover, the comparisons among these methods are still missing. \n\n3. This paper mainly conducts experiments on small datasets, such as MNIST and CIFAR. It would be better to consider some large-scale datasets, e.g., ImageNet.\n\n4. As mentioned in the paper, some weights of subnetworks may be unnecessary for one task but can be used to learn other tasks. In other words, the performance of subnetworks may vary a lot among different tasks. However, such differences are still unclear in the paper. It would be interesting to visualize the performance distribution of subnetworks on different tasks, similar to what is shown in [C]. Moreover, it is also very interesting to see how large the ranking of subnetworks changes from one task to another one (e.g., the best subnetwork on one task may be only located at the median on another task).\n\n\n5. As mentioned in Section 1, the main goal is to\u201cnegate the high computational cost of parameter isolation\u201d. Nevertheless, it seems that there are no results to show how much computational cost can be reduced.\n\n[A] Deep Residual Learning for Image Recognition, CVPR 2016.\n\n[B] Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows, ICCV 2021.\n\n[C] Improving Robustness by Enhancing Weak Subnets, ECCV 2022.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper provides sufficient implementation details.",
            "summary_of_the_review": "The idea is interesting. The experiments can be further improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2626/Reviewer_PPCR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2626/Reviewer_PPCR"
        ]
    },
    {
        "id": "RwS5QG3_ue",
        "original": null,
        "number": 3,
        "cdate": 1666597555192,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597555192,
        "tmdate": 1666597555192,
        "tddate": null,
        "forum": "FZAKltxF4y2",
        "replyto": "FZAKltxF4y2",
        "invitation": "ICLR.cc/2023/Conference/Paper2626/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a method that enables a pruned network to employ previously unused weights to learn subsequent tasks. The exploration along this work leads to multiple subnetwork hypothesis-- that a dense, randomly initialized feedforward ANN contains within its architecture multiple disjoint subnetworks which can be utilized together to learn and make accurate predictions on multiple tasks.",
            "strength_and_weaknesses": "The proposed idea looks very similar to lottery ticket hypothesis [1], such that there is a winning ticket for every task. The problem formulation, the expanded kernel tensor, looks very much like a mixture-of-expert layer while the sparsification and subnetwork identification are very much like the routing function used in a mixture-of-expert network. In this sense, this paper has very limited novelty and add-on contribution compared to related work. At least, the paper should explain why it is different from related work and why this method address multitask learning better.  \n\nEvaluations are not through by only comparing on tiny datasets like MNIST. The tables are extremely hard to interpret. \n\n[1]: https://arxiv.org/abs/1803.03635",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is poorly written with limited novelty:\n- There are not sufficient related work comparison including the lottery ticket, and sparsely activated mixture-of-expert networks. The key points are very similar. \n- The evaluation is not clearly explained and only contains small datasets. ",
            "summary_of_the_review": "The paper needs significant improvement before being accepted by any major conferences. First, please elaborate why this method is different from traditional approaches like lifelong learning with lottery ticket [1], and multitask learning with sparsely gated mixture-of-experts. In a MoE network, one or a few experts are selected for each data, which is a more general formulation that subsumes the proposed method in this paper. The paper should consider improving the baselines and evaluation benchmarks. For example, how does this proposed method improve against traditional lifelong learning methods and multitask methods. If the emphasis is on lifelong learning, then the paper should focus on a streaming of tasks.  If multitask is the priority, then should compare against approaches with stronger performance on multitask learning, and sparsely gated MoE can be a good one.\n\n[1]: https://openreview.net/forum?id=LXMSvPmsm0g ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2626/Reviewer_3BZv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2626/Reviewer_3BZv"
        ]
    },
    {
        "id": "VXQ3Xec6gN",
        "original": null,
        "number": 4,
        "cdate": 1666908988573,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666908988573,
        "tmdate": 1666908988573,
        "tddate": null,
        "forum": "FZAKltxF4y2",
        "replyto": "FZAKltxF4y2",
        "invitation": "ICLR.cc/2023/Conference/Paper2626/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper builds upon the lottery ticket hypothesis and posits the multiple subnetwork hypothesis in which weights of a single network are disjointly allocated to multiple tasks to avoid catastrophic forgetting while achieving competitive task-wise performance on all tasks.",
            "strength_and_weaknesses": "Pros:\n- Perhaps unsurprisingly, the authors are able to demonstrate that disjoint subsets of network weights can be used for different tasks without loss in performance.\n- The approach to more efficient multi-task learning with a single network is simple. \n\nCons:\n- The authors do not adequately discuss related work in multi-task learning.  Learning Sparse Sharing Architectures for Multiple Tasks by Sun et al., 2019 is especially relevant and not discussed at all.\n- The proposed approach is not very interesting since there is no knowledge sharing across tasks by design with disjoint subset of weights to avoid catastrophic forgetting. There is obvious benefit to sharing as recognized by the authors in Experiment 5 in which the UTKFace tasks share convolutional embeddings.  ",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\nThe clarity of the paper is poor in particular in the Methodology section.  In addition, for the experiments, it's unclear what the control set of results corresponds to.  \n## Quality\nThe quality of the experiments is low since there is no discussion of how many seeds were run or any notion of variance of the results.  Additionally, there was no comparison to meaningful competitors beyond the control. One specific question I have is why the result for CIFAR-10 in Table 5 is no better than random at 10%.   \n## Reproducibility\nThe authors do not provide enough detail about the experiments to reproduce results.  Experiments also appear to be run just once without any measure of variance/significance.",
            "summary_of_the_review": "I recommend reject for this paper due to the lack of novelty and insight in the proposed method for using disjoint weights of a single network to learn multiple tasks. There is no sharing at all between tasks unless introduced in an ad-hoc manner manually for specific tasks.  The experiments are also limited and of low quality. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2626/Reviewer_yCzy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2626/Reviewer_yCzy"
        ]
    }
]