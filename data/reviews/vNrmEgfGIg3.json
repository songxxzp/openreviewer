[
    {
        "id": "dl5LjZshuxk",
        "original": null,
        "number": 1,
        "cdate": 1666727638386,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727638386,
        "tmdate": 1666727638386,
        "tddate": null,
        "forum": "vNrmEgfGIg3",
        "replyto": "vNrmEgfGIg3",
        "invitation": "ICLR.cc/2023/Conference/Paper6275/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "A method is proposed for filtering spans to be labeled in a semi-Markov CRF, which empirically reduces training/inference complexity and increases evaluation metrics relative to basic CRF or basic semi-CRF.",
            "strength_and_weaknesses": "The paper is clearly written. The method is, to my knowledge, original.\n\nIt does not compare to state-of-the-art methods, which generally use something like transformer decoders with greedy decoding. (https://paperswithcode.com/sota/named-entity-recognition-ner-on-conll-2003, https://paperswithcode.com/sota/named-entity-recognition-ner-on-ontonotes-v5, https://paperswithcode.com/sota/named-entity-recognition-on-ace-2005) The results are a bit worse than SOTA.\n\nThe motivation is to address the shortcoming of semiCRF that it considers all spans. It turns out that the proposed method is not only faster but gets better evaluation scores than pure semi-CRF. (The authors might want to speculate on why that is-- it isn't clear to me a priori that the proposed method should be more accurate.) However the proposed method has some shortcomings: it introduces a hyperparameter beta, it has the possibility of discarding good spans in the filtering phase, and the worst-case complexity is higher. For many tasks it is possible to give a reasonable upper bound on the span length that needs to be considered, so complexity of semiCRF training/inference is reduced to linear from quadratic in sentence length. What about a baseline that uses a maximum segment length and allows null labels only for spans of length 1? I think that should address the weaknesses of semiCRF in a cleaner way and I would not be surprised to see it working quite well in terms of evaluation.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: mostly high\nQuality: a bit weak\nNovelty: probably new\nReproducibility: good",
            "summary_of_the_review": "The proposed method does well in ablation experiments (reducing to CRF, or expanding to full semi-CRF) but some obvious baselines were neglected (greedy decoding over Transformer, bounding maximum span of semiCRF).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6275/Reviewer_YFd7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6275/Reviewer_YFd7"
        ]
    },
    {
        "id": "A8t_RaDWkUy",
        "original": null,
        "number": 2,
        "cdate": 1666755094552,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666755094552,
        "tmdate": 1666755094552,
        "tddate": null,
        "forum": "vNrmEgfGIg3",
        "replyto": "vNrmEgfGIg3",
        "invitation": "ICLR.cc/2023/Conference/Paper6275/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on reducing the inference complexity of semi-Markov CRF and thus achieving better performance on NER by introducing a filtering step before forward algorithm and Viterbi decoding. More specifically, segments that are predicted to be null and whose label does not achieve the highest predicted score according to an additional local classifier will be filtered. Experiments are conducted on the task of NER and evaluated on three datasets: CONLL-2003, OntoNotes 5.0 and Arabic ACE.",
            "strength_and_weaknesses": "Strength:\n1. Experiment results on performance and throughput verify the claim that Filtered Semi-Markov CRF are better than CRF and Semi-Markov CRF on NER tasks.\n\nWeakness:\n1. Some parts of the paper is a little hard to follow, especially Section 3. For example, what is the model architecture of local classifier compared to the global ones?  Do they share parameters? Besides, without reading the rest part in Section 3, it is hard to recognize that Eq. (6) chooses segments whose labels are not predicted to null but also achieves the highest scores. There also exist some typos. For example, in the 4th row from the bottom of Page 3, $j_k<i_{k^*}$, which should be $i_{k^*} < j_k$.\n\n2. Lack of detailed analysis about why filtering leads to better performance. FSemiCRF does Viterbi decoding on a subset of segments compared to the original SemiCRF, whose optimum should be worse than that of SemiCRF in theory. I'm curious whether the introducing of local filtering models and corresponding segment filtering in training or segment filtering during inference is the primary cause of performance improvement. an ablation study and analysis will be helpful. For example, how FSemiCRF performs if it is trained as described in Section 3 but still follows original SemiCRF (i.e., run Viterbi decoding on all the segments) during inference?\n\n3. Model throughput needs further explanation. During inference, both SemiCRF and FSemiCRF have two steps: (1) to calculate segment scores for all segments; (2) Viterbi decoding, in which SemiCRF does Viterbi decoding directly while FSemiCRF runs segment filtering and does Viterbi decoding on the rest segments. It will be better to show which step is the bottleneck (e.g., to show empirical wall-clock time of these two steps) and how FSemiCRF improves SemiCRF in the second step (e.g., to show wall-clock time of segment filtering and Viterbi decoding on the rest segments).\n\n4. What about training complexity (e.g., wall-clock time) of FSemiCRF compared to SemiCRF.",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of accelerating the inference of SemiCRF is novel and the proposed filtering method is well-motivated. However, current analysis and experiment results do not support the claim well and more detailed analysis is needed. ",
            "summary_of_the_review": "In general, this paper focuses on an important problem w.r.t. SemiCRF which prevents its widespread application as CRF and proposes an applicable method for resolving it. However, this method lacks thorough analysis (especially experiments), which weakens the claims made in the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6275/Reviewer_JeoQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6275/Reviewer_JeoQ"
        ]
    },
    {
        "id": "2Imhof2H3M",
        "original": null,
        "number": 3,
        "cdate": 1667817830410,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667817830410,
        "tmdate": 1669647962548,
        "tddate": null,
        "forum": "vNrmEgfGIg3",
        "replyto": "vNrmEgfGIg3",
        "invitation": "ICLR.cc/2023/Conference/Paper6275/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a semi-Markov conditional random field model that integrates a filtering step to eliminate irrelevant segments when performing named entity recognition in text. According to the authors this helps reducing the complexity compared to a semi-Markov CRF and dramatically reduces the search space. \n",
            "strength_and_weaknesses": "Strengths\n\nAttempt to learn segmentation jointly with classification (here named entity recognition or NER) that does not rely on an expansion of the label space (as is commonly done in NER by specializing the labels into begin label, intermediate and outside label).\n\nWeaknesses\n\n-\tThe model relies on an extra classifier that filters segments. The model is jointly trained by considering the filter classification loss, the segmentation loss and the NER classification loss. It is not clear from the paper how during training the computational complexity is reduced as initially all potential segments need to be considered in the inference step during training. That means that the complexity is equal to the complexity of the semi-Markov model.  The authors acknowledge that \u201cduring training, especially in the first stage, the graph size can be large since the filtering is poor.\u201d It is not clear how exactly the filtering is performed during training. \n-\tBecause the segment filtering is the contribution of the paper, its approach (which should be explained in detail) should be separately evaluated, that is, its behavior and reduction in complexity during training and ablating the assumptions made here. \n-\tResults could have been evaluated on many more NER datasets. \n-\tThe claim that the proposed filtering model drastically reduces the search space compared to a CRF and SemiCRF model is not seen in the model\u2019s throughput numbers, especially not during training. Moreover, the effect of dynamic programming on the model\u2019s throughput during inference on the test data could be investigated. Is dynamic programming also used in the decoding of the CRF and SemiCRF models?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity (and consequently the reproducibility) could be improved (see remark above). The originality is difficult to judge given that the details of the contribution are missing. ",
            "summary_of_the_review": "The paper is below the threshold for acceptance at ICLR because of:\n- Important details and evaluations are missing.\n\nSeveral of my questions were answered during the rebuttal, for which I thank the authors. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6275/Reviewer_z7Y1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6275/Reviewer_z7Y1"
        ]
    }
]