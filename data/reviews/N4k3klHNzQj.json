[
    {
        "id": "y2j44uiDpG",
        "original": null,
        "number": 1,
        "cdate": 1665827367968,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665827367968,
        "tmdate": 1668735090228,
        "tddate": null,
        "forum": "N4k3klHNzQj",
        "replyto": "N4k3klHNzQj",
        "invitation": "ICLR.cc/2023/Conference/Paper4272/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "While graph neural networks (GNNs) and graph transformers (GTs) have been popular for graph representation learning recently, they have their own limitations: \n* GNNs, especially those based on message passing, are poor at modelling long-range dependencies and\n* GTs suffer from quadratic computation complexity. \n\nThis paper adapts a specialised multi-layer perceptron (MLP) architecture, viz., MLP-Mixer, initially proposed in the image domain [1], to graph-structured data for modelling long-range interactions with linear computational cost.\n\n[1] MLP-Mixer: An all-MLP Architecture for Vision, In NeurIPS'21.\n\nExperiments on benchmark graph datasets demonstrate the effectiveness of the proposed method.\n\n___",
            "strength_and_weaknesses": "\\\n**Strengths**\n\n\\+ The paper is well-organised and Table 1 clearly lists key differences of MLP-mixer for images and the proposed adaptation for graphs.\n\n\\+ The proposed method can potentially address important limitations of both GNNs and GTs.\n\n\n\\\n**Weaknesses**  \n\n\\- The authors claim that GNNs suffering from oversquashing leads to overfitting while existing literature [2] has shown that it actually leads to underfitting.\n\n[2] On the Bottleneck of Graph Neural Networks and its Practical Implications, In ICLR'21.\n\n\\- The authors attribute the poor long-range modelling ability of very deep GNNs to oversquashing but the poor performance could be due to a combined effect of multiple factors (e.g., oversquashing, oversmoothing, vanishing gradients, etc.).\n\n\\- There are no experiments to support the claim that Graph MLP-Mixer can capture long-range interactions in graph datasets.\n\n___",
            "clarity,_quality,_novelty_and_reproducibility": "\\\n**Clarity**\n\nWhile the paper seems well-organised, its clarity can be improved in technical reasoning aspects.\n\nSpecifically, more clarity on the relationship between oversquashing and overfitting is needed.\n\nThe relationship should also be positioned with existing literature which has argued that oversquashing actually leads to underfitting [2].\n\n[2] On the Bottleneck of Graph Neural Networks and its Practical Implications, In ICLR'21.\n\n\\\n**Quality**\n\nThe paper's quality can be improved through more empirical evaluations in support of the claims.\n\nExperiments on oversquashing are needed to support tha claim that Graph MLP-Mixer can mitigate oversquashing, e.g., TreeNeighboursMatch problem in prior work [2] (Section 4.1 in the paper).\n\nMore experiments are needed to confirm that Graph MLP-Mixer can capture long-range dependendcies, e.g., the authors could consider Code2 dataset used in prior work [3] (Section 5.4 in the paper).\n\n[3] Representing Long-Range Context for Graph Neural Networks with Global Attention, In NeurIPS'21.\n\n\\\n**Novelty**\n\nThis is the first adaptation of MLP-Mixer to graph structured data.\n\nMLP-Mixer can potentially address limitations of popular models (GNNs and GTs) on graphs.\n\n\\\n**Reproducibility**\n\nThe main part and the supplementary part include enough material, e.g., dataset details, baselines with references, hyperparameters, effect of different components, for an expert to replicate the results of the paper.\n\n___",
            "summary_of_the_review": "While the proposed method is potentially promising, more clarity and empirical evaluation are needed in support of the claims made in the paper.\n\n___\n\n\nUpdate:\nI have read all the reviews and their responses. I have raised my score from 5 to 6. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4272/Reviewer_jedP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4272/Reviewer_jedP"
        ]
    },
    {
        "id": "zOoEFPFCsP",
        "original": null,
        "number": 2,
        "cdate": 1666582147069,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582147069,
        "tmdate": 1666685674343,
        "tddate": null,
        "forum": "N4k3klHNzQj",
        "replyto": "N4k3klHNzQj",
        "invitation": "ICLR.cc/2023/Conference/Paper4272/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper explores the MLP Mixer based approach on graph learning. It claims that the architecture can get rid of over-squashing or high complexity problem for long-range connections.",
            "strength_and_weaknesses": "### Strengths\n1. The paper proposes to apply MLP mixer to graph learning.\n2. The authors introduce a method for extracting patches from the graph and adding positional information for the patch.\n3. The experimental results show that the graph MLP-mixer can improve the performance of the existing method.\n\n### Weaknesses\n1. The paper claims that the method can get rid of the over-squashing problem of the GNN model. It would be great to see the larger scale dataset with longer-range connections on the OGB dataset, such as OGBG-code2.\n2. The paper selects METIS as the graph partitioning algorithm to generate patches. It can be interesting to have the ablation study for how many benefits the METIS can provide against random partitioning.\n3. The Molpcba results in Table 5 do not seem very convincing, it would be better to see a fair comparison by having a similar number of parameters. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, and the method is evaluated on multiple benchmarks.",
            "summary_of_the_review": "It is an interesting exploration of the MLP mixer for graph learning and outperforms multiple existing works, but the experiments need to be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4272/Reviewer_Fxzu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4272/Reviewer_Fxzu"
        ]
    },
    {
        "id": "LMNrR-zzIA",
        "original": null,
        "number": 3,
        "cdate": 1666762463450,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666762463450,
        "tmdate": 1666762785767,
        "tddate": null,
        "forum": "N4k3klHNzQj",
        "replyto": "N4k3klHNzQj",
        "invitation": "ICLR.cc/2023/Conference/Paper4272/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Message passing-based GNNs suffer from over-squashing and poor long-range dependencies problems. \n\nGraph Transformers have been proposed because the global attention mechanism can alleviate the above problems.\n\nHowever, Graph Transformers have a computational complexity proportional to the square of the number of nodes, just like transformers in other fields.\n\nIn this paper, the authors propose an MLP-Mixer model for graph-structured data to overcome these issues.\n\nThe original MLP-Mixer, which is also unaffected by over-squashing and weak long-range dependencies, takes Multi-layer perceptrons (MLPs) in the place of the attention module.\n\nContrary to visual data, which has a grid structure, graphs have an irregular structure, so they use a clustering algorithm to extract the patches for MLP-Mixer.",
            "strength_and_weaknesses": "pros:\n\n[+] The paper is well-written and organized.  Related works and the background concepts are described enough to understand the main idea of Graph MLP-Mixer.\n\n[+] The problem definition and the development process are quite logical and straightforward.\n\n\n---\n\ncons:\n\n[-] It would be nice if there were more analyzes or novel modules for graphs.\nIt seems like a simple extension of MLP-Mixer with a little bit classic graph clustering module.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I wrote a point about novelty at the above section.",
            "summary_of_the_review": "If the authors provide a clear novel point or analysis on this, I'll raise the score.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4272/Reviewer_oFvZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4272/Reviewer_oFvZ"
        ]
    },
    {
        "id": "nYd2aUHUn4",
        "original": null,
        "number": 4,
        "cdate": 1666912042245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666912042245,
        "tmdate": 1666912042245,
        "tddate": null,
        "forum": "N4k3klHNzQj",
        "replyto": "N4k3klHNzQj",
        "invitation": "ICLR.cc/2023/Conference/Paper4272/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a Graph MLP-Mixer model that generalizes the MLP-Mixer from images to graphs. Specifically, it first adopts graph clustering algorithms to split the graph into overlapping patches. And then different GNNs are utilized to get patch embeddings. To encode the positional information in graphs, Node PE and Patch PE are used.  Experimental results show the proposed method can outperform the base GNN encoder. ",
            "strength_and_weaknesses": "Strength:\n- This paper generalizes MLP-Mixer from images to graphs and demonstrates the improvement in performance.\n- The designs of Graph MLP-Mixer are reasonable. And the authors show the differences between MLP-Mixer for images and graphs\n- This paper is well written and easy to follow. Detailed descriptions of the background are provided to make the paper more understandable.\n\nWeakness:\n- The contributions of this paper are limited. The authors simply move the MLP-mixer from CV domain to graph domain without explaining why this generalization is required for graphs.\n- The motivation of this paper is that GNNs suffer from over-squashing and poor long-range dependencies. But these two issues mainly occur in large graphs. For graph classification task, the graphs are usually small. Can the authors provide some experimental results to demonstrate these two problems?\n- The Positional Encoding and drop edges are  important for Graph MLP-mixer. How about just add these two component into the base models, such as GCN and GatedGCN?\n- The performance is not comparable to the state-of-the-art models, for example, GNN-AK[1] , Graph transformers and other models on  the OGB Leaderboards. Although the performance is not necessary to be very good, the authors should demonstrate the superiority of the Graph MLP-Mixer than other models. For MLP-Mixer in CV domain, it can achieve comparable performance with CNNs and ViT.\n\n[1] Zhao, Lingxiao, et al. \"From stars to subgraphs: Uplifting any GNN with local structure awareness.\" arXiv preprint arXiv:2110.03753 (2021).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Generally, the writing is relatively clear and follows a logical structure. However, the authors should demonstrate the superiority of the proposed method and why we should use the MLP-Mixer for the graph classification task. Just generalizing the MLP-Mixer from images to graphs makes the novelty limited. The writing and experimental settings are clear, which may help reproduce.\n",
            "summary_of_the_review": "This paper proposes a Graph MLP-Mixer model that generalizes the MLP-Mixer from images to graphs. The technical details are reasonable. But the authors should demonstrate the superiority of the proposed method and why we should use the MLP-Mixer for the graph classification task. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4272/Reviewer_BAFm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4272/Reviewer_BAFm"
        ]
    }
]