[
    {
        "id": "0-3uEfjWiq",
        "original": null,
        "number": 1,
        "cdate": 1666640290367,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640290367,
        "tmdate": 1666640290367,
        "tddate": null,
        "forum": "HUCgU5EQluN",
        "replyto": "HUCgU5EQluN",
        "invitation": "ICLR.cc/2023/Conference/Paper3483/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presented a new method for self-supervised learning on sparse time series data. Specifically, Transformers for EHR data with self-supervised learning -- TESS was proposed with an input binning scheme and a combination of both missingness masks and event values. Experimental analysis on two public EHR datasets shows the effectiveness of the proposed method. The main contributions of this paper are the proposed bin-based data pre-processing scheme and the joint processing model for multiple data types in EHR applications.",
            "strength_and_weaknesses": "**Strengths**\n\n\\+ This paper proposed a feasible and effective approach for self-supervised learning on EHR data.\n\n\\+ Each of the designed/used components within the proposed architecture was well-motivated.\n\n\\+ The proposed method was shown to perform better than the compared previous methods.\n\n\\+ An ablation study was presented to show the effectiveness of the components within the proposed method.\n\n\\+ The paper is generally well-written and easy to follow.\n\n\n**Weaknesses**\n\n\\- The technical novelty and contributions are a bit limited. The overall idea of using a transformer to process time series data is not new, as also acknowledged by the authors. The masked prediction was also used in prior works e.g. MAE (He et al., 2022). The main contribution, in this case, is the data pre-processing approach that was based on the bins. The continuous value embedding (CVE) was also from a prior work (Tipirneni & Reddy 2022), and also the early fusion instead of late fusion (Tipirneni & Reddy, 2022; Zhang et al., 2022). It would be better to clearly clarify the key novelty compared to previous works, especially the contribution (or performance gain) from the data pre-processing scheme.\n\n\\- It is unclear if there are masks applied to all the bins, or only to one bin as shown in Fig. 1.\n\n\\- It is unclear how the static data (age, gender etc.) were encoded to input to the MLP. The time-series data was also not clearly presented.\n\n\\- It is unclear what is the \"learned [MASK] embedding\" mean in the SSL pre-training stage of the proposed method.\n\n\\- The proposed \"masked event dropout scheme\" was not clearly presented. Was this dropout applied to the ground truth or the prediction? If it was applied to the prediction or the training input data, will this be considered for the loss function?\n\n\\- The proposed method was only evaluated on EHR data but claimed to be a method designed for \"time series data\" as in both the title and throughout the paper. Suggest either tone-down the claim or providing justification on more other time series data.\n\n\\- The experimental comparison with other methods seems to be a bit unfair. As the proposed method was pre-trained before the fine-tuning stage, it is unclear if the compared methods were also initialised with the same (or similar scale) pre-trained model. If not, as shown in Table 1, the proposed method without SSL performs inferior to most of the compared methods.\n\n\\- Missing reference to the two used EHR datasets at the beginning of Sec. 4.",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the clarity and the quality of this work are good with several contributions to EHR data processing. The novelty is a bit limited as mentioned above in the Weaknesses section. It seems to be able to reproduce the proposed method, but some technical details could have been better presented with sufficient details for better reproducibility.",
            "summary_of_the_review": "This paper is generally okay with clear motivation and a new approach to process EHR data with shown better performance on two widely used public datasets. There are a few concerns regarding the novelty and contributions, experiments and some statements/claims, but they could possibly be addressed within the rebuttal phase. As a result, I recommend a positive rating at the current stage and may change my recommendation after the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3483/Reviewer_EBR4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3483/Reviewer_EBR4"
        ]
    },
    {
        "id": "Q5dYvI09If",
        "original": null,
        "number": 2,
        "cdate": 1666666067627,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666067627,
        "tmdate": 1666666067627,
        "tddate": null,
        "forum": "HUCgU5EQluN",
        "replyto": "HUCgU5EQluN",
        "invitation": "ICLR.cc/2023/Conference/Paper3483/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper presents a novel solution to effectively handle the sparsity of data in EHR using a combination of techniques including self-supervision and aggregation based normalization of sparse inputs. The authors compared their model on two real world datasets for three different tasks. They claim that the scheme effectively improves modeling efficacy over state of the art methods\n",
            "strength_and_weaknesses": "Some of the key strengths of the paper are as follows:\n\n- The paper studies a very important problem that is prevalent in longitudinal EHR data. The results against the chosen baselines show strong performance and may have significant impact in using AI for clinical settings. Of special note the method performs better than XGBoost (see more below).\n- The proposed method of self-supervision is novel and simple. While the method is proposed for transformer based architecture, it could be adopted to most architectures such as LSTM as well other forms of attention based models.\n- The paper is well presented and the illustrations makes the paper easy to follow. The variables used in the model have been communicated. The experiments have also been conducted on two well known benchmarks and the results should be reproducible with some level of effort.\n\nThe paper can be improved upon by addressing some of the concerns below:\n\n- While the identified problem is well motivated, the clinical significance seems to lack sufficient depth. Compared to other forms of EHR, ICU data is well populated with less randomly missing data points. Also, typical EHR records, especially for chronic diseases, can go back for decades with large periods of data that are irrelevant to prediction for the time point of interest. The claim of the paper as a general solution for EHR data is thus not well supported. \n- Regarding novelty, the first aspect of the proposed method considers aggregation of data into bins, while novel in a model integrated manner, is a well known and often used pre-processing technique to handle EHR data. In fact, other forms of aggregation such as those based on higher level EHR concepts have also been used to reduce data sparsity. \n- In order to interpret the usefulness of the proposed method, it is critical to understand the performance over different sub-groups. At the very minimum, a typical \"Table one\" describing the data distribution should be presented. It is also important to understand whether the self-supervision can amplify biases in the data and discuss any proposed solutions to that\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of the paper lends itself well for comprehension and the details provided, along with the datasets used, should make the paper verifiable. The proposed SSL method is novel and can be important by its own merit\n",
            "summary_of_the_review": "The authors have analyzed an important and well-known problem in clinical AI. \nWhile the experiments on the chosen baseline is promising, the claims, especially considering the clinical perspective, is not well substantiated. Of note, the SSL method by itself is novel and from the results seem to contribute the most to the model performance. The method can be arguably applied to other architectures as well. It will be useful for the authors to deliberate on the claims e.g. either narrow the claims to ICU (i.e more regular) settings or provide experimental validations on longer term EHR data. Apart from the aforementioned aspects the authors may also want to consider the following aspects:\n\n- Provide intuitions in the paper (rather than form citation) about the CVE approach to capture temporal information over simpler methods such as cosine embedding\n- From Table 1, it seems TE(SS-) doesn't perform better than STraTS. This may indicate that SSL is the main driver of the performance. With this in mind, the authors may consider a simple extension of the SSL to base models such as LSTM or more classical DL method for clinical AI such as RETAIN and SAnD.\n- In the implementation section, the authors note that \n> We apply zero-mean and unit-standard deviation normalization of all inputs in x. We also clip outliers using a threshold of three median absolute deviations from the median. These steps allow for stable training without requiring clinical knowledge of normal variable ranges.\n    * While this is indeed beneficial from a ML modeling standpoint, it is not always clinically reasonable to neglect domain information such as typical values of clinical variables. EHR data is often noisy and such steps are often crucial to ensure that the model learns from reasonable manifestations of patient data\n- The authors claim that TESS handle the temporal granularity in a simpler manner than StraTS. While some justification is provided, it may be important to quantify these using some measures such as perplexity and/or computational requirement.\n- The discussions are model interpretations are not well supported. While t-SNE plots have been reported in Figure 3 there are two important problems with the analysis; (a) t-SNE can provide widely different manifolds based on the hyper-parameters. These should be reported. Comparison with other stable methods such as PCA could be considered. (b) the plots in Figure 3b can be argued to have some overlap - it is not evident from the naked eye whether TESS produces well separated representations. \n- On a similar note, for the model to be of real-world usage, the authors may want to investigate on the feature importance/attributions driving the improved performance\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The authors haven't conducted any group level analysis. If the model is to be applied in a real-world setting this is of paramount importance\n",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3483/Reviewer_HAwr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3483/Reviewer_HAwr"
        ]
    },
    {
        "id": "gkeuMS9L81c",
        "original": null,
        "number": 3,
        "cdate": 1666670000282,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670000282,
        "tmdate": 1666670000282,
        "tddate": null,
        "forum": "HUCgU5EQluN",
        "replyto": "HUCgU5EQluN",
        "invitation": "ICLR.cc/2023/Conference/Paper3483/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper focuses on the robust representation of sparse time series EHR data. The authors propose a new model TESS, Transformers for EHR data with Self Supervised learning, a self-supervised Transformer-based architecture, to represent the patients\u2019 data as fixed-size vectors with the consideration of irregular observations and missing values. The experiments on real-world datasets show that the proposed model outperforms the baselines.",
            "strength_and_weaknesses": "Strength:\n1. The authors present a transformer model to represent the patients\u2019 static variables and dynamic observations as fixed-size vectors. \n\nWeakness:\n1. The novelty is somewhat incremental. \n    a) Model training: SSL pre-training and fine-tuning are also common tricks for deep learning models [1,2,8]. The work is an application of Transformer to the healthcare domain.\n    b) Model framework: Lots of existing studies (e.g., [5,6,7]) have combined static variables and dynamic variables when learning patient representations and considering the time intervals and missing value problems. \n\n2. The aggregation methods (e.g., mean, maximal, minimal, last observed variable) used in the paper might not be enough for real-world clinical settings. The frequency of lab tests could imply the patients\u2019 disease severity. A simple aggregation might cause information loss.\n\n3. The experiments could be improved:\n    a) It would be better if the authors conducted experiments to show which aggregation methods are better for clinical settings (e.g., mean, maximal, minimal, last observed variable)?\n    b) The lengths of different patients\u2019 EHR data vary a lot (e.g., from 1~3 days to several weeks in ICUs). It is tricky to select the value of L for different patients. It would be better if the authors conducted experiments to show how the model is sensitive to L for different groups of patients, especially for patients with extremely short or long observation periods. Moreover, in Figure 4, when L is set 48, the performance becomes worse, which is unusual and worth explaining why.\n    c) The statistics of dataset (e.g., age, gender) are missing.\n    d) It is unclear how to learn patient representation [REP].\n\n4. Experiment results show that the proposed model performs just as comparable to baselines on some tasks (e.g., phenotyping).\n\n5. The paper writing could be improved. For example, the authors claim that \u201cthe use of Transformer in modelling sparse irregular time series with tabular data has not been widely explored.\u201d However, Transformer has been widely used in modeling EHR data (e.g., [1-4]).\n\n6. The implementation code is not available.\n\nReferences:\n\n[1] Rasmy L, Xiang Y, Xie Z, et al. Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. NPJ digital medicine, 2021, 4(1): 1-13.\n\n[2] Li Y, Rao S, Solares J R A, et al. BEHRT: transformer for electronic health records. Scientific reports, 2020, 10(1): 1-12.\n\n[3] Li F, Jin Y, Liu W, et al. Fine-tuning bidirectional encoder representations from transformers (BERT)\u2013based models on large-scale electronic health record notes: an empirical study. JMIR medical informatics, 2019, 7(3): e14830.\n\n[4] Zhang X, Qian B, Cao S, et al. INPREM: an interpretable and trustworthy predictive model for healthcare//Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020: 450-460.\n\n[5] Baytas I M, Xiao C, Zhang X, et al. Patient subtyping via time-aware LSTM networks//Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. 2017: 65-74.\n\n[6] Tipirneni S, Reddy C K. Self-supervised transformer for multivariate clinical time-series with missing values. arXiv preprint arXiv:2107.14293, 2021.\n\n[7] Yuan Luo, Peter Szolovits, Anand Dighe, and Jason Baron. 2018. 3D-MICE: integration of cross-sectional and longitudinal imputation for multi-analyte longitudinal clinical data. JAMIA 25, 6 (2018), 645\u2013653.\n\n[8] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is readable, but the writing could be improved.\n2. The paper is neither novel from machine learning nor from clinical informatics.\n3. There are some major concerns in the experiments.\n4. The implementation code is not available. \n",
            "summary_of_the_review": "The authors present a transformer model to represent the patients\u2019 static variables and dynamic observations as fixed-size vectors. The paper is neither novel from machine learning nor from clinical informatics. There are also some major concerns in the experiments.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3483/Reviewer_fWW7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3483/Reviewer_fWW7"
        ]
    },
    {
        "id": "3tLgjlguja",
        "original": null,
        "number": 4,
        "cdate": 1666672153007,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672153007,
        "tmdate": 1666672153007,
        "tddate": null,
        "forum": "HUCgU5EQluN",
        "replyto": "HUCgU5EQluN",
        "invitation": "ICLR.cc/2023/Conference/Paper3483/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper focuses on the problem of learning from irregularly sampled time series data. Similar to the NLP and vision community, it employs a self-supervised pretraining approach using Transformers to improve the modeling of irregularly sampled time series data. It first applies a binning technique to covert the irregularly sampled time series to regularly sampled data to apply Transformers models. The paper also proposes self-supervised learning tasks for such data to learn better representation. Experiments show that the proposed approach outperforms several recent approaches on multiple downstream tasks across two datasets.  ",
            "strength_and_weaknesses": "### Strengths\n1. The paper proposes a simple approach to improve the learning of irregularly sampled time series data using self-supervised pretraining.\n2. Experimental results show the effectiveness of the approach when compared to baselines and other recent approaches.\n3. The paper focuses on the task of learning from irregularly sampled data which is important in many domains.\n\n### Weaknesses\n1. The paper claims to propose the idea of using a binning technique to transform irregularly sampled time series into a regularly sampled sequence data with missing values. This has been widely studied in literature as discussed in this survey ( Section 4.1, [1]). \n2. The novelty is marginal as the self-supervised training is similar to mTAN and some other approaches mentioned in the paper and the input representation is widely known.\n3. The experimental results seem to be just marginally better than a standard baseline of XGBoost even when the proposed approache uses self-supervised pretraining. XGBoost clearly outperforms the TE(SS-) approach on all datasets. Could this be addressed by pretraining on a large scale time series data? As shown in NLP and vision community, scale of the data really matters. Its something the authors could try to further improve the performance. Since this seems like mostly an empirical work, I would also expect to see some study towards the transfer learning capabilities of the pretrained models. \n4. How do the authors deal when different features in the input have different sampling rate which is quite common in healthcare data? How does the proposed approach differentiates between missing data (from binning) and masked data for pretraining? \n5. It is not clear if the baseline approaches are trained with input representation mentioned in the current paper or their corresponding papers. Some of the baseline approaches (like mTAND) do not utilize the static inputs. Were these baseline approaches trained with static inputs? mTAND results on PhysioNet seem to be the same as mentioned in the original paper where no static inputs were used.\n\n\n#### References\n1. Shukla, Satya Narayan, and Benjamin M. Marlin. \"A survey on principles, models and methods for learning from irregularly sampled time series.\" arXiv preprint arXiv:2012.00168 (2020).",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** The paper mentions that using continuous value time embeddings where actual time values are taken into account is useful. It is not clear why would this matter as after binning they are just regularly sampled data. Can the authors describe how the baseline approaches were trained? How does the proposed approach differentiates between missing data (from binning) and masked data for pretraining? Are masks used during finetuning stage too? What aggregation function is used, comparison with different aggregation functions? \n\n**Novelty:** The novelty is marginal as the self-supervised training is similar to mTAN and some other approaches mentioned in the paper and the input representation is widely known.\n\n**Reproducibility:** Not reproducible. The paper doesnt include any code or provide hyperparameters required to reproduce the results.\n",
            "summary_of_the_review": "My recommendation for this paper is weak reject. Although the paper brings some interesting ideas from vision and NLP community, it fails to deliver in terms of the results.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3483/Reviewer_6eHs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3483/Reviewer_6eHs"
        ]
    }
]