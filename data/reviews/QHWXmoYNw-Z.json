[
    {
        "id": "44H0qjVm9qR",
        "original": null,
        "number": 1,
        "cdate": 1666612901099,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612901099,
        "tmdate": 1670338120823,
        "tddate": null,
        "forum": "QHWXmoYNw-Z",
        "replyto": "QHWXmoYNw-Z",
        "invitation": "ICLR.cc/2023/Conference/Paper158/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to ensemble multiple pre-trained models with different architecture by estimating and thresholding p-value for out-of-distribution detection. Experimental results show the effectiveness of the proposed method.\n",
            "strength_and_weaknesses": "Strengths\n\n+ Theoretical analysis on the p-value looks interesting.\n\n+ The proposed method clearly outperforms other methods in most cases.\n\nWeaknesses\n\n- No ablation study on the ensembling strategy. For example, authors could simply average the detection score or do majority voting and take them as baselines. \n\n- The comparison is not fair. Model ensembling is a kind of guaranteed strategy to improve the performance in machine learning. Even authors showed consistently good performance, empirical contribution is somewhat limited in that sense. Rather, comparison with other (baseline) ensemble strategies should be fair, but missed.\n\n- Is there a reason why the proposed method is combined with MSP, Energy, and KNN only? I wonder if the rank of methods holds, e.g., if ZODE-Mahalanobis is on average worse than ZODE-KNN. Authors could add more results in the appendix.\n\n- Typo: DensNet @ Table 2",
            "clarity,_quality,_novelty_and_reproducibility": "Writing is clear and well-written in general. The ensemble strategy is not novel, but the method to ensemble them, estimating and thresholding p-value might be novel. They also provided some theoretical analysis on it. I believe this work is reproducible with the provided code.",
            "summary_of_the_review": "I am mostly happy with this paper, except for missing ablation study on the choice of ensembling strategy. Again, I believe ensemble method almost always improves the performance of machine learning models, so comparison with other ensemble methods should be more reasonable than standalone models. Please answer my concerns above.\n\n\n**post-rebuttal**\n\nThe additional experiments for comparison with simple ensemble methods addressed my concern, but it turned out that the performance of the proposed method is not so better than baseline ensemble methods, in terms of AUC. Ensembling based on the p-value and corresponding analysis are interesting, but by looking at the performance, I can't see the reason why we need such a sophisticated ensemble strategy.\nAt this point, I decided not to change my initial rating.\n\nBy the way, authors claim that TPR is \"out of control\" and show FPR under somewhat random TPR for other ensemble methods, which I do not fully understand what they actually mean by. Maybe authors observed discrete changes on TPR?\n\nIt seems authors also count the fact that their work is first to take advantage of \"(publicly available) model zoo\" for OOD detection as the main contribution, but by looking at other reviews, it seems other reviewers do not care about it much?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Nothing special.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper158/Reviewer_FQTf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper158/Reviewer_FQTf"
        ]
    },
    {
        "id": "X51wesfj2S",
        "original": null,
        "number": 2,
        "cdate": 1666644354236,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644354236,
        "tmdate": 1666644354236,
        "tddate": null,
        "forum": "QHWXmoYNw-Z",
        "replyto": "QHWXmoYNw-Z",
        "invitation": "ICLR.cc/2023/Conference/Paper158/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the method ZODE, an ensemble scheme using pretrained models for OOD detection. The proposed scheme employs the Benjamini\u2013Hochberg procedure to decrease the false discovery rate of OOD samples by combining the base model outputs. The authors conduct experiments on CIFAR10 and Imagenet datasets to evaluate the ZODE performance compared with SOTA ODD methods, and they show that the method improves the ODD detection performance.",
            "strength_and_weaknesses": "- The proposed approach is interesting and presents promising results. \n- The idea of using an ensemble of classifiers for OOD is not new, but using the BH procedure to combine the base classifiers is clever. BH generally produces fewer Type I errors and performs best in sparse cases. What about the non-sparse cases?\n\n\n- I miss some qualitative discussions. What is the impact of the pretrained models on the results? Is there a minimum quantity of base classifiers? Do they need to be trained in some specific dataset? Is it necessary that the training dataset of the base classifiers be related to the test samples? Does the increase in the number of base classifiers improve the results? Is there a limit to that? What is the impact of adding diversity in the base classifiers in this context?\n- The authors do not consider the computational cost of using all base classifiers in the inference process when comparing the performance with the standalone SOTA methods. Could distilling the ensemble's knowledge using a distillation approach improve the process for computational cost purposes? \n- Is the approach generic to another type of data or just work with images?\n- How the SOTA methods were chosen? Are those the SOTA?\n- The authors did not discuss the limitation of the proposed method. It will be meaningful to discuss the gap between the experiments in the current version of the paper and the real-world applications.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and logically structured. Unfortunately, the code is unavailable, making it difficult to reproduce the experiments.\nThe idea of using an ensemble for OOD detection is simple but seems effective based on the paper's results. However, the idea is not entirely new, and the authors should compare the proposed model with other ensemble approaches. Therefore, the comparison with standalone methods seems not fair. \nThe analysis does not consider the computational cost added by the base classifiers. \n",
            "summary_of_the_review": "The trustworthiness of ML approaches is significantly related to their ability to recognize out-of-distribution samples. This is an important topic with a lot of applications. The proposed paper presents one interesting approach but fails to analyze different perspectives of the problem. The authors focus on error and do not consider the computational cost of the proposed method or the comparison with other ensemble methods. The author should discuss the proposed approach's limitations and explore the qualitative aspects of the results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no ethical concerns about the paper.\n",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper158/Reviewer_Wjqr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper158/Reviewer_Wjqr"
        ]
    },
    {
        "id": "7Z2isNvqJ1",
        "original": null,
        "number": 3,
        "cdate": 1666687062287,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687062287,
        "tmdate": 1666687062287,
        "tddate": null,
        "forum": "QHWXmoYNw-Z",
        "replyto": "QHWXmoYNw-Z",
        "invitation": "ICLR.cc/2023/Conference/Paper158/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents an approach\u00a0to detect out-of-distribution (ODD) samples by an ensemble\u00a0of pre-trained models. For a given test sample, OOD scores are computed based on a set of pre-trained models and a suitable p-value is determined whether the sample is OOD while maintaining a given true positive rate. experiments are conducted on common OOD\u00a0benchmarks such as CIFAR-10, ImageNet, and Places365.",
            "strength_and_weaknesses": "Strengths:\n\n1. Considering an ensemble of models to detect OODs is a simple yet effective idea.\n\n2. The draft is clearly written and easy to follow.\n\n3. Authors conducted extensive experiments of the standard benchmarks and the results are state of the art.\n\nWeaknesses:\n\n1. Overall novelty is limited as p-value based OOD detection is explored in conformal prediction literature.\n\n2. What is the score function used in this work? This is important and needs to be discussed.\n\n3. In OOD detection, FPR is more important than TPR. Does theorem 1 provide any guarantee on FPR?\n\n4. Authors are encouraged to discuss the computational complexity of Algorithm 1. For each test sample, it requires computing the score \nof all the training samples for all the models. This can be expensive.\n\n5. Does the OOD detection performance depend on the quality of pre-trained models?\n\n6. Some related references are missing \n\na. Cai, F.; and Koutsoukos, X. Real-time out-of-distribution detection in learning-enabled cyber-physical systems. ICCPS, 2020: \nb. Kaur, R. et al., iDECODe: In-distribution equivariance for conformal out-of-distribution detection. AAAI, 2022\n\n(a) considers p-value based conformal scores for OOD detection.\n(b) proposed an approach to combine multiple p-values with a bound on TPR. ",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to the above section for clarity and novelty. \nSome additional details are required to ensure reproducibility",
            "summary_of_the_review": "The idea of considering an ensemble of models for detecting OOD is interesting. However, the overall novelty is lacking. Please address the concerns in the weaknesses section.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper158/Reviewer_Ty7t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper158/Reviewer_Ty7t"
        ]
    }
]