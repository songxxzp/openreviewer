[
    {
        "id": "rJDhndHzRJ",
        "original": null,
        "number": 1,
        "cdate": 1666581518238,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666581518238,
        "tmdate": 1670553057264,
        "tddate": null,
        "forum": "z57WK5lGeHd",
        "replyto": "z57WK5lGeHd",
        "invitation": "ICLR.cc/2023/Conference/Paper3560/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a Regularized Lottery Ticket Hypothesis inspired network to deal with the well-known catastrophic forgetting problem. By only updating partial weights in the network, the proposed method claims to have a good performance on both the classes in previous and current sessions.",
            "strength_and_weaknesses": "Strengths: \n\n1). The paper is well present. In particular, Fig 1 and Algorithm 1 are clear and informative. \n\n2). The method is simple and easy to follow.\n\nWeaknesses:\n\n1). From the comparison experiments, it is hard to distinguish whether the proposed method has a good performance since there are plenty of works for few-shot class-incremental learning that the paper fails to have a comparison with. \n\n- Few-shot incremental learning with continually evolved classifiers. (CVPR \u201821)\n- Few-shot class incremental learning by sampling multi-phase tasks. (TPAMI)\n- Subspace regularizers for few-shot class incremental learning. (ICLR \u201822)\n- Metafscil: A meta-learning approach for few-shot class incremental learning. (CVPR \u201822)\n- Constrained few-shot class-incremental learning. (CVPR \u201822)\n- Few-shot class incremental learning via entropy-regularized data-free replay. (ECCV \u201822)\n- Few-shot class-incremental learning from an open-set perspective. (ECCV \u201822)\n\nAs an ICLR 23 submission, most of the baseline methods in this paper are from 2 years ago. An important baseline in FSCIL, CEC (Zhang et al., CVPR 2021), is even not compared with. I want to note that the above-mentioned methods seem to outperform the proposed method in this paper by a large margin. \n\n2). The proposed method seems to have a better result than F2M in the final session, but with a much better results in the session 1, which may indicate that the proposed method cannot prevent the network from forgetting, which is opposite from what is claimed in the introduction.\n\n3). The proposed method sounds not that new to me. I found the authors fail to give a reference for AANet (Adaptive Aggregation Networks for Class-Incremental Learning, CVPR \u201821). However, from my own perspective, the proposed method shares a similar motivation with AANet since both want to alleviate the catastrophic forgetting by fixing some parameters inside the network. Though CIL and FSCIL are different benchmarks, AANet seems to perform better and have a capability of preventing catastrophic forgetting. The authors should discuss the comparison with AANet in details. \n\n\n-------After rebuttal-----\n\nThe authors offer detailed response and clarification. I am happy to increase the score, but I sitll have the concern about the similar motivation to AANet. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of this work is good, but the novelty and experimental performance are limited, which impedes the quality of this paper.  Details are stated in the Weaknesses section. ",
            "summary_of_the_review": "Despite good paper writing, the novelty and the experimental performance are the largest issues of this paper. Most of the baseline methods are from 2 years ago. An important baseline in FSCIL, CEC (Zhang et al., CVPR 2021), is even not compared with. \n\nSee weaknesses for details. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3560/Reviewer_5XQs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3560/Reviewer_5XQs"
        ]
    },
    {
        "id": "-ge2vLhegsB",
        "original": null,
        "number": 2,
        "cdate": 1666631478724,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631478724,
        "tmdate": 1670811598013,
        "tddate": null,
        "forum": "z57WK5lGeHd",
        "replyto": "z57WK5lGeHd",
        "invitation": "ICLR.cc/2023/Conference/Paper3560/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Inspired by Regularized Lottery Ticket Hypothesis, which hypothesizes that smooth subnetworks exist within a dense network, this paper propose Soft-SubNetworks (SoftNet), an incremental learning strategy that preserves the learned class knowledge and learns the newer ones. The SoftNet jointly learns the model weights and adaptive soft masks to minimize catastrophic forgetting and to avoid overfitting novel few samples in Few Shot Class-Incremental Learning. Experiments are conducted on the CIFAR-100 and miniImageNet datasets under various settings.",
            "strength_and_weaknesses": "Strengths:\n\nS1) Few-shot Class Incremental Learning is an important and active topic in the community, which is addressed in this work.\n\nS2) The paper is generally well written and easy to follow.\n\nS3) The idea of jointly learns the model weights and adaptive soft masks to minimize catastrophic forgetting and to avoid overfitting novel few samples is interesting and makes sense. \n\nS4) The method significantly surpasses many State-Of-The-Art methods under various variants on the benchmarks.\n\nWeakness:\n\nW1) Some of the details are not fully discussed. For example in section 3.3, why the update scheme of $\\theta$ can effectively regularize the weights of the subnetworks for incremental learning.\n\nW2) Though the effectiveness of SoftNet is verified via extensive experiments. These experiments are conducted only on two datasets (CIFAR-100 and miniImageNet), which seem not adequate enough to show the robustness of the propsed method. Results on other popular benchmark datasets such as CUB200 may add to the convincingness of the SoftNet.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is well written and easy to follow. The idea is quite interesting and the methodology is in general clarified. The authors did not mention if they would release the source project on public.",
            "summary_of_the_review": "The paper tackles an important problem, proposing an incremental learning strategy that preserves the learned class knowledge and learns the newer ones.  Experiments are comprehensive and seem to show improved performance in two common benchmarks. \n\nI have carefully reviewed all the comments and responses. Though a reviewer show concerns about the inadequate comparison with SOTAs, I still consider that this paper provides an interesting and effective idea. I would like to keep my original rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3560/Reviewer_ZoLy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3560/Reviewer_ZoLy"
        ]
    },
    {
        "id": "JOzZI2LrR0a",
        "original": null,
        "number": 3,
        "cdate": 1666827245437,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666827245437,
        "tmdate": 1666827245437,
        "tddate": null,
        "forum": "z57WK5lGeHd",
        "replyto": "z57WK5lGeHd",
        "invitation": "ICLR.cc/2023/Conference/Paper3560/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a brand-new few-shot class-incremental learning method called soft-subNetworks (SoftNet). The two main problems in few-shot class-incremental learning contains catastrophic forgetting and overfitting. The former one asks the new session training not to interfering the former session and the latter one asks only updating a few parameters irrelevant to previous tasks. Inspired by Regularized Lottery Ticket Hypothesis, the paper points out that the subnet can perform on-par or better than the whole network. Taking usage of this hypothesis, the authors split the network into a major subnetwork and a minor one. In the base classes session, soft-subnetwork parameters and weight score are learned. In the incremental learning session, minor parameters of the subnetwork are updated. The SoftNet surpasses the state-of-the-art baselines over datasets.",
            "strength_and_weaknesses": "Strengths:\n1.\tBorrowing idea from regularized lottery ticket hypothesis to incremental learning is a novel and ingenious idea for its both maintaining performance in subnetwork and solving overfitting.\n2.\tThe paper is well written for its clear logic, figures, equations, and organization.\n3.\tThe outstanding performance of SoftNet is convincing.\nWeaknesses:\n1.\tThe choice of top-c weights is handcraft without theory support. Just comparing results on overall accuracy is not enough. The authors can refer to the works in subnetwork to refine their analysis.\n2.\tThe random choice of parameter follows uniform distribution, but condensation argues that there are always some weights much important than the others. The authors should explain why they use uniform distribution or the future improvement in this prior.\n3.\tIs there any possibility of extending SoftNet to the other kind of incremental learning? Or is the SoftNet limited to only class-incremental learning?\n",
            "clarity,_quality,_novelty_and_reproducibility": "see strength",
            "summary_of_the_review": "The paper outstands for its novel and the ingenious subnetwork training strategy. Also, the surpassing experiment results on benchmarks convince its idea. However, the theoretical analysis is quite week so that the readers cannot get the clear knowledge of the mechanism of subnetwork. The theoretical analysis should include how the split of major and minor help the forgetting and overfitting. Overall, the advantage outweighs its disadvantage so it can be recommended to the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3560/Reviewer_5Go7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3560/Reviewer_5Go7"
        ]
    }
]