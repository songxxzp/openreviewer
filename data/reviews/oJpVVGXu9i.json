[
    {
        "id": "Nv4SjvlqFdm",
        "original": null,
        "number": 1,
        "cdate": 1666844551448,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666844551448,
        "tmdate": 1669000586823,
        "tddate": null,
        "forum": "oJpVVGXu9i",
        "replyto": "oJpVVGXu9i",
        "invitation": "ICLR.cc/2023/Conference/Paper6462/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes studies federated learning under differential privacy with heterogeneous users, where the goal is to learn simultaneously learn good global and local models. The authors propose to only share weights of the representation networks and locally train the user-specific heads for personalization. Theoretical analysis is provided for linear networks, and experiment results verify the effectiveness of the approach.",
            "strength_and_weaknesses": "Strengths:\n- The idea of separately training representation and classification heads is intuitively nice. \n- The experiment results look promising. The proposed algorithm outperforms the DP-FedAvg and existing personalized FL baselines, and is the only method that outperforms local training.\n- Theoretical results demonstrate significant improvement for linear networks.\n\n\nWeakness:\n- The experiment results would be more convincing if more experiments on done on larger datasets like ImageNet.\n- This is just a curious question: how would the size of the classification head and representation network affect the performance, and what is a good way to select these parameters in practice? Would it depend on the training task, e.g. for simple classification it suffices to use a small head, but for more complicated tasks larger head is needed?\n\n\n=== Update ===\n\nAfter reading other reviewers' comments, it seems that there is a mismatch between the error metric used in this work (matrix L2 norm) and Jain et.al (Frobenius norm). Therefore, the theoretical improvement is unclear. I'm adjusting my score to 6 accordingly.\n\n=== Update ===\nThe authors have addressed my concern about the mismatch in the error metric. I also believe the authors have adequately addressed most concerns of other reviewers. Thus I will change back to my original score of 8. ",
            "clarity,_quality,_novelty_and_reproducibility": "The ideas and results are clearly presented.\n\nThe algorithmic ideas and experiment results are promising and exceed previous works.\n\nIt seems that the code is not provided via link nor in the supplementary materials. ",
            "summary_of_the_review": "This work proposes a framework for DP federated learning that separately trains the representation network and local heads. The performance surpasses previous works. The theoretical results for linear networks intuitively prove the effectiveness of the idea. I only have some minor issues stated in the weakness part.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6462/Reviewer_4btx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6462/Reviewer_4btx"
        ]
    },
    {
        "id": "GfGHQY91vg",
        "original": null,
        "number": 2,
        "cdate": 1666985709091,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666985709091,
        "tmdate": 1670286965044,
        "tddate": null,
        "forum": "oJpVVGXu9i",
        "replyto": "oJpVVGXu9i",
        "invitation": "ICLR.cc/2023/Conference/Paper6462/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the (federated) private model personalization problem, and provides a utility guarantee in the bilinear case.\nThe technical contribution is to prove a utility bound when using DPSGD to learn the shared representation, which improves upon the best known bound (that uses sufficient statistics perturbation instead) by a factor of $\\sqrt d$.",
            "strength_and_weaknesses": "## Strengths\n- The presentation is crisp.\n- The $O(\\sqrt d)$ improvement upon the method of Jain et al. is interesting (though the comparison may not be fair).\n- The empirical results are promising.\n\n## Weaknesses\n\nMy main concern is that the vast majority of ideas, which are presented as novel, are not new at all, they have been studied both in the federated learning literature and under differential privacy.\n\nThe idea of learning shared representations differentially privately while learning a personalized model for each user/client is exactly the setting of private model personalization (Jain et al. and others). In fact, they describe a general alternating minimization algorithm (their Algorithm 1), and explicitly mention DP-SGD as one possible way of solving the server-side problem, which is what Algorithm 1-2 in this paper does. Granted, their analysis is carried only for sufficient statistics perturbation and not gradient descent, so the utility bound in Theorem 5.1 is new in this setting. But the framework and ideas presented in the paper are well-established, for example arguments made in Sections 1-3 are well known.\n\nThe contribution of the paper is much more narrow than what the paper claims:\n1) Proving a utility bound (which includes a \\sqrt d improvement upon the SOTA) in the linear case, *in the low-privacy regime* (more discussion would help here, to understand how restrictive this condition is, for example some estimate of how large epsilon has to be in realistic scenarios).\n2) Good empirical results on CIFAR and EMNIST.\n\nI believe the presentation is misleading, and should be significantly updated to reflect the real contributions. All discussion up to and including Section 3 should be presented as a review of existing ideas.\n\nOn the technical side: the statement of results lacks rigor at times.\n- The privacy guarantee should be stated more carefully. The rigorous guarantee should use the joint-(R)DP notion.\n- Corollary 5.1 should be clearly stated -- how should one interpret statements such as \"we have the following trade-off\"? It is unclear, as stated, what conditions should hold, what parameters are fixed, what parameters need to be known for running the algorithm, etc.\n- As pointed out by other reviewers, the assumptions for the main result are not the same as Jain et al., this must be further discussed.\n\n#### Experiments:\nThe experimental results are promising. I have a few questions/suggestions:\n- Did the authors compare to DP-FedAvg-fb of Yu et al.? (i.e. fine-tuning the last layer instead of the entire model)? This appears closer to the setting under consideration.\n- It is always worrisome to see that all competing methods are worse than even the non-FL baseline. Does one really expect all these other methods to fail in practice? Surely there is a regime in which isolated training would fail (say in the extreme case of one example per client). Can the authors give a more nuanced discussion?\n\n\n==== Post-discussion update ====\nThe authors acknowledged that the idea of learning shared DP representation while learning a personalized classifier per client, is not new, and promised to rephrase their contributions to reflect this, and to better place the work in the context of existing work.\nOther improvements have been made, including discussions about the technical assumptions, and empirical support for the claims made in remark 3.1.",
            "clarity,_quality,_novelty_and_reproducibility": "Summary (see previous section for details):\n- Clarity: well-written, but misleading presentation.\n- Quality: some minor technical issues. Expanding the discussion of utility bound and/or empirical results can help.\n- Novelty: limited.",
            "summary_of_the_review": "Ideas are not new. The improvement of utility bound is interesting (but the low privacy condition needs to be a clear caveat) and the empirical results are promising (but need a perhaps more nuanced discussion).",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6462/Reviewer_CCKF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6462/Reviewer_CCKF"
        ]
    },
    {
        "id": "q0mtvGZxY7",
        "original": null,
        "number": 3,
        "cdate": 1667159284788,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667159284788,
        "tmdate": 1669743026553,
        "tddate": null,
        "forum": "oJpVVGXu9i",
        "replyto": "oJpVVGXu9i",
        "invitation": "ICLR.cc/2023/Conference/Paper6462/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose to solve the statistical heterogeneity problem in federated learning with differential privacy by separating the model into two parts, one part for each client is personalized and different while the other part on the server is shared for all the clients. The communication between the client and the server is the non-private gradient for updating the part on the server, and privacy is guaranteed by using the Gaussian mechanism on the server. The authors proved rates for their method on a linear setting for the utility-privacy tradeoff which improves the SOTA by $\\sqrt{d}$. The empirical performance is also very good.",
            "strength_and_weaknesses": "Major strengths: \n1. This idea is intuitive and the empirical performance is very good.\n2. The theoretical statements are very detailed.\n\nMajor weaknesses: \n1. The LRL problem assumes that the true model is $y_{ij}=(w_{i}^*)^T (B^*)^T x_{ij}$ where there is no noise in $y_{ij}$. This is different from Jain et al. (2021). I know that you can set the $\\sigma_F=0$ in Jain et al. (2021) Lemma 4.4, but this is not mentioned in the current paper, so the comparison may not be fair. Another problem is that the rate in Jain et al. (2021) Lemma 4.4 is for $\\\\|\\cdot\\\\|_F$ while the rate in Theorem 5.1 in this paper is for $\\\\|\\cdot\\\\|_2$. As we know $\\\\|\\cdot\\\\|_2 \\leq \\\\|\\cdot\\\\|_F$ for matrices, this comparison is not fair.\n2. Corollary 5.1 is not solid. It claims a lower bound but we only have an upper bound for the utility in Theorem 5.1. This can be revised and needs to be corrected.\n3. In the experiments, the authors said 'we do not perform any data augmentation, as we observe that naive data augmentation\nfor DP training leads to worse accuracy, as also reported in De et al. (2022).' But I think De et al. (2022) actually recommends doing augmentation in the correct way '... leveraging the benefits of data augmentation by averaging\nper-example gradients across multiple augmentations of the same image before the clipping operation\n(Hoffer et al., 2019) ...' This needs further explanations.\n\nMinor weaknesses:\nThe writing can be improved. There are many cases that one notation is used before it is introduced. For example,\n1. The first paragraph of Page 7 refers to Algorithm 3 which has $s_i$ but it is introduced in Section 5.1. \n2. The 'AGGREGATION' method set on Page 7 needs further explanation on how to transform $\\\\{G_i^t\\\\}_{i\\in\\mathbb{C}^t}$ to $G^t$. \n3. On Page 7, the authors said 'line 5 in Algorithm 4 uses Gaussian mechanism' twice, but this is not shown in the algorithm.\n\nOn page 16, there is a missing remark. Similar in Remark F.1.\n\nOn page 18, Equation (29) is based on Remark C.1 but I do not see how. By the way, I guess $\\mathrm{dist}(B^*, B^t)\\leq 1$ is from a property of $\\mathrm{dist}$, but please make it clear somewhere.\n\nThe proof for Lemma D.4 and Lemma F.1 is missing.\n\n\n\nSome typos:\n1. Page 5, Algorithm 1, Line 4, I guess $p_c$ should be $p_g$.\n2. Page 6, Remark 4.1, $S_\\alpha(p_c, \\sigma_g)=\\ldots$ needs to be corrected.\n3. Page 10, references, there are duplicates for 'Durmus Alp Emre Acar, et al. (2020, 2021)'.\n4. Appendix A.4, the sensitivity is $2\\zeta_g/n$.\n5. Appendix C, the equation above (MSP) should be $E_{x_{ij}}[\\langle \\frac{1}{\\sqrt{N}}A(X),\\frac{1}{\\sqrt{N}}A(X) \\rangle ]$. Similar problem in Page 23 last equation.\n6. Appendix Page 18, the first equation, the second line, the last part $\\eta_g\\sigma_g \\zeta_g W^t$ should be $\\eta_g\\sigma_g \\zeta_g W^t/n$\n7. Appendix Eq (42), I guess $F_t$ is $F^t$.\n8. On page 24, the first line in Section E.2, I think $I_k$ is $I_d$.\n9. On page 26, the second last line, Hardt & Price (2014) does not have Algorithm 6.\n\n----------------------------\nUpdate: The major weaknesses listed above are all solved by the authors. However, I have new concerns (listed below) about this paper, which made me change my score to 5 since those concerns are not easily addressable in the camera-ready version of this paper if it is accepted.\n1. The writing of this paper is a bit misleading since the proposed framework, CENTAUR, is very similar to existing methods where the only difference is in the implementation of the special setting, Linear Representation Learning. Therefore, the introduction of this paper needs rewriting. \n2. The question raised by reviewer CCKF about the comparison to DP-FedAvg-fb is unanswered. The authors have compared their method to DP-FedAvg-*ft*, which uses local fine-tuning of the full model. However, the *fb* variant fine-tunes the last layer only, which follows almost the same idea as the newly proposed method in this paper, therefore it is more valuable to compare with the *fb* variant.\n3. Although the empirical experiment shows good results in the given settings with $\\varepsilon\\in \\{0.25, 0.5, 1, 2, 4\\}$ for DP, there could be more settings (e.g., more choices of $\\varepsilon$) for a more detailed comparison between CENTAUR and other methods. The current results may be surprising to the experts in federated learning since other FL methods are mostly worse than the non-FL baseline even when $\\varepsilon=4$. Adding experiments with $\\varepsilon > 4$ or even $\\varepsilon \\rightarrow \\infty$, which deactivates the effect from DP, may help address this issue.\n\n---\nUpdate-2: The new responses from the authors include the code (which solves the name issue about 'DP-FedAvg-fb' and makes the results reproducible) and some experimental support for Remark 3.1. \n\nI would change my recommendation score to 6 since this paper points out a reasonable and promising direction of federated learning with differential privacy with theoretical and experimental evidence. I choose not to give an 8 because the theoretical analysis only covers the noiseless setting, which is much narrower than Jain et al. (2021), and the major theoretical contribution, 'the cross-validation based initialization scheme', is not used in the deep learning experiments.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read and the overall quality is good. It is not super novel in terms of the main algorithm (Figure 2 by Collins et al. (2021) looks almost the same as the Figure 2 in this paper), but the authors' insight is clear, and the analysis of utility and privacy guarantee is kind of solid. I believe the results are reproducible but the code is not provided so I cannot try it by myself.",
            "summary_of_the_review": "The authors propose to use a sharing-representation-only method to solve the FL+DP problem in deep learning, and they provide theoretical guarantees along with good empirical results. I would raise my recommendation score if the problems above are corrected and explained. \n\n\\-----\n\nAfter reading other reviewers' comments, I agree that this paper is misleading regarding its contributions. On Page 2, the authors said, 'In our scheme, we train a single differentially private global representation extractor while allowing each participant to have a different personalized classifier head.' However, such a scheme is already described in the existing literature, not a novel one. Remark 3.1 could be novel but needs further theoretical and experimental verification.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6462/Reviewer_Usxb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6462/Reviewer_Usxb"
        ]
    },
    {
        "id": "1jowgbYT3jk",
        "original": null,
        "number": 4,
        "cdate": 1667422765281,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667422765281,
        "tmdate": 1670291300152,
        "tddate": null,
        "forum": "oJpVVGXu9i",
        "replyto": "oJpVVGXu9i",
        "invitation": "ICLR.cc/2023/Conference/Paper6462/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a new algorithm for DP federated learning (with trusted central server) under model personalization assumptions. The algorithm (CENTAUR) works by having the clients only send a subset of their parameters to the central server to be trained privately. The final (classification) layer is trained on each client non-privately. The authors provide rigorous analysis for the specific problem of linear representation learning (LRL) and further support the efficacy of their algorithm empirically with experiments. ",
            "strength_and_weaknesses": "Strengths:\n\n- The paper provides a substantial rate improvement over existing work for the problem of LRL\n- The paper provides sound empirical reasoning for their method (i.e. that data representations are less prone to distribution shift)\n- The paper supports its results empirically\n\nWeakness:\n- The $O(d/n)$ rate is stated is for the matrix 2-norm but Jain et al. 2021 provide convergence guarantees for the Frobenious norm, making the comparison to existing work less clear.\n- The proposed algorithm is not incredibly novel compared to prior work. \n- The presentation of the analysis for the theoretical results is a bit verbose. Theorem 5.1 and 5.2 are not easily interpreted on their own and it seems more useful instead to provide a more direct statement of Corollary 5.1.\n- Given the rate improvement, an investigation into what DP lower bounds suggest is necessary would be nice.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper makes precise statements and provides adequately detailed proofs. The experiments are likewise well documented. ",
            "summary_of_the_review": "The paper makes progress for the problem model personalization under user level differential privacy and federated learning constraints. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6462/Reviewer_hVPm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6462/Reviewer_hVPm"
        ]
    }
]