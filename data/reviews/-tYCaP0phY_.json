[
    {
        "id": "KyrdPPazPc",
        "original": null,
        "number": 1,
        "cdate": 1666543134434,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666543134434,
        "tmdate": 1666679891947,
        "tddate": null,
        "forum": "-tYCaP0phY_",
        "replyto": "-tYCaP0phY_",
        "invitation": "ICLR.cc/2023/Conference/Paper3598/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a post-training quantization (PTQ) weight/activation quantization method.\nThe gist is to simultaneously optimize a multiplicative element-wise scale factor applied to the target tensor\nand the quantization step size, under the reconstruction loss. The method can be seen as closely related in spirit\nto AdaQuant and AdaRound, with the critical difference that instead of adding optimal noise to the tensor before\nrounding, multiplicative noise is used instead. If can also be seen to be related to the weight scaling method of\n(Nagel et al, 2019), in the sense that weight tensors are scaled prior to rounding, but with the critical difference\nthat DFQ does this prior to quantization, while FlexQuant does it during the quantization step.  \n\nExperiments carried out on image classification (ImageNet), natural language understanding (GLUE) and natural language\ngeneration (WikiText2 and PTB) show superior performance to AdaRound and AdaQuant.",
            "strength_and_weaknesses": "* Strengths:\n\n  * The main idea is simple and attractive from both a theoretical and practical standpoint, extending neatly the\n  existing algorithmic frameworks (e.g. BRECQ and QDrop) \n  * Evaluation scope covers a wide variety of domains (language, vision) and architectures (ResNet, MobileNet, ViT).\n\n* Weaknesses:\n\n  * There are points where the paper is unclear. Please see comments on clarity below.\n  * The experimental results reported for BRECQ in table 3 are different from those reported in the original papers.\n  I presume this is because a per-tensor quantization comparison is being used (but the authors should confirm this).\n  * No study in conjunction with the related weight range equalization method introduced in\n  (Nagel et al., 2019) is shown, nor is a comparison carried out. Please see comments below on quality.",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity:\n\n  * I found section 3.3 (Leveraging the Importance of Pre-Trained Weights) confusing. As it is part of section 3\n  (Methodology), one would expect that it describes some part of the algorithm. Furthermore, by the use of the word\n  \"Leveraging\", one would expect that the weight magnitude is somehow explicitly used to alter the rounding or scaling\n  policy introduced in the previous section (equation 2). It turns out (if my understanding is correct), that section\n  3.3 only shows an analysis on the rounding scheme resulting from experiments on MobileNetV2 and ResNet-18 using the\n  algorithm presented in section 3.2. I think this analysis should be moved to the experimental results section.\n  * It is not clear whether baselines have been modified to also work in a per-tensor PTQ reconstruction setup (this\n  may explain the difference with reported results in the original paper for BRECQ).\n\n* Novelty:\n\n  * Up to my knowledge, the formulation is technically novel and sufficiently different from previously introduced\n  techniques.\n\n* Quality:\n\n  * Similar to the range equalization method presented in (Nagel et al., 2019), this method attempts\n  to re-scale weights, albeit during quantization and not prior to it. While not applicable to all architectures,\n  weight pre-scaling is fast and works with any quantization algorithm. However, no experiment is run to see how this\n  method compares to this baseline, and whether using pre-scaled weights as input to this method further improves\n  results.\n\n* Reproducibility:\n\n  * The authors provide sufficient detail to reproduce the experiments and express their intention to make code publicly\n  available.",
            "summary_of_the_review": "I think this work provides a simple yet effective weight scaling approach within the PTQ framework, and,\nup to my knowledge, the problem formulation is qualitatively different from previous approaches. While evaluation\nis broad in terms of the benchmarks used, I have two concerns. First, the results reported for the \nBRECQ + AdaRound baselines are different from the ones in the original paper. Second, I would have liked to also see a\ncomparison/combination with the weight equalization approach of (Nagel et al., 2019). The clarity of the presentation\ncould be improved.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3598/Reviewer_y3vc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3598/Reviewer_y3vc"
        ]
    },
    {
        "id": "oXiAIRPQHI",
        "original": null,
        "number": 2,
        "cdate": 1666617693664,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666617693664,
        "tmdate": 1666617693664,
        "tddate": null,
        "forum": "-tYCaP0phY_",
        "replyto": "-tYCaP0phY_",
        "invitation": "ICLR.cc/2023/Conference/Paper3598/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This submission proposed to conduct post-training quantization by learning element-wise scaling parameters $\\boldsymbol{S}$. Pre-quantized parameters are divided by the scaling parameters and then rounded to integer. Basically, the proposed method is a subtle re-training for pre-trained parameters for quantization.",
            "strength_and_weaknesses": "Weaknesses:\n1. As the author mention that the previous works used element-wsie addition in parameters refine before quantization while the proposed method used division, why addition is not considered in the submission? Since both division and addition can refine parameters, would it be better if both are used?\n2. It seems author miss mentioning how much data is necessary to learn $\\boldsymbol{S}$.",
            "clarity,_quality,_novelty_and_reproducibility": "1. There are some confusing points, please refer to Strength And Weaknesses.\n2. As the $\\boldsymbol{S}$ shares the same size as original parameters, its learning space should be huge. Normally Quantization-aware Training (QAT) absorbs the refinement process into training, however as the refinement is left in PTQ, a natural question raises that: the volumn of training data affects the performance of the refinement, though more data could bring improvement, the same amount of data may bring more improvement if it is used in QAT. How much is data is a breakthrough point?\n",
            "summary_of_the_review": "Basically, the core method can be considered as a subtle re-training for pre-trained parameters for quantization. Besides, the author missed some important points for precise evaluation. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3598/Reviewer_vGda"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3598/Reviewer_vGda"
        ]
    },
    {
        "id": "3G3w7s6cvB",
        "original": null,
        "number": 3,
        "cdate": 1666626366292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626366292,
        "tmdate": 1666681498658,
        "tddate": null,
        "forum": "-tYCaP0phY_",
        "replyto": "-tYCaP0phY_",
        "invitation": "ICLR.cc/2023/Conference/Paper3598/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new approach named FlexRound to improve the post-training quantization of deep neural networks. Previous methods primarily rely on element-wise addition for the rounding function. The proposed FlexRound, instead, uses element-wise division, which can exploit the parameter magnitude to update the quantization scale. The authors validate the efficacy of FlexRound over various CV and NLP tasks, together with a variety of network architectures.",
            "strength_and_weaknesses": "Strengths:\n\n- The authors propose FlexRound, a new approach that applies element-wise division to exploit the parameter magnitude to update the quantization scale. Figure 3 and Figure 4 provide good explanations on the advantages of FlexRound. \n\n- The authors conduct comprehensive empirical studies over the image classification, natural language understanding and natural language generation tasks. Moreover, the authors testify various network architectures across CV and NLP, and especially, GPT-Neo and OPT, for post-training quantization. \n\n- The writing is clean.\n\nWeakness:\n\n- Lack of comparisons with existing approaches, especially BRECQ and Q-Drop. According to the published results of BRECQ and Q-Drop, the results in this paper seem to be worse than these two baselines (e.g., W3-A3 Q-Drop for MobileNet-v2 is 57.98%, but only 41.51% in this paper), even under the same configuration. More comparisons and explanations should be presented.\n\n- While BRECQ and AdaRound rely on element-wise addition for quantization, it is still unclear what is their disadvantage according to the paper.\n\n- The proposed approach seems to be only applicable to weight quantization, yet this is hardly mentioned. For Table-3, it should be made clear that what type of activation quantization is adopted. \n\n\n\nDetailed comments:\n\n- Despite ablated, it is still not clear the benefits of introducing $\\mathbf{s}_3$ and $\\mathbf{s}_4$ in Equation 2. Intuitively, both factors can be absorbed into $\\mathbf{S}_2$. \n\n- What is the initialization method for $\\mathbf{S}$? This should be important as PTQ does not allow intensive training like those in LSQ.\n\n- Provide more derivations (in appendix) for the gradient w.r.t. the scaling factor $\\mathbf{S}^{'}$.\n\n- As a minor suggestion, a loss curve or accuracy curve to compare FlexRound with BRECQ during the PTQ iterations could help illustrate FlexRound. This is based on the intuition that parameters with larger magnitude tend to move faster and thus converge better given the limited training iterations and samples.\n\n- Some recent research on PTQ for your reference.\n\n  - Liu Z, Wang Y, Han K, et al. Post-training quantization for vision transformer. NeurIPS 2021.\n\n  - Bai H, Hou L, Shang L, et al. Towards efficient post-training quantization of pre-trained language models. NeurIPS 2022.\n\n  - Park G, Park B, Kwon S J, et al. nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models. arXiv preprint arXiv:2206.09557, 2022.\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "See the above.",
            "summary_of_the_review": "Despite the methodological change is minor, the authors provide a reasonable explanation why FlexRound can exploit the importance of pre-trained weights to better calibrate the quantized model (Figure 3 and Figure 4). Experimentally, I appreciate the authors provide comprehensive studies across computer vision, natural language understanding and natural language generation tasks, and results demonstrate the success. Nonetheless, there are still issues with the experimental designs and comparisons, as mentioned above. There are also concerns w.r.t. the methodology, e.g., if FlexRound can be applied to activation quantization; and the necessity in introducing multiple scaling factors $\\mathbf{s}3$, $\\mathbf{s}4$, e.t.c..",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3598/Reviewer_mQzD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3598/Reviewer_mQzD"
        ]
    },
    {
        "id": "ec-0JL27Rcm",
        "original": null,
        "number": 4,
        "cdate": 1666631087917,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631087917,
        "tmdate": 1669996280783,
        "tddate": null,
        "forum": "-tYCaP0phY_",
        "replyto": "-tYCaP0phY_",
        "invitation": "ICLR.cc/2023/Conference/Paper3598/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a new post-training quantization algorithm called FlexRound. Unlike prior algorithms such as AdaRound, the learnable rounding factors are division parameters instead of an addition parameters. It has the advantage that it can learn the integer scale and rounding jointly, and also take the weights \u2018importance\u2019 into account according to the authors. The proposed approach is well evaluated on CV and NLP tasks and shows good empirical performance compared to their baselines.\n",
            "strength_and_weaknesses": "\nStrength:\n* The paper is well written and easy to follow. The visualizations are nice and helpful and the work is put well in context of existing literature.\n* FlexRound allows learning the scale and \u2018rounding\u2019 at the same time which can potentially be an advantage over SOTA current methods (AdaRound/BRECQ).\n* The proposed method is extensively evaluated on image classification and NLP tasks and shows good empirical performance (except some missing comparisons, more on that later).\n* Has ablation study on empirical choices such as introducing the factors s3 and s4 which should be in theory not needed. \n\nWeaknesses:\n* A proper comparison to literature and prior work is missing. While table 2 should in theory be comparable to the papers, their numbers do not match. To my understanding,\u2019 B+AdaRound\u2019 should be exactly what BRECQ is, however, the stated numbers are significantly below the results in the original BRECQ paper. Where does the difference come from? Also the 4-bit MobileNet v2 results are below the AdaRound paper while actually BRECQ+AdaRound improves over vanilla AdaRound (and the original AdaRound paper uses per-tensor quantization and the first/last layer are in 4 bits).\n* Claim 3 states that they are the first that do extensive per-tensor study on image classification and NLP. This is not fully true. The white paper of Nagel et al. 2021 (which the authors also cite) has in its PTQ chapter (table 6) a similarly extensive study which also includes per-tensor quantization. On the per-tensor vs per-channel point later, the original AdaRound paper is also with per-tensor quantization (not sure about BRECQ).\n* Claim 2 says they demonstrate that element-wise division includes the importance of the pre-trained weight. This is a fairly strong claim and IMO they only show this partly. They show that the gradient is proportional to the magnitude of the weights but then the link to importance is a bit soft/vague.\n* The authors argue that addition schemes may change the sign of a weight. However, for the most compared schemes, AdaRound and BRECQ, this can not happened based on how it is defined. Only in the case of AdaQuant, which they show performs poorly, this could theoretically happen.\n* Arguing that FlexRound has no extra hyper-parameters (compared to BRECQ/AdaRound) is only somewhat a benefit as the AdaRound paper keeps al hyper-parameter constant inter experimentation and is therefore de-facto also hyper-parameter free.\n\nQuestion: \n* It seems s1 (the general scaling factor) is learned jointly with the \u2018rounding\u2019. Could this be also a reason why FelxRound is empirically better than AdaRound/BRECQ? Due to their formulation, AdaRound/BRECQ can not learn the scales jointly with the rounding which is a clear drawback. An additional ablation (in table 1) with a fixed s1 could potentially give some interesting insights into this.\n* Did the author explore combining the additive approach (AdaRound/BRECQ) with the devision based approach? Given that empirically they need 3 new learnable scaling factors (s2, s3, s4,  cf table 1), it might be interesting to see if there comes benefits from both and additive and multiplicative term.\n\n\nEditorial:\n* Would suggest to use $\\lfloor \\cdot \\rceil$ for rounding such as in most prior literature (BRECQ, AdaRound etc).\n* Section 3.2: \u201cper-tensor quantization schemes facilitate higher parallelism for implementation compared to per-channel quantization schemes (Nagel et al., 2021)\u201d. This seems a not widely known/acknowledged statement and I could not find such a statement in the referred work. Could the authors please point me to the sections where this is discussed or other resources?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. It has some ablation studies on empirical choices (like s3/s4) which I appreciated. The proposed method is fairly simple and a closely related to AdaQuant and AdaRound, though somewhat novel as the additional factor is a division instead of an addition as in prior work. The paper seems that it should be reproducible but there are some questions wrt the comparisons to prior work (AdaRound and BRECQ, see above).",
            "summary_of_the_review": "The paper proposes a simple adaptation to AdaRound/BRECQ/AdaQuant which has some novelty as the learned parameter is relative instead of additive. In general the paper has a good empirical evaluation except a few questions/inconsistencies with respect to prior work. A few of the papers claims are a bit strong or can be misleading and should be adapted accordingly. Overall the paper is borderline. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3598/Reviewer_oCmz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3598/Reviewer_oCmz"
        ]
    }
]