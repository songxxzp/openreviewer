[
    {
        "id": "nk7iWJm7DJ",
        "original": null,
        "number": 1,
        "cdate": 1666358266465,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666358266465,
        "tmdate": 1666358266465,
        "tddate": null,
        "forum": "2nocgE1m0A",
        "replyto": "2nocgE1m0A",
        "invitation": "ICLR.cc/2023/Conference/Paper2564/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In the current paper, the authors present a new framework to perform data augmentation on low-resource NLP tasks. The core innovation is the idea of Knowledge mixture training (KoMT), that allows representing a large variety of tasks in a uniform way, in the form of keys and values pairs. This allows training the LM in a large range of NLP tasks through a denoising objective, where different combinations of full keys or values are masked. Through additional tools, such as the usage of demonstrators, and by also harnessing the zero-shot capabilities of the large LM model used as initial checkpoint (T5), the authors show the capabilities of KnowDA on tasks that require generating long text sequences. Furthermore, at generation time, the authors propose an autoregressive scheme, through some fine-tuning of the model after KoMT, to better adapt to the structure of each task, by establishing a dependency relation between the different features of the task at hand.\n\nThrough all the proposed innovations, the authors are able to provide significant improvement on many low-resource NLP tasks, more notably the fewGLUE benchmark, when compared to previous approaches such as FlipDA. Besides, they perform several ablation studies to underline the significance of the proposed methods.",
            "strength_and_weaknesses": "The paper presents interesting and meaningful methodologies for the important problem of tackling low-resource NLP tasks. All the ideas proposed, even though based on previous well-known works, still offer a substantial degree of advancement, and move in reasonable and intuitive directions, which is always appreciated when discussing large LMs. The core idea of KnowDA, of harmonizing tasks' representation and training objectives, is clear, well-argued and nicely implemented. Besides, some additional tricks, such as the zero-shot plus demonstrators for long text generation, and the autoregressive approach for better generation, are quite reasonable and grounded to tackle some of the most complicated and challenging tasks.\n\nFurthermore, the authors\u00a0do an excellent job in terms of exhaustively benchmarking their methodology, both from the point of view of the specific tasks considered, varied and challenging, and the number of baselines they compare against. And in most of these tasks, the methods proposed outperform previous ones. And when it is not the case, the authors try to give some intuition on why the methodology might down-perform in these cases, which is appreciated. From my point of view, the results are quite good, and prove the proposed methodology advances in the right direction in the problem of low-resource NLP.\n\nFinally, I also appreciate the extensive work done for the supplementary material, as it helps clarify many of the doubts that might arise during the main manuscript, and provide further valuable results and information to better understand the value of the current method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Even though the manuscript requires some deep understanding of the specific problem of low-resource NLP tasks and some previous methods, such as FlipDA and PET, if we consider the page limitation, the authors do a great job of trying to make the core ideas clear and understandable. I would have appreciated some extra details on some previous concepts that are key for the understanding, such as how PET handles semi-supervised training, but I understand the space is limited, and any other reader well-versed on this concrete field will perfectly understand. Beyond that, the manuscript's quality is high, it is really well written, and all provided figures and tables are clear and help further understand all the ideas. And for the novelty, despite not having a deep understanding of previous approaches, I believe the authors advance sufficiently the state of the art, and present enough, and well-justified innovations, to claim that they are making a contribution to the field. \n\nI cannot assess the reproducibility, but I guess the authors will be able to provide at least the core model after KoMT training as a pretrained LM model, which will help future researchers reproducing all the results. Still, it is to question what happens with the fine-tuned models, used for autoregressive generation. Here actually is where I have some questions to the authors. You train a different copy of KnowDA for generation, at each stage, for each NLP task. This seems extremely laborious and time and space-consuming. Therefore:- Could you elaborate more on this, and present more precise numbers on how many fine-tuned models are then obtained? \n- How much time does it require to perform these fine tunings? Are those performed on the fly, with the generation, and then discarded?\n- In case these models are stored, are they saved as the diff on the weights with respect to the parameters of the \"base\"-KoMT KnowDA?\n\nBesides, I would have another question, just out of curiosity: \n- In the case of long text generation, should not fine-tuning plus transferring task knowledge be even more efficient? If not, why?",
            "summary_of_the_review": "I believe the authors confront a challenging problem, not from the point of view of a reduced set of tasks, but from a really holistic perspective, tackling a large set of challenging NLP problems. The results reported are quite competitive, outperforming substantially previous approaches. Besides, the paper is really well written, and provides a clear intuition of the advancements implemented, understandable also for the readers non-versed on this particular field of NLP. For these reasons, I believe the paper paper is quite relevant, and should be accepted as it is. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2564/Reviewer_Fev8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2564/Reviewer_Fev8"
        ]
    },
    {
        "id": "ZVUnRc3Cdk",
        "original": null,
        "number": 2,
        "cdate": 1666636588321,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636588321,
        "tmdate": 1666636800412,
        "tddate": null,
        "forum": "2nocgE1m0A",
        "replyto": "2nocgE1m0A",
        "invitation": "ICLR.cc/2023/Conference/Paper2564/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores a data augmentation method, called KnowDA, for low-source NLP tasks, aiming to capture task knowledge to improve the relevance and diversity of augmented data. The method first attempts to pre-train text-to-text generation model that will be used to generate augmented data for NLP tasks for data augmentation. To learn from diverse NLP tasks, the method select multiple NLP tasks with public datasets in Huggingface. Afterward, a key-value list format is introduced as a unified framework to represent instances of different NLP tasks. The key idea is to use keys to indicate feature functions (e.g., premise, hypothesis, tag) and values to capture string representations of feature content. KnowDA then uses denoising objectives to pre-train the model over converted instances of the NLP tasks (i.e., via masking selected values in the key-value list format for reconstruction from remaining information). In addition, KnowDA adds some selected demonstrations/instances from the same tasks to the input for the text-to-text formulation during pre-training. To leverage KnowDA for data generation, task-specific feature dependency is used to provide an order to generate keys and values to create new training data for a target task. To generate long values (e.g., documents), KnowDA uses zero-shot learning framework. The method is evaluated over low-resource experiments using FewGLUE, CoNLL'03 and WikiAnn (for sequence labeling). Experiments show better performance of KnowDA over several selected baselines. The paper also conduct some analysis to demonstrate the benefits of KnowDA, including ablation study, long text generation, data diversity, and human evaluation.\n",
            "strength_and_weaknesses": "Strength:\n\n+The key-value list format to unify different NLP tasks for multi-task learning is interesting and seems helpful.\n\n+KnowDA achieves strong performance on the benchmark datasets for low-resource learning.\n\nWeakness:\n\n-Although interesting, the key-value list format is similar to the text generation formulation of NLP tasks that is studied extensively recently (e.g., [1]). These work should be discussed to highlight the model's benefits.\n\n-Compared to the baselines (e.g., FlipDA) that do not require training data of external NLP tasks, KnowDA needs to use datasets of multiple NLP tasks to pre-train its model. This might make the comparison less compatible and convincing in this work.\n\n-Given the training data of multiple NLP tasks for pre-training KnowDA, another possible baseline is to formulate the NLP tasks into the text-to-text problems using the key-value list format. Given a target NLP task, a text-to-text model can be trained over the external task data and the provided training data to directly solve the task (i.e., not using data augmentation). It might be helpful to discuss or evaluate this baseline to show the benefits of data augmentation given training data from external tasks.\n\n[1] Paolini et al. Structured prediction as translation between augmented natural language, 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper provides an appendix to show more details for method and experiment designs. Code is also included to facilitate reproducibility.\n",
            "summary_of_the_review": "The fairness of the comparison can be improved as KnowDA uses training data from multiple external NLP tasks that is not employed in the baselines. A text-to-text baseline can be considered to show the benefits of data augmentation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2564/Reviewer_CnYA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2564/Reviewer_CnYA"
        ]
    },
    {
        "id": "JR-FTcDkqQ",
        "original": null,
        "number": 3,
        "cdate": 1666669583302,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669583302,
        "tmdate": 1666669583302,
        "tddate": null,
        "forum": "2nocgE1m0A",
        "replyto": "2nocgE1m0A",
        "invitation": "ICLR.cc/2023/Conference/Paper2564/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a novel data augmentation model and technique for improving performance of few shot and low resource NLP tasks. Based on a seq-to-seq pre-trained language model (T5 large), this paper first present a multi-task pre-finetuning on 100+ NLP task using a new Knowledge Mixture Training (KoMT) framework, where all the datasets are represented in an unified  key-value pair format, adding some demonstration exemplars at the beginning, and training using a de-noising objective. For augmenting a low resource NLP task, the new samples are generated autoregressively according to task specific feature dependency. To accomplish this, different copies of the pre-finetuned model are trained for each auto-regressive step. The proposed method is evaluated on benchmark low resource NLP datasets (FewGLUE, CoNLL\u201903, and WikiAnn) and shown improvements over previous SOTA baselines.",
            "strength_and_weaknesses": "Strengths:\n \nThe paper is well written and easy to follow;\nThe proposed KoMT pre-finetuning technique is novel and the paper claims to be the first work to perform large scale multi-task pre-training on over 100+ NLP tasks;\nThe model is thoroughly evaluated, and shows improved data augmentation performance on low-resource NLP tasks. Human evaluation demonstrates that the generated data samples are closer to human generated samples than previous SOTA models.\n \nWeaknesses:\n \n1. Despite the improved data generation performance, several proposed techniques make the overall system less practical for real applications. For example, the auto-regressive generation based on feature-dependency requires training multiple copies of the same pre-trained model;\n\n2. It is unclear how the authors define/choose different task specific feature dependencies. Is it done randomly or using some domain specific knowledge? How will it be chosen for a brand new task? There are no experiments on the how sensitive is the performance on the choice of such feature dependency;\n \n3. Many key details are missing:  What is meant by using KnowDA as task solver? Does it mean the pre-trained seq-to-seq model is directly being finetuned on the sequence labeling task? If so, is this model different than the one being used to generated the augmentation samples?\nHow many exemplars are chosen (per sample) for each task during pre-training and fine-tuning? Is one exemplar sufficient? If more than one is needed then how sensitive is the model to this choice?\n\nTable 4 and its description on page 6 is not clear.\nFigure 3 caption: Please add what is meant by T and D. Figure captions should be self-contained and understandable.\n \nTypos:\n \nPage 2: \u201c.. KoMT is more scalable and comprehensive because those works heavily reply on the human-crafted prompts ..\u201d -> heavily rely on\nPage 2: \u201cTo summarize, contributes of this paper are following\u201d -> contributions of this paper\nPage 6: \u201cWe conduct the shot-10 setting where 40 samples for CoNLL\u201903 and 30 samples for WikiAnn\u201d  -> setting with 40 samples",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. \nThe proposed KoMT pre-finetuning method, and KnowDA model is novel. \nThe experiments should be mostly reproducible.",
            "summary_of_the_review": "The paper develops a novel multi-task pre-training technique for large language models, trained over 100+ NLP tasks, which can be subsequently used for data augmentation in a new unseen low resource NLP task. The paper demonstrates improved performance of the proposed data augmentation method on benchmark low resource NLP datasets. These new findings could be beneficial to the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2564/Reviewer_U7D8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2564/Reviewer_U7D8"
        ]
    }
]