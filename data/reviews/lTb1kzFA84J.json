[
    {
        "id": "TtuI03odUo",
        "original": null,
        "number": 1,
        "cdate": 1666703615025,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703615025,
        "tmdate": 1666703615025,
        "tddate": null,
        "forum": "lTb1kzFA84J",
        "replyto": "lTb1kzFA84J",
        "invitation": "ICLR.cc/2023/Conference/Paper235/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The author has discussed the problem of transition samples in the bellman operator, which determine the operator's behavior. The author has proposed a novel idea of approximation of the Bellman operator known as the Projected Bellman Operator (PBO). Given the value function's parameter, PBO outputs the new value function's parameter. PBO is a parametric operator on the parameter space of the given value function. Finally, the author has analyzed PBO in terms of stability, convergence, and approximation error on different problems. The author claims that the PBO method overcomes the classical methods' limitations.",
            "strength_and_weaknesses": "Strength:\n\n  The paper discusses an essential problem in the literature.\n\n  Also, the author provides a nice solution to tackle the problem, but the analysis and intuition part is very poorly represented in this paper.\n\nWeaknesses:\n\n\" first to deal with the original problem of operator learning in RL.\"\nI don't think this statement is correct. Please refer [R1] and BBO(Fellows et al., 2021) from the paper.\n\nThe author should compare the results of other Bellman operators like the Bayesian Bellman operation. It would be nice to observe the performance of the PBO with other Bellman operators' methods.\n\nIt is not clear in the paper how the parameter approximation is formulated in function space (Operator learning space). Which function approximation method is used in PBO? How does it work in the operation space? The author should add a paragraph on function approximation in the literature.\n\nThe author should show the parameter distribution of V before the approximation and after the approximation. \n\nHow does the quality of the sample improve in PBO techniques? The author should show results both qualitatively and quantitatively. \n\nHow about the direction of the update by the current sample? The decision is taken by a single instance or batch of samples.\n\nThe author should report ablation analysis on various parameter approximation methods.\n\nThe experiment section is not represented properly. The author should report stability results to compare with another method. Also should report the approximation error results.\n\nReference:\nR1. Song, Zhao, Ron Parr, and Lawrence Carin. \"Revisiting the softmax bellman operator: New benefits and new perspective.\" In International conference on machine learning, pp. 5916-5925. PMLR, 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to the weakness section.",
            "summary_of_the_review": "The overall writing quality is ok, and the proposed method is simple and beneficial. However, the experiment comparison lacks justification, and the technical contribution is plain.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper235/Reviewer_bapX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper235/Reviewer_bapX"
        ]
    },
    {
        "id": "iOc33cAy-m",
        "original": null,
        "number": 2,
        "cdate": 1666724134385,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666724134385,
        "tmdate": 1669065072614,
        "tddate": null,
        "forum": "lTb1kzFA84J",
        "replyto": "lTb1kzFA84J",
        "invitation": "ICLR.cc/2023/Conference/Paper235/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes learning a parametric model of the Bellman operator, and using this Projected Bellman Operator (PBO) in place of the true Bellman operator to optimize a learned value function. The paper gives examples of PBOs for several settings, and derives an approximate value iteration algorithm based on PBOs. Experiments on several environments are conducted.",
            "strength_and_weaknesses": "**Strengths:**\n1. Very clearly written. Thank you!\n1. Examples are given when explaining concepts, which is very helpful.\n1. Plots are well-done, large enough to read, and include 95% confidence intervals.\n\n**Potential Weaknesses/Questions:**\n1. The main advantage of learning a PBO as stated in the introduction is that it can be applied an arbitrary number of times without using samples. Applying it repeatedly in this way would require the PBO to be extremely accurate, as any error would be propagated and magnified with each application. However, parametric models are usually biased by the initial values of the model parameters, and therefore are likely to be inaccurate while the parameters are optimized. In addition, asymptotic accuracy is limited by how well the chosen function class can represent the true Bellman operator. For these reasons, it seems like it would be difficult to actually take advantage of the main strength of a PBO in practice, especially on larger/harder problems where the function approximator is not powerful enough, where it's unclear what parameterization is appropriate, or whenever a huge batch of data is not available (such as the usual online setting).\n1. When the PBO parameterization is not suitable, the algorithm can fail completely. This seems like a problem, because knowledge of which parameterization is sufficient for a given environment is not available in general.\n1. ~~How is learning a PBO different from learning a parametric model of the transition and reward functions in model-based RL (e.g. Dyna)? This connection with model-based RL should definitely be discussed in the Related Works section.~~ The authors' comments and the updated version of the paper made it clearer to me that there is no actual model of the environment being used to generate samples in the update.\n1. Experiments are on simple problems with powerful function approximation. How does the method perform on harder problems with weaker function approximation?\n1. The paper does not sufficiently discuss weaknesses of the proposed method.\n1. The paper only presents the parameter settings used in the experiments, but does not mention how these parameters were chosen, what other values were tried, or how different values of parameters change the resulting performance of each method (although this is done for a subset of the parameters). Hence, the experiments seem more like demonstrations than rigorous comparisons between algorithms.\n1. PBO seems to only be specified for the offline batch setting, which is at odds with the usual online sequential RL setting. How could it be used in the online setting?\n1. Learning a PBO requires transition samples, so why would using transition samples to learn a PBO and then using the PBO to learn a parametric value function be better than using the transition samples to learn a value function directly?\n1. ~~What is the vertical dashed black line in the plots?~~ The authors addressed this in their response.\n1. Consider using a colour scheme for the plots that is more friendly to people with colourblindness. Alternatively, the legend could be replaced with labels that point to each line, so that the ability to discern colour is not actually necessary to tell which algorithm corresponds to which line.\n1. The name \"projected Bellman operator\" is very similar to the \"projected Bellman error\" (i.e., the Bellman error with a projection operator) used in gradient TD algorithms, which was a bit confusing. This is an extremely minor point, but it might be good to explicitly clarify the difference.\n\n**References:**\n- Sutton, R. S., Szepesv\u00e1ri, C., Geramifard, A., & Bowling, M. (2008, July). Dyna-style planning with linear function approximation and prioritized sweeping. In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (pp. 528-536).",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\nThe paper is very clearly written, and the included Figures are clear and helpful for understanding/summarizing concepts.\n\n**Quality:**\nThe only concerns I have about the paper's quality are: ~~(1) no connection is made between the proposed idea and model-based RL (including related works),~~ (2) the lack of a comprehensive discussion of the limitations of the proposed algorithm, and (3) the experiments seem more like demonstrations than rigorous comparisons of each algorithm due to not checking other values of parameters used in the experiments.\n\n**Novelty:**\n~~I'm unsure of how exactly the idea of PBOs differs from existing algorithms that learn parametric models of the transition dynamics and reward function for use in learning a value function (e.g., the Dyna algorithm). However,~~ the idea seems to be explored in a different way, resulting in different theoretical analyses and yielding a different final algorithm.\n\n**Reproducibility:**\nEnough details are given in the paper and appendices that the results could likely be reproduced without much difficulty. Thank you, authors!",
            "summary_of_the_review": "~~My initial recommendation is to reject the paper due to some of the weaknesses mentioned above. However, I actually quite like the paper and the idea of projected Bellman operators, and feel that the paper is not far from clearing the bar for acceptance; if the authors respond satisfactorily to my concerns, I would recommend acceptance.~~\n\nThe authors have responded satisfactorily to most of my concerns, although I would still like to see a discussion of the weaknesses/limitations of the proposed algorithm. For that reason I'm comfortable recommending the paper be accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper235/Reviewer_emHW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper235/Reviewer_emHW"
        ]
    },
    {
        "id": "YFuV_XMsjAX",
        "original": null,
        "number": 3,
        "cdate": 1667104041119,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667104041119,
        "tmdate": 1667104041119,
        "tddate": null,
        "forum": "lTb1kzFA84J",
        "replyto": "lTb1kzFA84J",
        "invitation": "ICLR.cc/2023/Conference/Paper235/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes the projected Bellman operator (PBO), an operator on the parameter space of the value function, for value-based reinforcement learning. The paper argues that PBO can approximate repeated applications of Bellman operator at once. To show the advantage of this, the paper considers three example of applying PBO, including tabular MDP, low-rank MDP and LQR, where the convergence and approximation error of PBO are provided. Finally, the paper also provides empirical studies in simple synthetic environments. \n",
            "strength_and_weaknesses": "Strength \n\nThe paper considers a new paradigm for value-based RL, which could be potentially interesting for both theoretical and practical applications. Other than some notational ambiguity, the paper is well written. I also like that the authors give some specific examples of the proposed general operator. \n\nWeakness \n\nThe major weakness of this paper is that the significance of the contributions is not clear to me. \n\nSince the main contribution is PBO, the key to decide if the contribution is significant enough is to understand if this new operator can bring some new findings from either empirical or theoretical perspective. However this is not clear to me as explained in details below. Please correct me if I missed anything important in the rebuttal. \n\n \n1. The so called PBO, defined in Equations 3 and 4, is defined as the solution of solving a single step of fitted value iteration for a general parameterized function class. But I believe this operator has already been considered in the literature. For example, [1] defines it in a similar way, and analyze FVI with this operator in a batch setting. \n\n2. The authors claim that \u201cour PBO can approximate repeated applications of the true Bellman operator at once, as opposed to the sequential nature of the standard Bellman operator\u201d. This is what motivates the learning objective 13. However, to avoid a sequential implementation when computing $T_\\phi^k$, doesn\u2019t it need the assumption that the operator is linear, or at least can be written as a closed form? I believe this is definitely not true for general non-linear function approximation. \n\n3. The authors consider three cases as examples of PBO and gives convergence analysis for all three examples. However, it seems that all these results are already known? For example, a more general case of tabular MDP (Section 4.2) is under the assumption of linear function approximation (as one-hot feature is linear), in which case similar results have been provided in [2]. \n\n4. Some notations might not be well defined. For example, What is F in Theorem 1? What is $\\Pi$ in Equation 6? Equation 13: there is no (s,a). \n\n\n\n[1] Chen, J. and Jiang, N., 2019, May. Information-theoretic considerations in batch reinforcement learning. In\u00a0International Conference on Machine Learning\u00a0(pp. 1042-1051). PMLR. \n\n[2] Parr, R., Li, L., Taylor, G., Painter-Wakefield, C. and Littman, M.L., 2008, July. An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning. In\u00a0Proceedings of the 25th international conference on Machine learning\u00a0(pp. 752-759).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the detailed comments in the previous section. \n\n",
            "summary_of_the_review": "I recommend rejecting this paper as the contribution of the proposed method is not clear to me. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper235/Reviewer_Kjbj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper235/Reviewer_Kjbj"
        ]
    },
    {
        "id": "ykepugwhpp",
        "original": null,
        "number": 4,
        "cdate": 1667461480442,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667461480442,
        "tmdate": 1667461480442,
        "tddate": null,
        "forum": "lTb1kzFA84J",
        "replyto": "lTb1kzFA84J",
        "invitation": "ICLR.cc/2023/Conference/Paper235/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to obtain an approximation of the standard Bellman operator, called projected Bellman operator (PBO). Specifically, PBO is a parametric operator on the parameter space of a given value function. Based on the idea of PBO, the authors develop a novel algorithm for value estimation. Experiments demonstrate the effectiveness of the proposed method. ",
            "strength_and_weaknesses": "Strengths:\n\n1.\tTo the best of my knowledge, the idea of obtaining a parametric operator on the parameter space of a given value function is novel. \n\n2.\tThe proposed method is theoretically sound. The authors show how to derive different PBOs according to the class of problems at hand.\n\n3.\tThe paper is well organized and easy to follow.\n\nWeaknesses:\n\n1.\tThe experiments can be improved. It would be more convincing if the authors could evaluate PBO on more challenging continuous control tasks, such as Mujoco [1].\n\n2.\tThe authors may want to provide a detailed discussion on advantages of PBO over the standard Bellman operator. \n\n[1] Todorov et.al \"Mujoco: A physics engine for model-based control.\" international conference on intelligent robots and systems. IEEE, 2012.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Reproducibility: The paper is well organized and easy to follow. However, the authors may want to provide a detailed discussion on advantages of PBO over the standard Bellman operator. \n\nQuality and Novelty: The idea of this paper is novel to me. However, the experiments need to be improved.",
            "summary_of_the_review": "To the best of my knowledge, the idea of obtaining a parametric operator on the parameter space of a given value function is novel. The paper is well organized and easy to follow. However, the experiments can be improved (please see above for details). ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper235/Reviewer_3oQg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper235/Reviewer_3oQg"
        ]
    }
]