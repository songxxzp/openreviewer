[
    {
        "id": "mrNZ9xOgGt",
        "original": null,
        "number": 1,
        "cdate": 1666280363886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666280363886,
        "tmdate": 1666280363886,
        "tddate": null,
        "forum": "_JScUk9TBUn",
        "replyto": "_JScUk9TBUn",
        "invitation": "ICLR.cc/2023/Conference/Paper6257/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers two-layer networks with one output dimension in the mean-field parametrization. In the limit of infinite hidden-layer width, these can be described in the infinite dimensional space of parameter distributions (across hidden unit weights) by a nonlinear Fokker-Plank equation, which has been done in the past. The paper justifies the relevance of this approach by computing bounds on the difference between finite-size discretization and the infinite-width limit, showing that the difference in any sufficiently smooth test function and for any time in the training process scales as $1/N$, given relatively mild assumptions. This extends previous finite-size bounds which suffered from an exponential dependence on training time.",
            "strength_and_weaknesses": "**Strengths:**\n - Technically demanding proof with relevant outcome (1/N scaling) under mild conditions.\n\n**Weaknesses:**\n - The progress in proof technique is stated multiple times in abstract/intro/conclusion (not requiring weak interaction while showing uniform propagation of chaos), but not technically discussed/explained in the relevant technical section 4 or appendices. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Quality, Novelty:** yes.\n\n**Clarity:** In general clear presentation of context and results, however the central technical advance is advertised but not clearly detailed (see weaknesses).\n\n**Reproducibility:** The calculations are clear and reproducible. The code for the numerical experiments should be made available.\n\n\n**Additional small comments:**  There are a number possible typos \\\n -p.1: $h_z(x)=\\tanh(r\\sigma(w^Tx)) $ seems a typo (while on p.5 below A1 $\\tanh(r)\\sigma(w^Tx)$ is given) \\\n-p.2: \"using neural network\"; \"$M$ neurons\" ; also $x\\in \\mathbb{R}^p$ and $X \\subset \\mathbb{R}^d $ should both be $\\mathbb{R}\\times\\mathbb{R}^d$? \\\n-p.4: \"provides meaningful guarantee\" \\\n-p.5 in Def.1: $\\mathbb{R}^p$ should be $\\mathbb{R}^d$? \\\n-p.6: \"under quadratic regularizer\" \\\n-p.9: \"[our] analysis do not cover\" \\\n-p.13: \"xB\"; \"having form\"; \"such the\"; \"(1) uniform\"; \"(2) local\"; \"[they showed a..] but it also requires\" \\\n-p.20: \"We also a\"; \"for related quantity\" \\\n",
            "summary_of_the_review": "This paper seems mathematically sound and provides a relevant result for the study of feature learning in two layer networks in MF-parametrization. It is suitable for ICLR. The technical description of the novelty in approach should be improved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6257/Reviewer_713A"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6257/Reviewer_713A"
        ]
    },
    {
        "id": "zgH8SFdVfz",
        "original": null,
        "number": 2,
        "cdate": 1666553734279,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666553734279,
        "tmdate": 1666554089205,
        "tddate": null,
        "forum": "_JScUk9TBUn",
        "replyto": "_JScUk9TBUn",
        "invitation": "ICLR.cc/2023/Conference/Paper6257/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the mean-field regime of an overparametrized two layers neural network $f_X$ where $X\\in\\mathbb{R}^p$ stands for the trainable parameters. Indeed, it have been shown previously taking the number of neurons $N\\to \\infty$ that optimizing such network can be seen as minimizing the functional over the space of probability laws \n$$\nF(\\mu)=\\frac{1}{n} \\sum_{i=1}^n \\ell_i\\left(f_\\mu\\right)+\\lambda_1 \\int r(x) \\mathrm{d} \\mu(x) \\quad , \\quad f_\\mu(z)=\\int f_x(z) \\mathrm{d} \\mu(x).\n$$\nIn particular, the paper aims to extend and complete existing literature regarding convergence of the neurons following the continuous limit of the \"noisy\" gradient descent dynamics to a mean-field Langevin equation which is the gradient flow dynamics associated with $F$. \nMore precisely, the authors consider the particle system\n\n$$\n\\mathrm{d} \\hat{X}_t^i=-b(\\hat{X}_t^i, \\mu_t^N ) \\mathrm{d} t +\\sqrt{2 \\lambda} \\mathrm{d} W_t^i \n$$\n\n$$\n\\mu_t^N=\\frac{1}{N} \\sum_{i=1}^N \\delta_{\\hat{X}_t^i}, \n$$\n\nwhere $W_t^i$ are independent Brownian motion and $b$ corresponds to the gradient of the first variation $F$\n$$\nb(x, \\mu)=\\nabla \\frac{\\delta F}{\\delta \\mu}(\\mu)(x)=\\frac{1}{n} \\sum_{j=1}^n \\ell_j^{\\prime}\\left(f_\\mu\\right) \\nabla h_j(x)+\\lambda_1 \\nabla r(x).\n$$\n\nThey then show that for a nice class of functional $\\Phi$, a uniform propagation holds for the sequence of dynamics $(\\mu_t^N)$: there exist $C_1,C_2,C_3$ such that for any $t \\geq 0$ and $N \\geq 1$,\n$$\n|\\mathbb{E}\\left[\\Phi\\left(\\mu_t^N\\right)\\right]-\\Phi\\left(\\mu^*\\right) |\\leq C_1 e^{C_2t} + C_3/N.\n$$\n\nwhere $\\mu^*$ is the minimizer of \n$$\n\\mathcal{L}(\\mu)=\\frac{1}{n} \\sum_{i=1}^n \\ell_i\\left(f_\\mu\\right)+\\lambda \\mathrm{KL}\\left(\\mu, \\nu_r\\right),\n$$\n\nand the limit of the mean-field Langevin dynamics\n\\begin{aligned}\n&\\mathrm{d} X_t=-b\\left(X_t, \\mu_t\\right) \\mathrm{d} t+\\sqrt{2 \\lambda} \\mathrm{d} W_t, \\\\\n&\\mu_t=\\operatorname{Law}\\left(X_t\\right).\n\\end{aligned}",
            "strength_and_weaknesses": "Strength:\n- the result is mathematically interesting and brings some new tools for establishing uniform propagation of chaos for particle systems which emerged from overparametrized neural network optimization.\n\nWeakness:\n- I am not really sure how interesting this result is to the machine learning community. I think the authors should better motivate the question they are addressing and provide some insights why their results may help in understanding of the optimization of overparametreized neural networks.  Ideally, could the authors give practical implications of their findings? \n- The condition on the regularization term is not really satisfactory. In particular, the authors say that previous works were limited by assuming either weak interaction or large noise. To my view, the authors get rid of this constraint by imposing a very strong regularization term which always counterbalances the interaction drift. This choice could potentially miss important and interesting phase-transition phenomena. \n- I felt that some parts of the paper would needed to be rewritten and  proofread if the authors want that their paper is accessible to the machine learning community. ",
            "clarity,_quality,_novelty_and_reproducibility": "I think the writing and presentation of the document should be improved. In particular, section 4.1.2, which is two pages of the main document, is in my opinion too difficult to parse.\nIn addition, Some details which are given are inessential whereas more would have required more care. \n\n- For example, it is a bit odd in my opinion to note that a distribution is also called a probability law (p4). \n- Why referring particularly to Huang for existence and uniqueness of solutions for (3) while no assumption and detail are given and there are many other results on this topic.\n- The use of both $\\lambda$ and $\\lambda_1$ is really necessary? I think you could only introduce one of them since you work with a generic $r$.\n- From assumption on $r$, it is hard to guess functions satisfying such a condition except $\\Vert x\\Vert^{2\\alpha}$, for $\\alpha >1$.\n- The authors make use of higher order derivative of function $\\Phi :\\mathcal{P} \\to \\mathbb{R}$ with respect to a measure $\\mu$ but I did not catch how it is defined only based on the definition of the first order derivative. In fact one of the main arguments is based on a second order differential computation derived in Delarue and Tse. However, I was not able to understand the tools used by the authors and how they use them to show Theorem 2 in the main paper. Finally, I think the authors should really comment more on what their contributions differ from those of Delarue and Tse.\n- I did not follow how the smoothness of $\\Phi$ implies that $\\Phi(\\mu) - \\Phi(\\mu^*) \\leq C W_2(\\mu,\\mu^{*})$ (end of p6).\n- the notation $\\mu_t$ and $m(t,\\mu_0)$ is really confusing.\n- Finally, I was not been able to recover the informal statement of the authors:\n$$\n\\mathbb{E}[(f_{X_t}-f_{\\mu_t})^2] = O(1/N).\n$$\nIn my opinion, it should be \n$$\n\\vert \\mathbb{E}[f_{X_t}-f_{\\mu_t}] \\vert = O(1/N).\n$$\n",
            "summary_of_the_review": "Overall I think that it could be a nice contribution to the literature on propagation of chaos.\n\nHowever, in my opinion, the writing of the paper has to be improved and the relevance of the contributions for the machine community should be better emphasized. \n\nSecond, I do not really know if ICLR is a good venue for the paper which I think should be suitable for more theoretical conferences or journals. Especially because the proofs of the results are quite delicate and require a relatively long time to be verified. To be honest, I did not have the time to do it and I am afraid no reviewer has either. \n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6257/Reviewer_FGcq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6257/Reviewer_FGcq"
        ]
    },
    {
        "id": "YCs5fIdGo6F",
        "original": null,
        "number": 3,
        "cdate": 1666563328870,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666563328870,
        "tmdate": 1666564389620,
        "tddate": null,
        "forum": "_JScUk9TBUn",
        "replyto": "_JScUk9TBUn",
        "invitation": "ICLR.cc/2023/Conference/Paper6257/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proves uniform-in-time chaos in the context of mean-field (MF) Langevin dynamics, motivated by noisy gradient descent for 2-layer MF neural nets. The analysis exploits the fact that a super log-Sobolev inequality (LSI) for the so-called proximal Gibbs measure, enabled by a super-quadratic regularizer, translates to the LSI of the corresponding flow measure at any sufficiently large time.",
            "strength_and_weaknesses": "This paper is the first I have known of to prove a uniform-in-time type of result for MF neural networks. This is a good result, since there is currently no applicable tool from the uniform-in-time chaos literature. Uniformity in time is also a highly desirable property, since one drawback of typical MF analyses is that the approximation between the large-width neural nets and the MF limit blows up exponentially with time. The observation that $\\mu_t$ satisfies LSI thanks to the super-quadratic regularizer, leading to Theorem 2, is an interesting one. The work also circumvents the trouble of doing the large-$N$ approximation at each time $t$ by going for a large (ideally infinite) $t$, hence allowing for the error decomposition as well as the exponential convergence result that was proven by previous works.\n\nI do not have much to complain about the paper, nor do I have sufficient time and background to check the proofs. Of course, there are typical concerns about the (potentially bad) dependency among the constants, as well as the rather unusually strong regularizer, but I would not worry much about it for now in light of a new result, the first of its kind. I have two questions:\n\n- The numerical illustration basically shows that the approximation is increasingly better for larger $N$, but it doesn\u2019t quite show that the approximation is getting better / no worse with time. Is there a way to better illustrate this?\n\n- The strong tail of the regularizer essentially shapes the curvature at infinity and intuitively prevents bad things to happen at infinity. However the analysis centers around the optimal solution $\\mu*$, for which the faraway region likely matters much less. Is there a way to intuitively understand this supposed dilemma? One possibility is that the regularizer might not be needed for uniform-in-time chaos.\n\nHere my concern is that the strong regularizer is more of a technical device than something insightful, so there is some doubt whether this is the right way to understand uniform-in-time chaos.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear. The result is new and significant.",
            "summary_of_the_review": "The paper proves the first uniform-in-time chaos result ever known for MF 2-layer neural nets. This is a good result with new technical insights, though I\u2019m unable to go through the proofs. I would expect researchers in the area take serious interests in this result, even if the strong regularizer does not appear convincing to be the right way to understanding the phenomenon.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6257/Reviewer_tT78"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6257/Reviewer_tT78"
        ]
    },
    {
        "id": "a2GIJYd8ASh",
        "original": null,
        "number": 4,
        "cdate": 1666597246487,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597246487,
        "tmdate": 1666597246487,
        "tddate": null,
        "forum": "_JScUk9TBUn",
        "replyto": "_JScUk9TBUn",
        "invitation": "ICLR.cc/2023/Conference/Paper6257/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the convergence of the mean-field gradient Langevin dynamics for the regularized mean-field neural network model. Under certain regularity assumption, it is proved that the dynamics in mean-field limit and the finite particle approximation are close for all time t>0, and certain statistic of these two dynamics are close with the order of O(1/N). As a consequence, it can be show that the finite particle approximation of the mean-field dynamics exponentially convergence towards the optimal solution up to an error of O(1/N).",
            "strength_and_weaknesses": "Strength: This paper provides the first rigorous uniform-in-time propagation of chaos result in the context of mean-field neural networks. This is achieved without adding strong entropy regularization or imposing strong assumptions on the complicated interaction term. \n\nWeakness: To achieve the claimed result, the convex regularizer on parameters is assumed to have a super-quadratic tail, an assumption that is not satisfied for the most common $\\ell_2$ regularizer.\n\nSome minor comments:\n1. Below the equation after Eq.(4), there is a statement \"We therefore see that $\\mu_t$ decreases ... $\\neq 0$.\". It is difficult to understand. \n2. In the paragraph of \"Particle discretization\" on page 4, it is assume that $\\hat X_0^i \\sim \\mu_0$, but in Lemma 1, it is claimed that you replaced the KL divergence on the R.H.S. with the Wasserstein distance since $\\mu_0$ is a discrete distribution. Why can't we simply take $\\mu_0$ to be some simple continuous initial distribution?\n3. At the end of Proposition 1, I guess it should be $\\mu^* = p_{\\mu^*}$.\n4. The concept of \"test function\" used in section 4.1 is not the same as the ones commonly used for defining weak convergences. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The presentation of this paper is overall clearly written. Section 4.1.2 is difficult to follow.\n\nNovelty: While this paper achieves the uniform-in-time type control between the mean-field dynamics and its finite-particle counterpart without adding strong entropy regularization or imposing strong assumptions on the complicated interaction term, the super-quadratic tail of the convex regularizer seems strong to me. I was not able to go through the details of the proof and I believe it is not easy. However, a strong regularization basically confines the parameter in a compact space (especially the gradients are assumed to be bounded) and LSI naturally holds in that case. I am not certain how meaningful this result is.\n\n",
            "summary_of_the_review": "This paper shows that the dynamics in mean-field limit and the finite particle approximation are close for all time t>0, and certain statistic of these two dynamics are close with the order of O(1/N). This result is interesting, but is obtained under a relatively strong assumptions on the parameter regularizer.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6257/Reviewer_k2b8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6257/Reviewer_k2b8"
        ]
    },
    {
        "id": "rDrds709_r",
        "original": null,
        "number": 5,
        "cdate": 1666817837942,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666817837942,
        "tmdate": 1666817837942,
        "tddate": null,
        "forum": "_JScUk9TBUn",
        "replyto": "_JScUk9TBUn",
        "invitation": "ICLR.cc/2023/Conference/Paper6257/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper proposed a uniform-in-time analysis of the propagation of chaos for the mean-field Langevin dynamics to conduct neural network optimization. The authors avoid the double-loop structure and enable to deal with convergence guarantee based on finite-width neural network with vanilla noisy gradient descent algorithms.",
            "strength_and_weaknesses": "**Pros:**\n1. The first quantitative discretization error ensures analysis based on neural networks of finite/ limited neurons. The uniform nature ensures that the deviation of the output reduces rapidly.\n2. The analysis leverages the advantages of propagation of chaos and overcomes the bottleneck suffered by the Growall inequality in Mei'18 and the error bound remains stable when the time t is large.\n\n**Cons:**\nThe assumption of the super-quadratic tail of the regularization term is strong indeed.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors clearly stated the contributions and limitations of the proof. \n\nNovelty: I am not an expert in neural network theory and I am not able to evaluate the novelty.\n\nThe technical tools based on gradient flow stuff seem standard and reasonable.\n\nTypos: $\\mu_t$ decreases $\\mathcal{L}(\\mu_t)$ unless $\\frac{\\delta \\mathcal{L}(\\mu)}{\\delta\\mu}=0$?",
            "summary_of_the_review": "Convergence analysis for stochastic gradient descent on a finite-width neural network where the upper bound is table w.r.t. time.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6257/Reviewer_LXtr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6257/Reviewer_LXtr"
        ]
    }
]