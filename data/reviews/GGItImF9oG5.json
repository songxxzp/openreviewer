[
    {
        "id": "grYtPuvaVWg",
        "original": null,
        "number": 1,
        "cdate": 1666474401357,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666474401357,
        "tmdate": 1666677096106,
        "tddate": null,
        "forum": "GGItImF9oG5",
        "replyto": "GGItImF9oG5",
        "invitation": "ICLR.cc/2023/Conference/Paper6203/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors examine how a wide variety of transformer architecture choices (and a few non-transformer architectures such as MLP-Mixers and dynamic convolutions) scale over a wide range of model sizes. Their models are implemented in a sequence-to-sequence manner, and evaluated with pretraining perplexity and SuperGlue accuracy. They make the following observations: the vanilla transformer has the best scaling behavior even though its absolute performance in each compute region is not the best; performance does not scale consistently for a given architecture; pretraining quality (negative log-perplexity) does not always correlate with downstream quality (SuperGlue and SQuAD accuracy); some architectures scale negatively.",
            "strength_and_weaknesses": "**Strengths**\n\nI agree with the motivation that new methods are not always evaluated across a sufficiently broad range of resource scales.\n\nThese sorts of studies are resource-intensive and not particularly exciting to conduct, but provide high-level perspective that can be invaluable to the field.\n\n**Weaknesses**\n\nThe authors point out the shortcomings of different measures of model \u201cscale\u201d (parameters and flops). In the end, what most people care about is the resources required to use these models. The authors provide \u201cspeed\u201d (steps/sec) values for each model in their main results table, but do not spend much time discussing it in the text. I think speed should be more strongly emphasized.\n\nInsufficient hyperparameter information (see comment in below section)\n\nNumerous works have shown that the optimal amount of training data scales proportionally to the size of the model being trained. But you train all your models with the same amount of data. Is it possible that the scaling results you observe for larger models are confounded by their being trained on an insufficient amount of data?\n\nPerhaps I\u2019m misunderstanding something about the scaling quality metric, but it seems like it may be a poor metric if it is showing the vanilla transformer as a superior choice to the GLU-Transformer. The GLU-Transformer is superior to the vanilla Transformer for every model size when evaluated w/ perplexity (Figure 2) and every model size except one when evaluated on SuperGlue (Figure 3), yet the vanilla transformer \u201cscales better\u201d than the GLU-Transformer and is repeatedly emphasized as the best model choice. Similarly, the Switch Transformer has superior perplexity and similar SuperGlue accuracy to the vanilla transformer at all but one model size.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Datapoint labels (model names) are illegible in Figure 1. Please increase the text size if possible.\n\nIn the Related Work section, you say \u201c\u200b\u200bFedus et al. (2021) scaled a sparse model based on Mixture of Experts (MoE) models up to trillion parameters.\u201d but you don\u2019t report their findings.\n\nThere is insufficient information about hyperparameters. You mention training for 2^19 steps on C4, but do not mention batch size or LR/LR schedule, for example.\n\nNeeds another round of proofreading: \u201cImprovements at a a specific scale\u201d; \u201cDetails on scaling details\u201d; many of the section titles are followed immediately by a single sentence that restates the same information that\u2019s provided by the section title, e.g. \u201cDo All Models Scale the Same Way\u201d is followed by a single sentence \u201cThis section investigates if all model architectures scale in the same way\u201d.\n",
            "summary_of_the_review": "Well-motivated and will be of useful to the field if the methodological and editorial shortcomings are addressed.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6203/Reviewer_4um9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6203/Reviewer_4um9"
        ]
    },
    {
        "id": "LMihT-QYgNc",
        "original": null,
        "number": 2,
        "cdate": 1666651789140,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651789140,
        "tmdate": 1666651789140,
        "tddate": null,
        "forum": "GGItImF9oG5",
        "replyto": "GGItImF9oG5",
        "invitation": "ICLR.cc/2023/Conference/Paper6203/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper conducts extensive experiments to study the scaling behavior of different neural network models on language problems. The experiment results show: \n1. Different models architectures scale differently when the sizes of the models are changed,\n2. At different compute regions, the model that perform best is different.\n3. Scaling models in different ways, such as scaling by width or depth, shows different scaling behaviors.\n\n",
            "strength_and_weaknesses": "Strength:\n\n1. The experiments are comprehensive. Many model architectures are tested in a large range of model scales. \n2. Both upstream and downstream performances are considered and studied.\n3. Careful attention is paid on how to scale the models.\n\nWeaknesses:\n\n1. The paper presents a lot of empirical results, but does not summarize new patterns from the observations.\n2. The paper does not discuss the mechanism behind the results, such as why different models have different scaling behaviors. \n2. No new insight is provided on how neural networks work in different scales. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is clearly written.\n\nQuality: The empirical study is brought out with high quality, with extensive experiments on many different settings.\n\nNovelty: Empirically, experiments on the scaling behavior of neural networks with such a large coverage have not been done before. Technically, no new message is given. \n\nReproducibility: I did not check the reproducibility of the experiments. ",
            "summary_of_the_review": "This work conducts extensive experiments, but lacks deeper thinking and analysis of the mechanism that causes the empirical results. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6203/Reviewer_7UyJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6203/Reviewer_7UyJ"
        ]
    },
    {
        "id": "EUCAsMe1DX",
        "original": null,
        "number": 3,
        "cdate": 1666655523769,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666655523769,
        "tmdate": 1666655523769,
        "tddate": null,
        "forum": "GGItImF9oG5",
        "replyto": "GGItImF9oG5",
        "invitation": "ICLR.cc/2023/Conference/Paper6203/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates the scaling laws of different sequence model architectures, showing that different Transformer and non-Transformer models scale differently with model size. This study is carried out throughout, as it compares many mainstream models with a wide range of model sizes for both upstream and downstream tasks.",
            "strength_and_weaknesses": "Strength: The evaluation in this paper is extensive as it examines a wide range of inductive bias/different model architectures from tiny to large model scales. In addition, this paper also provides results on both upstream and downstream tasks and points out the potential discrepancy in the performance of upstream and downstream tasks.\n\nWeaknesses:\n1. Why use FLOPS instead of speed to show scaling laws? Although FLOPS is a good indicator of speed, FLOPS does not really reflect how well the model can scale on real-world hardware.\n2. It is clear to me that some architectural changes might not generalize well to large-scale models or downstream tasks. However, it remains unclear to me how to leverage lessons learned from this study to help designing new architectures. For example, Chinchilla [1] suggests using more data to achieve compute-optimal language models.\n3. This paper also lacks enough studies on scaling protocols. Although the paper touches slightly on the impact of scaling protocols for different architectures, I wonder if it is possible to get a general scaling rule for sequence models.\n\n[1] Training Compute-Optimal Large Language Models, arxiv'22\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and easy to understand. The quality of the work is good, as the approaches to studying scaling laws, the evaluation setup, and the derived conclusions all seem reasonable to me. The novelty of the paper is moderate, as it does not really provide new practices for designing new architectures and scaling up models.",
            "summary_of_the_review": "The paper is well motivated, clearly written, and technically sound. However, I would rate the paper slightly above the acceptance threshold due to the lack of new guidance/practice on scaling models.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6203/Reviewer_6T2s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6203/Reviewer_6T2s"
        ]
    },
    {
        "id": "meY-CGN9WoL",
        "original": null,
        "number": 4,
        "cdate": 1666678079316,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678079316,
        "tmdate": 1666678079316,
        "tddate": null,
        "forum": "GGItImF9oG5",
        "replyto": "GGItImF9oG5",
        "invitation": "ICLR.cc/2023/Conference/Paper6203/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper conducts a systematic study of scaling behavior of different model architectures. \n",
            "strength_and_weaknesses": "*Strength:*\n\nExtensive experiments have been done in this paper.\n\n*Weaknesses:*\n\n(1). There is a lack of innovation. All of the conclusions drawn from experiments are pretty under expectation and there are no surprises. Majority of findings have been seen before.\n\n(2). This paper mainly compares a series of models published by google such as Evolved Transformers, Universal Transformers, Switch Transformers, Performer, Funnel Transformer, Albert, MLP-mixer, and ignores some other state-of-the-art models. For example, although the Switch Transformer was published first, the subsequent models such as BASE layers [1], Hash layers [2] are much better than the Switch Transformer.  As a systematic study paper, the author should choose SOTA models for comparison. \n\n(3). One of the conclusions from this paper is \u201cthe MLP-Mixer is hard to scale\u201d.  I doubt whether this can explain the structure of MLP hard to scale, or because the author only uses mlp-mixer at input encoder part. Actually, MLP structure can also be applied to decoder-only models such as [3]. \n\n(4). I wonder why this paper does not compare encoder-decoder model architecture v.s. decoder-only model. The encoder-decoder model seems to be rarely used for large language models. The largest T5 is only 11B, however, decoder only models can easily pass 100B parameters. In addition, I wonder how to scale up encoder-decoder models (increase depth or width? How to balance the encoder part and decoder part). This article systematically compares model structures, but ignores this one.\n\n*References:*\n\n[1]. Lewis, Mike, et al. \"Base layers: Simplifying training of large, sparse models.\" International Conference on Machine Learning. PMLR, 2021.\n\n[2]. Roller, Stephen, Sainbayar Sukhbaatar, and Jason Weston. \"Hash layers for large sparse models.\" Advances in Neural Information Processing Systems 34 (2021): 17555-17566.\n\n[3]. Yu, Ping, et al. \"Efficient Language Modeling with Sparse all-MLP.\" arXiv preprint arXiv:2203.06850 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThis paper is well written and easy to understand.\n\nNovelty:\n\nAs stated in weaknesses, this paper lacks novelty. In addition, instead of trying to compare SOTA model architectures, most models in this paper come from google. I understand that this will greatly reduce the workload, but as an article of a systematic study, more SOTA model structures should be selected.\n",
            "summary_of_the_review": "This article systematically studies model architecture\u2019s influence on scaling law. Extensive experiments are carried out. However, All of the conclusions drawn from experiments are pretty under expectation and there are no surprises. Majority of findings have been seen before. In addition, some SOTA model architectures are not included in this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6203/Reviewer_1jM9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6203/Reviewer_1jM9"
        ]
    }
]