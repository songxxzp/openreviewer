[
    {
        "id": "SrlVtkg-pO9",
        "original": null,
        "number": 1,
        "cdate": 1666643549523,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643549523,
        "tmdate": 1666643760969,
        "tddate": null,
        "forum": "SZYXyhE2c6f",
        "replyto": "SZYXyhE2c6f",
        "invitation": "ICLR.cc/2023/Conference/Paper5838/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose a probabilistic framework to model modular architectures for the problem of continual learning (CL). They divide the problem of choosing modules in two parts: perceptual transfer (PT), and non-perceptual transfer (NT). In PT, the first l layers are assumed to be pre-trained and the model must choose between those modules to maximize performance when training new modules on top. NT works the other way around. From this, they derive 3 CL algorithms: (i) PeCL, which does perceptual and few-shot transfer, (ii) NoCL, which does non-perceptual transfer, and (iii) CCL, which combines both strategies. To evaluate their model, the authors introduce a compositional version of CTrL and evaluate plasticity, backward transfer, forward transfer, perceptual transfer, few-shot transfer, and non-perceptual transfer. In experiments they show that their models perform favourably compared to SA, MCL-RS, HOUDINI, MNTDP.",
            "strength_and_weaknesses": "Strengths\n=======\n* The method is sound and it tries to approach modularity from first principles with a divide-and-conquer approach\n* The proposed method outperforms MNTDP\n\nWeaknesses\n=========\n* The model description is not clear, which makes it difficult to understand. Overall the submission looks unpolished. I highly encourage the authors to carefuly rework sections 3, 4, and 5, to make it clearer.\n* The authors cite LMC but do not compare with it, is there any reason?\n* The divide-and-conquer approach has a clear disadvantage: it introduces the additional complexity of establishing where to split the preceptual part from the non-perceptual part. Maybe the authors could include a limitations section to talk about that.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n=====\nI found this work difficult to read and understand. Here are some unclear points I found while reviewing the submission:\n\n* To my understanding, only one module is chosen per layer. Then, at the beginning of Section 3, you mention that you split $\\Pi_t$ in subsets $\\Pi_t^i$. Does that mean that you choose modules layerwise? If so, why do you call $\\Pi_t^i$ a subset if it is a single module? Maybe I am missing something.\n* In equation 3, what is the j subscript after the parenthesis?\n* In equation 6, q is introduced without any definition. I could get it from the context but a more explicit definition would help. The same with $\\mathbf{A}$\n* In section 4.2: \"we freeze this selection and reuse the same l modules for PT paths which transfer more modules\" what do you mean?\n* \"We define the augmented search strategy\" why augmented?\n* It took some time for me to parse equation 7, I think some explanation or guidance to the reader (particularly about $\\pi^{*PT,l-1}[l-1]$) could make it much easier to understand.\n* \"The difference between two NT paths ... of length l is that their last l pre-trained modules compute different functions.\" If they are of length l, the last l pre-trained modules is the whole path no?\n* I understand equation 10 but how it is used in your algorithm is not clear to me.\n* \"number of transfer which need to be transferred in order to improve the performance.\" what do you mean by the transfer that needs to be transferred?\n* Eq 11. What is the $j$ after the parenthesis?\n* You introduce the term LCB but I could not find an explanation for what it is. The search brought me to the appendix, where I could find what the letters mean as well as many \"??\" for the references.\n\nBesides these points there are some minor typos like \"as it does not does not involve\" in Section 3.\n\nQuality\n=====\n* For the most part, the technical quality is good, however, the lack of clarity hinders my understanding of the method. In experiments,  it is not clear why the authors did not compare to LMC.\n* It is not clear whether the number of parameters of their method to put them in comparison to the other baselines (if it is the same I did not find it clearly stated in the text, maybe appendix C.2 last paragraph?)\n\nNovelty\n======\nThe proposed method is novel to the best of my knowledge.\n\nReproducibility\n===========\nThe authors did not provide the code neither a reproducibility statement, however, the appendix contain some implementation details.",
            "summary_of_the_review": "The proposed method is sound and it tackles an interesting problem for continual learning. However, the lack of clarity of the text makes it difficult to properly asses the quality of this work. In its current form, this work does not meet the standards of ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5838/Reviewer_ifu1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5838/Reviewer_ifu1"
        ]
    },
    {
        "id": "SlQl73QV6lp",
        "original": null,
        "number": 2,
        "cdate": 1666660316588,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660316588,
        "tmdate": 1666660316588,
        "tddate": null,
        "forum": "SZYXyhE2c6f",
        "replyto": "SZYXyhE2c6f",
        "invitation": "ICLR.cc/2023/Conference/Paper5838/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper extends prior work on Continual Learning in two directions: 1) It introduces new learning sequences designed to evaluate non-perceptual transfer (transfer the last l layers) and few-shot transfer (transfer all layers) as well as more challenging sequences for the evaluation of perpetual transfer (transfer the first l layers). 2) Building on modular approaches to CL, e.g., (Venital et al. 2020), a probabilistic framework is proposed to tackle the exponential space of module combinations. One probabilistic model is proposed for perceptual (and few-shot) transfer and a separate model is proposed for non-perceptual transfer.\n\nExperiments compare the proposed approach to some recent methods and the results are somewhat favorable to the proposed method.\n",
            "strength_and_weaknesses": "## Strengths\n\n**S1.** The paper identifies limitations of prior work and seeks to address them.\n\n## Weaknesses\n\n**W1.** The contributions of the submission are very incremental. I see it as an extension of the work of (Venital et al. 2020) in the two directions outlined in the summary. Further, the theoretical contributions are obfuscated by the rough presentation (more on this below) and the validation is, in part, designed for the proposed method (i.e., some of the evaluation sequences are designed to not be handled properly by prior work).\n\n**W2.** The presentation has many issues:\n\n1. often symbols are introduced without definition and sometimes never get defined, e.g., Z in eq. (10), d in eq. (10) does not match the d previously defined; A at the end of page 4;\n\n2. evaluation metrics like those in Table 1 are never defined; etc.\n\n3. some explanations do not make much sense, e.g., \u201cgives preference to modules which helped achieve a higher accuracy, on the problems which they have been trained on,\u201d \u201cminimum number of transfer which need to be transferred,\u201d \u201ccan be explored by applying this search strategy to each sequentially,\u201d etc.\n\n4. the probabilistic models proposed are not explained in detail, e.g., eq. (5) is simply stated with no explanation and the Gaussian Process in section 5.1 is not even specified.\n\n**W3.** The paper disregards other benchmarks in CL like (Lin et al. NeurIPS 2021.) Such a benchmark should be discussed, and results on it should either be included or there should be a discussion explaining why those would not be relevant.\n\n## References\n\nLin et al. The CLEAR Benchmark: Continual LEArning on Real-World Imagery. NeurIPS 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "In my view this work has potential but is work in progress, and is not quite ready for publication.",
            "summary_of_the_review": "The paper\u2019s contribution is limited as outlined above and the quality of the presentation is very poor.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5838/Reviewer_LPZE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5838/Reviewer_LPZE"
        ]
    },
    {
        "id": "NM-8OR716yz",
        "original": null,
        "number": 3,
        "cdate": 1666719738451,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666719738451,
        "tmdate": 1666719763596,
        "tddate": null,
        "forum": "SZYXyhE2c6f",
        "replyto": "SZYXyhE2c6f",
        "invitation": "ICLR.cc/2023/Conference/Paper5838/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper tackles the idea of doing inference over a set of modules such that the resulting model can generalize as well as adapt quickly. Different approaches differ as to how they do search over an ensemble of modules. The proposed paper deals with both \"perceptual\" transfer and \"non-perceptual\" transfer. They introduce a probabilistic framework for selection of pre-trained modules.\u00a0The key idea for driving the selection (i.e., which module to select) is\u00a0 how well the module can transform the input. Each module is parameterized as a neural network. The paper define a posterior and prior, where the posterior is a function of how well the module can explain the input, and prior gives preference to modules which help achieve a higher accuracy.\u00a0",
            "strength_and_weaknesses": "Strengths:\n\n-  The paper tackles an important problem i.e., exploring better ways of searching through possible combinations of modules.\n\nWeaknesses:\n\n- It would be useful to study how the proposed method scales as a function of number of modules.\n- It would also be useful how the proposed method compares to methods which does a \"soft\" selection of the output of modules (like in LMC). \n- The paper seems a bit hard to parse. It may be useful to give a high level \"overview\"of the proposed method before jumping to describe how the paper works. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Different sections are well written.\n\nQuality: The paper tackles an important problem. \n\nNovelty: The underlying idea seems interesting, and to the best of my knowledge has not been explored in this context before. \n\n",
            "summary_of_the_review": "The paper proposes a probabilistic framework to do inference over an ensemble of modules.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5838/Reviewer_ARaq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5838/Reviewer_ARaq"
        ]
    },
    {
        "id": "Rdt30v_NJk",
        "original": null,
        "number": 4,
        "cdate": 1666780417301,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666780417301,
        "tmdate": 1666780417301,
        "tddate": null,
        "forum": "SZYXyhE2c6f",
        "replyto": "SZYXyhE2c6f",
        "invitation": "ICLR.cc/2023/Conference/Paper5838/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a probabilistic framework for modular continual learning.\nThey built on the idea in [1] that modules can be chosen locally based on their input distribution.\nContrarily to [1], this paper opts for a Bayesian treatment of the module composition problem.\n\nThe paper proposes two transfer learning settings for compositionality: perceptual vs non-perceptual transfer.\nIn the former, we assume that most of the drift in p(x,y) happens in p(x), whereas the in the latter, it occurs in p(y|x).\nThey prescribe one probabilistic method for both settings.\n\nSome experiments in CTRL [2] and a modified version show that the method can outperform some baselines.\n\n[1] Continual Learning via Local Module Composition. Oleksiy Ostapenko, Pau Rodr\u00edguez, Massimo Caccia, Laurent Charlin\n[2] Efficient Continual Learning with Modular Networks and Task-Driven Priors. Tom Veniat, Ludovic Denoyer, Marc'Aurelio Ranzato",
            "strength_and_weaknesses": "**Strengths**\n\nThe proposed method is sensible and adequately scales w.r.t. the number of training modules, which was a limitation of [1].\nThis work can inspire future work in continual learning.\n\n**Weaknesses**\n\nI wish the paper offered a more profound contrast with [1], both technically and empirically.\n\nThe idea of using the modules' input distribution as a signal for activating the modules was proposed in [1], but section 4 does not acknowledge that. Section 4 should explain how previous work has solved the problem above and then explain how their Bayesian treatment relates. The Section should further explain the pros and cons of such treatment compared to the previous solution.\n\nThe paper could instead (or additionally) benchmark their method against [1], which is the most relevant baseline for the paper.\nI understand that running new experiments during rebuttal is not ideal. \nI'm not expecting the authors to do that.\nAt least the others might want to add the reported results of [1] for the CTRL benchmark while mentioning that [1] operates in a much more challenging setting, i.e., **class**-incremental learning and not task-incremental.\n\n  ",
            "clarity,_quality,_novelty_and_reproducibility": "great on all accounts.",
            "summary_of_the_review": "I think the paper is worthy of a publication if the weakness mentioned above are addressed. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5838/Reviewer_E5fq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5838/Reviewer_E5fq"
        ]
    }
]