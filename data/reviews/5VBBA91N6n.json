[
    {
        "id": "VlaFMEoSYmU",
        "original": null,
        "number": 1,
        "cdate": 1666599408933,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599408933,
        "tmdate": 1666599408933,
        "tddate": null,
        "forum": "5VBBA91N6n",
        "replyto": "5VBBA91N6n",
        "invitation": "ICLR.cc/2023/Conference/Paper1871/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This research proposes a subgraph sampling method(LMC) with provable convergence for graph neural networks. Training Graph neural networks suffer from neighbor explosion problems. Subgraph sampling methods apply to a wide range of GNN architectures by directly running GNNs on the subgraphs constructed by the sampled mini-batches and showed great empirical success in prior works, while inaccurate mini-batch gradients hurt the accuracy and performance of GNNs. The key idea of LMC is effective compensation in backward and forward passes, which computes accurate mini-batch gradients and thus accelerate convergence.",
            "strength_and_weaknesses": "Strength:\n1. Key idea of effective compensation is great, and they explain their idea clearly.\n2. First subgraph sampling method with provable convergence for graph neural networks. Their theoretical analysis shows that mini-batch gradients are unbiased and their algorithm can converge to the stationary point of GNNs.\n3. Numeric experiments are sufficient, it's worth mentioning that the experiments on compensation terms are great.\n\nWeaknesses:\n1. Some typo, denote the j-th columns of A by $A_j$ not $A_i$, forward not froward.\n2. More explanations are needed for the difference of GAS and LMC, why more information keeping can get the algorithm more robust the the graph size. On some datasets, GAS performs better, why?   ",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of the whole research is good especially since they explain some details for their numeric experiment.  ",
            "summary_of_the_review": "This research proposes a subgraph sampling method(LMC) with provable convergence for graph neural networks.  The key idea of LMC is effective compensation in backward and forward passes, which computes accurate mini-batch gradients and thus accelerate convergence. Numeric experiments and theoretical analysis are great, while more explanations are needed for the difference and key development with the prior works.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1871/Reviewer_7N9L"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1871/Reviewer_7N9L"
        ]
    },
    {
        "id": "3_l8B1NFKdy",
        "original": null,
        "number": 2,
        "cdate": 1666625228934,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625228934,
        "tmdate": 1666625228934,
        "tddate": null,
        "forum": "5VBBA91N6n",
        "replyto": "5VBBA91N6n",
        "invitation": "ICLR.cc/2023/Conference/Paper1871/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors proposed a subgraph sampling training technique to solve the neighbor-exploding issues commonly seen in GNN training. The proposed methods named Local Message Compensation is claimed to debias the mini-batch gradient in a one-shot sampling setting hence accelerate convergence of training. Both theoretical and experimental results shows the efficacy of the proposed method. \n",
            "strength_and_weaknesses": "Pros: \n- The paper is well-written with wide coverage of related works. \n- The deviate of backward SGD is intuitive and make it easy to follow the further approximation and compensation. \n- The claimed theorems and illustration in Figure 1 make the difference between LMC and other methods straightforward. \n- Space and time complexity analysis in Appendix adds more concrete supports to the efficiency of LMC\n- The uniform sampling and variance reduction technique using neighbors\u2019 historic inform resemble some of the accelerated SGD method, and the authors provided theoretical rate of convergence in a similar way which is good to compare the ideal performance. \n- Complete and detailed experiments showing the convergence and efficiency of proposed method.  \n\nCons: \n- In equation (7), on the left hand side shouldn\u2019t the gradient for \\theta^l depends on V_j^{l + 1} instead of V_j^l  (same for u_{\\theta}) ? \n- Does the classification performance of GAS and LMC always stay at the same level? What if you change a different base GNN (maybe less expressive and prone to error by gradient variance)? \n - Please add indicator (such as different font, color, etc) in the tables so that its easier to find the winner. \n- Is the method universally adapted? What is the performance when applying to small/medium size graph?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to understand with clear definition and intuitive illustrations. The novelty comes mostly from the historic compensation part but the authors shows the necessity in ablation studies. Results should be easily reproduced once they publish the code. ",
            "summary_of_the_review": "This work is a step forward from FAS. Though the two shares some similarity, the authors shown the improvements with both theoretical guarantee and detailed experiments. Overall I would recommend the paper to be accept. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1871/Reviewer_p4iX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1871/Reviewer_p4iX"
        ]
    },
    {
        "id": "8ZkntwpW5TY",
        "original": null,
        "number": 3,
        "cdate": 1667222258115,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667222258115,
        "tmdate": 1667235924804,
        "tddate": null,
        "forum": "5VBBA91N6n",
        "replyto": "5VBBA91N6n",
        "invitation": "ICLR.cc/2023/Conference/Paper1871/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a new subgraph sampling method: Local Message Compensation (LMC) for speeding up GNNs training. The general subgraph sampling method will dscard information outside the mini-batches, and thus brings in performance degradation and slow convergence. LMC deals with this issue by retrieving the discarded information in backward passes based on a message passing formulation of backward passes. This paper also provides convergence analysis. The experiment is promising and achieves good convergence speed and accuracy.  ",
            "strength_and_weaknesses": "Strength:\n\n1: A reasonable method for retrieval information outside the mini-batches. \n2: Providing convergence analysis. \n2: Comprehensive experiments show fast convergence.\n\nWeakness:\n\n1) One thing I am confused is that do you need to maintain the all the node embeddings for all the layers? If that is the case, what if the number of layer is too large, would it be OOM issue? Traditional subgraph sampling methods such as ClusterGCN does not need to main such embeddings for all the layers, as it is computed on-the-fly, and thus more memory efficient.\n\n2) how is the subgraph sampling method playing a role during the convergence? For example, what if using random partition vs metis?\n\n3) how is the per epoch time comparing with different methods? The result presented in the paper is mostly on total training time  (or total epochs ) vs accuracy or loss. It would interesting to see per epoch time as well. As I feel seems there are more variables needing to be updated from traditional subgraph sampling method, LMC will need more time per epoch.\n\n4) how the proposed method compared with GraphFM, which seems quite similar to the proposed method.\n\n5) also some ablation over the combination coefficients beta_i.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to understand and seems novel.",
            "summary_of_the_review": "The paper is in easy to understand and the proposed method is making sense and achieves good convergence. I am mostly concerning about the memory usage and space to maintain all the layers historical information. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1871/Reviewer_LGPo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1871/Reviewer_LGPo"
        ]
    },
    {
        "id": "4-_utS5IQo",
        "original": null,
        "number": 4,
        "cdate": 1667268071575,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667268071575,
        "tmdate": 1668790700542,
        "tddate": null,
        "forum": "5VBBA91N6n",
        "replyto": "5VBBA91N6n",
        "invitation": "ICLR.cc/2023/Conference/Paper1871/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors proposed a novel subgraph-wise sampling method, Local Message Compensation(LMC), to accelerate the training of GNNs on large-scale graphs. They mainly focus on solving the neighbor explosion problem. Their main concern is finding a solution to the neighbor explosion issue. I think their research has a significant impact on the advancement of GNNs.",
            "strength_and_weaknesses": "Strength:\n(1) Their approach helps to avoid the neighbor explosion problem to a certain extent.\n(2) The convergence of their method is provable.\n\nWeaknesses:\n1. one typo is that in table 1 GraphSAINT's 97.0 reaches the best.\n2. It seems to me that the authors claim that their method is more efficient than other methods, which means the \"proportion of reserved messages (%) in forward and backward passes\" based on their response, but in terms of memory in Table 2, for example, 1892 for LMC is about 2 times more than 1193 for CLUSTER and 557 is about 1.3 times more than 424, which may confuse others in terms of memory efficiency. It would be clearer to modify Table 2 and add \"proportion of reserved messages (%) in forward and backward passes\" to it.",
            "clarity,_quality,_novelty_and_reproducibility": "This manuscript is excellently written. Their method is clearly explained in the notations, equations, and descriptions. Their novelties include a new backward SGD technique and theoretical guarantees.",
            "summary_of_the_review": "Even though their work aids in the development of GNNs, these are insufficient for being strongly accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1871/Reviewer_DdvT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1871/Reviewer_DdvT"
        ]
    }
]