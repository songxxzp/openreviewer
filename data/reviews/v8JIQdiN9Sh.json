[
    {
        "id": "U2p7LLoGeB7",
        "original": null,
        "number": 1,
        "cdate": 1666270639630,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666270639630,
        "tmdate": 1669198445028,
        "tddate": null,
        "forum": "v8JIQdiN9Sh",
        "replyto": "v8JIQdiN9Sh",
        "invitation": "ICLR.cc/2023/Conference/Paper2604/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies self-supervised long-tailed learning with the auxiliary of out-of-distribution data by designing a sampling strategy together with a distribution-level supervised contrastive loss. Empirical validation demonstrates the effectiveness of the proposed method on a range of datasets compared with previous methods. ",
            "strength_and_weaknesses": "Strength\n\n(1) The experiments on a range of datasets demonstrate that it is promising to improve the performance of self-supervised long-tailed learning via external data. \n\n(2) The proposed method is orthogonal to a range of existing methods and can be consistently improve the current self-supervised long-tailed learning methods.\n\nThere are some concerns that need to be improved.\n\n(1) The first contribution in the Introduction is not very meaningful, since MAK has given such a setting and a rough answer to this problem. It is advised to refine this point, better claiming the challenges in this direction.\n\n(2) The proposed method combines dynamic sampling guided by tailed area estimation and an additional supervised contrastive learning, which slightly makes the novelty incremental. Although it shows the additional supervised contrastive learning helpful in Figure 3(c), it also confuses the key challenge of this topic. \n\n(3) The external datasets are also relatively controversial, since it seems that the authors also choose the datasets tightly in the same domain, e.g., ImageNet-100 with ImageNet-R, instead of the non-curated open-world datasets. Here, some details are not clear. What is 300K random Images in Hendrycks 2018b. for CIFAR-LT. \n\n(4) It is not proper to roughly compare the computational overhead between COLT and MAK, since the periodic sampling on the basis of the clustering is also expensive. It might be more reasonable to compare the time cost relative to the plain backbone without the sampling augmentation.",
            "clarity,_quality,_novelty_and_reproducibility": "The authors design an effective method to help self-supervised long-tailed learning with the auxiliary of out-of-the-distribution data and demonstrate the promising performance on a range of datasets. My main concerns are about the technical novelty, which is relatively ad-hoc and some claims need to be refined.",
            "summary_of_the_review": "It is an interesting and promising work, and the technical novelty and some claims should be improved. I can consider to raise the score if above concerns can be well addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2604/Reviewer_SHmZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2604/Reviewer_SHmZ"
        ]
    },
    {
        "id": "lpaTUFNCWE",
        "original": null,
        "number": 2,
        "cdate": 1666545893822,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666545893822,
        "tmdate": 1668787265825,
        "tddate": null,
        "forum": "v8JIQdiN9Sh",
        "replyto": "v8JIQdiN9Sh",
        "invitation": "ICLR.cc/2023/Conference/Paper2604/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies self-supervised learning (SSL) for long-tailed datasets. In particular, it proposes to leverage out-of-distribution (OOD) data to improve model performance when facing imbalanced data, with a new framework proposed called Contrastive with Out-of-distribution (OOD) data for Long-Tail learning (COLT). It first proposes to localize tail samples by assigning a tailness score to each OOD sample, and then uses an online OOD sampling strategy to dynamically re-balance the feature space. Finally, a distribution-level supervised contrastive loss is proposed. The paper performs experiments on several long-tailed datasets and verifies that COLT improves the performance over other baselines.",
            "strength_and_weaknesses": "# Strength \n+ The investigation of OOD data for improving long-tailed recognition is under explored. The problem itself is important and practical, and the technique introduced is well motivated and clearly stated.\n+ The experiments are thorough and cover large-scale benchmark datasets in the field. The performance is good -- compared to other CL baselines, the improvements on certain datasets are even larger than 10%.\n+ The ablation studies are thorough. They confirm that each of the components should be useful for long-tailed data.\n+ The analysis and visualization of the feature balanceness / ability for tail discovery are effective and clear.\n\n---\n# Weaknesses\n\nUnfortunately, there are several major weaknesses that exist in the current paper.\n\n### Methods & Experiments\n\n- One drawback is that the paper fails to compare with semi-supervised learning methods for long-tailed recognition.\n1. The main contribution of the paper is to consider OOD dataset in long-tailed learning, and design SSL algorithm to incorporate the OOD samples into the training process. However, one natural choice when given more unlabeled data is to use *semi-supervised* learning. For example, how does COLT compare to SOTA semi-supervised learning methods [1-3] when leveraging OOD data?\n\n[1] FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence.\n\n[2] FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling.\n\n[3] SelfMatch: Combining Contrastive Self-Supervision and Consistency for Semi-Supervised Learning\n\n2. Moreover, in the long-tailed literature, methods have already been proposed to use unlabeled data for combating data imbalance [4-6]. Although they might use ID samples, it is still encourgaed to compare with these standard methods in the field to demonstrate the performance is convincing.\n\n[4] CReST: A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning. CVPR 2021.\n\n[5] ABC: Auxiliary Balanced Classifier for Class-imbalanced Semi-supervised Learning\n\n[6] Distribution aligning refinery of pseudo-label for imbalanced semi-supervised learning\n\n- Another part I find confusing is the choice of OOD dataset.\n1. How does OOD defined in the first place? Does it refer to covariate shift from the original data? Is the OOD strength controlled across different datasets? Overall, the authors did not give a principled way to define / contruct the OOD dataset.\n2. Is the OOD dataset used also imbalanced / long-tailed? Will the imbalance of OOD dataset influence the results of COLT?  If so, how does it influence the performance? Unfortunately the analysis is missing here.\n3. What is the amount (i.e., # of samples) of the OOD dataset? Will the number of OOD samples influence the performance?\n\n- It is unclear how many epochs needed for training with COLT. In fact, no descriptions are provided for the actual training details. Does COLT need more training epochs, similar to contrastive learning methods like SimCLR / MoCo? What about training details for other baselines? \n\n- Similar to above, contrastive learning-based methods usually use strong data augmentation tricks during training. However, many current methods do not use them, making the comparison unfair. Does COLT involve strong augmentations during training? This again goes to the comment that no detailed training setups are provided. If so, the authors should comprehensively compare COLT with current non-contrastive learning-based methods (such as RIDE) by adding such tricks to them. \n\n- The proposed method can obtain enhancement on all many / medium / few-shot classes (e.g., on ImageNet-100-LT), which is perhaps surprising and a great benefit, but also leads to many questions. Many current methods sacrifice the head performance. Can the author explain where the benefit comes from, and why the current methods cannot achieve this goal?\n\n### Writing\n- The overall writing quality is not good. The writing needs to be significantly improved.\n- The references in the text are somehow mixed with the main text without clear separation, making it really hard to read, especially for the related work. Please fix accordingly.\n- There are many typos in the text, I suggest a careful rereading. Here are some examples:\n  - page 7, \"about 12% for longtail CIFAR-100\" -> \"about 12% for long-tailed CIFAR-100\"\n  - page 7, \"COLT yields balancedness feature space\" -> \"COLT yields a balanced feature space\"\n  - page 8, \"but also significantly alleviates the imbalancedness\" -> \"but also significantly alleviates the imbalance\"\n\n---\n# Other questions\n- The authors showed that using a distribution-level supervised contrastive loss to separate ID / OOD samples can lead to better performance. I do not fully understand the intuition here. The OOD samples used in the experiments are still largely overlapped with the ID samples (e.g., Places-LT uses still \n\"places\" samples as OOD dataset; the contents do not change). That being said, the high-level semantic information are still there. Directly separating them may not lead to better feature space as many of the features can be shared across ID / OOD samples. Can you explain the rationale behind using this SCL loss?\n- The distribution-level supervised contrastive loss is essentially try to discriminate between two sets of samples (ID or OOD). I wonder why not directly using a discriminator loss? What would be the differences?",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\nFair.\n\nx) The language is fine, but needs to be further improved.\n\nx) The structure and flow are ok.\n\nx) There are many typos in the text, I suggest a careful rereading.\n\nPlease refer to the weaknesses part for a complete review.\n\n\n## Quality\nFair.\n\nThe paper is based on an idea that I find nice, but I feel that the paper could have developed and explored it more in depth.\n\n## Novelty\nGood.\n\nThe main message of the paper is a recipe to use OOD samples for improved long-tailed learning performance, and to exploit this knowledge for better training. As far as I know, this is novel.\n\n## Reproducibility\nGood.\n\nThe source code is provided with detailed running setups; although I still insist that more implementation details should be provided in the main paper, such as the training epochs, augmentation choices, etc.\n\nDisclaimer: I did not check the code in detail.",
            "summary_of_the_review": "The paper studies an interesting yet important problem, utilizing OOD samples for long-tailed learning. The paper proposed several interesting components to form a self-supervised learning framework called COLT, which is later demonstrated to deliver better results.\n\nThe overall idea of leveraging OOD data seems to be novel and not explored before in this field, which is plausible.\n\nHowever, currently there are several drawbacks and weaknesses, in terms of Methods and Exepriments. The writing quality also needs to be significantly improved.\n\nIn summary, in my view, the current elaboration and presentation do not meet the standards of ICLR. I am open to increase my evaluation if my principal concerns in the Weaknesses section are addressed in a convincing manner.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2604/Reviewer_QM2L"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2604/Reviewer_QM2L"
        ]
    },
    {
        "id": "pbNpBD6_bE",
        "original": null,
        "number": 3,
        "cdate": 1666646240456,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646240456,
        "tmdate": 1668845195690,
        "tddate": null,
        "forum": "v8JIQdiN9Sh",
        "replyto": "v8JIQdiN9Sh",
        "invitation": "ICLR.cc/2023/Conference/Paper2604/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a method where out-of-domain (OOD) data are used to learning better representations via self-supervised learning (SSL) in a setting where we have unlabeled in-domain (ID) data that follow a long-tail distribution. The paper proposed to define an unsupervised \"tailness\" score, while an online sampling strategy is proposed for sampling the OOD data mostly closer to the tail classes. Experiments are conducted on 4 long-tail datasets",
            "strength_and_weaknesses": "Strengths:\n\n- Self-supervised Learning (SSL) on long-tail data is a very important and interesting problem; training on balanced datasets like imagenet is highly artificial in that regard.\n\n- Improving SSL on long-tailed datasets effectively with external unlabeled OOD data is a realistic scenario that was shown to work for supervised learning by Wei et al (2022).\n\n- The method seems to give strong gains for SimCLR for long-tail SSL.\n\nWeaknesses:\n\n- The authors say that they \"obtain C feature prototypes from ID training set S_id via K-means clustering.\". How does one expect to capture a long-tail distribution with kmeans? How well does this clustering work, also when looking at clustering accuracy for the head and tail classes? Also, how do you set K? Is there a further assumption that we know how many classes exist? \n\n- Defining the \"tailness\" score as simply the sum of the \" top-k%\"  largest negative logits seems like an unstable heuristic. Does k transfer across datasets? Did you consider other alternatives, eg radius-based? \n\n- Although tackling a realistic problem in theory, the datasets used are mostly toy-level. Results on a real long-tail dataset like iNaturalist would be more convincing.\n\nNotes:\n- It is hard to undestand which line is which in Fig 3e, please fix the typo of two methods having the same line color and style",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, contains technical novelty, while its results seem reproducible. I encourage the authors to open source their code for better reproducibility.",
            "summary_of_the_review": "The paper deals with an interesting task and offers a solution that to my knowledge is novel, works better than recent works on the datasets it is tested on. Adding more realistic evaluations on bigger and real long-tail datasets, as well as expanding on aspects like the use of k-means clustering or different, more robust criteria for the tailness loss would make me more positive towards acceptance. Looking forward to the authors' responses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2604/Reviewer_9FQm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2604/Reviewer_9FQm"
        ]
    },
    {
        "id": "m1K-gcIcK9",
        "original": null,
        "number": 4,
        "cdate": 1666723933384,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666723933384,
        "tmdate": 1669318348926,
        "tddate": null,
        "forum": "v8JIQdiN9Sh",
        "replyto": "v8JIQdiN9Sh",
        "invitation": "ICLR.cc/2023/Conference/Paper2604/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors aim to employ OOD data to improve self-supervised learning in the long-tailed setting.  To achieve that, they use tailness score estimation, dynamic sampling strategies, and additional contrastive losses for long-tailed learning with additional OOD samples.  The authors conduct experiments on various datasets and several state-of-the-art SSL frameworks to evaluate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\n- The idea in this work is novel. Before this work, It has not been shown how to exploit the benefits of OOD data for SSL long-tailed setting. \n- The improvements shown in the reported results are significant. From the reported results, the proposed method can effectively utilize the benefits of OOD examples in the SSL long-tailed settings.\n\n\nWeakness:\n- The contribution of this work is limited. This work can be seen as an SSL version of the recent work [1], which has shown that OOD can be beneficial to long-tailed learning. Although this work provides some technical contributions by adapting OOD data for SSL methods, this work cannot provide new insight for this problem.\n- This work does not provide any in-depth understanding about why OOD examples can improve SSL in long-tailed setting. In this paper, the authors simply propose a method with three components, but do not explain why we could use OOD examples there.\n\n\n[1] Wei, H., Tao, L., Xie, R., Feng, L., & An, B. (2022, June). Open-Sampling: Exploring Out-of-Distribution data for Re-balancing Long-tailed datasets. In International Conference on Machine Learning (pp. 23615-23630). PMLR.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The citation format should be improved. In the current version, the citation is mixed with the text part, which makes it difficult to read. I suggest the authors to use \\citep instead.\n\n- The writing should be improved. As shown in the weakness, the authors should not simply describe the method without any in-depth understanding. \n\n- Originality: From my perspective, the proposed method is novel but not interesting enough.",
            "summary_of_the_review": "The proposed method is novel and has shown significant improvement from the reported results. However, this work does not provide any insight or understanding about why it can work. Since this work can be treated as an SSL version of the recent work [1], the contribution of this work is not sufficient for this conference. So, I recommend a weak reject for this work.\n\n-----\n\nThe response from the authors has addressed my concerns so I decide to improve my score to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2604/Reviewer_EuL2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2604/Reviewer_EuL2"
        ]
    }
]