[
    {
        "id": "2lXzXgqkdm",
        "original": null,
        "number": 1,
        "cdate": 1666563383089,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666563383089,
        "tmdate": 1666563383089,
        "tddate": null,
        "forum": "3vOtC1t1kF",
        "replyto": "3vOtC1t1kF",
        "invitation": "ICLR.cc/2023/Conference/Paper2420/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new approach to personalized federated learning via sparse model adaptation. The key idea is to configure local models as subnets of a master net representing the global model. \n\nThe subnets are generated via a block-wise binary mask that (1) zeroes out less important components; and (2) enforce a certain sparsity level such that the no. of non-zero parameters of resulting subnets fits with local hardware constraints. \n\nLocal models are therefore heterogeneous compression of the same global model. This importantly allows local models to have heterogeneous architectures, which is, to my understanding, a less studied aspect of personalized FL. Most existing personalized FL approaches only consider data heterogeneity (but not model heterogeneity).   \n\nThe generative mechanism of the subset is parameterized by a two-flow model: (1) one flow generates crude masks (that might violate local sparsity constraints); (2) the other flow generates an importance score for each parameter blocks. The output of two flows are combined via solving a knapsack problem, resulting in a binary mask that meets the hard sparsity constraint.\n\nComparing to existing model compression or gradient quantization techniques, which are often post-training editing approaches, the proposed approach allows learning both the compression mechanism & the updates to the global model. ",
            "strength_and_weaknesses": "Strengths:\n\n+ Both problem formulation & proposed solution are novel to me. I have not seen similar approaches to this before.\n+ The technical presentation is very well-organized, particularly the well-structured appendix that makes it easy to zoom into each component of the proposed work\n+ The empirical results are reasonably extensive with comparison to multiple baselines in the same direction\n+ There are interesting theoretical results showing both convergence of the proposed algorithm and the existence of optimal local solution: among optimal local solution that meets local sparse constraints, there exists one that can be acquired via masking the global model\n\nWeaknesses:\n\n- Lacking comparison with a known baseline -- Fallah et al., 2020\n- Lacking discussion with other related works on FL with heterogeneous models\n- Theoretical results are established under strong convexity of the loss function, which is likely not the case unless the model is linear\n- Lacking experiments showing advantages over simpler knowledge distillation technique for FL with heterogeneous model\n\nI will elaborate on those weaknesses with specific comments & suggestions below.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe paper is very well-written in general. As I commended the authors above, their appendix is very well-structured. I do, however, have a few nitpicks:\n\n1. The presentation of the proving techniques is too terse. It would always be good to start with a narrative of a general proving strategy, followed by step-by-step derivation annotated with explanation why each step holds\n2. Section 5.2.3 is very much lacking in detail, which actually raises concerns about the differentiability of the entire workflow -- I will comment more on this below.\n\nNovelty & Quality:\n\nAs stated above, problem formulation & solution idea are new so I acknowledge the novelty here. But, I am not sure I follow why the last key step in combining output of the two flows (mask generator & importance score generator)  via knapsack (Section 5.2.3) is differentiable.\n\nI agree that using the previous technique of binarized neural net, the authors can make the representation of the \"knapsack matrix\" I differentiable. But, it seem the max operator is still not differentiable & so if the solution to Eq. (6) is to be integrated into the \"end-to-end\" pipeline, it would come out in the form of a bi-level optimizable task -- further treatment of this probably needs to be detailed because otherwise, it is not immediately clear to me how gradient flows through that bi-level structure.\n\nAs for the empirical experiment, please do consider comparing with Fallah et al. 2020 too for thoroughness. In addition, there should be key discussions & demonstrations of how this proposed mechanism has an advantage over simpler engineering solution for FL with knowledge distillation. For instance, clients can download big model from the server but will distill into smaller local model with heterogeneous structure before baking in local data; in turn, the server can pull local models from clients and distill soft labels from them to train its global model. How would the proposed model perform against this vanilla approach?\n\nMore suggestions:\n\nAlthough the idea of using heterogeneous models in personalized FL is quite new; it has been explored before (in a different context with a different probabilistic formulation) in standard FL setting. For example,\n\nBayesian Nonparametric Federated Learning of Neural Network (ICML-19)\nStatistical Model Aggregation via Parameter Matching (NeurIPS-19)\n\nBoth approaches in these works would allow local models to be heterogeneous in size so as to accommodate for the heterogeneity in their hardware capacity. The formulation is different but it addresses the same aspect so some discussions regarding pros/cons between the two lines of approaches would be nice to have.\n\nMore nitpicks:\n\nIt would be good to repeat all experiments a few times to generate error bars\nIncluding in the appendix more detail about the differentiable parameterization detailed in (Hubara et al. 2016) \nDiscussing potential directions to expand the proving technique beyond the strong convexity assumption which is tied to simple linear model\n\nReproducibility:\n\nThe code is released and the algorithm description is sufficiently clear so I believe reproducibility is possible. But still, for a thorough understanding of the approach, Section 5.2.3 needs a significant flesh-out. \n ",
            "summary_of_the_review": "The paper presents an interesting formulation of personalized FL & a new solution addressing a less studied issue of model heterogeneity. The novelty of this work is strong but its execution needs more polishing, particularly regarding missing experiments with both related work & vanilla baseline (to make sure such proposed mechanism is needed & cannot be bypassed by simpler, trivial engineering solution). The presentation of proving techniques can also be improved. Please refer to my specific comments above. Overall, I believe this will be an acceptable paper if the above are addressed sufficiently. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2420/Reviewer_wTvc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2420/Reviewer_wTvc"
        ]
    },
    {
        "id": "snzwPGx_BYd",
        "original": null,
        "number": 2,
        "cdate": 1666661598867,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661598867,
        "tmdate": 1666661598867,
        "tddate": null,
        "forum": "3vOtC1t1kF",
        "replyto": "3vOtC1t1kF",
        "invitation": "ICLR.cc/2023/Conference/Paper2420/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a personalized federated learning framework that selects a subset of model parameters (thus sparse) for each training example via personalized gating units for each client (Fig.1). The proposed method, pFedGate, has a gating unit/layer for each client, consisting of a rescaling factor M, an importance factor G and learns the binary mask by solving a knapsack problem per training example (Fig.2). Theoretical analysis on generalization, convergence and complexities are provided. Experiments on common personalized FL benchmark datasets show that pFedGate can outperform existing methods.",
            "strength_and_weaknesses": "Strengths\n- Writing is clear and easy to understand\n- Thorough theoretical analysis\n\nWeaknesses\n- The framework fails to solve the resource heterogeneity issue since the clients still need to store the whole (global) model.\n- Some implementation and experiment details remain unclear.\n\nDetail comments:\n\n1. Sec.4 first paragraph: the two-step process \u201cstill requires computational and communication costs corresponding to the un-compressed models during the FL process\u201d. However, according to Fig.1, pFedGate also needs to store the un-compressed model locally in order to compute the compressed model using M\u2019, which is also mentioned in Sec.5.1 \u201call clients share the same global model \\theta_g that downloaded from server\u201d. In addition, since every example uses different blocks of the model, each client has to store the whole model for inference. Therefore, both training and inference are dense, making the main contribution unsubstantiated. (Appendix E is also misleading.)\n\n2. The gradient computation requires further discussion. The forward pass of every example requires solving a knapsack problem as shown in Sec.5.2.3. Combing with the fact that we are training multiple networks (\\theta and \\phi), it would be helpful to elaborate on how the gradient of each network is computed using the knapsack solution. \n\n3. What is the relative sparsity of gating layer s_{\\phi_i} in Sec.6.3?\n\n4. For the experiments\n- How many clients are used for the experiments in Sec.7.2?\n- Sec.5.2.2 discusses that different DNN layers can have different block structures. Then how are the layers being partitioned in the experiments?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear, but the proposed method does not live up to the expectation of handling resource-heterogeneous clients. Some of the implementation details are missing. ",
            "summary_of_the_review": "The paper proposes a framework for personalized FL and it can work well in the experiments. However, it does not provide sufficient justification for the main claim/contribution of solving the resource heterogeneity issue as all clients still need to store the global model locally for training and inference.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2420/Reviewer_PyZd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2420/Reviewer_PyZd"
        ]
    },
    {
        "id": "f9dJkvusjWF",
        "original": null,
        "number": 3,
        "cdate": 1667192947934,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667192947934,
        "tmdate": 1667192947934,
        "tddate": null,
        "forum": "3vOtC1t1kF",
        "replyto": "3vOtC1t1kF",
        "invitation": "ICLR.cc/2023/Conference/Paper2420/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new framework called pFedGate for training personalized models at client devices, such that each model can have different sparsity level depending on the devices computational and memory capacities. The idea is to generate personalized masks with different sparsity levels for each device, using a small gating layer per device. The gating layer itself is small in size and hence the paper claims that it does not add much overhead. The empirical results show that this technique indeed achieves better personalization than existing methods.",
            "strength_and_weaknesses": "Strengths:\n1. The proposed framework seems to achieve better personalization accuracy than existing models.\n2. The idea of generating personalized masks (gating weights) using a computationally cheap network is novel.\n\n\nConcerns / Questions:\n1. I think Assumption 1 might be a strong assumption, since it assumes bounded diversity between the globally optimal model, and the optimal sparse local model. In particular, I think $\\sigma_i$ might itself depend on the level of sparsity for the device.\n\n2. In the proof of proposition 1, I do not follow how the Reverse Cauchy-Schwarz inequality is applied, since the left hand side of Eq. (13) has the l2 norm squared of element-wise multiplication of vectors instead of inner product. To be precise, the left hand side will look like $(\\sum_j (M^*_{i,j}\\theta^*_{g,j})^2 )$ instead of $(\\sum_j (M^*_{i,j}\\theta^*_{g,j}) )$. Also, I think the denominator of Eq. (4) should not have the square in norm of $\\theta^*_{g}$.\n\n3. The experimental results do not provide the time (in seconds) and memory requirements (in GB / MB) for the training of the models. Further, the convergence vs communication round curve is only provided for EMNIST dataset. This data is important since one of the motivations for this paper is to perform Federated Learning efficiently on computationally slow devices.\n\n4. In Eq. (7), is the gradient w.r.t $\\phi$ as well? In other words, does Theorem 2 prove convergence for the gating layer as well?\n\n5. On page 6, section 6.1, I think the definition of $L(\\theta_g, \\phi_i)$ and $\\hat{L}(\\theta_g, \\phi_i)$ should not have the $i$ in subscript on the left hand side since it also appears in the summation on the right.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has typos, but otherwise is well written. It also have novelty in the method proposed. I have not verified all the proofs or tried to run the provided code.",
            "summary_of_the_review": "While the proposed idea of generating personalized masks (gating weights) is novel and has potential, I have some concerns regarding the theory and experimental results that I have mentioned above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2420/Reviewer_sfrQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2420/Reviewer_sfrQ"
        ]
    },
    {
        "id": "X30uHMeAlD0",
        "original": null,
        "number": 4,
        "cdate": 1667311857922,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667311857922,
        "tmdate": 1669040937446,
        "tddate": null,
        "forum": "3vOtC1t1kF",
        "replyto": "3vOtC1t1kF",
        "invitation": "ICLR.cc/2023/Conference/Paper2420/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "\n\nThe draft proposed an approach for efficient personalized FL by adaptively and efficiently learning sparse local models(a subset of the global one) with a shared global model. With a lightweight trainable gating layer, the proposed algorithm enables clients to reach their full potential in model capacity by generating different sparse models accounting for both the heterogeneous data distributions and resource constraints. ",
            "strength_and_weaknesses": "Strength:\n\nS1). The authors provided a theoretical analysis of the proposed algorithm.\n\nS2). The method is validated with sufficient experiments.\n\nS3). The results are very promising.\n\nWeakness:\n\nW1). The novelty of the paper is not very high since there are already some papers with similar ideas in FL. As claimed in Related work \u201cour method generates different sub-models from the whole global model with a larger learnable parameter space to handle heterogeneous data distribution\u201d, actually the following work is using a similar idea. It also adopts the block-wise selection from the global model and generates personal and sparse architecture for different clients. \na). Personalized Federated Learning via Heterogeneous Modular Networks. IEEE ICDM 2022. \nhttps://arxiv.org/pdf/2210.14830.pdf\n\nb). Personalized Neural Architecture Search for Federated Learning, Minh Hang et al. Neurips Workshop 2021.\nhttps://neurips2021workshopfl.github.io/NFFL-2021/papers/2021/Hoang2021.pdf\n\nActually, these two papers are using more elegant solutions for PFL. The proposed solution adopts the mixture method of ideas of NAS and FL. \n\n\nW2). It is strongly suggested to include the two papers above in the revised version for discussion of novelty of the proposed approach. B) is discussing the fine-grained subnetwork selection and A) is discussing block-wise subset network selection for communication efficiency that is similar as this paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly expressed with good quality.\nNovelty needs to be discussed in the two papers not cited.\nReproducibility is uncertain.",
            "summary_of_the_review": "Basically,  The paper is of good quality. Although the novelty is not very high since there are two existing papers that adopt similar ideas for Personalized FL, this paper provides a good theoretical analysis and the results are very promising. If the authors can add a good discussion on the novelty compared with the two papers mentioned below, I would champion the acceptance.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2420/Reviewer_gH8r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2420/Reviewer_gH8r"
        ]
    },
    {
        "id": "9iK1ugcpwUa",
        "original": null,
        "number": 5,
        "cdate": 1667378411562,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667378411562,
        "tmdate": 1669713958639,
        "tddate": null,
        "forum": "3vOtC1t1kF",
        "replyto": "3vOtC1t1kF",
        "invitation": "ICLR.cc/2023/Conference/Paper2420/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "### Summary\n\nThis paper addresses data and resource heterogeneity of each client in Federated Learning. This paper proposes a gating function $g_{\\phi}(x, s_i)$ that allows easily adaptation to each client while taking sparsity into account like computation resources. As gating function generates masking parameters for local models, it can response to various model-size, heterogeneous dataset and computation resources. Especially, the authors suggest that sample-wise model is better effective than a client-wise local model in personalized FL. \n\nTechnically, the authors demonstrate that the proposed method does not hurt the previous theoretical convergence bound and generalization gap in FL. This empirically achieves the state-of-art average performance in personalized FL, ensuring bottom decile performance in a variety of experimental results like EMNIST, FEMNIST, CIFAR10 and CIFAR100. ",
            "strength_and_weaknesses": "### Strength\n\n- This paper consistently presents empirical performance improvement in various experimental results. \n- It also suggests sample-wise local models by using gating function $g_{\\phi}$, so that we do not practically need to design sophisticated manipulation for it, since this gating function can be learnt during training. It obviously leads to work in FL that disregard data and resource heterogeneity.\n- This paper offers many ablation studies of learned FL models, including ones that distinguish each client without additional training. This empirical studies appears relevant for deriving performance improvement of all cases, and supports author's claims.  \n\n### Weakness\n\n- This paper should clarify a limit of data and resource heterogeneity of FL framework. In the manuscript, their voices are likely to be misunderstood, so that the proposed method may be used to totally different devices like edge devices and high-performance cloud computation. According to my understanding, this method supposes that all devices have sufficient memory to simultaneously load global and gating function, but each device's computation capability varies.\n\n- Along with the above comment, the authors should reveal the how extents of sparsity this model can support. Particularly, they should investigate when a gating function rapidly deteriorates. Also, they need to show characteristic of representation capability between global and local models. Lastly, they should clearly reveal a limitation and boundary of this method by changing communication frequency and data distribution of each client.\n\n- In technical aspects,  theoretical reasonings in this paper somewhat mismatch their claim that emphasizes sparsity of each client. Especially, in theorem 1 and 2, they do not consider the sparsity of each client, as well as not provide any high-level understanding of this sparsity based on those theorems. Instead, through very strong assumption like strong convexity of all losses and independence with respect to global, gating functions and these parameters $\\phi_i$, it simply reveals that the proposed method does not violate the previous FL framework from point of the number of effective samples and convergence analysis.\n\n- This paper must compare of FedMask[1] in terms of masking characteristics by varying sparsity. The FedMask also employs masking parameters for each client, so it is completely compatible with the proposed method excluding sample-wise models. If the authors want to highlight practical usage of heterogeneity of data and resources, this comparison must be presented for better understanding of this paper. However, in experimental results on the manuscript and appendix, they simply describe the proposed method outperform those algorithms. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Presentation** : A presentation of this paper appears quite dense, so I\u2019m concerned about violation of the ICLR2023 template.",
            "summary_of_the_review": "In summary, I believe the paper holds an interesting research direction worth exploring in personalized FL. However, in my opinion, the current manuscript does not meet the standards for acceptance at ICLR 2023. The authors are strongly encouraged to present the limitations of the method by empirical manners, and improve technical aspects as I do believe this is a promising idea.\n\n-------------------\n\n**After rebuttal** : I'm satisfied with the authors feedback related to experimental results. They provide several experimental evidences to support the efficiency of their model and performance improvement via this rebuttal phase. However, I believe that theoretical aspects must be revised and re-organized in the camera-ready version. Furthermore, since the authors have responded my comments after the end of discuss period, so they were not able to revise the manuscript. Under the condition that author assure all comments will be included in the camera ready version, I score this paper as 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2420/Reviewer_eJXn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2420/Reviewer_eJXn"
        ]
    }
]