[
    {
        "id": "AGhglDIu_zR",
        "original": null,
        "number": 1,
        "cdate": 1666654070728,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654070728,
        "tmdate": 1666655780917,
        "tddate": null,
        "forum": "yzHn1QejdT4",
        "replyto": "yzHn1QejdT4",
        "invitation": "ICLR.cc/2023/Conference/Paper2992/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper assumes limited label availability and propose a weighted multi-source distillation method, in practice that means distill multiple (diverse) source models trained on different domains, weighing them by their relevance for the target task, assuming such target task is available",
            "strength_and_weaknesses": "Strengths:\n- The paper shows there are sources that may represent better some given target tasks than others, including the baseline of ImageNet pretrained networks\n- The methods show superior results in particular for fine-grained data such as CUB200\n\nWeaknesses:\n- FixMatch is a semi-supervised technique where the unlabeled data is assumed to be from the same domain\n- The methods assumes the target task is available during distillation time\n- A stronger baseline would be ViT/DINO\n- It seems that, overall, it would be better to select a few tasks (more relevant) than to weigh them all. Maybe a preselection would reach similar results without computational burden (single is almost as good as task similarity)\n- It is interesting that MobileNet performs better than ResNet (among others), which contradicts previous work, and no justification is given.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and complete in terms of reproducibility. Although some findings are interesting, they are not completely surprising. In particular Single Source is quite similar to the proposed one. Also, some backbones are weak: InceptionV3, InceptionResNet and others could be used as usually those transfer better (Kornblith 2019), which puts into question the claims of \"distillation is better than fine-tuning from ImageNet\" and \"distilling to efficient architecture could be better than fine-tuning large models\".",
            "summary_of_the_review": "The paper is interesting overall but there are better backbones available, including transformed-based ones. Also, single source is quite similar to the proposed method. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2992/Reviewer_fFzJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2992/Reviewer_fFzJ"
        ]
    },
    {
        "id": "jGk7eAf_F2P",
        "original": null,
        "number": 2,
        "cdate": 1666701030767,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666701030767,
        "tmdate": 1666701030767,
        "tddate": null,
        "forum": "yzHn1QejdT4",
        "replyto": "yzHn1QejdT4",
        "invitation": "ICLR.cc/2023/Conference/Paper2992/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper address the challenge of knowledge distillation for image recognition tasks with few training samples. This paper proved that simply distilling knowledge from a single task will not get good performance. The authors present a weighted multi-source distillation method to distill multiple source models trained on different domains by weighting these models with a task relevance score. Experiments demonstrate that more datasets will bring performance improvement.",
            "strength_and_weaknesses": "*Strength\n1. The idea of distiallting knowledge from multiple datasets with weights is interesting and reasonable. The authors conduct a lot of experiments to validate the corresponding claims.\n2. The paper is overall clear and well-organized in illustrating the methods, motivation, and experimental results.\n\n*Weakness\n1. The task relevance metric is very important for knowledge distillation, but the authors don't discuss the influence of the different choices of the task relevance metric.",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of this paper is interesting and the paper is well-written. But there is not too much technical contribution presented in this paper. The task relevance metric is important for the proposed method, but the authors didn't discuss this issue.\n",
            "summary_of_the_review": "I think the method and experiments are not strong enough to support the motivation of this paper. Importance issues are forgotten to be discussed. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2992/Reviewer_Tp8o"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2992/Reviewer_Tp8o"
        ]
    },
    {
        "id": "m5LsJ5UACuV",
        "original": null,
        "number": 3,
        "cdate": 1666746378326,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666746378326,
        "tmdate": 1666746378326,
        "tddate": null,
        "forum": "yzHn1QejdT4",
        "replyto": "yzHn1QejdT4",
        "invitation": "ICLR.cc/2023/Conference/Paper2992/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a method to train efficient models with limited labeled data. By using task similarity metrics, they conduct weighted knowledge distillation with pretrained models from different sources. It achieves better results than transferring from ImageNet pretrained model or FixMatch.",
            "strength_and_weaknesses": "Strength:\n- Learning with limited labeled data is meaningful to be solved.\n- The experiments and ablation studies are well-designed.\n\nWeakness:\n- In the main table it's good to show how multi-source distillation works for all architectures.\n- The empirical results are not convincing enough comparing to the single-source distillation with task similarity. Seems it only provides marginal improvement with multi-source but adds way more computation overhead.\n- Extra hyperparameters are required to be tuned (especially p).\n- I don't quite get why this method especially targets efficient models and doesn't work well for large models like resnet50. More analyzes will be very helpful.",
            "clarity,_quality,_novelty_and_reproducibility": "The method overall is novel but the technical contribution and empirical results are not significant enough.",
            "summary_of_the_review": "In conclusion, this work shows task similarity is useful metrics when it comes to distillation with teachers from multiple sources. I think more evidences are needed to prove multi-source distillation is indeed better than single-source distillation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2992/Reviewer_QfTL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2992/Reviewer_QfTL"
        ]
    }
]