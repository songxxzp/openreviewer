[
    {
        "id": "AoAcBpPNRN",
        "original": null,
        "number": 1,
        "cdate": 1666644629284,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644629284,
        "tmdate": 1666644629284,
        "tddate": null,
        "forum": "sE-9hkZL5wV",
        "replyto": "sE-9hkZL5wV",
        "invitation": "ICLR.cc/2023/Conference/Paper5246/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose the problem of progressive distillation: approximating a large model with an ensemble of smaller models. They seek a solution that \\emph{decomposes} the large model, such that the quality of the approximation improves monotonically with the size of the ensemble. They formulate progressive distillation as a two player zero-sum game and present B-DISTIL, an algorithm for solving this game. One notable feature of their algorithm is that it allows models to reuse intermediate representations from other models in the ensemble. They evaluate their algorithm on image classification (ResNets on CIFAR10 and DenseNets on SVHN), speech classification (LSTM on Google-13), and sensor data (GRU on DSA-19). They find that their B-DISTIL-generated ensembles allow favorable tradeoffs between computation time and accuracy.",
            "strength_and_weaknesses": "**Strengths**\n\nGood motivation\n\nClever method\u2014reusing intermediate representations within an ensemble is an elegant idea.\n\n**Weaknesses**\n\nExperiments are not very convincing; baselines and benchmarks are insufficient.\n\nMinor clarity issues.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Empirical Rigor**\n\nIn general, the experiments feel like a bit of an afterthought.\n\nThe experiments are lacking baselines. What are the current best methods to reduce latency at the cost of reduced accuracy? One simple baseline that comes to mind is training a smaller model from scratch. For a given efficiency ensemble latency, find a model architecture that has similar latency and compare accuracy. It\u2019s possible you do this in Figure 5, but I did not find that figure particularly easy to understand.\n\nCIFAR10 and SVHN are not particularly convincing benchmarks. I would encourage the authors to attempt to scale their experiments to at least CIFAR100 or Tiny-ImageNet.\n\n**Clarity**\n\nYou mention that student models have \u201csignificantly fewer parameters and resource requirements\u201d. This is very vague and needs to be quantified.\n\nX- and Y-axes in Figure 5 need labels.\n",
            "summary_of_the_review": "Clever idea, needs more rigorous empirical evaluation. Could be publishable with more thorough experimentation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5246/Reviewer_GAfE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5246/Reviewer_GAfE"
        ]
    },
    {
        "id": "tAkbOlmVPl",
        "original": null,
        "number": 2,
        "cdate": 1666792681418,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666792681418,
        "tmdate": 1669926969036,
        "tddate": null,
        "forum": "sE-9hkZL5wV",
        "replyto": "sE-9hkZL5wV",
        "invitation": "ICLR.cc/2023/Conference/Paper5246/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a method (B-DISTILL) to distill an (expensive) trained model into an ensemble of weak learners. These learners are meant to provide efficient inference, with progressively better results as the ensemble size increases: this allows for a straightforward way to trade off accuracy and compute at inference time, in cases where inference may be interrupted abruptly, or when variable levels of accuracy are expected.\n\nThe distilled ensemble is obtained by solving a two player zero-sum game, with convergence and generalization bounds provided under weak assumptions. In order to the $n+1$-th model to refine the prediction of the previous $n$ models, the authors also allow each successive model to _reuse_ the feature transformation of the previous ensemble member.\n\nExperimentally, authors verify their method on simulated, vision, speech, and sensor-data classification tasks, and across a variety of architectures.",
            "strength_and_weaknesses": "# Strengths\n- The paper is very well written, and the exposition is overall clear (aside from issues with baselines and $K$ matrices mentioned below)\n- The method has a clear application and is of similarly clear value to the community (anytime and efficient inference).\n-  The authors provide convergence guarantees ($\\mathcal O(\\sqrt{\\ln 2N / T})$ for $T$ the ensemble size and $N$ the dataset size)\n\n# Weaknesses\n- The description of the experimental baselines should be much clearer. As it stands, I found it difficult to be certain whether the authors consider Fixed-model to be an ensemble (which appears to be the case), and how successive ensemble members differ from each other.\n- It does not appear that the authors included early exit baselines; it would strengthen the paper significantly to compare to work such as (Dennis et al., 2018), despite the fact that such work is not distillation based.\n- The Figures (and Figure 3 in particular) are difficult to view (due to size, even on a screen and zooming in), and to understand in general. Am I correct in understanding that round $T$ reports the FLOP percentage for an ensemble of size $T$ (assuming each round of FIND-WL succeeds)? For the FLOP analysis, can you confirm that distinction between fixed-model and B-Distill is inconsequential?\n\n# Questions to the authors\n- Could you clarify why you need to split the matrices $K$ into $K^+$ and $K^-$?\n- I'm confused about why $p$ in (3) has is of shape $N \\times L$. I understand that $N$ is for the distribution over samples of the dataset; is the $L$ dimension to reweight the loss for different logit dimensions on each sample?\n- In Theorem 2, is $\\mu$ a distribution (and the inequality stochastic dominance), or a scalar?\n- Experimentally, how often did FIND-WL fail?\n\n# Nitpicks\n- Please add a $y$-axis to Figure 5. Are you reporting accuracy?\n- Consider moving the legend above the sub-figures when possible, to improve readability. \n- Is there an extraneous index $j$ in (3)?",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: Overall, the paper is clear, although the current discussion surrounding the probability matrices $K$ and the exact setup for the baselines is insufficient. Should these issues be addressed, there would be no issues with clarity.\n\n**Quality**: The results seems of good quality (assuming my understanding of the baselines is correct).\n\n**Novelty**: The work is fairly novel, although its experimental results should be placed more clearly within the context of early exiting/anytime inference.\n\n**Reproducibility**: The paper appears mostly reproducible; however, the authors should include the detailed architectures used as base learners in all experiments.\n\n",
            "summary_of_the_review": "This paper addresses an interesting problem (distilling a model into an ensemble of weak learners to control inference cost and accuracy trade-offs). Taking a game theory approach, authors introduce an algorithm with convergence guarantees, and show experimentally that their approach successfully trades off computational costs with model accuracy. \n\nHowever, I found it quite difficult to understand the detail of the experimental results, and of the baselines in particular. This work would be very much strengthened by a clearer experimental section, and the inclusion of baselines from the multiple-instance learning / anytime inference literature.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5246/Reviewer_9Tcs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5246/Reviewer_9Tcs"
        ]
    },
    {
        "id": "tI2lT7TiiJ",
        "original": null,
        "number": 3,
        "cdate": 1666940124672,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666940124672,
        "tmdate": 1666940124672,
        "tddate": null,
        "forum": "sE-9hkZL5wV",
        "replyto": "sE-9hkZL5wV",
        "invitation": "ICLR.cc/2023/Conference/Paper5246/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of progressive distillation, which is defined as approximating a large model with an ensemble of multiple small models. The paper poses this problem as a two-player zero-sum game where a \u201cdistribution player\u201d poses a distribution over example assignments to learners to maximize loss, and the \u201chypothesis player\u201d chooses hypotheses for each learner to minimize loss.",
            "strength_and_weaknesses": "The problem setting is interesting and well-motivated. Standard distillation (one big model to one small model) is likely to result in underfitting, whereas considering an ensemble of small models increases the capacity while ideal latency can be kept low through parallelization.\n\nThe intermediate layer connection idea (section 3.1.2) is interesting - instead of just considering a bigger model class, you can start from embeddings from previous generation nets; this is probably better for when you want to do early-exit or anytime inference with limited compute.\n\nIn experiments, the paper reports benefits in six different datasets, each with different network architectures. However, I think the evaluation would be more convincing if it considered more baselines such as a basic boosting algorithm.\n\nMinor comment: the paper says \u201cSVNH dataset\u201d repeatedly; is this a typo for SVHN (Street View House Numbers)?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing wasn't perfect, but clear enough to understand the main points. I think section 3 in particular was structured in a way that was hard to parse at first. To my knowledge, the problem setting and approach is novel. The paper and appendix do not provide enough information to reproduce the experiments, such as chosen hyperparameters or exactly how the smaller capacity models were constructed.",
            "summary_of_the_review": "I think the problem setting and method is interesting, but the experimental evaluation is somewhat weak.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5246/Reviewer_kWeE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5246/Reviewer_kWeE"
        ]
    }
]