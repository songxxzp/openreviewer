[
    {
        "id": "gf1tLl9maO",
        "original": null,
        "number": 1,
        "cdate": 1666660665599,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660665599,
        "tmdate": 1666660665599,
        "tddate": null,
        "forum": "IDJx97BC38",
        "replyto": "IDJx97BC38",
        "invitation": "ICLR.cc/2023/Conference/Paper438/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a new benchmark for situated embodied scene understanding. Mechanical Turkers are given tasks of diverse scenes, which include writing text descriptions of situations and human activities, writing questions given these tasks and their text descriptions, and finally, answering these questions. The dataset collected spans over 650 rooms and 6 types of questions. Experiments with 3 different model variants and qualitative analysis show there exist a large gap between embodied AI models and humans.",
            "strength_and_weaknesses": "Strengths:\n- Table 1 presents a clear overview of related work and the contributions of the proposed SQA3D dataset.\n- Experiments with 3 model variations, 3D, Video-Image and text-based.\n\nWeaknesses:\n- Evaluating GPT-3 on 10% of testing data make results uncomparable. Would be better to replace this with an open-source model, e.g., BLOOM.\n- Assuming results reported in Table 3 are top-1 accuracy, the ablation on the important of situation understanding seems to showcase that there is a marginal decrease in most question types when the situation description is removed from the input.\n- Please further explain: \"No further metric is not included as we find it sufficient enough to measure the differences between baseline models with exact match\".\n\n\nTo my understanding, the dataset is built on a standalone web interface rather than on top of an exciting simulator such as AI2-Thor, which could facilitate the evaluation of embodiedQA agents on completing, for example, object navigation tasks. Another comment is on the lack of limitations and societal impact sections. Perhaps articulating similar limitations would make the work more complete, e.g., on the expected usage of the dataset.",
            "clarity,_quality,_novelty_and_reproducibility": "While the reviewer understands the amount of work and detail that has to be put in for a dataset construction of such scale, section 2 which describes the dataset format, curation and cleaning are quite dense, especially \"Multi-staged collection\". This makes it difficult for the reader to extract the most important information about the dataset construction.\n\nSome figures could also be improved, e.g., in figures 12-13, the font size is too small. The boxes for the 3D scenes in Figure 7 are too small as well, and it takes quite a lot of zoom-in to understand what each scene contains.\n",
            "summary_of_the_review": "While there exist several benchmark datasets on embodied QA, to the best of my knowledge, this seems to be one of the largest ones and most diverse wrt questions. Concerns involve whether the standalone nature of such datasets and the evaluation with exact match / top-1 accuracy metrics is sufficient for embodied agent scene understanding.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper438/Reviewer_ZAer"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper438/Reviewer_ZAer"
        ]
    },
    {
        "id": "T0d81qACYp6",
        "original": null,
        "number": 2,
        "cdate": 1666689649072,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689649072,
        "tmdate": 1670168346563,
        "tddate": null,
        "forum": "IDJx97BC38",
        "replyto": "IDJx97BC38",
        "invitation": "ICLR.cc/2023/Conference/Paper438/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This article proposes a task to benchmark the understanding of specific agents, scenes, situational questions, and answers in 3D scenes (SQA3D). Given the 3D scene background, then answering a question that requires a lot of situational reasoning from this perspective. Based on scenes from ScanNet, it provides a data set centered on 6.8k unique scenes, 20.4k descriptions, and 33.4k different reasoning questions for these scenes for scenario answers in 3D scenes. At the same time, the author studies the most advanced multi-mode reasoning models, and the results show that these two models still lag behind human performance to a large extent. It shows the key\nrole of correct 3D representation and the need for better situational understanding in specific scene understanding.",
            "strength_and_weaknesses": "Strength\uff1a  \nDifferent from other embedded scene understanding tasks, SQA3D tasks propose to understand and complete tasks in the first person to answer questions, which is more practical and has a broader range of practical task needs. And it has knowledge-intensive questions and a larger scale of the collection. The paper is well-organized, which is easy to read and understand. The current experiment of baseline performance on SQA3D tasks is relatively complete. \n\nWeaknesses\uff1a  \nIt is good to introduce how to control the potential biases in the dataset.  \nThe paper should also refer to several recent Situated Reasoning benchmarks or  Video QA benchmarks as they proposed situated reasoning with QA tasks or similar scenarios. For example, STAR: A benchmark for situated reasoning in real-world videos; Agqa: A benchmark for compositional spatio-temporal reasoning, etc.   \nThe experiment in Table 1 shows that the blind model reached 43.65 performance. So the hints or potential connections in the language part are strong which probably will be easy to correlate to answers.   \nProviding more module-level experiments or ablation studies will be good.   ",
            "clarity,_quality,_novelty_and_reproducibility": "Quality\uff1a \nThis article is well-organized. It has completely raised the problems and analysis of 3D scene answering questions, pointed out the key to 3D scene understanding, proposed a new benchmark to reflect scene understanding, and introduced and analyzed the SQA3D dataset in detail, as well as the process of data processing.  \n\nClarity:  \nThe clarity of this article is good, the layout of the article is appropriate, and the figures and tables are useful to describe the data of the proposed dataset, the analysis of the 3D scene understanding model, and the analysis of SQA3D qualitative results enable people to better understand, and the description of the formula in the article is also relatively clear.  \n\nOriginality:  \nThe paper reflects relatively original contributions. It has proposed a 3D QA benchmark on the basis of ScanNet and proposed a different combination of QA tasks, which has a certain degree of novelty.",
            "summary_of_the_review": "Please refer to the above comments for the pros and cons.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper438/Reviewer_cywf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper438/Reviewer_cywf"
        ]
    },
    {
        "id": "YX1twqmESd",
        "original": null,
        "number": 3,
        "cdate": 1666923522206,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666923522206,
        "tmdate": 1670166721665,
        "tddate": null,
        "forum": "IDJx97BC38",
        "replyto": "IDJx97BC38",
        "invitation": "ICLR.cc/2023/Conference/Paper438/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a new dataset and benchmark for 3D scene question answering. It is based on ScanNet and it consists of a large number of situations, with descriptions and reasoning questions. These questions test the agent in various ways including relation comprehension, commonsense understanding, and navigation. The authors tested several existing QA methods on this dataset and achieved a very low score in comparison to what humans achieve leaving large room for improvement.",
            "strength_and_weaknesses": "Strengths\n\n- The paper goes into great detail in describing how the dataset SQA3D has been acquired, I appreciate the quality check made in generating the situations.\n- The experiments show a thorough analysis of how the methods perform for different question types. \n- SQA3D seems to be a useful challenging benchmark where current methods for question answering are far off from human performance opening doors for a lot of advancement opportunities\n- I enjoyed reading the detector section as different methods were thoroughly compared in terms of strengths and weaknesses and useful practices like having balanced data were provided.\n\nWeaknesses\n\n-More descriptions need to be given how SQA3D\u2019s challenges differ from existing related benchmarks (Azuma et al., 2022; Ye et al., 2021; Yan et al., 2021). One question to ask, does having a model that performs really well on these existing benchmarks enough to deploy the model, or do we need to test it on SQA3D as well?\n\n- It would be useful to have more detailed descriptions of the dataset challenges and why the scores by the AI algorithms are so slow. One way to reconcile that is to pinpoint several challenges and implement variations of existing methods that can at least slightly address those challenges. This strategy is similar to how some of the challenges of imagenet were addressed using a deep network in its original paper\n\n- No multiple runs of the same experiments in Table 3, it would help to see what  the variance is like if different random seeds were used to run the experiments in order to identify if the differences between the results are significant.\n\n- No code was released to verify and reproduce the results and interact with the dataset to get a deep understanding of how it is structured. Could you add a script that allows us to run one of the methods onto one of the situations? Since this is a benchmark there should be an easy-to-use codebase that makes it smooth to run the baselines and extend them for new algorithms.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the Strengths and Weaknesses.",
            "summary_of_the_review": "Please see the Strengths and Weaknesses. Overall this work provides a useful benchmark to encourage research in 3D question answering. However, more description is needed for how this benchmark compares to existing ones and it would be useful to have a variation of existing Q&A methods that can at least slightly address this benchmark's specific challenges.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper438/Reviewer_4eyF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper438/Reviewer_4eyF"
        ]
    },
    {
        "id": "KAVksrZ0aY",
        "original": null,
        "number": 4,
        "cdate": 1666963036498,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666963036498,
        "tmdate": 1666963036498,
        "tddate": null,
        "forum": "IDJx97BC38",
        "replyto": "IDJx97BC38",
        "invitation": "ICLR.cc/2023/Conference/Paper438/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel task, called SQA3D, to benchmark and help improve 3D scene understanding of embodied agents. The embodied agent is provided a visual scene context and a textual description of its own location and state in the scene. The task is to answer an input question which requires both an understanding of the scene and the agent\u2019s location in it. The questions encompass spatial relations, common sense understanding, native and multi-hop reasoning. The authors build the task using scenes from the ScanNet dataset and crowdsourcing with human curation is used to prepare the ground truth. Human benchmark (using amateurs) is prepared on the task yielding an overall score of ~90%. Several state of the art approaches are evaluated with the best reaching 47.2% accuracy. This large gap in performance is expected to facilitate further research on embodied scene understanding. ",
            "strength_and_weaknesses": "### Strengths\n\n- The paper addresses creates a novel shared task (challenge) for the emerging field of embodied AI, in particular embodied 3D scene understanding. \n- The task is similar to recent work in 3D language grounding and embodies QA but consists of a larger diversity of context-dependent, knowledge-intensive questions on a much larger dataset. \n- Human benchmarking is performed. \n- Several recent multimodal reasoning approaches are evaluated revealing a large performance gap of ~43%. The choices of these seem reasonable and provide a good coverage of different approaches.\n- The large, labeled SQA3D dataset (650 scenes, 20.4k descriptions of ~6.8k situations and 33.4k questions) and the identified performance gap should spur further research on this important topic.\n\n### Weaknesses\n\n- __(W.1)__ I suppose the authors are planning on releasing the code, data and metadata related to the task:  the data cuts used for training their models, the trained models employed to get the results in the paper, the code which changed the SOTA models etc. I might have missed it if the authors explicitly state this. Kindly confirm. \n\n- __(W.2)__ There are some surprising, quantitative results which need further probing and a discussion. \n\n- __(W.2.a) 3D scans__: The VSQ setting should be better than the max of SQ and VQ. This goes to the heart of this task. The improvements seem low on {Is, How, Can} questions. What seems to be going on? It will be good to have human benchmarks in these settings. \n\n- __(W.2.b)  BEV, Ego. videos__: While I agree with the general observations in the paper, low performance on {Is, How, Can, Others} with respect to the blind test is disconcerting. This means that those models are not \u2018working\u2019 at all.\n\n - __(W.2.c) Zero Shot__: Clearly captioning is the bottleneck but for a \u2018reasonable\u2019 system, the performance shouldn\u2019t degrade below the \u2018Blind test\u2019. The low performance on {Is, How, Can, Others} mirrors the above. Can the authors clarify?\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The ideas are presented clearly and in a structured manner.\n\n**Quality**: The quality of the paper is mostly good, with some minor issues pointed out below which are easily fixable.\n- some grammatical mistakes and typos.\n- Appropriate use of capitalization in the \u2018References\u2019 section.\n\n**Novelty**: \n- Low on technical novelty, high on practical novelty and impact as the paper introduces a new task and SOTA baselines on the task in an important area.\n\n**Reproducibility**:  \n- As noted above, I don\u2019t see authors explicitly stating the release of the code, data and metadata related to the task. If they don\u2019t, then the community may have some difficulty reproducing the results (based on my past experience, authors don\u2019t respond if others\u2019 efforts don\u2019t get the same results and my rating below on reproducibility reflects this.)\n",
            "summary_of_the_review": "A good contribution introducing a novel shared task in the general area of embodied AI along with a human study, reasonable baselines based on SOTA identifying the large gap between the two. The paper quality is acceptable. I have some concerns regarding reproducibility and the analysis of the baselines could\u2019ve been better. My lower rating reflects these concerns.\n\nSubject to the above, this task is expected to have a positive impact on the research in the area.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper438/Reviewer_neRZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper438/Reviewer_neRZ"
        ]
    }
]