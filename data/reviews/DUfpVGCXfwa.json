[
    {
        "id": "WAp3i_Urgk",
        "original": null,
        "number": 1,
        "cdate": 1666343119868,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666343119868,
        "tmdate": 1666343119868,
        "tddate": null,
        "forum": "DUfpVGCXfwa",
        "replyto": "DUfpVGCXfwa",
        "invitation": "ICLR.cc/2023/Conference/Paper4909/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new routing strategy, Routing Entropy Minimization (REM), for capsule networks. The core idea of REM is to minimize the entropy of capsule parse trees by pruning so that the activated connections between capsules are stronger and fewer, which will improve the interpretability of the capsule networks. Experiments on Fashion-MNIST, SVHN, smallNORB, and CIFAR10 validate that the proposed method achieves comparable performance with a significant lower number of parse trees.",
            "strength_and_weaknesses": "Strength:\n1) The paper gives detailed instructions on how to extract the parse tree of capsule networks (capsule connections) and how to penalize the sparsity of the connections.\n2) The proposed method is easy to be plugged into the current capsule networks, which effectively improves the sparsity of the connections while keeping comparable performance.\n\nWeakness:\n1) Lack of novelty. The main contribution of the paper is to prune the capsule connections by penalizing the activations and votes of capsules. While the method for pruning is borrowed from other papers (LOBSTER, Tartaglione et al, 2022).\n2) In table 1, the paper combines REM with the current methods and evaluates them on different datasets to show the generalization ability, which is not convincing. To validate the generalization, there should be cross-data evaluation, where the new capsule network should be trained on one dataset and tested on another dataset. As I can see, the results in Table 3 show that the performance is reduced when applying the capsule network to novel viewpoints.\n4) The interpretability of the sparse capsules need to be further illustrated, i.e., show the sparse part-whole relations learned by the satisfied capsule networks.\n\nSome details:\n(1) Following the formatting instructions of ICLR2023, the abstract must be limited to one paragraph.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to understand.",
            "summary_of_the_review": "The paper gives a detailed analysis of how to sparsify the connections of capsules. However, the effectiveness and the interpretability of the sparsified networks have not been well discussed and explored. Thus, my recommendation is to reject it in its current form.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4909/Reviewer_uZJ5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4909/Reviewer_uZJ5"
        ]
    },
    {
        "id": "oIiCnthBcj-",
        "original": null,
        "number": 2,
        "cdate": 1666588069640,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588069640,
        "tmdate": 1666588069640,
        "tddate": null,
        "forum": "DUfpVGCXfwa",
        "replyto": "DUfpVGCXfwa",
        "invitation": "ICLR.cc/2023/Conference/Paper4909/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "RSM is proposed to minimize the entropy of the parse tree-like structure in Capsule Networks. Pruning is used to reduce the entropy of the model parameters distribution. These methods generate a significantly lower number of parse trees, with no performance loss. Experimental results show the effectiveness of the proposed method.\n",
            "strength_and_weaknesses": "Strengths:\n1 The motivation of this paper is clearly described. \n2 Empirical results indicate that the proposed method achieves good performance.\n3 The paper is well organized.\n\nWeaknesses:\n1 Pruning and Quantization are standard methods in compression and acceleration literature. Intuitively, the paper just applies these two techniques to the capsule networks. What are the contributions of these techniques?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper appears to be technically sound, and the experiments are relatively sufficient, which supports the initial claims.\n",
            "summary_of_the_review": " I am not familiar with this topic. I score this paper marginally above the acceptance threshold, based on the solid contribution by minimizing the entropy of the connections and comprehensive experiments. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4909/Reviewer_1ErG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4909/Reviewer_1ErG"
        ]
    },
    {
        "id": "bwiy9whajxo",
        "original": null,
        "number": 3,
        "cdate": 1666603879971,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603879971,
        "tmdate": 1666603879971,
        "tddate": null,
        "forum": "DUfpVGCXfwa",
        "replyto": "DUfpVGCXfwa",
        "invitation": "ICLR.cc/2023/Conference/Paper4909/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "I would have hoped to be clearer on how all the components proposed fit together, but in a nutshell, the authors propose a way to prune CapsNets without compromising performance, and they do so by pushing the model parameters to low entropy spaces. They achieved competitive results in some datasets they compared with, demonstrating that one can reduce entropy and cardinality in the parse trees extracted.",
            "strength_and_weaknesses": "Strengths: a) There is some new methodology described to extract parse trees and use pruning and quantization in capsule networks b) Results are promising and it seems that the method does work competitively with respect to other solutions presented in the literature. However, I do think authors are missing two important works in the area, i.e. subspace capsules (Edraki et al.) and Capsule Routing via Variational Bayes (De Sousa Ribeiro et al.)\n\nWeaknesses: a) Table 3 and other tables (wherever relevant) needs to include other methods as well (e.g. ones I mentioned above and also ones already cited). Authors do not need to reimplement anything as all these (or most of them) methods do have results in such datasets.\n\nThe abstract starts: \"Capsule Networks ambition\" this needs to be paraphrased, as it is our ambition to make them interpretable not their ambition - or \"for Capsule Networks to be interpretable we need to.....\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well presented and includes useful experiments - I think experiments need to be contextualised with respect to other work, irrespective of whether the method achieves SOTA or not. Reproducibility is not the strongest part of this paper (no code is included), but they provide enough information to reproduce some parts of this work.\n",
            "summary_of_the_review": "This is a good submission which helps to provide a new dimension to capsule neural networks. There is a good experimental setup presented but I think it requires further improvements to demonstrate correspondence with other methods presented in the literature - this is the major limitation of this paper, as it is hard to put this paper in context (e.g. to compare with other methods which include number of parameters etc.)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4909/Reviewer_Ni7P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4909/Reviewer_Ni7P"
        ]
    },
    {
        "id": "CQ4J84YYla",
        "original": null,
        "number": 4,
        "cdate": 1666661015805,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661015805,
        "tmdate": 1666661015805,
        "tddate": null,
        "forum": "DUfpVGCXfwa",
        "replyto": "DUfpVGCXfwa",
        "invitation": "ICLR.cc/2023/Conference/Paper4909/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method (REM) to reduce the connection in the dynamic routing step of the capsule network, thereby reducing the entropy of routing. By doing so, the paper shows improved interpretability of the capsule networks. Specifically, the work proposes to simply prune connection via quantization of coupling coefficients in routing and thus get the parse trees. The paper validates its method and analyzes the routing in multiple datasets. ",
            "strength_and_weaknesses": "## Strength\n- This work studies the interpretability of dynamic routing algorithms, which is very interesting to the community. \n- The paper reports lots of experimental analysis and reveals some interesting results  -- e.g., entropy changes curves along the training. \n\n## Weaknesses\nThe paper has several weaknesses which I'll detail below:\n\nWriting:\n- Motivation for the method is not strong. Fewer parameters are better for computation overhead but it doesn't mean that we can better interpret the capsule network. However, this paper motivates their story from the viewpoint of interpretability, which I personally don't agree. I didn't see how the routing of lower entropy can help understand the behavior of the network. I would recommend having better motivation.\n- Low clarity in the method description. I sometimes have difficulties understanding the idea/motivation behind the design choices. For example, in Eq. 16, why is the method trained with a special optimizer? Any reason behind this? I would recommend a better justification for this choice. And I'm also not very sure how you define entropy. \n\n\nMethod: \n- The paper claims the proposed method minimizes the entropy of the routing algorithm. It seems to be an over-claim from my perspective. Because I didn't see any regularizers/optimization loop inside. I don't quite get how entropy is minimized for dynamic routing. Is that just through a connection pruning step?\n\nExperimental results:\n- I didn't find any experiments that support the proposed method. By that I mean the results show the benefit/advantages of the proposed algorithm compared with other methods in terms of efficiency. The only comparison seems to be an ablation study of the proposed method. It's not solid. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality is good. And the idea to reduce the entropy of routing is novel. But the paper is of low clarity in terms of motivation and method description, thus leading to reproducibility. ",
            "summary_of_the_review": "The paper proposes an interesting idea for training the capsule network while pruning the connection of routing. The experimental results show some improvement. But it's very marginal. And also I'm not sure about the motivation behind it. In other words, I'm not very clear about its contribution. Furthermore, I feel the experimental validation is not solid, as I detailed above. I thus tend to reject this paper. But I'm very happy if the author can provide feedback just in case I misunderstand anything. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4909/Reviewer_LZej"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4909/Reviewer_LZej"
        ]
    }
]