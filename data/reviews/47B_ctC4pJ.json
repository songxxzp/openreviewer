[
    {
        "id": "NndzBw9Vxl",
        "original": null,
        "number": 1,
        "cdate": 1666161306939,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666161306939,
        "tmdate": 1666161306939,
        "tddate": null,
        "forum": "47B_ctC4pJ",
        "replyto": "47B_ctC4pJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4130/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to improve the GlobalDirection method of StyleCLIP to further consider the inter-channel correlations when finding the editing vectors in StyleSpace. A novel dictionary learning loss is proposed to learn such correlations and experiment validates the effectiveness of the method compared to the StyleCLIP. ",
            "strength_and_weaknesses": "**Strengths:**\n+ The idea to consider the inter-channel correlations during latent editing is intuitive and reasonable. The paper is well motivated. \n+ In general, the visual and editing quality of the proposed method is significantly better than the baseline method. \n\n**Weaknesses:**\n+ The idea of the second loss term in Eq. (4) is hard to understand to me. What is the intuitive motivation or physical meaning behind this loss term? Is there any visual results with and without this loss term to help readers better understand its effect?\n+ In ablation study, the effect of second loss term is better to be validated by setting lambda=0. Current, only lambda=0.1 and 0.01 is compared quantitatively. Visual results are suggested to be given for better comparison.\n+ The limitation is suggested to be discussed. For example, why in Fig. 11, the text is smiling eyes, but the eyes have nearly no changes while the mouth changes a lot.\n+ The conclusion is suggested to be given.\n\n**Some small issues:**\n+ The text in Fig. 6 is too small.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This motivation of this paper is clear and the idea to consider the inter-channel correlations is novel to me. About the reproducibility, the authors promise to release the code. ",
            "summary_of_the_review": "This paper proposes to improve the GlobalDirection method of StyleCLIP to further consider the inter-channel correlations when finding the editing vector. I can generally understand the main idea of dictionary learning but cannot fully nail down the meaning and function of the second loss term. From the experiment results, it seems that the proposed dictionary learning is effective to improve the editing quality. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4130/Reviewer_jy1p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4130/Reviewer_jy1p"
        ]
    },
    {
        "id": "_3NNZ9tEAE",
        "original": null,
        "number": 2,
        "cdate": 1666525701560,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666525701560,
        "tmdate": 1670391047048,
        "tddate": null,
        "forum": "47B_ctC4pJ",
        "replyto": "47B_ctC4pJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4130/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a dictionary learning framework to improve StyleCLIP, one state-of-the-art image manipulation method via text guidance. Compared to StyleCLIP which operates on one single channel independently, this method tries to embed the multiple style channels into CLIP space jointly. Some experiments are performed to support the advantages of the proposed method.",
            "strength_and_weaknesses": "Strengths:\n\n+ The authors design experiments to support their motivations by showing that the GlobalDirection can not provide consistent embeddings. I like these experiments, which makes the statement clear.\n+ The shown visual results look pleasing and demonstrate some improvement over GlobalDirection.\n+ The writing is clear and easy to follow.\n\nWeakness:\n\n+ The proposed approach is an incremental improvement upon StyleCLIP and GlobalDirection methods. Specifically, this submission can be regarded as a straightforward extension of GlobalDirection by extending the single-channel editing direction to multi-channel editing direction. Also, the idea of discovering the editing direction and training the dictionary also borrows the results of unsupervised latent direction discovery methods such as SeFA and GANSpace. This makes me feel like this submission might suffer from limited novelty.\n\n+ As mentioned in Section E, the proposed method uses the output of previous methods (e.g. SeFA and GANSpace). So the authors need an ablation study (only use multi-channel learning strategy) along with corresponding discussions to rule out the influence of this point and then highlight the influence of multi-channel modeling. And the fact that this method depending on SeFA and GANSpace's outputs, also might make it suffer from similar limitations to SeFA and GANSpace.\n+ How to evaluate the image synthesis results is still an open problem. Thus this submission mainly depends on the qualitative comparisons, which might be very subjective. Although this submission presents some basic quantitative comparisons, I suggest to use some widely used quantitative metrics, like Inception score or more advanced alternatives, to evaluate the generation results instead of MSE and Cosine similarity. At least, the user studies are needed for evaluations over randomly generated results.\n+ In this submission, the authors only compare with the original StyleCLIP but there are other variants. So more empirical comparisons need to be conducted with more SOTA methods.\n+ While the proposed method shows better results over chosen baselines, I am curious about the comparisons in terms of computation overhead and other additional limitations introduced by multi-channel modeling.",
            "clarity,_quality,_novelty_and_reproducibility": "In general, this submission is well-written and easy to follow. It has good results compared to chosen baselines. However, as for me, it suffers from limited technical novelty and still lacks some experimental results. Since it would be tricky to train networks from scratch, codes are welcome for reproducibility.",
            "summary_of_the_review": "This submission details how it extends from GlobalDirection to achieve multiple channel modeling. However, this motivation seems to be somewhat trivial and just a straightforward revision, even given its superior results. The authors can also improve their paper by adding more experimental results to support their claims. But I would not like to object acceptance if other reviewers prefer to do so.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4130/Reviewer_bYKo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4130/Reviewer_bYKo"
        ]
    },
    {
        "id": "mPq8Qgk3ok",
        "original": null,
        "number": 3,
        "cdate": 1666623864385,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623864385,
        "tmdate": 1666623864385,
        "tddate": null,
        "forum": "47B_ctC4pJ",
        "replyto": "47B_ctC4pJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4130/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new method for image manipulation with \u2018global direction\u2019, which is case-agnostic and applicable to all input images. The authors argue that the previous work StyleCLIP provides a dictionary to quickly find out the channel-wise manipulation while the collective and interactive relation among multiple channels is not considered. Thus the manipulation performance is limited. The authors propose a new manipulation method by considering the manipulation effect coming from the interaction with multiple other channels.\n",
            "strength_and_weaknesses": "Strength:\n1. Global direction editing is charming since it can be transferred to different new images without local optimization. An effective method around text-guided editing is a promising task.\n2. The analysis is interesting. Especially the experiment is impressive, i.e., single-channel editing provides no change while joint-channel editing can change the global shape. A corresponding method is derived based on the observation, which makes it more convincing. The results look good compared with the original StyleCLIP dictionary. The joint-channel editing may provide other unsupervised editing work with several insights (the collective and interactive relation among multiple channels should be considered).\n3. The authors conduct adequate experiments on sufficient datasets to prove the manipulation performance of the proposed method surpasses StyleCLIP.\n\nWeakness:\n1. One question, beyond text-guided editing, is it possible to use such a global direction to provide a robust editing method on the large pose or expression change? Existing methods have attempted to use 3DMM representation or other unsupervised editing directions to inject into 2D StyleGAN, but the results are not that robust.\n2. Some figures are pretty messy. For instance, there are too many texts of various fonts/sizes/colors in Fig.1 and Fig.4. Please consider unifying some of them.\n3. Some terms lacked explanation when first mentioned, such as \u25b3s and e(i) in Eq(2), easily confusing the reader.\n4. The process of StyleCLIP\u2019s obtaining a dictionary should be represented more clearly in case the reader is unfamiliar.",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality of the paper is pretty good, but some formulations could be made more clear (such as eq.2) and some figures could be further improved (such as fig.1 \u2013 currently there is too much text thus it looks messy). ",
            "summary_of_the_review": "The reviewer recommends a weak accept of the paper and is pretty confident.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4130/Reviewer_RdgP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4130/Reviewer_RdgP"
        ]
    },
    {
        "id": "lUnpPC_LW2H",
        "original": null,
        "number": 4,
        "cdate": 1666660257546,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660257546,
        "tmdate": 1670391442899,
        "tddate": null,
        "forum": "47B_ctC4pJ",
        "replyto": "47B_ctC4pJ",
        "invitation": "ICLR.cc/2023/Conference/Paper4130/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper works on leveraging the StyleGAN model for text-guided image manipulation. Starting from latent directions discovered by the unsupervised method, the authors propose an approach, with a sparsify trick, to further disentangle the editing directions. Experiments conducted on the FFHQ, LSUN, AFHQ datasets show that the proposed method performs favorably against the state-of-the-art method.",
            "strength_and_weaknesses": "*Strength*\n1. The authors promise to release the source code in the future, which can stimulate further research in this field.\n2. From the quantitative and qualitative results, the proposed method seems to have favorable editing capability compared to the state-of-the-art methods.\n\n*Weakness*\n1. The paper is not well written. Particularly, Section 3 and 4 is hard to follow. The syntaxes in the figure and the text description are not alighted. For example, $\\Delta{\\mathcal{S}}$ is used in Figure 1, but in the text descriptions it seems to be $\\Delta{s}$. There are many confusing and unclear descriptions as well. For instance, in Section 3.2, what do the authors mean by \"a large portion of the instances from unsupervised directions\"? What do the \"instance\" and \"directions\" refer to? Moreover, what does \"represent new directions to the fullest capacity\" mean? The notation $\\alpha$ is also not properly defined. What is $alpha$? What is the dimensionality? For section 4, how do the authors use the sparsified $\\alpha^{(r)}$ in Equation 4?\n2. The experiments shown in Section Table 1 are also confusing. For Table 1, how is the direction term $\\alpha$ computed for the GlobalDirection method computed?\n3. As the goal of this paper is to edit images using text, what is the importance of recovering the unsupervised directions in this paper? \n",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity*: The paper is not well-written, and is really hard to follow.\n\n*Quality and Novelty*: It is hard to determine the quality and novelty of this work since the motivation and proposed framework is not clearly introduced in the paper.\n\n*Reproducibility*: Although the proposed method is not well described, the author promise to release the source code in the future.",
            "summary_of_the_review": "I suggest the authors to carefully revise the manuscript, especially Section 2 and 3. Although favorable quantitative and qualitative results are shown, the motivation, intuition, technique, and novelty behind this paper is not well-demonstrated.\n\n--- post rebuttal ---\nThe authors have revised the manuscript, and now Section 2 and 3 are more clear, so I'm not against to accept this work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4130/Reviewer_v5mm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4130/Reviewer_v5mm"
        ]
    }
]