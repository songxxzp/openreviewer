[
    {
        "id": "7E5ovuago-p",
        "original": null,
        "number": 1,
        "cdate": 1666039483790,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666039483790,
        "tmdate": 1666039483790,
        "tddate": null,
        "forum": "fGm87trHel_",
        "replyto": "fGm87trHel_",
        "invitation": "ICLR.cc/2023/Conference/Paper2193/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the calibration of differentially private learners based on stochastic gradient descent. The paper first observes the miscalibration  is due to the per-example gradient clipping. Then, the paper provides differentially private recalibration to reduce  calibration errors. The basic idea is to divide the training dataset into two parts: one part is used to train a classifier, while the other is used to train a recalibration function. Extensive experimental results are reported to show the effectiveness of the differentially recalibration method. ",
            "strength_and_weaknesses": "**strength**\n\nThe proposed method is simple and general. It provides a framework which includes several post-hoc recalibration techniques. There are many experimental results, which show the proposed method works well in practice.\n\n**weakness**\n\nThere is no theoretical analysis of the proposed method. There is no theoretical guarantee and therefore it is not clear how the method would behave in general.\n\nThe idea of extending recalibration to differentially private recalibration seems to be natural. It is not clear whether the extension is novel enough.",
            "clarity,_quality,_novelty_and_reproducibility": "The description of Algorithm 1 is very general. There is no detailed explanation of how DP-SGD is applied in both step 2 and step 3 of Algorithm 1. The authors should give more details.\n\nThe idea of calibration is not quite clear. It is not clear to me the underlying motivation on why the calibration is a big issue we need to consider in algorithm design. What is its connection to accuracy? There is not enough background on calibration.\n\nIn the abstract, the paper says their analysis identifies per-example gradient clipping as a major issue of miscalibration. However, I do not find such an analysis in the paper.\n\nBelow eq (1): $l_i$ should be $L_i$",
            "summary_of_the_review": "The paper presents a simple and general method for recalibration in a differentially private setting. The paper conducts many experiments to show the effectiveness of the method. The proposed algorithm is not presented in detail and there is no theoretical analysis.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2193/Reviewer_s6au"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2193/Reviewer_s6au"
        ]
    },
    {
        "id": "Cl2Qk34HSaX",
        "original": null,
        "number": 2,
        "cdate": 1666301643475,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666301643475,
        "tmdate": 1669745051557,
        "tddate": null,
        "forum": "fGm87trHel_",
        "replyto": "fGm87trHel_",
        "invitation": "ICLR.cc/2023/Conference/Paper2193/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the calibration of state-of-the-art classifiers trained using differentially private stochastic gradient descent (DP-SGD). It shows that there are miscalibration issues in these DP classifiers even when their accuracy matches that of their non-private counterparts. The likely cause of that is conjectured to be the repeated per-example gradient clipping in DP-SGD. The other contribution is addressing these miscalibrations using DP versions of temperature scaling (TS) and Platt scaling (PS), which are almost as good as their non-private counterparts. This work is mostly empirical in nature, and uses multiple datasets and models, with applications in text-processing and computer vision, and has experiments to evaluate both in-domain and out-of-domain calibrations.",
            "strength_and_weaknesses": "Strengths:\n1. The paper successfully depicts the issue of miscalibrations in DP classifiers (trained via DP-SGD). All the experiments indicate the DP counterparts having more miscalibration than their respective non-private counterparts.\n2. This paper also establishes its second goal of providing a DP method to lower the miscalibration errors significantly. These methods involve the use of DP-SGD again. The experimental results indicate very low miscalibration errors that are similar to those from the non-private methods.\n\nWeaknesses:\n1. I have some concerns about the writing quality. Please, see the section regarding clarity of the submission.\n2. I'm also concerned about the significance of the problem being solved here. I'm curious as to why this problem under DP has not been studied much before this. In other words, why is reducing miscalibrations that important a problem to solve, especially under DP?\n3. The algorithmic contributions don't seem to be that interesting, unfortunately -- just optimising a calibration function using DP-SGD. The techniques aren't novel, and simply involve blackbox uses of DP-SGD. Is the goal of this paper to bring the issue of miscalibrations in DP classifiers to people's attention?",
            "clarity,_quality,_novelty_and_reproducibility": "Writing Quality:\n1. I think the paper is a bit vague at different places, and the notations are not well-defined. In Section 3.3 while defining canonical calibration, what is $h_{\\theta}$? What is the probability over? Is $Y$ the true label or is it the prediction of $h$? Also, is Equation 2 like an analogue of $\\ell_{\\infty}$ error? In the same subsection, what is $\\hat{p}_i$?\n2. Might want to motivate in a paragraph why calibration is an important problem to study even if we have good accuracy. Understanding the motivation of this is important, and I don't think the authors do a great job at conveying that.\n3. The DP-PS and DP-TS are unexplained in this draft. I know you use DP-SGD here, but giving some kind of an analysis or more detailed descriptions of the two would make it much more meaningful.\n4. Might want to give a clear intuition of what calibration actually means or entails. Might make sense to explain the formal definitions a bit in words to show what it means to have bad calibration even if we have good accuracy.\n\nNovelty:\nThere is novelty in the sense that there are previous works that address this issue under DP in settings different from the one in this paper.",
            "summary_of_the_review": "I am not convinced about the overall quality of this paper. The contributions don't seem that significant to me, unfortunately, especially given the lack of motivation of the problem in this draft and the not-so-novel technical ideas. The writing quality wasn't that great either, but that doesn't matter much to me as far as the quality of the work is concerned -- my feedback there is just to help the authors improve their manuscript. I do acknowledge that the authors seem to have achieved the claims of this paper. All this said, I will be open to changes in my final score based on the authors' responses though.\n\nUpdate: Based on the responses, I have bumped the score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2193/Reviewer_zkmx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2193/Reviewer_zkmx"
        ]
    },
    {
        "id": "4OrxUVnsev",
        "original": null,
        "number": 3,
        "cdate": 1666443271136,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666443271136,
        "tmdate": 1666443271136,
        "tddate": null,
        "forum": "fGm87trHel_",
        "replyto": "fGm87trHel_",
        "invitation": "ICLR.cc/2023/Conference/Paper2193/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the trade-off between differential privacy and classifier calibration. In particular, the main observation is that classifiers trained with the standard DP-SGD algorithm can be highly miscalibrated. To this end, the authors propose a natural and intuitive solution to fix this issue using differentially private recalibration. First, the algorithm runs DP-SGD in order to obtain the standard classification model with a subset of the data and then runs DP-SGD again in order to find a post-hoc recalibration function using the remaining data. Finally, it outputs the composition of the two functions.\n",
            "strength_and_weaknesses": "The paper provides extensive experimental evidence about the impact of differential privacy to the calibration property of classifiers. I find that the provided empirical observations are interesting and may raise some nice theoretical questions too. More to that, the paper is nicely written. The only issue is that the paper does not have any theoretical contribution.\n\nA potential question is whether variants of DP-SGD or other training methods would also cause such miscalibration phenomena. Is gradient clipping the only obstacle towards private training that is well-calibrated?",
            "clarity,_quality,_novelty_and_reproducibility": "The problem and the experiments are clearly presented. The comparison with previous work is also discussed. ",
            "summary_of_the_review": "In short, even if there is no theoretical contribution, I believe that the paper is beyond the acceptance threshold, since its message can potentially raise some theoretical questions. The experimental observations are interesting.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2193/Reviewer_9fSP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2193/Reviewer_9fSP"
        ]
    },
    {
        "id": "_3YXQtK2D0m",
        "original": null,
        "number": 4,
        "cdate": 1666647848183,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647848183,
        "tmdate": 1666647848183,
        "tddate": null,
        "forum": "fGm87trHel_",
        "replyto": "fGm87trHel_",
        "invitation": "ICLR.cc/2023/Conference/Paper2193/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors  identify the per-example gradient clipping in differentially private stochastic gradient descent (DP-SGD) as a major cause of miscalibration, and argue that existing baselines for improving private calibration only provide small improvements in calibration error while occasionally causing large degradation in accuracy. Authors propose using post-processing calibration. \n\n",
            "strength_and_weaknesses": "Strengths:\n\n\nAuthors have a good grasp of some of the challenges of differentially private computations in the context of the paper. The paper is generally well-written. The empirical context is well motivated. \n\n\nWeakness:\n\n\nThis paper is primarily on empirical evidence for a recalibration idea. It is not clear to me that the recalibration preserves differential privacy, although there is claim in the paper about this. Perhaps additional discussion on this in an eventually revised manuscript is in order. There does not seem to be adequate theoretical justification about when, why and how the standard DP-SGD method fails, or when, why and how the proposed recalibration will work, or when it will fail. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, the quality of the empirical work seems fine but there is a lack of adequate theoretical developments. The algorithm may have some elements of novelty, and the results look reproducible.\n\n",
            "summary_of_the_review": "The main emphasis of this work is empirical evidence. More detailed analysis is needed. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2193/Reviewer_ckRn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2193/Reviewer_ckRn"
        ]
    }
]