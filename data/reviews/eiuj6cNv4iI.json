[
    {
        "id": "wnqUuGBqjhQ",
        "original": null,
        "number": 1,
        "cdate": 1666497473814,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666497473814,
        "tmdate": 1669781361318,
        "tddate": null,
        "forum": "eiuj6cNv4iI",
        "replyto": "eiuj6cNv4iI",
        "invitation": "ICLR.cc/2023/Conference/Paper5974/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new benchmark for generating the first paragraph of wikipedia articles (with references) from supporting paragraphs. The authors compare a retrieve-and-generate baseline against several non-retrieval baselines (e.g., BART, GPT-2, and FiD + BART). None of these methods (including GPT-3) perform well on the task, suggesting room for future improvement.",
            "strength_and_weaknesses": "Strengths:\n- This is an interesting and important task, and I think it's one that could be a fruitful combination of lot of existing work on both generation and understanding.\n\nWeaknesses:\n- Despite the grandiose motivations, the authors consider an extremely narrow instantiation of this task:\n  - Only the first paragraph is used as the generation target, rather than entire articles. When I read the abstract and introduction I was excited to see how the authors would handle generation / understanding of long sequences, since this is an area where I think this benchmark would be particularly useful, but was disappointed to see that they largely punted on this problem.\n  - Furthermore, the authors don't use the entirety of the reference articles, instead taking only a single paragraph per article that is specifically chosen to maximize informativeness, which I think gives a bit of an unfair advantage to the retrieval-based systems (since they're much more likely to retrieve something useful).\n\n- In general, I feel like the paragraph \"Reference Passage Selection \" in 3.2 completely misses the mark. In particular:\n\n> The original Wiki articles and reference articles tend to be very long. To adapt the capacity of most pre-trained language models (e.g., BERT has a 512 token limit), we use the first section of Wiki articles as the generation target, and we select the passage from the reference article with the highest PST value as the supporting passage.\n\nI'm not sure that I feel very good about our benchmarks having to be \"adapted\" to be consumable for existing models while trading off the essence of the task (synthesizing information from multiple long documents to generate a long document). We should set the benchmarks as goals, see how current models and techniques perform on them, and allow them to be targets for future work.\n\n- I feel like the baselines are pretty weak, and the ReGen model is a pretty standard retrieve-and-generate baseline. I don't think there's too much modeling novelty here.\n\n- Furthermore, the query distribution seems pretty unnatural. For example, in the appendix, the query used is \"how google translate work\". This is a query with a pretty clear information-seeking / question intent that can be answered by a paragraph (or paragraphs). However, the wikipedia page for \"Google Translate\" answers far more than just this question, it contains a variety of general information about the query \"google translate\" (which itself could have many different question intents). So, I feel like the source of supervision provides a different sort of information than what a query like this would actually want (let alone that in practice, only the first passage is used, which contains even less information because of the way that lead sections in wikipedia are structured to contain general high-level information: https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Lead_section).\n  - The original motivation mentioned \"unseen factual queries\", but it's not really clear that generating an article is what you want for this. If it's factual, you can probably answer it with a paragraph or two. I think this work would benefit from a clearer exposition of what these queries would actually be, and what the right source of supervision / data for effectively addressing these queries would look like.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper was reasonably clear, and I thought that the setup was pretty solid. It's an entry in a long line of work on generating Wikipedia paragraphs from references or other external knowledge.",
            "summary_of_the_review": "While I like where this paper is going, I feel like the practical artifact and problem tackled here are a bit too small-scale and narrow. We're at a point in NLP where things actually kind of (?) begin to work, and there have been a variety of impressive results on generating and understanding long documents. I think this work would really shine in pushing this direction, but instead, it compromises itself to fit the limitations of existing models, limiting its usefulness for future work. Thus, I'm not sure that I can recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5974/Reviewer_Ej3Q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5974/Reviewer_Ej3Q"
        ]
    },
    {
        "id": "5--_YvGLGN",
        "original": null,
        "number": 2,
        "cdate": 1666675697395,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675697395,
        "tmdate": 1666676395386,
        "tddate": null,
        "forum": "eiuj6cNv4iI",
        "replyto": "eiuj6cNv4iI",
        "invitation": "ICLR.cc/2023/Conference/Paper5974/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new task called Web-Brain which aims to generate short factual articles for queries by mining supporting evidence from Web. The paper also proposes a new large scale dataset with English Wikipedia. The paper also provides a new framework called ReGen based on SPLADE and FiD. The model is evaluated with both n-gram overlapping metrics and factual correctness metrics. The paper analyze the impact of retrieval, number of references in a quantitative way. The paper also did both human and automatic evaluation. ",
            "strength_and_weaknesses": "Strength \n1. The paper introduces WebBrain, which lets the model retrieve supporting evidence and generate factual articles given a factual query. The proposed dataset is somewhat similar to the Wizard of Wikipedia (Dinan et al., 2018). The newly proposed dataset is interesting and large-scale. The authors crawled and cleaned Wikipedia. The proposed task and corresponding dataset are very interesting and worthy of future research. \n\n2. The paper proposes a new retrieval-augmented generation framework based on SPLADE and FiD. The proposed methods achieve the best results over automatic and human evaluation. The experiment section is very comprehensive. The authors conduct an ablation study with different retrieval models and show the impact of the different numbers of retrieved references. The paper also checks the impact of a number of references. Those results are clearly represented in tables or charts with detailed explanations. The paper shows the case study, human evaluation, and reference mark correction strategy in the appendix.\n\nWeaknesses\n1. The paper uses n-gram overlapping metrics for automatic evaluation. The paper needs to include some newer metrics such as BERTscore (Zhang et al., 2019), and BARTScore (Yuan et al., 2021) which can check semantic similarity. \n\n2. Most of the experiment analyses are in quantitative way. I would like to see more qualitative analysis. \n\n\n\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2019). Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.\n\nYuan, W., Neubig, G., & Liu, P. (2021). Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34, 27263-27277.\n\n\n\nDinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., & Weston, J. (2018). Wizard of Wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:1811.01241.",
            "clarity,_quality,_novelty_and_reproducibility": "1. Some parts of the paper are not very clear. The steps to create WebBrain-R and WebBrain-G is unclear. \n\n2. The paper attached the implementation details in the appendix. It also provides examples from the dataset for readers to check. However, it does not provide any code for reproduction. It shows the limitation and system demonstration in the Appendix. ",
            "summary_of_the_review": "Overall, the paper proposes a new interesting task with a corresponding large-scale Wikipedia-based dataset. The experiment part is quite comprehensive.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5974/Reviewer_892z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5974/Reviewer_892z"
        ]
    },
    {
        "id": "cMVGNXf-tNq",
        "original": null,
        "number": 3,
        "cdate": 1666819890074,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666819890074,
        "tmdate": 1666819890074,
        "tddate": null,
        "forum": "eiuj6cNv4iI",
        "replyto": "eiuj6cNv4iI",
        "invitation": "ICLR.cc/2023/Conference/Paper5974/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new task called \u201cWebBrain\u201d. The objective of the task is to learn to generate a fluent, informative and factually correct short article from a query given some search results. The task is essentially a combination of 2 components:\n  1. retrieval of evidence passages given a wikipedia page title,\n  2. multi document summarization of the retrieved evidence with the first paragraph of the wikipedia page as target. \n\nThe authors contribute a dataset \"WebBrain-Raw\", comprised of English wikipedia plus all crawlable references. The dataset is cleaned with an interesting set of heuristics.\n\nThe authors additionally propose a baseline system called \"ReGen\". ReGen combines several modeling choices:\n  * training a SPLADE retriever for this task with hard negatives are mined,\n  * tuning sparsity in document representations for retrieval,\n  * filtering retrieved documents trading off consistency and diversity,\n  * fusion-in-decoder for generation,\n  * making generation more grounded in the references, by pre-training the generation component to generate individual sentences from a reference passage at time.\n\nThe authors finally present analysis showing the usefulness of these modeling choices, compared to more vanilla choices.",
            "strength_and_weaknesses": "Strengths:\n* The task examined in this paper (retrieval + multi-document summarization) is important.\n* The dataset released by this paper is much larger than comparable datasets and could be useful for furthering work on this topic.\n* The authors describe in detail many technical details of ReGen. These details can be useful to practitioners for reproducing ReGen\u2019s results and in their own work.\n\nWeaknesses:\n* Limiting the generation to only the first passage of wikipedia pages is a pretty strong limitation, also making this work very close to existing multi-document summarization works. The authors do acknowledge this similarity though.\n* In \u201cReference Passage Selection\u201d and \u201cDataset Generation\u201d, the authors select input passages for training with simple word overlap or BM25. It seems like this could be easily improved by using an entailment model, a dense retriever or even SPLADE.\n* The proposed \"WebBrain-Raw\" dataset would be more interesting if it was multilingual instead of English-only.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and good quality. The system is described in detail and should be reproducible.\n\nThe novelty of the approach is mainly in the size of the released corpus and in including retrieval as part of the task, together with multi-document summarization.\n\nThe techniques proposed by the authors are very interesting, but they are only applied to the dataset they created, so it's not totally clear how their modeling choices would stack up against other methods on a more competitive benchmark.",
            "summary_of_the_review": "This looks like a useful dataset+baseline contribution for the important task of multi-document summarization. The techniques used by the authors seem solid.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5974/Reviewer_dCpa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5974/Reviewer_dCpa"
        ]
    },
    {
        "id": "5z-M9diMDnw",
        "original": null,
        "number": 4,
        "cdate": 1666983569179,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666983569179,
        "tmdate": 1666983569179,
        "tddate": null,
        "forum": "eiuj6cNv4iI",
        "replyto": "eiuj6cNv4iI",
        "invitation": "ICLR.cc/2023/Conference/Paper5974/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new dataset and associated task that involves generating the first section of Wikipedia pages from a set of retrieved references. The dataset is similar in form to previous work like WikiSumm, but it is significantly larger and the authors promise to release the references, which have been downloaded from non-Wikipedia sites.\n\nThe dataset generation process involves a number of filtering steps, based on term overlap, that narrow down the reference data to passages that are likely to be related to the target Wikipedia article and associate these passages with sentences in the target. The goal, at test time, is to retrieve reference passages that may be related to a topic, and then\ngenerate the target section of the Wikipedia article.\n\nAlong with the dataset, this paper presents a new model, ReGen. ReGen is composed of a retriever (based on SPLADE) and a reference encoder and target decoder (based on FiD). When trained on the WebBrain data, ReGen outperforms both large language model baselines and also a retrieval augmented model with a dense retriever (FiD + BART) according to a range of automatic metrics, and also annotator judgements of a small set of predictions. I have some questions about some of these comparisions below, but it is clear that ReGen is capable of generating coherent and informative text. It is also capable of providing references for this text, which is a significant advantage over methods that don't include retrieval.\n\nThere are a number of nice ablations that show how the different systems' performances change with different numbers of retrievals and gold references. These comparisons are useful in understanding the tradeoffs between retrieving a lot of supporting evidence, with lower precision, vs fewer high quality references. ",
            "strength_and_weaknesses": "#### Strengths\n- This paper promises a massive new dataset for retrieval augmented text generation, that could be very useful to the NLP community. Having said that, I do have questions about the release with respect to copyright and privacy concerns (detailed below in the ethics section).\n- The end system can retrieve references, and link them to generated sentences, which is a very nice end product which could have real utility beyond unconstrained text generation, which cannot provide any form of attribution for its predictions.\n- There are a number of interesting modeling and training choices made, which lead to significant improvements over a very large language model (GPT3) and also similar retrieval agumented approaches with different choices of retriever and training procedure.\n\n#### Weaknesses\n- There are a lot of separate contributions here that are not independently evaluated (data filtering / retrieval filtering / warmup strategy). It would be nice to see evaluations that validate these choices.\n- The reference passage filtering process selects the references included in the retrieval corpus references according to term overlap with the target Wikipedia article. If I'm understanding this process correctly, this means that the references stored in the corpus have been selected according to observations of the test time targets, so there is some leakage of information from the test targets into the model input. The paper should discuss this.\n- The prompt given to GPT3 \"Introduce [Page Title]\" does not mention that the target is Wikipedia-style text, and the example in Figure 1. does not look much like the start of a Wikipedia article. Meanwhile, GPT3 is definitely able to generate Wikipedia style text if prompted to do so. The comparision to GPT3 would be stronger if the prompt was better tied to the actual task.\n",
            "clarity,_quality,_novelty_and_reproducibility": "#### Clarity & Reproducability\nThe paper is clearly written and all parts are pretty well described. The authors commit to releasing the data, which will allow easy replication.\n\n#### Novelty\nThe dataset is similar in form to previous work, but extends that previous work to the scenario where references must be retrieved from a very large corpus. Similarly, the model is a small modification of existing approaches but it appears to work better than well chosen baselines for this task.\n\n#### Quality\nThere are a lot of details in the filtering procedures used to select the contents of the dataset, and assign references to sentences. These are all justified int the text, but are not supported by any sort of analysis. If this is going to become a benchmark dataset, going forward, it would be good to see a discussion of how these choices affect the task. In particular, see my question about test target leakage into the retrieval corpus above (under Weaknesses).\n",
            "summary_of_the_review": "This paper presents a significant new dataset and task that could be useful to the community, going forward, in benchmarking methods of retrieval augmented generation. The new model is also quite different from previous work, which has generally relied on dense DPR-style retrievers.\n\nOverall, I think this paper is a nice contribution and I am currently leaning toward acceptance. However, I also have some serious questions below about the data release strategy, and how the authors propose to handle concerns about releasing multiple terrabytes of data which may include copyrighted works or personal information. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety",
                "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
            ],
            "details_of_ethics_concerns": "This work promises the release of 260M non-Wikipedia documents without any discussion of potential copyright or privacy issues. Both of these should be addressed. The authors could also consider methods of supporting user data removal requests or, alternatively, release pointers into a store like CommonCrawl, which has support for data removal requests.\n",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5974/Reviewer_TtTt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5974/Reviewer_TtTt"
        ]
    }
]