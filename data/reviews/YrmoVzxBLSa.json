[
    {
        "id": "60xGVaHX3oJ",
        "original": null,
        "number": 1,
        "cdate": 1666104950586,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666104950586,
        "tmdate": 1669642524444,
        "tddate": null,
        "forum": "YrmoVzxBLSa",
        "replyto": "YrmoVzxBLSa",
        "invitation": "ICLR.cc/2023/Conference/Paper1796/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper uses the NTK to discuss the effect of momentum on the training success of PINNs. It is shown that momentum (or adaptive momentum) can improve the convergence speed compared to standard SGD-based training, using both theoretical and empirical arguments.",
            "strength_and_weaknesses": "The paper is well-written and covers an interesting and timely topic. While the authors do not propose a novel method, their experimental evidence is in favor of their hypothesis. The supplementary material contains additional information that helps shedding light on the interplay between the spectral properties of the solution, the selection of the optimizer, and training success.\n\nWhat prevents me from giving a better recommendation is that some of the experimental evidence is unconvincing, and that -- I believe -- alternative hypotheses first need to be ruled out. Indeed, judging from the results, it appears as if spectral bias is not the problem underlying convergence problems. Looking at, e.g., Fig. E.2 or Fig. 2, we see that SGD manages to learn the high-frequency components, but with a shift, trend, or offset added to it. Therefore, the spectral bias, which suggests that the high-frequency components are learned later, seems not to be a valid explanation. As an alternative hypothesis, it may be that learned solutions $u$ satisfy the PDE. While it is known that trivial (zero) solutions are attractive for PINNs (arXiv:2109.09338,arXiv:2203.13648), I assume that the same holds for non-trivial solutions that satisfy the PDE. Essentially, I argue that the solution $u$ provided by the PINN satisfies the PDE, but not the initial/boundary conditions. This needs to be confirmed or rejected by investigating the loss term $\\mathcal{L}_r(w)$ throughout training, and not only the compound loss $\\mathcal{L}$. If this former loss is small, then the PDE is at least approximately satisfied.\n\nFurther, the results in Fig. E.4 seem questionable, as a standard neural network should be capable of learning this simple function. Please correct me if I am missing something important.",
            "clarity,_quality,_novelty_and_reproducibility": "### Quality, Novelty, and Reproducibility:\nThe paper treats an interesting topic and the perspective about the effects of momentum is definitely novel (to the best of my knowledge). However, some important questions regarding the experimental evidence and possible alternative hypotheses need to be taken care of to increase the paper's quality.\n\n### Clarity:\nGenerally, the paper is clearly written. Below are pointers to a few exceptions, that can be taken care of easily.\n- End of Sec. 2.1: What does \"optimal\" in \"optimal solution $u(x,w)$ refer to? The solution of the PDE, or the minimum of the optimization loss landscape? These may be different.\n- Equations at the beginning of Sec. 2.2: There seems to be something wrong. If we insert $u_{h-1}$ into $g_h$, then the weight matrix $\\Theta_{h-1}$ is applied twice. Please check.\n- In eq. (4), what are $s(x)$ and $b(x)$?\n- In Th. 1, what is $m$? Can $m$ be chosen arbitrarily, or does it depend on the parameters of the PDE/the PINN?\n\n### Minor Comments:\n- \"they are limited to some week empirical evidence\" -> weak\n- Proposition A -> Proposition 1\n- After Th. 1, the references to eq. (7) should be a reference to eq. (6), right?\n- In Sec. 4.3, \"unperformed\" -> underperformed; Section (4) -> Section 4; the results for the Poisson equation with $C=10\\pi$ are described twice.\n\n*EDIT* Increased score during discussion period.",
            "summary_of_the_review": "An interesting paper that studies the effect of momentum on the training success of PINNs. While interesting and novel, the experimental evidence is not yet fully convincing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1796/Reviewer_3LnV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1796/Reviewer_3LnV"
        ]
    },
    {
        "id": "mX6zXKitPZ",
        "original": null,
        "number": 2,
        "cdate": 1666624982702,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624982702,
        "tmdate": 1666841036343,
        "tddate": null,
        "forum": "YrmoVzxBLSa",
        "replyto": "YrmoVzxBLSa",
        "invitation": "ICLR.cc/2023/Conference/Paper1796/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the spectral bias phenomenon when using momentum-based methods to optimize physical-informed neural networks. The authors simplify the neural networks as a linear model (i.e., NTK), and prove that the continuous approximation of GDM converges to the high-frequency solution faster than SGD. The authors also investigate the convergence of Adam over PINN through a specific model. Experiment results are provided to show that Adam and SGDM converge faster than SGD when solving the PINNs.\n\n\n",
            "strength_and_weaknesses": "**Weakness**\n\n1. The biggest weakness of this paper is that the analysis does not utilize the property of physical-informed loss. In other words, the same results also hold for image classification tasks as the analysis does not rely on which loss is used. This makes the result hard to unveil any information specific to the NN-solving-PDE task.\n\n2. There are some overclaims in the paper. It is said that \"using the SGD with momentum (SGDM) optimizer can reduce\nthe effect of spectral bias in the networks\", but the paper only proves the result for a continuous approximation of GDM with no approximation error provided.\n\n2. The paper is hard to follow, and many places lack explanations. For example, at the end of page 3, it is said \"the norm of the Hessian $\\mathcal{H}(w_t)$ is order $\\mathcal{O}(C^4)$\". However, the parameterization of $u(w,x)$ is not even provided (although it is explained in the appendix). On page 4, it is said that \"the decay rate analysis becomes more involved as Eq. 7\". However, there is no Eq. 7 in the main text. Also, why does using SGDM lead to faster convergence for high-frequency components than SGD? How do we know it from Theorem 1?",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "Based on the evaluation above, I believe that this paper is below the bar of ICLR. The paper can be improved by refining the writing and the theoretical results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1796/Reviewer_droF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1796/Reviewer_droF"
        ]
    },
    {
        "id": "eMqnVeE9szI",
        "original": null,
        "number": 3,
        "cdate": 1666668130531,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668130531,
        "tmdate": 1666668130531,
        "tddate": null,
        "forum": "YrmoVzxBLSa",
        "replyto": "YrmoVzxBLSa",
        "invitation": "ICLR.cc/2023/Conference/Paper1796/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper the authors study the convergence of infinitely wide PINNs on PDEs that have high frequency modes to examine the effects of spectral bias. Their analysis proceeds using NTK theory of convergence. They show that while SGD can learn high frequency modes that the learning rate has to be small. Further they show that SGD with momentum as well as ADAM reduces the effects of spectral bias and speed up the learning. The paper also features empirical results on the poisson equation, reaction-diffusion equation and transport function.  ",
            "strength_and_weaknesses": "-The authors do a good job of introducing the reader to spectral bias in a general context in addition to spectral bias in the case of PINNs.---Very clear and concise analysis for section 3.1 and 3.2.  \n-Nice experimental results, they seem to confirm theoretical convergence results. \n\n\n-The authors could flush out section 3.3; maybe move some of the material in the appendix for convergence results into main paper and list equations in format similar to 3.1 and 3.2\n-Would be nice if authors concocted a numerically tractable ay to extract exact eigenvalues of Hessians to see how it matches with theory.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is clearly written and results are reproducible. I don't believe anyone has used NTK to analyze PINNs for this context. ",
            "summary_of_the_review": "Overall a good paper, easy to understand what the authors aimed to do. Seems like an important and useful result and ties in considering the spectrum of the physics problem in consideration with convergence guarantees of the network.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1796/Reviewer_Ntw5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1796/Reviewer_Ntw5"
        ]
    },
    {
        "id": "nfC6haACNe",
        "original": null,
        "number": 4,
        "cdate": 1666689772863,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689772863,
        "tmdate": 1666689772863,
        "tddate": null,
        "forum": "YrmoVzxBLSa",
        "replyto": "YrmoVzxBLSa",
        "invitation": "ICLR.cc/2023/Conference/Paper1796/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "PINNs often fail to converge. The authors propose to use neural tangent kernel to solve this issue, and propose to use the Adam optimizer. They empirically demonstrate this to work on problems with high-frequency features.",
            "strength_and_weaknesses": "# Strengths\n\n- for the first time, PINNs under SGDM, and Adam are analyzed, and their relation to solving spectral bias is discussed\n- theoretical analysis\n\n# Weaknesses\n\n- it has been shown that the Adam optimizer works better than vanilla SGD in many cases (unrelated to PINNs), why would you assume that it does not work for PINNs?\n- the paper aims to solve spectral bias. However, the phenomenon of spectral bias is not clearly defined or introduced in the paper\n- figures are of bad quality\n\n\n\nminor issues:\n- spelling mistakes (week -> weak)",
            "clarity,_quality,_novelty_and_reproducibility": "# clarity\n\nThe main motivation of the paper, the existence of spectral bias, is not introduced very well. It is only mentioned that it is a known problem, but the referenced papers do not give clear definitions of spectral bias either. There should be some discussion of how to measure spectral bias, and what it is. Otherwise the paper is fairly well written, except for some typos and grammatical mistakes. The figures have bad quality and are too small.\n\n# quality\n\nseems to be good\n\n# novelty\n\nthe analysis is important, but novelty is limited, there is no specific new method introduced. \n\n# reproducibility\n\ncode is not given, but should be possible to reproduce from the paper",
            "summary_of_the_review": "PINNs and their convergence behaviour is an understudied area where any insights can be very valuable to future research. The present work attempts to do some fundamental work to investigate PINNs. It is good to do some theoretical analysis, but novelty is limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1796/Reviewer_3BS7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1796/Reviewer_3BS7"
        ]
    }
]