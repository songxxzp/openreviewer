[
    {
        "id": "LIIV18fqD3",
        "original": null,
        "number": 1,
        "cdate": 1666542789009,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666542789009,
        "tmdate": 1669998879355,
        "tddate": null,
        "forum": "UG8bQcD3Emv",
        "replyto": "UG8bQcD3Emv",
        "invitation": "ICLR.cc/2023/Conference/Paper3915/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a deep learning-based Granger causality model for discovering the Granger causal graph and imputing missing values at the same time. The author claim this model can handle missing completely at random and periodic missing scenarios.\nThe main methodology is to iteratively apply the data imputation and graph learning, where the graph learning stage is further split into three sub-stages: (1) without imputed value; (2) with imputed value but not as model supervisory; (3) with imputed value. \nThe graph is sampled from an independent Bernoulli distribution with a Gumbel-softmax gradient estimator.\n\nEmpirically, the author conducts 2 synthetic experiments with the Netsim fMRI dataset, where the model achieves better results. ",
            "strength_and_weaknesses": "## Strength\nGranger causality with deep learning has been a popular research topic in recent years, and this model is an extension of some well-known previous work. The idea behind this model is intuitive and easy to follow. Despite some typos in the math, the model formulation is easy to understand. Although I think the author should conduct more real-world experiments (e.g. DREAM3/4 dataset), the author has conducted sufficient ablation studies for synthetic experiments to demonstrate some of the properties of the model. \n\n## Weakness\nAfter reading the paper, I have some concerns regarding the proposed method: (1) theoretical results and soundness; (2) clarity regarding the periodic missing. \n\nSpecifically, first, I am not fully sure the proposed objective in eq.10 is correct. Since the adjacency matrix is sampled from an independent Bernoulli distribution, which is modeled by the parameter $\\theta$. This means this is very similar to a variational framework, where the Bernoulli distribution is a mean-field approximation, see the formulation [1] and [2] for details. This means the with Gaussian noise variable, the $L_{pred}$ in eq.10 is similar to the likelihood term in ELBO and $\\Vert \\sigma(\\theta) \\Vert_1$ is similar to the prior, however, the entropy of the Bernoulli distribution is missed in eq.10. This means the proposed objective is not a proper likelihood, so how it can guarantee it recovers the ground truth graph? (I will explain my concern of theorem 1 in the following). How to explain the differences?\n\nThe author provides theorem 1 to justify the convergence of the CUTs. However, I am not fully sure this will hold. For equation 15, the inputs to $f_{\\phi_i}$ should be the parents of node $x_{t,i}$, why it is $x_{t-\\tau:t-1, i}$? Is this a typo?\nIn eq.17, when $S$ and $f_\\phi$ are not the ground truth, how this two cases with $S_{ij=1}$ equals $S_{ij=0}$? Also, the negative gradient has a derivative of the sigmoid function, meaning that the magnitude of the gradient will go toward 0, meaning that more rigorous analysis is needed to claim convergence.\n\nFor equations 18 and 19, I am not sure the assumption \"$f_\\phi$ accurately models the $f_i$\" is a reasonable one. Without learning a good graph, it is hard to learn a good $f_\\phi$. Thus, I don't think this assumption will hold in general. In addition, due to a similar reason as above, careful analysis is needed to claim convergence. \n\n\nFor the clarity, if I understand correctly, the author tries to model the non-uniform sampling interval as the missing value problem (i.e. periodic missing). In this case, it can only tackle a subset of \"non-uniform sampling interval\" since it assumes a fixed period. If that is the case, I wonder how CUTs will work in the following example: $x_1$ is sampled with interval T and $x_2$ is sampled with interval 2T, In that case, $x_2$ will be missing at $t=T, 3T, 5T, ...$ (assuming start t=0). So how CUTs can impute this value $x_2$ in-between since no observation is provided?\n\n[1] Geffner, T., Antoran, J., Foster, A., Gong, W., Ma, C., Kiciman, E., ... & Zhang, C. (2022). Deep End-to-end Causal Inference. arXiv preprint arXiv:2202.02195.\n[2] Morales-Alvarez, P., Lamb, A., Woodhead, S., Jones, S. P., Allamanis, M., & Zhang, C. (2021). VICause: Simultaneous Missing Value Imputation and Causal Discovery with Groups. arXiv preprint arXiv:2110.08223.\n ",
            "clarity,_quality,_novelty_and_reproducibility": "In general, this paper is written clearly apart from the \"periodic missing\" part and the idea is easy to follow. Empirically, I prefer more real-world experiments but the ablation study here is also useful to demonstrate the model properties. As for reproducibility, I wonder how the hyper-parameters are tuned. Is it based on a validation dataset and tuned with MSE error? What is the data splitting ratio?\n\nFor novelty, I think the author should include the citation of [2], which also tackles the missing value and structure learning at the same time. The formulation is very similar. Overall, I think it is not a very novel model, and similar ideas have been explored in static settings. ",
            "summary_of_the_review": "My main concern regarding this paper is the technical soundness and claims made by the author. In general, I think this paper can be improved if the author can clarify the concerns and elaborate more on its novelty.\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3915/Reviewer_jtS8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3915/Reviewer_jtS8"
        ]
    },
    {
        "id": "twfFS8-CP0",
        "original": null,
        "number": 2,
        "cdate": 1666836498426,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666836498426,
        "tmdate": 1666836498426,
        "tddate": null,
        "forum": "UG8bQcD3Emv",
        "replyto": "UG8bQcD3Emv",
        "invitation": "ICLR.cc/2023/Conference/Paper3915/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors present a Granger causal discovery framework for unstructured time series data which leverages an alternating formulation comprising of a) data imputation and b) subsequent causal discovery. The basic premise is centered around providing empirical results on how data imputation aids the causal discovery and vice versa. Ablation studies based results along with comparisons using different combinations of Causal Discovery and data imputation techniques (Random and periodic missing patterns only considered) indicate the superior performance of the proposed CUTS method.",
            "strength_and_weaknesses": "**Strengths**\n\n- Problem of causal discovery from unstructured time series data is an important problem and the paper presents a framework which is intuitive and relatively easy to follow.\n\n- The authors have put in significant effort to benchmark their technique with various combinations of causal discovery and missing pattern imputation techniques.\n\n**Weakness**\n- The paper as a whole does not do a good job in providing a technically rigorous narrative on why or how Data imputation boost Causal Discovery.  The vice versa argument seems reasonable enough, but given that the imputation is just confined to addressing two patterns in time series data (random and periodic patterns), I wonder if the causal discovery would benefit with the limited insight provided by the data imputation framework.\n- To support these claims, I do notice that in general the AUROC improvements for VAR with Random missing are marginal compared to other competing methods in Table 1. \n\n**Minor writing issues**\n\n- ParCorr first occurrence is not defined on Page 6.\n- ZOH first occurrence is not defined on Page 5.\n- Table 2 title should be Quantitative and Table 3 title should also be Quantitative",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is reasonably clear and the authors have described the algorithms in good detail in the Appendix also.  I believe the work can be reproduced with significant effort (due to multiple algorithms involved).",
            "summary_of_the_review": "I believe the paper presents a good empirical study of a framework which nicely combines two techniques a) data imputation and b) causal discovery in an alternating style framework to deal with the problem of causal discovery from unstructured time series data. While the approach is interesting and the paper is reasonably easy to follow, the results for some cases do not indicate significant improvements. However, the more major concern I have is with the technical rigor is missing to support some claims on data imputation supporting causal discovery. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3915/Reviewer_ADkU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3915/Reviewer_ADkU"
        ]
    },
    {
        "id": "Ud29FWHTR4d",
        "original": null,
        "number": 3,
        "cdate": 1666863959244,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666863959244,
        "tmdate": 1670922256070,
        "tddate": null,
        "forum": "UG8bQcD3Emv",
        "replyto": "UG8bQcD3Emv",
        "invitation": "ICLR.cc/2023/Conference/Paper3915/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents an auto-regressive style model for joint causal graph learning and missing data imputation for unstructured temporal data. The model is trained in a two-step procedure alternating between training the imputation model and the graph learning (EM-style algorithm). The model parametrises the graph using an independent Bernoulli distribution and the Gumbel softmax for enabling gradient based learning. The paper compares the method to relevant temporal discovery methods and achieves competitive performance.",
            "strength_and_weaknesses": "Strengths:\n- The paper tackles the important problem of modelling timeseries data with missing entries.\n- The experimental results show competitive or superior results to relevant baselines.\n- The proposed method seems to incorporate a lot of small tweaks that improve the model performance - there seems to be value in analysing the training behaviour a bit further.\n\nWeaknesses:\n- Some important related works are missing: [1] tackles temporal causal discovery with Neural ODEs that would be able to handle inconistent sampling intervals, [2] performs joint structure learning and data imputation, [3] performs temporal causal discovery using the NOTEARS framework for continuous DAG learning. Methods based on the same framework have been applied to static data for joint causal discovery and data imputation [4]. All these weaken the novelty of this paper.\n- The paper mentions that it is based on Granger causality. However, the current formulation also allows for an interpretation as an additive noise model: ie $x_i = f(pa_i) + e_i$. Could you please comment on this? This interpretation would also allow for the identification of the temporal causal graph $A_{0, \\tau}$ rather than just the summary graph $\\hat{A} = max_t A_{t}$.\n  - Please add some comments about the difference between Granger causality and ANMs or PCMCI that also identify the temporal causal graph.\n- It is unclear how $\\tau_{max}$ is chosen. Is this assumed to be known? What if this isn't known?\n- All experiments use missing data. It would be great to see a baseline comparing to datasets with full observability.\n\nMisc:\n- What's the intuition of using the moving average as a training signal for the imputation network?\n- For the graph discovery stage - do you calculate an expectation over multiple graph samples or is this amortised over different batches? Or do you use the same graph sample for optimising this loss?\n- Please explain ZOH earlier in the text.\n- Eq 3: what is $e$? What's the assumption about it? This might make or break the use of the L2 loss.\n- Please pay attention to the use of `\\citep` and `\\citet`.\n- Eq 5: You use inconsistent $\\tau=0...$ and $\\tau=1,...$.\n- p5 just above eq 11: I believe $n_2$ should be $n_3$.\n- p2: \" to conduct causal inference and ..\" - should this be \"causal discovery\"? Causal inference tackles the question of inferring causal estimates (e.g. ATEs).\n\n[1] Bellot, Alexis, Kim Branson, and Mihaela van der Schaar. \"Neural graphical modelling in continuous-time: consistency guarantees and algorithms.\" International Conference on Learning Representations. 2021.\n[2] Morales-Alvarez, Pablo, et al. \"VICause: Simultaneous Missing Value Imputation and Causal Discovery with Groups.\" arXiv preprint arXiv:2110.08223 (2021).\n[3] Pamfil, Roxana, et al. \"Dynotears: Structure learning from time-series data.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.\n[4] Geffner, Tomas, et al. \"Deep End-to-end Causal Inference.\" arXiv preprint arXiv:2202.02195 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written and easy to follow. The novelty of the method is limited in light of the papers mentioned [1-4]. Most relevant details are included for reproducibility.",
            "summary_of_the_review": "The paper tackles and important and interesting problem of discovering causal graphs from temporal data with missing values. However, the evaluation only compares to baselines without proper data imputation capabilities and does not offer a comparison on fully observed data. Furthermore, important literature is missing, and it would be good to discuss some of the model assumptions around Granger causality vs other causal discovery assumptions. Lastly, the model choices about noise distributions and loss functions should be better justified.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3915/Reviewer_SHw1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3915/Reviewer_SHw1"
        ]
    },
    {
        "id": "5BHHpSzTDi",
        "original": null,
        "number": 4,
        "cdate": 1667167952353,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667167952353,
        "tmdate": 1670347334825,
        "tddate": null,
        "forum": "UG8bQcD3Emv",
        "replyto": "UG8bQcD3Emv",
        "invitation": "ICLR.cc/2023/Conference/Paper3915/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an architecture to infer causal link from irregularly sampled time series data. The authors embrace the Granger causality paradigm. The method works by interleaving missing data imputation scheme and causal graph learning. The causal graph is used to structure the imputation predictions by using only causal predictors. The inferred causal graph is assumed to get more accurate as the imputation gets better. The model proceeds in three stages. The first is learning the forecasting / imputation and causal graph using the initial imputation. The second is similar but replaces the missing values with the predictions from stage 1. The third further refines the causal graph learning by replacing the missing values by the imputation from the second stage. The causal graph of the last stage is the predicted causal graph of the model.",
            "strength_and_weaknesses": "### Strengths\n\n- This paper addresses an important problem in causal discovery : inferring causal dependencies from irregular time series. \n- The method appears to outperform baselines it is comparing against.\n- The iterative refinement of the imputation is a good idea and the authors show that the data fitting and causal graph discovery are helping each other.\n- The paper proposes different ablations (though I believe complementary experiments would be required - see below).\n\n\n### Weaknesses\n\n\n- The biggest weakness of this paper is the clarity. I think Figure 1 and Figure 2 should be mixed together so you make very clear that each stage 1-2-3 corresponds to doing one iteration of Figure 1. The notation at the beginning of Section 4.2 is uncommon as well (what is $p(x \\rightarrow p(x))$, this lacks rigour. Similarly, $\\theta$ is not directly understood as parametrising $m$. Only in point of Theorem 1 does it become apparent, which is, I believe, the wrong place to introduce this concept. \n\n- In the definition of Granger causality, I think there is a mistake in the causal direction. If I\u2019m not mistaken, you mean $j$ causes $i$.\n\n- The implications of the assumptions of Theorem 1 should be discussed in the main text. It\u2019s important for the reader to have an intuitive grasp of when this theorem holds. \n\n- It seems that each stage is a refinement of the causal graph learning by using more accurate imputations. In that respect, I think it would make sense to have an experiment that investigates the impact of a different number of refinement steps. It does not seem to be present in the ablation study section.\n\n- The related section misses the whole line of work of causality in time series using convergent cross mapping. I encourage the authors to show how their paper situates with respect to that literature.  In particular, extensions of such methods have been proposed for irregular time series (e.g. Latent Convergent Cross Mapping - https://openreview.net/pdf?id=4TSiOTkKe5P), which represents a relevant baseline for the problem discussed here. \n\n- You are comparing your approach with a GP for imputation. I believe a GP is non causal as it may use observations from the future to predict information from the past. I have few questions regarding this. First do you think it is an issue that the imputation is non-causal. Second, if not, would your approach fare better if another non-causal imputation strategy would be used (e.g. BRITS that you are referring to). In which case, a single step of imputation followed by a causal graph learning step could be enough to reach similar performance. \n\n- I think unstructured time series is not the appropriate name for the type of time series you are considering. In my experience, this is more often referred to as *irregular* time series. \n\n- Despite the bold numbers, it seems difficult to appreciate the magnitude of the improvement in performance this method is giving. Indeed, the difference between AUC results are very small. \n\n- Minor : Right above equation 11, I believe you mean last $n_3$ Epochs and not $n_2$.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nThe paper could clearly improve on clarity as I wrote above. I think the idea is good and that it would be possible to convey it in a more straightforward fashion to the community. I pointed to key points that should be addressed but I encourage the authors to work on the presentation.\n\n### Quality\n\nThe authors provide several experimental results and perform many ablation studies to study the impact of each part of their model. I pointed above to additional experiments that would help the reader understand more deeply the behaviour of this model.\n\n### Novelty\n\nWhile Granger causality for time series is very well established, the contribution of this work relies on the adaptive training scheme (which further justifies why experiments with varying number of refinement stages are needed). \n\n### Reproducibility\n\nReproducibility is inherently linked to clarity, which can be improved. ",
            "summary_of_the_review": "Interesting idea that would deserve additional experiments to fully grasp the contribution of the method. The clarity and the positioning with respect to existing works could be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no ethical concerns.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3915/Reviewer_fTfw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3915/Reviewer_fTfw"
        ]
    }
]