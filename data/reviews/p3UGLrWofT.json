[
    {
        "id": "VFNNVDiVeg",
        "original": null,
        "number": 1,
        "cdate": 1666650015721,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650015721,
        "tmdate": 1670006993672,
        "tddate": null,
        "forum": "p3UGLrWofT",
        "replyto": "p3UGLrWofT",
        "invitation": "ICLR.cc/2023/Conference/Paper5391/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies efficient neural network design for faster inference. The authors propose Structured Coarse Block Pruning (SCBP) that first partitions all layers into segments, then assigns a width multiplier to each stage, and finally explores the best configuration by grid search. The proposed SCBP delivers 20% faster and 50% smaller models on CIFAR-10 and CIFAR-100 benchmarks.",
            "strength_and_weaknesses": "---\nStrengths:\n- This paper targets a critical problem: designing efficient neural architecture without much human effort and computational resources.\n\n---\nWeaknesses:\n- The proposed method is essentially a grid search over a simplified design space. The main contribution of this paper lies in partitioning layers into segments. However, the authors have provided no insights into why layers with the same feature map size should have the same width multiplier (i.e., pruning ratio). The current solution appears like an arbitrary design space simplification without proper justifications.\n- The paper is hard to follow. The algorithm boxes in Section 3 are composed of many sub-procedure (function) calls, which I find hard to understand without more clarification. Though the authors claim that they base their method on filter pruning, the proposed algorithm seems to retrain all the candidate networks from scratch without inheriting the weights from the original model.\n- This paper does not provide any baselines in its experimental evaluation. As the proposed method is highly related to pruning and neural architecture search, it is more than necessary to include some numbers of related baselines.\n- The experimental results are all on small-scale benchmarks, which is less representative. It would be essential to include some results on large-scale datasets, such as ImageNet. The authors claim that their search cost is much lower than other methods, which is actually because the dataset they are using is much smaller.\n\n---",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has poor quality, clarity and novelty.",
            "summary_of_the_review": "My recommendation is based on the limited novelty, insufficient evaluation, and poor writing of this paper. Therefore, I think it is not ready for publication in its current shape.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5391/Reviewer_yX2W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5391/Reviewer_yX2W"
        ]
    },
    {
        "id": "RHDDEeDWZR5",
        "original": null,
        "number": 2,
        "cdate": 1667173081995,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667173081995,
        "tmdate": 1667173081995,
        "tddate": null,
        "forum": "p3UGLrWofT",
        "replyto": "p3UGLrWofT",
        "invitation": "ICLR.cc/2023/Conference/Paper5391/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a systematic method of creating variations of a convolutional architecture by reducing the number of filters at certain groups of layers. Subsequently training and evaluating these new architectures shows that some are significantly more efficient that the initial \"seed\" architecture achieving similar accuracy scores with significantly less computational resources.",
            "strength_and_weaknesses": "The method proposed in the paper requires retraining every variation from scratch which is not network compression but simply architecture search. Since there is very little information proposed to guide this architecture search the paper performs either brute force architecture search (using a grid search) or manual architecture search which is simply CNN training.\n\nTo strengthen the paper the authors should compare with network compression techniques (such as the ones listed in the related work section) and show whether the proposed method finds architectures with specific efficiency characteristics more efficiently than network compression.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the method is reproducible. There is very little novelty in the proposed method as it amounts to manual CNN design based on a seed architecture.",
            "summary_of_the_review": "Based on the lack of comparison with baselines as well as the low novelty I propose rejection.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5391/Reviewer_aX8y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5391/Reviewer_aX8y"
        ]
    },
    {
        "id": "WC5rQHDFL0_",
        "original": null,
        "number": 3,
        "cdate": 1667227594433,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667227594433,
        "tmdate": 1667227805199,
        "tddate": null,
        "forum": "p3UGLrWofT",
        "replyto": "p3UGLrWofT",
        "invitation": "ICLR.cc/2023/Conference/Paper5391/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper demonstrates SCBP, a framework to compress convolutional networks, by first segmenting the layers into superblocks and then associate them with different compression ratios. The SCBP framework constructs a pool of models with different attributes that meet different needs.",
            "strength_and_weaknesses": "The method proposed is effective and not hard to follow.\nHowever, it brings limited new things/insights to me.\n1. The authors claim the core of its method is the segmentation and the c-ratio set.\nHowever, is the segmentation really needed?\nThe authors claim that the segmentation can reduce the search cost by binning similar layers together in a bucket/segment. It appears to me that this is very trivial. \n2. The c-ration, in the paper, is also trivial to set.\n3. Therefore, the framework is a bit of engineering with very limited research insight.\n4. A previous work, AutoSlim[1], associates a ratio to each single layer, and applies algorithm to automatically learn the ratio, which should be discussed and compared.\n5. The work does not compare to other methods.\n6. The experiment evaluation is also limited on only CIFAR.\n7. The authors claim there is no retrain cycle. However, after the compression, it requires to train and evaluated the candidates' architectures. This step is 'retrain' and would cost a lot of time.\n8. The resulted network size and FLOPS are not reported.\n9. The writing should be largely improved with many typos.\n\nReference:\n[1] https://arxiv.org/abs/1903.11728",
            "clarity,_quality,_novelty_and_reproducibility": "The work is not of good quality, novelty.",
            "summary_of_the_review": "Overall, I think the work lacks novelty, research insights, experiments and analysis. It is not appropriate to be accepted in its current form.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5391/Reviewer_PfGt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5391/Reviewer_PfGt"
        ]
    }
]