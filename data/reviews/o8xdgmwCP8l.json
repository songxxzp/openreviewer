[
    {
        "id": "KVwLWRCEFm",
        "original": null,
        "number": 1,
        "cdate": 1666406855458,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666406855458,
        "tmdate": 1670465998830,
        "tddate": null,
        "forum": "o8xdgmwCP8l",
        "replyto": "o8xdgmwCP8l",
        "invitation": "ICLR.cc/2023/Conference/Paper2335/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies how augmentations induced by object interaction sessions help with self-supervised learning performance. This paper uses two synthetic environments and two real world data environments, each with different manipulation, ego-motion, and fixation related augmentations for self-supervised learning. The authors showed that these types of augmentations are more helpful than traditional image-space augmentations, and best if combined with those augmentations as well.\n\n",
            "strength_and_weaknesses": "#### Strength\n\nThis paper excels at performing well designed and controlled augmentations to study their performance in various settings. Through this design and careful control, it is convincing to show that the type of augmentation introduced are helpful for SSL.\n\n\n#### Weaknesses\n\nMy main concern for this paper is the notion of \"time-based\" augmentations. I find this term a bit misleading: most of the augmentation introduced seems time-invariant, but more as 3D augmentations. By time-invariant I mean in the case that if all the frames captured each session are randomly shuffled then piped to the SSL frameworks, augmentations like manipulation and ego motion seems unaffected at all, where all that matters are the set of positive and negative pairs, but not their temporal relations. Next frame positive pair might be affected, which is truly time-based. But according to [1], this may not be a good strategy for mining semantic correspondences. \n\nI therefore would like to ask the author for more arguments or evidence to support the use of \"time\", in addition to the 3D-based augmentations provided in the paper. Or a more proper name should be used. \n\n[1] Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective",
            "clarity,_quality,_novelty_and_reproducibility": "- This paper is well written and easy to follow.\n- The authors are well thought on the experiments to support their claim, making it a good quality paper\n- The augmentations introduced in this paper are not significantly novel perse, but the careful design of the augmentations to mimic human response is likely to be novel.\n- If the authors would release their data generation pipeline, this paper is reproducible.\n",
            "summary_of_the_review": "In summary, I find this paper slightly below the bar of acceptance. My major concern is the notion of 'time-based augmentations', where a lot of those augmentations are just 3-D based and are time ignorant. To match this term more closely, the augmentations, or the positive frame selection must be time sensitive. To support the claim that this is helpful, the baseline should be a time-ignorant positive frame sampling strategy with the same set of augmentations. \nAlso, the types of augmentations shown here are marginally novel in pure SSL sense, but this could be an underestimate since the augmentations are designed more closely to mimic human responses.\n\n## Update after the rebuttal \nMy main concern is mostly around the term 'time-based': some of the augmentations used in this paper is not quintessential in terms of 'time'. One example is rotation of the object, which can be seen as an instance-centric 3D augmentation, where the ordering of different poses along the temporal dimension does not matter. \nThe longer phrase \"time-based augmentations during natural interactions\" is more accurate, and I apprecitate the change proposed by the authors. I do hope the title of the paper can be more accurate as well, but it is a relative minor issue.\nGiven the author's response, I would like to increase my rating from 5 to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2335/Reviewer_S6Vj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2335/Reviewer_S6Vj"
        ]
    },
    {
        "id": "_GtKhINMbj",
        "original": null,
        "number": 2,
        "cdate": 1666540909913,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666540909913,
        "tmdate": 1670444120028,
        "tddate": null,
        "forum": "o8xdgmwCP8l",
        "replyto": "o8xdgmwCP8l",
        "invitation": "ICLR.cc/2023/Conference/Paper2335/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose to use temporal changes in object views that occur naturally in developing infants' visual environment as data augmentations for time-based self-supervised learning. Using a rendering environment they create positive and negative training pairs using 3d object manipulations such as rotations, saccadic eye movements, moving objects (to change the background). They compare to standard self-supervised learning using data augmentation and evaluate the learned representations on both the rendered and some real-world video datasets of humans manipulation objects.\n",
            "strength_and_weaknesses": "### Strengths\n\n + Well motivated overall question\n + Important question how active vision could support visual representation learning beyond the simple data augmentation strategies currently used\n + Using rendering could be an interesting way forward to generate realistic view changes synthetically\n + Implemented an interesting and diverse set of view changes caused by active vision not captured by data augmentation\n\n\n### Weaknesses\n\n 1. Method does not actually result consistently in improved visual representations\n 1. Small size of datasets seems to be a major limiting factor preventing strong conclusions\n 1. Motivation for the individual experiments and how they each relate to the main research question is not very clear\n 1. Analyses are lacking depth\n\n\n\n### 1. No consistent improvement\n\nOne major concern is that the results do not really show clear evidence that the proposed approach indeed outperforms standard data augmentations. In more than half of the dataset/learning method combinations it actually performs worse.\n\n\n### 2. Small datasets\n\nAnother major concern is what we can actually learn about self-supervised learning in practically relevant scenarios from such small datasets. The fact that both methods (standard and time-based) combined work significantly better than each one individually suggests that in both cases the number of unique image sources/scenes is a major limiting factor and the combination of both methods simply leads to substantially more \"effective\" data. How each of the methods would fare in a more typical self-supervised setting where the datasets are large is not really answered by these experiments.\n\n\n### 3/4. Motivation + lack of depth\n\nThe authors present a collection of results, each of which addresses a question that is in principle interesting. However, it was not very clear to me why exactly these questions were asked and what we learn from the analyses. To name just one concrete example, why are saccades only simulated on CORe50 but not VHE? \n\nEach question is addressed very superficially by one manipulation, often on a single dataset, without discussing robustness and generality of these results: how much do they depend on the particular way the authors operationalized the question, what about different datasets or change in hyperparameters?\n\nOverall, after reading the paper I'm left wondering: what now? What follows for self-supervised learning? The fact that invariance to 3d rotations and changes in background are useful is undoubted, but such changes simply cannot be easily generated from unlabeled images and deriving them from unlabeled video is not a unique contribution of this paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall motivation and introduction are very clear and compelling. Although several aspects of the work are creative and novel, unfortunately the experiments and results do not really live up to the expectations. \n\nI expect reproducibility to be limited, since the relatively large number of individual experimental manipulations are not described in much detail, albeit these things could be inferred from the code, which is promised to be published. *[note: in my original review, I had not seen the note about code being published]*\n",
            "summary_of_the_review": "A well-motivated paper with creative and original aspects but relatively poor execution and somewhat unclear conclusions.\n\n### [Update after rebuttal]\n\nBased on the discussions and the other reviewers' impressions I increased my score and gave the paper the benefit of the doubt.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2335/Reviewer_Qi6u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2335/Reviewer_Qi6u"
        ]
    },
    {
        "id": "d6OvifJLi1q",
        "original": null,
        "number": 3,
        "cdate": 1666675851390,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675851390,
        "tmdate": 1670481925785,
        "tddate": null,
        "forum": "o8xdgmwCP8l",
        "replyto": "o8xdgmwCP8l",
        "invitation": "ICLR.cc/2023/Conference/Paper2335/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes the idea of using time-based data augmentations to aid general purpose machine vision systems. Authors provide a thorough and very detailed psychological motivation for why this may work, and later use simulated agents's view points to simulate the time-based components of learning. Authors find that stacking time-based representations on top of other data-augmentation schemes aids in recognition.",
            "strength_and_weaknesses": "See Summary of Review in section below. TLDR: The main paper's strengths are the original scientific question, and methodological execution (dataset, networks and learning regimes used). The main weakness is a tentative missing control to justify the main claim (see summary review section for details).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read, very clear, and the appendix and level of detail is high enough to guarantee reproducibility. Authors also have states in the paper that they will share their code if the paper is accepted and that is a great idea.",
            "summary_of_the_review": "This is a type of paper that everyone talks about that \"someone will have to do this\", but no one is aware of anyone doing it, so I am happy that a paper regarding training a NN that incorporated the temporal dynamics in a SSL framework is studied and properly evaluated. This paper deserves an accept just because: 1) the authors have tried (and this isn't an obvious thing to do both in terms of finding an interesting scientific question and also the methodology, since most of the CV community is unfortunately still thinking about ImageNet, and developing a convoluted new architecture to increase the validation set performance by 0.1%); 2) authors find a set of interesting results and the cohesive story of the experimental design + evaluations mostly make a lot of senses (however, there's a couple of questions I have later on this).\n\nThe main **weakness** I find in this paper is that I think at some point there was some conversation (in the paper) about having a control condition where the video frames were shuffled as a control to eliminate the confounding variable of \"more data\". Without this control, the paper almost falls short because it still raises the question: **\"Do time-based augmentations drive this increase in performance due to their time-based nature, or by pure virtue of adding more training data?\"** Maybe authors have added these plots in the Appendix and I've missed this?\n\nOther than that, **this is a fantastic paper(!). I'm willing to increase my score if authors address my concerns in this review.**\n\n* Missing References:\n\n   * On eye-movements and multiple views for robustness, it may be worth adding the recent work of **Harrington & Deza. ICLR 2022** that showed through a set of psychophysical experiments that learning of multi-view foveal and peripheral template potentially aids in robust representations for humans (through adversarial robustness experiments in machines as well).\n\n   * Another key paper that is missing is the theme of time-based contrastive/self-supervised models are: ``Are models trained on temporally-continuous data streams more adversarially robust?'' by **Kong & Norcia. SVRHM 2021.**\n\nOf course, it would be quite interesting to see how time-based learning affects the agents performance in adversarial robustness and/or common corruptions. Perhaps this is not doable to do for the rebuttal (that is ok), but I'd be curious what the authors would think would happen. I expect stronger robustness, but will this actually be the case if some studies have shown that a robustness-accuracy tradeoff exists? (Tsipiras, ICLR 2019)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2335/Reviewer_fkWg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2335/Reviewer_fkWg"
        ]
    },
    {
        "id": "bUVUOZPnZNR",
        "original": null,
        "number": 4,
        "cdate": 1666932517611,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666932517611,
        "tmdate": 1669153495741,
        "tddate": null,
        "forum": "o8xdgmwCP8l",
        "replyto": "o8xdgmwCP8l",
        "invitation": "ICLR.cc/2023/Conference/Paper2335/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes and experiments with the idea of time-based augmentations: Human infants manipulate and move objects in front of themselves, move their eyes, making multiple fixations on an object, and carry objects into different rooms, providing independence of object appearance from background. All of these manipulations can be thought of as time-based augmentations of the data. Using these ideas with self-supervised learning provides a biologically-inspired way of learning representations of objects. The paper tests these ideas on three different self-supervised algorithms on four different datasets, and finds that adding time-based augmentations to standard ones significantly increases performance. The improvements are not in the 1-2% range that some papers show; rather, many results show a 10-20% increase, suggesting a real impact on self-supervised learning. Interestingly, time-based augmentations are not by themselves particularly effective (except in simulation); it is only in combination with standard augmentations that there is a substantial effect.\n\nI have read the other reviews (which I think are substantially unfair, and/or are from reviewers who, for some reason I fail to understand, want to kill the paper), the authors' rebuttals, and skimmed the updated paper. \n\nMy evaluation still stands: This is an excellent and exciting paper, well worthy of publication. It could be a good talk at the conference.",
            "strength_and_weaknesses": "Strengths:\n\n+ The idea is well-motivated by data from infants\n+ The improvements are substantial, and impact three different categories of self-supervised systems.\n+ The analysis of the different factors affecting the outcome is fairly good.\n+ The method is tested in simulation, where the kinds of interaction can be controlled, and on real-world video of object manipulations.\n\nWeaknesses, with concrete, actionable feedback\n\n- The idea is not completely novel; several authors have proposed similar ideas. The difference here is the thoroughness of testing, the systematic analysis, and the very large datasets used, generated via simulation.\n\n- The paper is somewhat dense, due to all the experiments conducted, and is a little difficult to follow at times. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Due to the complexity of the experiments, it is not always easy to follow what was being done.\n\nQuality: This is a very thorough analysis of the effects of \"natural\" interactions on object recognition. The results are impressive.\n\nNovelty: While temporal augmentation has been done before, this paper investigates it in a very thorough and systematic manner.\n\nReproducibility: the code will be made available. \n\nI didn't see any figures showing what the stimuli look like for 3DShapeE? Did I miss it?\n\np2: have shown -> have been shown\nallowing to discard _> allowing the model to discard\nproposed to use -> proposed using\nproposes to learn -> proposes learning\n\np5: we approximate different kind of -> we approximate different kinds of\n\np6:, near top: replace the neither nor construction with either or.\n\np7: 2nd line from bottom: It means that -> This means that \n\nI'm unclear on what the \"speed of object rotations\" means in terms of the sequence of stimuli. Does this mean that object pairs may be in completely different orientations? A picture would help a lot here. It could be in the appendix, if necessary. \n\np8: allows to encode -> allows the model to encode\n\nencoding the shape similarity makes easier category ->\nencoding the shape similarity enables easier category\n\nsimilarity makes easier -> similarity enables easier \n\np9: embodied agents that interacts -> embodied agents that interact\n",
            "summary_of_the_review": "This is an exciting paper, showing that \"natural\" temporal interactions with objects significantly boosts the performance of self-supervised algorithms. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2335/Reviewer_UnyN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2335/Reviewer_UnyN"
        ]
    }
]