[
    {
        "id": "HlUaP2LhipW",
        "original": null,
        "number": 1,
        "cdate": 1666386841213,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666386841213,
        "tmdate": 1666386841213,
        "tddate": null,
        "forum": "osei3IzUia",
        "replyto": "osei3IzUia",
        "invitation": "ICLR.cc/2023/Conference/Paper4995/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides a general derivation of Evidence Lower Bounds in Multivariate Diffusion Models and some parametrization schemes. The paper also introduces MALDA diffusion model under its framework and evaluates it on MNIST and CIFAR10.",
            "strength_and_weaknesses": "## Strength:\n\nThe derivation seems to be unfied and generic. The experimental results seem good.\n\n## Weakness:\n\n### Novelty:\n\nThe novelty of the paper is marginal: \n\n(1) For the theoretical derivations, all the results are readily known from previous papers (e.g., Huang et al. (2021); Durkan & Song (2021), Dockhorn et al., (2021)). The paper is just summarization of the previous derivations using a generic framework. \n\n\n(2) The empirical results are not novel and lack motivation: ALDA is introduced in Mou et al. (2019). The modified version (MALDA) is not well-motivated (at least not shown in the main text) and only marginally different from the ALDA method.\n\n\n### Clarity:\n\nThe paper is a ill-structured and unclear. \n\n(1) Most of the details are deferred to the Appendix, making the paper hard to follow. \n\n(2) The theoretical part of the paper is not structured. It is written as derivations of formula without clear goals. Assumptions are added to the derivations in the middle, e.g. Section 3.3 assumptions on $f(A,s)$; Section 3.4. linear case. Instead, the best way to present theoretical results is through a list of assumptions and theorems. This makes follow-up papers easy to verify the assumptions and use the results without knowing the details of the derivations. (Same philosophy as abstracting codes through APIs, if the authors are more familiar with coding and engineering.)\n\n(3) The experimental part lacks motivations and exposition of the proposed method. E.g., what is the difference between ALDA and MALDA? what is the motivation for MALDA? Do you claim ALDA as your own contribution, as in the abstract and introduction section; or do you give full credit to  Mou et al. (2019)?\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see above.",
            "summary_of_the_review": "The paper is not novel and ill-structured.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4995/Reviewer_rSRA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4995/Reviewer_rSRA"
        ]
    },
    {
        "id": "X7EKpYqXUh_",
        "original": null,
        "number": 2,
        "cdate": 1666543865274,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666543865274,
        "tmdate": 1666543865274,
        "tddate": null,
        "forum": "osei3IzUia",
        "replyto": "osei3IzUia",
        "invitation": "ICLR.cc/2023/Conference/Paper4995/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces Multivariate Diffusion Models which extend standard diffusion models to include auxiliary variables. Towards this end, the authors provide a new learnable inference and generative process that generalizes many familiar diffusion SDEs (e.g. VP) and provides a modified ELBO required for training. The authors also introduce two linear diffusion models in ALDA and MALDA and show that these lead to state-of-the-art BPD and NLL on MNIST and CIFAR10.",
            "strength_and_weaknesses": "First, the paper is exceptionally well written, and the main ideas and motivation are clear and convincing. I really found the idea to include auxiliary variables as a compelling step forward given the numerous examples previously in machine learning (e.g. Augmented normalizing flows). I was initially concerned that having $K$ variables per dimension might incur a high computational cost but table 1 suggests otherwise. \n\nFrom a theory standpoint, I must admit I did not have time to go through the details in the appendix but the items presented in the main paper appeared sound. The main insights of using linear diffusion and computing the transition kernels (eq 15-16) felt a bit opaque to me in the main text but I understand this is fleshed out more in Appendix C. It would be nice if the authors could move some of this material to the main text as well to help readability. The fixed inference parameterization and its extension to a learnable parameterization sounds reasonable, but as a question it appears that we get this for free? So are all MDM models have learnable inference processes by construction? It would be nice to see an ablation on this as table 2 is not very clear on this point.\n\nI did not find many clear weaknesses in this paper. But I will outline some areas of improvement. First tables 2 and 3 can really be combined into one table, the results are identical for MDMs. Secondly, the experiments are a bit limited as they only include MNIST and CIFAR10. Most diffusion papers also include a larger dataset in Imagenet 32 X 32 or Imagenet 64 X 64. It would be good to see if MDMs are equally performant in this setting. Also, it would be nice to get a convergence vs. timestep analysis on ELBO for MDMs vs other diffusion models. Do MDMs converge faster due to higher modeling flexibility? Finally, it appears ALDA is significantly worse than MALDA? The intuition for this is not very clear to me, can the authors provide more details here?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "As I mentioned above the paper is exceptionally well-written and polished. The quality of the presented theory is very high and appears to be thorough based on a quick skim of the appendix. With regards to novelty, the paper is certainly novel but leans on similar themes in past ML research in using auxiliary variables to improve modeling flexibility. \n\nMinor:\n- minor grammatical errors throughout (e.g. \"by compute objectives\" in the hybrid score matching section)",
            "summary_of_the_review": "Overall this paper is of high quality and has a nice theoretical component. The experiments are limited to small datasets but there does not appear to be a significant bottleneck in scaling them. Based on this I recommend accepting this paper with a score 8.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4995/Reviewer_bfsX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4995/Reviewer_bfsX"
        ]
    },
    {
        "id": "1sYR6nF-IA",
        "original": null,
        "number": 3,
        "cdate": 1666680328844,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680328844,
        "tmdate": 1666680328844,
        "tddate": null,
        "forum": "osei3IzUia",
        "replyto": "osei3IzUia",
        "invitation": "ICLR.cc/2023/Conference/Paper4995/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to improve diffusion probabilistic models by considering multivariate diffusions. The motivation inherits previous probabilistic modeling methods that use auxiliary variables. By augmenting the diffusion space, it enables mixing between data dimensions and auxiliary dimensions, leading to better-aligned generative and inference processes. Based on this the paper further demonstrates how to parameterize the inference process, e.g., MALDA. Experimental results are promising in terms of test likelihood estimation.",
            "strength_and_weaknesses": "Strength:\n\n- The paper is well-written and introduces an interesting novel approach for establishing diffusion probabilistic models in auxiliary space.\n- The motivation of the work is explained very well in Section 1 and Section 4, which is helpful for understanding the paper.\n- Technical contributions are solid (to the best of my knowledge).\n- Experimental results achieve state-of-the-art likelihood (in terms of bits-per-dim).\n\nQuestions:\n- How is the auxiliary variable initialized in time zero? Is it sampled from $q_\\phi(\\mathbf{y}^v_0|x)$ during training? It seems that $q_\\phi(\\mathbf{y}^v_0|x)$ can be any distribution in theory. In the paper, an example of $q_\\phi(\\mathbf{y}^v_0|x)=\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ is given, but this could be quite different from the data distribution, i.e., $q_\\phi(\\mathbf{y}^z_0|x)$. Since in the practical implementation the score for each dimension is output from different channels from the same model, I wonder whether a better choice of $q_\\phi(\\mathbf{y}^v_0|x)$ could alleviate the training burden and perhaps results in a better likelihood or faster training?\n\nMinors:\n- In Eq.(18) no text explanation under the underbrace.\n- On page 6 there is something wrong with the typography.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the technical part is of high quality. The novelty is good. The experiments seem reproducible.",
            "summary_of_the_review": "Overall, I think this paper is of good quality, in that it presents a novel and interesting perspective of understanding and a concrete approach for diffusion models, achieving good results on standard benchmarks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4995/Reviewer_yDwB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4995/Reviewer_yDwB"
        ]
    }
]