[
    {
        "id": "ANvN_cpzs4q",
        "original": null,
        "number": 1,
        "cdate": 1666311471162,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666311471162,
        "tmdate": 1666311471162,
        "tddate": null,
        "forum": "8thVleggPV_",
        "replyto": "8thVleggPV_",
        "invitation": "ICLR.cc/2023/Conference/Paper3060/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper tackles the problem of robustness against gradient-free perturbations. It formulates AutoJoin, which is a gradient-free technique consisting of two steps:\n* Perturb the images using a perturbation from the predetermined perturbation set at a sampled intensity level and obtain the latent representations through the encoder, \n* The encoder representations are then passed to the decoder to jointly learn the denoising autoencoder (DAE) and regression model for steering angle prediction.\n\nThe proposed method is tested against gradient-free and gradient-based attacks, improving performance over prior methods while decreasing the per-epoch training time significantly.\n",
            "strength_and_weaknesses": "## Strengths\n \n* **Originality and Quality**\n    * The paper focuses on an important problem of formulating efficient gradient-free adversarial training methods for robust maneuvering against image perturbations. This is the first step in that direction and would interest the community.\n    * The proposed method is novel and simple, improving both the performance and computational cost.\n    * The experiments cover benchmark datasets and compare them with state-of-the-art methods for gradient-free attacks.\n    * The evaluation also spans gradient-based attacks, which strengthens the experimental results.\n\n* **Clarity** Overall, the paper is well-written and easy to read. \n\n---\n\n## Weaknesses\n\n* **Originality and Quality**\n    * **Sanity check for robustness:** While the proposed method is the first attempt for a gradient-free technique, many generative methods have been proposed to defend against gradient-based attacks and have later proven ineffective [1]. Can the authors evaluate the defense against random perturbations with random restarts as sanity check? \n   * **Importance of DAE and missing confidence intervals:** The ablation in subsection 4.2.2 shows that Autojoin obtains marginal better performance with DAE. The paper mentions that it is vital for clean and single MAE; however, without the confidence intervals, it is challenging to analyze the effectiveness of this significant component of the proposed method.\n    * **Additional ablation studies:** The ablation Table 7 highlights the importance of RGB and HSV perturbations. Can the authors include the rows with only RGB and HSV perturbations to dissect the gains in the rows where they are combined with Gaussian noise? I also checked the appendix, but it only contains the same settings on different datasets.\n    * **Gradient-based attacks:** Despite the paper\u2019s focus on gradient-free attacks, the paper includes an evaluation on gradient-free attacks. However, the evaluation is extremely limited due to two reasons. First, the paper evaluates FGSM and PGD attacks. I\u2019d suggest including the evaluation with AutoAttack \u2013 this should strengthen the results significantly. Second, the defenses used for comparison are relatively weak, instead the authors should evaluate with more recent methods (refer RobustBench [2] for the list of recent advances in gradient-based defenses).\n\n* **Clarity**\n    * **Redundant text:** The paper contains redundant text in a few places that can be removed to include more important material from the supplementary material. For example, the paragraph below subsection 4.2 is not required, and instead of having the following as subsubsections, I\u2019d suggest including them as subsections directly. Further, the details in the first paragraph in subsection 3,1 can be moved to the appendix.\n  * Third last line \u2013 Chen et al. in elated work is missing citation. \n   * The paper should include more details on the selection process for $\\lambda_1$ and $\\lambda_2$, along with an ablation showing their effect on the performance.\n* **Reproducibility** The code was not provided with the submission. Since the main contribution of this paper is the empirical analysis of this new strategy, it is essential to provide the code and detailed implementation to evaluate the paper.\n\n---\n## References\n[1] Tram\u00e8r et al. On Adaptive Attacks to Adversarial Example Defenses. NeurIPS 2020.  \n[2] Croce et al. RobustBench: a standardized adversarial robustness benchmark, NeurIPS 2021   ",
            "clarity,_quality,_novelty_and_reproducibility": "The review above elaborates on all these points in detail.",
            "summary_of_the_review": "The paper tackles a significant problem; however, analyzing the effectiveness of the proposed components in the current ablation settings is challenging. The evaluation of gradient-based attacks is not the paper's primary focus but could be stronger in its current form. Without the sanity checks/adaptive attacks, I am worried that the proposed method might be effective due to weak attacks. Therefore, my current score is weak reject, but I am happy to increase my rating during the discussion period if the authors address my concerns.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3060/Reviewer_wNFL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3060/Reviewer_wNFL"
        ]
    },
    {
        "id": "cS9w62cAhI4",
        "original": null,
        "number": 2,
        "cdate": 1666514233108,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666514233108,
        "tmdate": 1666514233108,
        "tddate": null,
        "forum": "8thVleggPV_",
        "replyto": "8thVleggPV_",
        "invitation": "ICLR.cc/2023/Conference/Paper3060/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The author introduces a new efficient adversarial training method, AutoJoin, to efficiently produce robust models for imaged-based maneuvering. Compared with Shen, the method adds a new denoising autoencoder to reconstruction the image so as to remove the noise added to the input image. The method achieves better empirical robustness and efficiency than baselines.",
            "strength_and_weaknesses": "Strength\n\n1. The proposed method, AutoJoin, achieves better robustness and efficiency than baseliens.\n\n2. The experiments and the ablation study are intensive, demonstraing the effectivenss the of proposed method.\n\nWeakness\n\n1. The result about the gradient-based adversarial examples in Table 8 is not complete. If the claim of the paper contains the adversarial robustness of this kind of adversarial examples, AutoAttack [1] should be applied to test the robustness in order to remove the effect of gradient masking.\n\n2. As one of the paper's focus is the efficiency, it is better to list all the details about the training time in a table in the Appendix. Besides, I wonder whether the improvment of the efficiency comes from using less training data. If so, will applying the random select of the training data in line 4 of Alg 1 to Shen reduces as much as training time as AutoJoin?\n\n3. The contribution of the paper should be highlighted. For example, which components are proposed in previous papers and which ones are newly proposed?\n\nMinors\n\n1. It would be better for the authors to add description about Single, Combined and Unseen in the main text.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper needs some modification of writing to highlight the contribution. I think the algorithm is a little complicated with many augmentations. The authors need to release their code to make the result reproducible. ",
            "summary_of_the_review": "The proposed method, AutoJoin, achieves better robustness and efficiency than previous baselines. Some clarification and more experiments are needed. \n\n[1] Francesco Croce et.al., Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks, ICML 2020\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3060/Reviewer_mroR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3060/Reviewer_mroR"
        ]
    },
    {
        "id": "vBktI9zFQL",
        "original": null,
        "number": 3,
        "cdate": 1666605998729,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605998729,
        "tmdate": 1666605998729,
        "tddate": null,
        "forum": "8thVleggPV_",
        "replyto": "8thVleggPV_",
        "invitation": "ICLR.cc/2023/Conference/Paper3060/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is on the topic of  robust maneuvering in view of autonomous driving. The authors propose to augment the training images with gradient free perturbations. Furthermore, they propose to regularize the model training by adding a denoising task where a decoder should reconstruct the original images without the gradient free perturbations.",
            "strength_and_weaknesses": "Strength:\nThe paper is clear (Figure 1 resumes quite well the proposed method) and proposes an extensive set of experiments to support their claim: experiments on several datasets, an ablation study to show that both gradient free perturbations and the denoising auto-encoder regularization improves performance.\n\nWeaknesses:\n1) The authors should more clearly state the differences with Shen et al., 2021 which also propose gradient free perturbations but no denoising auto-encoders.\n2) Maybe adding data augmentations such as RandAugment and adversarial perturbations methods like AdvProp could further support the experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and somewhat novel with the refining of the perturbations and the auto-encoder regularization.",
            "summary_of_the_review": "The paper is clear and well supported with experiments, it reaches SOTA results but the only clear novelty seems to be the denoising auto-encoder as regularization.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3060/Reviewer_Jfom"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3060/Reviewer_Jfom"
        ]
    },
    {
        "id": "JtJDGjpr3_",
        "original": null,
        "number": 4,
        "cdate": 1666697696445,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697696445,
        "tmdate": 1666697696445,
        "tddate": null,
        "forum": "8thVleggPV_",
        "replyto": "8thVleggPV_",
        "invitation": "ICLR.cc/2023/Conference/Paper3060/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a gradient-free adversarial training technique called AutoJoin, which attaches a denoising autoencoder to the original regression model. It shows superior performance compared with the baselines on the benchmarks. The joint learning of steering and denoising reinforce each other.",
            "strength_and_weaknesses": "## Strength\n\n1. This method is simple but demonstrates huge performance improvement.\n1. This paper is well-written and easy to understand, especially with algorithm 1.\n\n## Weakness\n\n1. The novelty is limited. The introduction of a denoising autoencoder for adversarial robustness is already introduced in [1][2].\n\n[1] Feature Denoising for Improving Adversarial Robustness\n[2] Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser",
            "clarity,_quality,_novelty_and_reproducibility": "The method is simple and can be easily reproduced.",
            "summary_of_the_review": "Though the performance improvement is huge, the novelty of this paper is limited, which doesn't meet the ICLR standard.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3060/Reviewer_LDET"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3060/Reviewer_LDET"
        ]
    }
]