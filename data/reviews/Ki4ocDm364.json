[
    {
        "id": "vaTucca-sP",
        "original": null,
        "number": 1,
        "cdate": 1666365460247,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666365460247,
        "tmdate": 1666365460247,
        "tddate": null,
        "forum": "Ki4ocDm364",
        "replyto": "Ki4ocDm364",
        "invitation": "ICLR.cc/2023/Conference/Paper3967/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies offline multi-objective reinforcement learning (offline MORL). In the first half of the paper, the authors introduce a new benchmarking dataset. For 6 tasks it contains trajectories with different preferences, where for each preference we have data from an expert policy (with similar preference profile) and a noisy amateur policy. In the second half of the paper, the authors introduce a new approach for offline MORL, based on return-conditioned sequence modeling. Experiments show that their method outperforms baseline approaches on their own benchmark. ",
            "strength_and_weaknesses": "Strength:\n* The paper is well written and clear. \n* The benchmarking dataset is a useful contribution for the community. \n* Related work is well covered. \n* The paper contains very useful illustrations, such as Fig 1 to illustrate Sec. 3, and Figures 2 and 3 to illustrate the data generation process for the benchmark.\n* Extensive experiments. The generalization performance in Fig. 5 is very impressive. \n\nWeaknesses: \n* For data generation of the amateur policy, you set the exploration parameter p to 65%. This seems rather high to me: in most problems you will not get very far with such high exploration. Couldn\u2019t you vary the amount of exploration, from low to high, for different episodes? Or gradually increase the noise during the episode, to make sure that your amateur policy sometimes gets a bit further in the domain? I think you see this effect in Fig 2: in MO-Swimmer for example you see that the amateur policies cover a quite demarcated area of the return space, indicating that your exploration/noise scheme covers a too small region of the overall return space. \n* You extensively mention Decision Transformers in your abstract and introduction, but actually your best performing models are the MORvS(P) ones, that do not use a transformer. I think you would need to phrase this differently. \n* On the algorithmic side, the innovation is mostly in the application of the (return-conditioned) sequence modeling approach to the offline RL setting. There are some details about how to feed the preferences and returns into the model, with some claims about what worked and did not work (without results though). It is a useful insight to use sequence modeling for offline RL though, since you typically want to stay close to the original data. \n* Top of page 7: You say that a scalarized \\hat{g} and the preference vector omega can recover the vector valued g. This is not true right? Imagine omega = [0.5,0.5], and \\hat{g] = 0.5, then both g = [1,0] and g=[0,1] would work (or any g for which sum(g) = 1.0). I think this explains why you need the elementwise product between g and omega (but why not feed them in completely separately?). \n\nMinor:\n* Sec 2: previous work is \u201cprimarily\u201d demonstrated on tabular tasks \u2192 But not only right? Try to be precise here, i.e., what is your extension? \n* Sec 3: I miss an explanation why \u201csparsity\u201d is a relevant measure? \n* What type of noise distribution do you inject in generating the amateur policy? These are continuous tasks right, so do you use Gaussian noise, or simply a uniform distribution within the bounds of the action space? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good\nQuality: Good\nNovelty: Reasonable\nReproducibility: Work should be reproducible. ",
            "summary_of_the_review": "This paper has two contributions: 1) a new benchmarking dataset for offline multi-objective RL, and 2) a new algorithm class for offline MORL based on return-conditioned sequence modeling. I think the first aspect is a clear contribution, which will be of merit to the community. The second part is also interesting although slightly incremental. However, together I think the paper is a good candidate for acceptance at ICLR. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3967/Reviewer_gXRB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3967/Reviewer_gXRB"
        ]
    },
    {
        "id": "JqtuEyXZzyp",
        "original": null,
        "number": 2,
        "cdate": 1666523627665,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666523627665,
        "tmdate": 1666523627665,
        "tddate": null,
        "forum": "Ki4ocDm364",
        "replyto": "Ki4ocDm364",
        "invitation": "ICLR.cc/2023/Conference/Paper3967/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a reference benchmark for multi-objective offline learning data set which is built on a formerly proposed set of benchmark environments for multi-objective RL consisting of 6 MuCojo environments.\nIn addition, the authors adapt decision transformers and the RL via Supervised learning method to the multi-objective setting. This is achieved by concatenating the preference vector to state, action and return. Furthermore, the return to go is modified to represent the scalarized return.\nIn the experiments, the authors compare to behavioural cloning and conservative Q-learning which is modified to receive the input as preference as well. Results indicate that the supervised learning-based offline-RL approaches outperform compared methods.",
            "strength_and_weaknesses": "Strength:\n* The paper is easy to follow.\n* The paper examines the questions of how well decision transformers (and RLvSL) can be adapted to the multi-objective setting\n* Multi-objective offline RL was not examined on Mucojo environments before.\n\nWeaknesses:\n* The proposed extension of the offline RL methods is relatively straightforward.\n* Results are not too surprising.\n* The novel data set is helpful, but given that the online benchmark was already available it is not too difficult to sample an offline data set.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and mostly easy to follow. \nThough multi-objective RL wasn't examined in the offline setting on the given environments before, the paper's novelty is limited. The technical contribution consists of feeding the preference as an extra input token to the method.\nThe paper provides a variety of information, and thus, I expect the reproducibility is quite high.",
            "summary_of_the_review": "The paper is a solid study of offline multi-objective RL on the MuCojo settings. However, the paper lacks novelty and technical depth. All solutions are kind of relatively straightforward and as compared methods do not fit the problem results are not too convincing. The sampling scheme to generate the offline data seems solid but sampling the data did not require overcoming any technical problems.\nTo conclude, the paper yields interesting results but does not come up with the technical depth and novelty of most accepted ICLR papers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3967/Reviewer_wtvp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3967/Reviewer_wtvp"
        ]
    },
    {
        "id": "7BmewdEMxO",
        "original": null,
        "number": 3,
        "cdate": 1666737504124,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666737504124,
        "tmdate": 1666737504124,
        "tddate": null,
        "forum": "Ki4ocDm364",
        "replyto": "Ki4ocDm364",
        "invitation": "ICLR.cc/2023/Conference/Paper3967/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper the authors propose a flexible multi-objective RL framework that can handle different preferences over objectives, that are not known in training time. They achieve this with an offline RL approach. First, they present a novel dataset of demonstrations designed for offline multi-objective settings. Second, they propose a Pareto-efficient decision mechanism that builds on Decision Transformers with a novel approach for conditioning the policies on preferences and returns. The authors continue to show that their proposed framework closely matches the behavior policies in the demonstrations, meaning it effectively approximates the Pareto front.  ",
            "strength_and_weaknesses": "strengths\n-------------\n- An interesting approach to an interesting (and nontrivial) problem.\n\n- The idea of a sample-efficient way for approximating the Pareto front at execution time seems quite promising. \n\nWeaknesses\n---------------\n- The dataset itself doesn't seem like that much of a contribution, compared to how it is framed in the paper.\n\n- Adding random perturbations to expert demonstrations is not a good way of approximating amateur demonstrations.\n\n- It's unclear how this translates to a more organic multi-objective setting (consider a navigation task where vehicles need to maximize transport throughput while minimizing risk and travel time). \n\n- Not clear how well this would generalize to remote parts of the trajectory space (and preference space).",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper seems fairly well written and organized. The approach seems fairly novel (though incrementally so). Reproducibility seems like it might be challenging from the paper alone, but not egregiously outside the current standard for ML publications.",
            "summary_of_the_review": "Overall an interesting paper with some caveats that need addressing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3967/Reviewer_6PPS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3967/Reviewer_6PPS"
        ]
    },
    {
        "id": "xT1N4AJR_Oa",
        "original": null,
        "number": 4,
        "cdate": 1667117607032,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667117607032,
        "tmdate": 1667117607032,
        "tddate": null,
        "forum": "Ki4ocDm364",
        "replyto": "Ki4ocDm364",
        "invitation": "ICLR.cc/2023/Conference/Paper3967/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This manuscript did two things. One is to propose a new dataset that collects agent trajectories from multiple independent agents with different preferences on the objectives. This dataset contains trajectories from both well-trained agents and semi-trained agents. HalfCheetah is a typical example where actions that are large in absolute value will consume more energy while the agent wants the cheetah to run as fast as possible. Two is to propose a decision transformer-like algorithm that handles an offline multi-objective dataset (which is possibly the one proposed). This algorithm is quite intuitive and the main idea is to condition its output on the preference of the newly tested task. Several experiments are presented.\n\n",
            "strength_and_weaknesses": "Pro:\n\n1. This paper introduces a new dataset that targets the multi-objective RL with offline data.\n\n2. This paper introduces a decision transformer-like algorithm which is intuitive and works in experiments.\n\nCons:\n\nThe baselines in the experiments seem lacking as only two methods (BC/CQL) are compared with. Is any offline RL and inverse RL approach relevant?\n\nQuestions\n\n1. One question is how the two contributions - namely the dataset and the algorithm - are entangled to each other. With the presentation of this paper one would expect the contributions to be independent, i.e. we expect the algorithm to work on other datasets as well. Is there any way to validate this? I understand a similar dataset might not be present in the community for now.\n\n2. Following the last question, is the dataset general enough to be an independent contribution? From the generation of this dataset I could many variants in the process. Should the community use this dataset in the future or it's good only for this work. \n\n3. What is the difference of this work and multi-objective inverse RL? I'm not seeing much difference despite the work follows the vein of offline RL mostly. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is in high clarity and quality. See the previous part for novelty. I have no tell on its reproducibility.",
            "summary_of_the_review": "I think it's quite an interesting problem and the work is solid (intuitive, works well, and is presented well). I believe it's a borderline paper and I lean slightly towards acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3967/Reviewer_oRVV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3967/Reviewer_oRVV"
        ]
    }
]