[
    {
        "id": "QgAFZsz4gy",
        "original": null,
        "number": 1,
        "cdate": 1666530545478,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666530545478,
        "tmdate": 1666530545478,
        "tddate": null,
        "forum": "9eT2pA9P-vI",
        "replyto": "9eT2pA9P-vI",
        "invitation": "ICLR.cc/2023/Conference/Paper1037/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focus on the problem of reducing memory usage in parallel large-scale DNN training. The problem itself is indeed an important research problem that can benefits both the research community and industry. The authors proposed a method (AdamA) to improve upon previous memory reduction methods. The method is that: during gradient accumulation over multiple micro-batch, we can directly integrates gradients into optimizer states and release the gradients. \n\nFrom emipirical study, the authors demonstrated 1) AdamA can further reduce memory compared to previous work, on various settings. 2) AdamA shows similar convergency as Adam baseline. Authors also provide a mathmatical analysis for the convergence. In addition, the authors provide a easy to apply training pipeline that can be used by other researchers. ",
            "strength_and_weaknesses": "Strength:\n1. novelty of the proposed method: to the best of my knowledge, this method is novel and adds quite significant value on top of the baseline gradient accumulation method, in terms of memory reduction. It enables combining benefits of different memory reduction methods.\n\n2. the contributions are mostly well-supported.\n\na) [well-supported] the claimed contribution of the proposed AdamA can enable reducing memory footprints of activations and gradients simultaneously, leading to good amount of memory reduction compared to previous work. This claim is well-supported by multiple empirical evidences, including directly comparing memory allocation and the max model size on both common vision and language datasets/models. Examples include ResNet-50 and Bert-large.\n\nb) [well-supported] the proposed method in parallel training setting could have suffered from heavy communication overhead due to the micro-batch all-reduce operation. The authors proposed to address this by updating local optimizer states without micro-batch all-reduce and update once at the end of mini-batch. This method made the throughput of the propoesd method to be similar to original Adam method. The empirical results supports the claim.\n\nc) [still have concerns] the authors demonstrate some evidence that the proposed method has similar convergence as Adam: mathmatically analysis about the convergence and empirically showing similar performance on ImageNet and GLUE. However these evidence may not be sufficient, plesae see the weakness part.\n\n3. clarity, reproducibility, quality are good (as mentioned in the next question).\n\n\nWeakness (or unaddressed concerns):\n\nThe proposed method made two major changes compared to Adam: one is the 2nd momentum accumulation in the optimizer states by replacing the square of the accumulated gradients to accumulating the square of gradients. the other one is in parallel setting, replacing the all-reduce per micro-batch with local update per micro-batch. These two intuitively may have impact on model performance/convergence, and thus need evidence to show their impact is negligible for the proposed method to be useful.\n\nWhile the authors have shown some evidences, I still have some concerns unaddressed:\n\n1. the convergence analysis is done for the first change, but not the 2nd change (even though it only happens with parallel setting, but I think large-scale DNN basically means we use parallel setting all the time). Also the analysis is based on non-pratical assumptions, such as convex cost function. I understand non-convex scenarios are usually not provided in such analysis. But I do want to call out this weakness to emphasize that we would need strong empirical evidence to support the \"similar convergence\" claim.\n\n2. The empirical evidence of convergency versus memory reduction misses some important results. The authors separately shows the convergence results in Sec 4.1 and memory reduction in Sec 4.2. However for some experiments settings, we can only see one but not the other (e.g. we have memory reduction results of Adafactor etc but not its convergence/performance). Could the authors provide the full comparison (note this should not require new experiments, just need all the metrics that should have been obtained during authors' original experiments)? Specifically, examples could be a table of <method, memory reduction, accuracy metrics> and a table of <method, max model size, accuracy metrics>, on top of table 2 and table 3.\n\n3. The evidence shown on the vision domain is not very strong. The authors used ResNet-50 with Adam as baseline. However we know that Adam is much weaker that SGD on ResNet-50 and can only achieve sub-optimal performance on ImageNet (fig. 3 shows <75% but SGD gives more than 75% I remember). Thus ResNet-50 is not a good model to compare AdamA and Adam. If the author would like to strengthen the claim on vision domain, I would recommend to use some models that originally uses Adam as its strongest optimizer, like transformer based models. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and reproducibility of the paper is good. The paper gives details and sources of the experiments and also provide source code to reproduce. Overall writing quality is good as well: easy to follow the main claims and their supports.\nNovelty of the paper: to the best of my knowledge (however I am not expert in memory reduction in DNN), the method of intergrating the gradient accumulation into the optimier states is novel. The claimed contributions around this method are mostly well-supported (as above mentioned). So the novelty of the paper is good in my opinion.",
            "summary_of_the_review": "As mentioned above, I think the quality of the paper, the novelty, and most of the evidence for supporting the claimed contributions are quite good. My major concern lies on the convergence part. For the proposed method to be pratically helpful to the research community and industry, this convergence part needs to strongly supported. If the author can provide explanantions and evidence to address my concerns, I would be happy to accept the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1037/Reviewer_a8MG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1037/Reviewer_a8MG"
        ]
    },
    {
        "id": "brsBYy3HH5c",
        "original": null,
        "number": 2,
        "cdate": 1666547607677,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666547607677,
        "tmdate": 1666547607677,
        "tddate": null,
        "forum": "9eT2pA9P-vI",
        "replyto": "9eT2pA9P-vI",
        "invitation": "ICLR.cc/2023/Conference/Paper1037/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes AdamA, a memory-efficient version of Adam when combined with gradient accumulation. The idea is simple: instead of accumulating the gradients after each forward and backward step for the micro-batch, AdamA directly updates Adam optimizer states so that the memory footprint for the gradients will be reduced.  ",
            "strength_and_weaknesses": "Strength:\n\n- The idea is simple and direct, it is clear that the proposed method can reduce the memory footprint.\n\n- The presentation of the paper is clear because of the simplicity of the methodology.\n\nWeakness:\n\n- There is a lack of more direct alternatives, e.g.,  why not simply use a smaller batch? Would AdamA be better than small batch in any case?\n\n- AdamA introduces 2X communication overhead compared with the standard data parallelism. \n\n- The experiments are inappropriately designed in the distributed scenario.  \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear because of the simplicity of the methodology.\n\nThis is a mismatch of AdamA's goal and the empirical study, especially for the distributed scenario. Why is the benchmark based on the combination with ZeRO-DP instead of more memory-efficient stages in ZeRO to support larger-scale models (where the memory reduction technique does matter)? For example, ZeRO-S3 (which is also known fully sharded data parallelism)? Can AdamA be combined with ZeRO-S3?\n\nThe source code is attached to provide good Reproducibility.",
            "summary_of_the_review": "The idea is simple, straightforward but with limited novelty; some discussion about more direct alternatives is missing (e.g., why not directly use a smaller batch?). Additionally, AdamA actually introduces more communication overhead (running allreduce over m and v introduces 2X communication compared to allreduce over g). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1037/Reviewer_3RgH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1037/Reviewer_3RgH"
        ]
    },
    {
        "id": "oUHY5kfrcwb",
        "original": null,
        "number": 3,
        "cdate": 1667167363015,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667167363015,
        "tmdate": 1667167363015,
        "tddate": null,
        "forum": "9eT2pA9P-vI",
        "replyto": "9eT2pA9P-vI",
        "invitation": "ICLR.cc/2023/Conference/Paper1037/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers the problem of reducing memory usage for training neural networks. It proposes a memory-efficient version (AdamA) of the popular Adam optimizer. The motivation behind AdamA is to enable gradients to be released after computation over each micro-batch (micro-batching computes the gradient sequentially over a mini-batch to reduce the memory footprint of storing the activations over the mini-batch, while still maintaining the convergence properties of the original mini-batch). To enable the gradients to be released, AdamA incorporates the gradients into the optimization state (m_t and v_t for Adam). Allowing for gradient release enables AdamA to use prior work which computes the backward pass in a layer by layer manner, releasing the gradients for previous layers. By using both micro-batching and gradient release, AdamA can lead to considerable memory reductions.\n\nThe paper empirically validates that AdamA has similar convergence properties as Adam on several benchmarks, while being able to reduce the memory usage by order of a few GBs. \n\nThe paper also provides a proof that AdamA has similar convergence behavior as Adam, though I have some issues with that part.\n\n",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper provides a simple and clean approach to reduce memory consumption for neural network training. The idea is compatible with several other approaches for reducing memory usage, and could be composed with other methods to yield even more savings.\n2. The experimental evidence appears to be quite thorough. The experiments are systematic, and performed over a variety of vision and NLP tasks. They demonstrate that AdamA has very similar convergence to Adam, while providing substantial memory savings.\n\nWeaknesses:\n\n1. The paper proves convergence of AdamA by following the proof of convergence for Adam. Unfortunately, the proof of convergence for Adam has fatal flaws as pointed out by \"On the Convergence of Adam and Beyond\". I tried to go through the proof, and I think the proof in the paper inherits the same flaws. In particular, \"On the Convergence of Adam and Beyond\" shows that Adam in fact does not always converge, and the same counterexample should apply to AdamA as well. It is possible that some of proposed variants to rectify this convergence issue (such as AMSGrad) can be adapted to this setup as well, but it would have to be analyzed.\n\nTo be clear, I do not believe that the theoretical properties are the selling point of the approach. Instead, the selling point is the strong empirical results, which appear to be sound as far as I can see.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is quite well-written. It explains the previous approaches to reduce memory usage quite well, and places the proposed work properly in that context. Several figures, diagrams and detailed algorithm descriptions make the approach quite easy to understand.\n\nOriginality: The paper borrows quite a lot from previous approaches, but as the experimental results demonstrate the proposed modifications lead to significant improvements in practice.\n\nMinor question:\n\nCan the authors comment on the running time of AdamA, as compared to Adam, in some of the considered settings?",
            "summary_of_the_review": "In summary, though the theory in the paper appears to be flawed and this should be addressed, I don't see that as being a fatal flaw. I think the paper could lead to improvements in neural network training and I tend towards acceptance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1037/Reviewer_6Jc2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1037/Reviewer_6Jc2"
        ]
    },
    {
        "id": "0hXX3W4pb_",
        "original": null,
        "number": 4,
        "cdate": 1667783991298,
        "mdate": 1667783991298,
        "ddate": null,
        "tcdate": 1667783991298,
        "tmdate": 1667783991298,
        "tddate": null,
        "forum": "9eT2pA9P-vI",
        "replyto": "9eT2pA9P-vI",
        "invitation": "ICLR.cc/2023/Conference/Paper1037/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides a modification to Adam that allows for gradient updates to be applied \"online\" during gradient accumulation instead of after gradient accumulation has finished. The main selling point of this approach is that it allows for the memory savings from what the authors call \"gradient release\" (also equivalent to the optimization in Zero-2 or Zero-DP P_{g}) to be combined with the memory savings from gradient accumulation.\n\nThe authors demonstrate that this can lead to increased memory savings compared to Zero-DP P_{os+g}, and negligible throughput drop compared to gradient accumulation.",
            "strength_and_weaknesses": "Overall, the optimization makes sense, and why the optimization delivers improved performance in the author's benchmarks is understandable. It's also useful to know that this modification to Adam *can* be done without impairing its convergence qualities.\n\nUnfortunately, from my understanding of this paper, it seems to be of limited applicability. In particular, AdamA *cannot* be combined with Zero-DP P_{os} (i.e. optimizer state sharding) and gradient accumulation without significantly increasing communication costs.\n\nTo summarize my understanding of AdamA, the primary idea is that it modifies the optimizer state update to allow for online updates of the optimizer state. Thus, instead of needing to compute all of your gradient minibatches before updating the optimizer state, you can update the optimizer state (and thus release the gradient minibatches) before all of your gradient minibatches are completed. *Note* that this does require a change to how data-parallel training is typically done. Typically, data-parallel training accumulates your gradients and then all-reduces your gradients across all devices. However, as AdamA would like to release your gradients as soon as possible, AdamA must all-reduce the *optimizer state* across all devices after finishing microbatches. This is a typically an increase in communication volume, as gradients are only one value while optimizer states (i.e. of Adam) are two floating point values.\n\nTo review Optimizer State Sharding, the idea is that instead of replicating *both* your parameters and your optimizer state across all devices (like typical data parallelism), you only replicate your parameters, and shard your optimizer state across your devices. Then, after you compute your gradients, you can perform a reduce-scatter communicate your gradients to the devices that need them (as only the device with the optimizer state of layer N needs the gradients for layer N), perform your parameter update, and then perform an all-gather to replicate your parameters across all devices again. This, notably, is the *same* communication volume as regular data-parallelism (which only performs an all-reduce), as both reduce-scatters and all-gathers are half the communication volume of an all-reduce. Also, optimizer state sharding does not interfere with gradient accumulation.\n\nNow, the issue here is that, from my understanding, one cannot combine all 3 of optimizer state sharding, AdamA, *and* gradient accumulation without a significant increase in communication volume.\n\nThe way that AdamA avoids communication after each minibatch in gradient accumulation is by doing an online update of the optimizer state. *However*, this can only be done as long as your optimizer state is present on every device, which is not true in Optimizer State Sharding.\n\nThus, although the paper presents memory reduction in Figure 6b through combining AdamA with optimizer state sharding, I suspect that the training throughput is substantially decreased.\n\nOTOH, from Figure 6a, we can see that although it reduces the gradient memory, the bulk of the memory is *optimizer states*. So in that setting, I suspect that using optimizer state sharding would reduce the memory more compared to AdamA.\n\nThe rest of the paper seems clear enough to me, but this is my primary issue with the paper. **As my review heavily relies upon my understanding of this point, if I've misunderstood the paper (or am missing other considerations), please let me know and I would be happy to adjust my rating.**\n\n### Conclusion\n\nThe authors claim that \"AdamA can reduce the memory footprint up to 23% with less than 2% degradation in training throughput [compared to Adam baseline without other memory optimization techniques]\" and that \"AdamA can fit 1.26x larger models over ... Deepspeed baselines\". However, from my understanding of the paper, it seems that AdamA cannot *also* fit 1.26x larger models over Deepspeed baselines without a substantial degradation in training throughput. Thus, it seems to me that I don't see a situation where one would use AdamA over optimizer state sharding. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "See above.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1037/Reviewer_u5im"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1037/Reviewer_u5im"
        ]
    }
]