[
    {
        "id": "KKvYwDBc-Nj",
        "original": null,
        "number": 1,
        "cdate": 1666538178885,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666538178885,
        "tmdate": 1666538308149,
        "tddate": null,
        "forum": "XSEBx0iSjFQ",
        "replyto": "XSEBx0iSjFQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3396/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This proposes a retrieval-augmented text-to-image model Re-Imagen for text and image guided image generation. The proposed method Re-Imagen interleaves classifier-free guidance during sampling to ensure both text alignment and entity fidelity and achieve good performances on the two datasets. This paper also introduces a more challenging benchmark EntityDrawBench for the text-to-image generation.",
            "strength_and_weaknesses": "Strength: \n1) This paper introduces an interesting viewpoint for text-to-image generation by involving the retrieval-based images during the generation procedure. \n2) The proposed methods achive good performances on the datasets and the newly-introduced benchmark may be helpful for the text-to-image task.\n\nWeakness:\n1) The comparison is not fair in the experiments. As for the most of the previous methods, the images are generated from the only text inputs. While, the proposed methods involve the images retrieved from texts by the datasets, which will make a big difference on the evaluation of the generated images, especially the FID scores. \n2) The novelty is limited. Although the retrieval-augmented methods achieve good performances and provide another viewpoint for image-guided text-to-image generation, the multiple conditions for diffusion models are not novel. The UNet architecture is also widely used in the diffusion models.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The clarity of this paper is good. The pipeline of the proposed methods is clear and the illustrations are pretty helpful for the readers although there are still lots of typos in the paper.\n\nQuality:The completion of this paper is still good. However the comparisons in the experiments may be not fair. The experiment results especially the FID score cannot support the effectiveness  of the proposed methods.\n\nNovelty: The novelty of this paper is limited. Although  this paper introduces a new challenging benchmark for text-to-image generaiton, the methods of  this paper are not very novel. The Interleaved Classifier-free Guidance seems an implementation trick for the multiple conditions for diffusion models.\n\nReproducibility: The clarify of the proposed methods are clear. It is possible to reproduce the performances of the proposed methods but it may be very challenging due to the lackness of the key codes and details.",
            "summary_of_the_review": "This paper introduces a novel retrieval-augmented text-to-image model and a more challenging benchmark for text-to-image generation. The retrieval-augmented methods may be helpful for the application for the text-to-image generation. However, the retrieved image involving in the image generation procedure makes it not fair to be compared with the previous methods which generates the images only from the texts. The experiements cannot support to prove the effectiveness of the proposed methods. Futhermore, the novelty of the proposed methods is limited. This paper is not good enough for publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3396/Reviewer_oSTW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3396/Reviewer_oSTW"
        ]
    },
    {
        "id": "hWKMcI0sitW",
        "original": null,
        "number": 2,
        "cdate": 1666589415361,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589415361,
        "tmdate": 1666589415361,
        "tddate": null,
        "forum": "XSEBx0iSjFQ",
        "replyto": "XSEBx0iSjFQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3396/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a retrieval augmented diffusion model for the task of text to image synthesis. The idea is to condition the diffusion model on the external knowledge base in addition to the text embeddings that are used in text-to-image diffusion models. Conditioning on the retrieved nearest neighbor embeddings happens using the cross-attention layer. During inference, the model retrieves the image-text pairs corresponding to the input text prompt and then samples the result. Experiments show SOTA results on zero-shot FID on COCO, other benchmark datasets and human evaluation.",
            "strength_and_weaknesses": "Strengths:\n\nThe use of external knowledge base to augment the diffusion models is intuitive and a powerful idea. It is very hard to encode the entire world knowledge in a 2.5B parameter model, hence, these models are bound to fail in rare modalities. External knowledge base comes to rescue in this case.\n\nThe idea proposed by authors is simple, yet quite effective (as shown in  experimental studies). This model can also be finetuned from the original text-to-image model (Imagen in this case), which is something really nice. This way, a lot of compute can be saved.\n\nThe modification to the existing classifier free guidance to have optional conditioning on both nearest neighbor image features and text is also interesting.\n\nThe experimental results are strong. Both on zero-shot FID and human evaluation, the authors achieve SOTA.\n\nWeakness:\n\nI have a few comments, mainly with respect to some of the design choices made. I feel like the authors mainly mentioned these choices, and did not have a good discussion / justification.\n\nOn the choice of number of nearest neighbors during training: The authors just mention that they use k=2. But why just k=2? I can imagine that as k increases, the number of queries and keys in the cross attention would increase, hence the computational burden would go high. Is that the reason why k=2 was chosen? In a ideal case, performing an ablation on k would be good. But I can understand that ablations might be too expensive in these models. Hence, having a discussion on why this decision was made and what considerations were taking into account to end up in this decision would help the research community.\nOn the choice of number of nearest neighbors during inference: Should the same k be used during training and inference? While it might be true that performing an ablation on k during training might be infeasible, performing an ablation on k during inference might not be that hard. It would be great if authors had done that. K is an important parameter in retrieval, hence understanding the sensitivity of k might be beneficial.\nModified classifier-free guidance: The modification to the classifier free guidance makes sense to me. In the implementation, the authors randomly flip a coin and choose one of the two scores - text score and neighbor score in reverse diffusion. Why should this be done? Why not use a weighted combination of both scores. That is, why not use $\\eta \\hat{\\eps}_{p} + (1-\\eta)\\hat{\\eps}_{n}$ as the score? This might increase the NFE\u2019s, but it might be worthwhile to study if this gives any gains. An ideal experiment would be to plot NFE-FID tradeoff for using this combined score vs using the proposed algorithm.\nI would have liked it if authors provided more visualizations. One of the power of techniques like Imagen is the ability to image zero-shot content. For instance, \u201cCorgi lives in a house made out of sushi\u201d. But none of such visualizations is provided in this paper. I am wondering if this ability got worse due to the nearest neighbor finetuning because such samples are not present in retrieval dataset. COCO FID might not measure this as captions in COCO are not containing such novel texts. A discussion on this might be beneficial.  \nA discussion on computational overhead: This is related to 1, but I am curious about what the additional computational overhead is because of the added attention tokens?\nBM-25 seems to be a very simple retrieval algorithm. Will you get more gains by using CLIP text?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The ideas in the paper are reasonably well conveyed. However, as I pointed above, more discussions on design choices might have been nice.\n\nQuality: This is a good quality work. Results are quite strong.\n\nNovelty: The idea of using retrieval in diffusion models is not something new. There are many concurrent papers as authors point out. However, showing these impressive results at this scale is indeed a contribution, one that is valuable to the community.\n\nReproducibility: This is something I am a bit concerned about. Many of the ablations and design choice discussions are missing. The authors did provide other details, but adding some more detailed discussions would help with reproducibility.\n",
            "summary_of_the_review": "I think the paper is a good contribution to the community since it presents good results. However, as I pointed above, some of the ablations, discussions and visualizations are missing. Adding these would have made it a very strong submission.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3396/Reviewer_UC3H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3396/Reviewer_UC3H"
        ]
    },
    {
        "id": "9m0I9aZhW9E",
        "original": null,
        "number": 3,
        "cdate": 1666603723861,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603723861,
        "tmdate": 1666603896746,
        "tddate": null,
        "forum": "XSEBx0iSjFQ",
        "replyto": "XSEBx0iSjFQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3396/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed an approach to deal with the problem, that state-of-the-art models often have difficulty generating images of uncommon entities. The main contributions are 1) a novel retrieval-augmented text-to-image model Re-Imagen. 2)  interleaved classifier-free guidance. 3) a benchmark.",
            "strength_and_weaknesses": "**Strength**\n\n1) The issue that this paper tried to solve is important and critical in nowadays text2image generation. The topic is meaningful.\n2) This paper is well-written and easy to follow.\n3) The qualitative results is compelling.\n\n**Weaknesses**\n\n1) Research topic definition changing.\n\nThe research topic is text-to-image generation, which is presumed to take text as input and image as output. However, though this work means to solve the problem of uncommon entities generation in text-to-image generation, it brings subtle but certainly important changing of the \"research topic definition\". An extra input of a set of knowledge base is needed for the inference of this model, which makes the problem to be like text (with knowledge base)-to-image generation, with more strong conditions. The changing of the definition mainly brings a problem that the usage bar for normal users is actually pulled up.\n\n2) The comparisons on Table 1, Table 2 and Table 3 have some concern in fairness.\n\n(a) For the comparion between Re-Imagen (\u0000=BM25; B=COCO; k=2) to Imagen on Table 1, (i) the # of Params are different, (ii) the training set is different (iii) the metric FID-30K and Zero-shot FID-30K is different, it seems to be arbitrary and misleading to say: \u201cRe-Imagen (with the COCO database) can achieve a significant gain on FID-30K without fine- tuning: roughly a 2.0 absolute FID improvement over Imagen.\u201d. \n\n(b) For the comparison on Table 2, the # of Params are different, it hard to deduce meaningful and solid conclusions. The factors should be disentangled to disclose the key insights and observations in the experiments.\n\n(c) For the comparison on Table 3, the motivation why these three types of visual entities (dog breeds, landmarks, and foods) are picked may be clarified. A column of results of broader entities can be more convincing.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. Will the code and dataset be public available? If yes, which parts of the code/data used will be released?\n2. For the datasets that used in this work, what the licences are? Some concerns are about the validity of the large-scale datasets.",
            "summary_of_the_review": "This paper tried to solve an important problem, and the method is reasonable and works. The main concerns are in the fairness in the comparisons in experiments and the reimplementation of this method for both data and model.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3396/Reviewer_xjWB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3396/Reviewer_xjWB"
        ]
    },
    {
        "id": "AqP78s70jNj",
        "original": null,
        "number": 4,
        "cdate": 1666610424166,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666610424166,
        "tmdate": 1666610424166,
        "tddate": null,
        "forum": "XSEBx0iSjFQ",
        "replyto": "XSEBx0iSjFQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3396/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to utilize the retrieval image(s) for extending the model's ability for generating images. In this way, the model takes the text prompt as the input, by fusing with the noisy predicted by the retrieval images, it can generate the images that the dataset seldom contains. To balance the generation ability among text prompts and retrieved images, the paper also introduces a hand-crafted weighted scheme. Several experiments have been conducted to generate the efficiency of the method.",
            "strength_and_weaknesses": "Strengths:\n\n+ The proposed idea of taking the power of retrieved images for generating better results is simple but effective. The effectiveness has also been demonstrated through the experiments.\n\n+ I enjoy reading this paper, and it writes generally well, though with some typos (e.g., \"on two dasets:\").\n\n+ This paper works on a recent hot topic, which is text-based image generation. The idea of customizing with more conditional images can push forward the development of this area. \n\n+ I appreciate the limitations \n\nWeaknesses:\n\n+ Some parts of the model descriptions may not be clear. To my understanding, the Re-Imagen allows multiple retrieved images as inputs, but how exactly the model balances them or takes them as inputs? How is the model architecture designed? Does this mean the number of retrieved images should be the same as the training? What if the retrieved images share much diverse appearance, how the model handles such a case?\n\n+ I feel the way of improving the classifier-free guidance is rather heuristic. Are there any insights that why the undesired imbalance happens?\n\n+ Though the idea is effective, its underlying technical contribution is rather limited, as adding the retrieval images as another condition is straightforward for implementing the such retrieval-image-augment idea. \n\n+ As an image generation work, I would like to see much more visual results than the paper and the supplementary document provided. It is encouraged to release a project webpage containing more results for demonstrating the generality of the proposed method.\n\n+ Will the model be released publicly? If not, how this work can contribute to the research community is encourag",
            "clarity,_quality,_novelty_and_reproducibility": "The paper writes clearly. Because of the simple idea, I believe it can be easily reimplemented. However, as far as I know, the behind diffusion model has not been released, which makes it hard for reproducibility and benefits the general research. The authors should justify how their works can contribute to general research without many computational resources properly.",
            "summary_of_the_review": "Generally speaking, the effective idea and good results make me lean toward acceptance. However, the rather hand-crafted designed combination way makes the method rather heuristic. The technical contribution is still limited. Thus, I can only put it a little bit over the boundary.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3396/Reviewer_T7pn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3396/Reviewer_T7pn"
        ]
    }
]