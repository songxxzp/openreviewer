[
    {
        "id": "d7uC4y_8dR",
        "original": null,
        "number": 1,
        "cdate": 1666141852710,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666141852710,
        "tmdate": 1666142806309,
        "tddate": null,
        "forum": "DvMDIEFtyjV",
        "replyto": "DvMDIEFtyjV",
        "invitation": "ICLR.cc/2023/Conference/Paper3908/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper targets the topic of unsupervised environment design (UED) in curriculum reinforcement learning. In particular, it modifies PAIRED by using the decoder of a VAE as the task generator. With this improvement, the model decouples task representation and curriculum learning into a two-stage optimization. Thus, the authors observe that the training is more stable and the final reward is higher than existing UED methods.",
            "strength_and_weaknesses": "### **Strength**\n\n* **Interesting topic.** Unsupervised Environment Design is an important direction in curriculum reinforcement learning. Previous works usually suffer from unstable training problems. This paper targets at solving this problem by using the decoder of VAE to generate tasks.\n\n* **Comprehensive experiment.** The experiment part looks good to me. Some important factors are explored respectively.\n\n### **Weaknesses**\n\n* **Things to be clarified.** \n    * The proposed method heavily depends on PAIRED but details of PAIRED are missing in Section 4.3. \n    * In the 4th line of Algorithm 1, what does it mean by \u201cUse adversary to sample latent task vector\u201d? If the adversary is a parametrized model, what distribution is used for sampling z?\n    * What is single-step RL? It seems not a widely used term but the definition is missing. I guess the authors mean that their method does not do sequential generation with the interaction with the environment.\n    * The second difference between CLUTR and PAIRED is hard to follow. More explanation is needed. Why does the state space of PAIRED rely on the underlying POMDP?\n    * No example of permutation parameters is given in the experiment environment. The authors say that \u201cfor a navigation task, a set of obstacles corresponds to factorially different permutations of the parameters.\u201d What is the obstacle in the Car racing experiment? \n    * I don\u2019t think VAE(z, E) in equation (1) is a standard notation. It should be clarified.\n\n\n* **Formulation of CLUTR.** The derivation in Appendix B seems redundant to me. According to Figure 1, variable R actually does not involve the VAE inference since it only depends on another observable variable E. Therefore, Equation (1) is just the ELBO of VAE plus an arbitrary regularization term that relates R and E. I think the authors can directly say that rather than taking a detour.\n\n* **Motivation.** If I understand correctly, this paper proposes to use VAE to generate tasks because of two advantages, i.e., long-horizon credit assignment and permutation invariant. However, searching in the latent space itself is not an easy task since the smoothness of latent space heavily depends on the training and quality of the dataset. So, does this method always outperform PAIRED in general? For example, when most of the uniformly sampled environment parameters are invalid.\n",
            "clarity,_quality,_novelty_and_reproducibility": "### **Clarity**\nThere are some clarification problems (as mentioned in the weakness part) that need to be solved. Although the general idea of the proposed method is easy to understand, a lot of details are missing.\n\n### **Quality**\nThe quality of this paper is limited by the presentation. A clearer organization would improve the quality.\n\n### **Novelty**\nThe novelty of this paper is ok but maybe around the average level. If I understand correctly, the only modification of this paper is replacing the task generator with a pre-trained VAE.\n\n### **Reproducibility**\nNot sure. Code is not provided.\n\n\n",
            "summary_of_the_review": "Due to my concerns about the clarification, I suggest rejecting this paper. The authors are welcome to address my questions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3908/Reviewer_eq4e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3908/Reviewer_eq4e"
        ]
    },
    {
        "id": "iK_ip0H82R",
        "original": null,
        "number": 2,
        "cdate": 1666530164297,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666530164297,
        "tmdate": 1666532022892,
        "tddate": null,
        "forum": "DvMDIEFtyjV",
        "replyto": "DvMDIEFtyjV",
        "invitation": "ICLR.cc/2023/Conference/Paper3908/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes CLUTR, a curriculum learning algorithm based on PAIRED, which decouples\ntask representation and curriculum learning. The task representation is learned \nusing a LSTM-based recurrent VAE, after which the teacher updates the curriculum and \nthe protagonist and antagonist policies are updated according to the regret\nof the generated tasks. The experiments showed empirically that CLUTR outperforms\nstate-of-the-art UED methods in terms of sample efficientyc and generalization\n on a range of tasks, and performs compratably to the non-UED state-of-the-art on the CarRacing task. \n",
            "strength_and_weaknesses": "\nstrength: The paper is well written.\nThe experiments showed promising results and covered a range of hypothesis claimed in the paper. \n\n\nweakness: the novalty of the paper is somewhat limited. The main techniques used in CLUTR are based on VAE and PAIRED, which are existing methods. \n\nI am curious to know the effect of learning the representation of the tasks. \nFor example, how sensitive does the performance depend on the fitting of the VAE representation? Is the VAE represntation is mis-specified, how much does it worsen the performance?\n\n\nMinor:\n* E is not defined in 4.1 \n* equations in 4.1 are better written with $\\log q(z)$ \n* missing space in section 4: Section4.4 -> Section 4.4\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the paper is well presented and well written. The authors claim they will release the code upon acceptance. \n",
            "summary_of_the_review": "The paper tackles the problem of sample inefficiency in RL problems by pre-training a VAE to embed the tasks in to a manifold and update policies based on sampled trajectories from the tasks, however, the novelty of the paper is a bit limited. Having said that. given the empirical performance on the extensive sets of experiments, the proposed method may be helpful for the community in practice. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3908/Reviewer_Ui8Z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3908/Reviewer_Ui8Z"
        ]
    },
    {
        "id": "xZ5HWnkJMH",
        "original": null,
        "number": 3,
        "cdate": 1666547138645,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666547138645,
        "tmdate": 1666618796542,
        "tddate": null,
        "forum": "DvMDIEFtyjV",
        "replyto": "DvMDIEFtyjV",
        "invitation": "ICLR.cc/2023/Conference/Paper3908/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates learning a task representation space for use in unsupervised environment design (UED), with an algorithm called PAIRED. PAIRED trains an RL agent, the adversary, to configure the environment parameters throughout training, in tandem with two RL agents that learn to solve these configurations (each configuration is termed a \"task\"). The adversary is allied with one of the agents, called the antagonist, and seeks to maximize the margin of return by which the antagonist outperforms the other agent, called the protagonist. Thus, PAIRED produces a curriculum that maximizes a lower bound on the regret experienced by the protagonist, leading to a robust minimax-regret policy at equilibrium. This paper then proposes CLUTR, a method that first trains a recurrent VAE over a large number of randomized tasks in a specific domain (e.g. mazes or car racing tracks) and then performs PAIRED over the learned latent space of the VAE. Their results show that PAIRED in such latent-spaces produces more robust protagonist policies than standard PAIRED, which designs tasks in the direct task space.",
            "strength_and_weaknesses": "### Strengths\n- The benefits of performing UED on a learned latent manifold of the task space is well-motivated.\n- The connection between CLUTR and prior works is clearly discussed.\n- The CarRacing experiments compare against a comprehensive set of baselines.\n\n### Weaknesses\nDespite what seem like promising results showing CLUTR outperforms PAIRED, there are several important points that are left unclear and claims that seem to be incorrect:\n\n**The regret objective used by CLUTR is unclear**\n\n- It is not clear whether CLUTR uses the flexible PAIRED objective or the standard PAIRED objective. Appendix D.2 seeks to compare CLUTR with the _standard PAIRED regret objective_, implying the main results use the flexible PAIRED objective.\n- If CLUTR uses the flexible regret objective, then the main experiment results should compare to PAIRED with the flexible regret objective, also known as \"flexible PAIRED\" in Dennis et al, 2020. It must be shown that CLUTR's gains over PAIRED with the standard regret objective are not in fact due to the choice of regret objective, rather than the task representation, as claimed.\n- If CLUTR does in fact use the flexible regret objective, then Theorem 1 from Dennis et al, 2020, which guarantees a minimax-regret policy for the protagonist at Nash equilibrium of the PAIRED game, is no longer directly applicable\u2014since the flexible regret objective changes the nature of the underlying game between the three agents.\n- Moreover, the flexible regret objective was not introduced in Gur et al, 2021 as stated in the first paragraph of Section 3.2, but clearly detailed and evaluated in the appendix of Dennis et al, 2020.\n\n**CLUTR does not have access to the full task space**\n\nWhile PAIRED, in principle, has the ability to generate any 50-block maze, this is not necessarily true for CLUTR. This is because CLUTR is trained on a limited subset of all possible tasks, and the learned latent space is not guaranteed to contain all possible mazes. Therefore, Property 3 of CLUTR, stated on Page 5, is incorrect.\n\n**Interpretation of Figure 4 and 5 is flawed**\n\n- In Figure 5, CLUTR's adversary achieves lower regret than that of PAIRED. This result implies that the latent task representation space makes the optimization of the regret objective a harder problem for the teacher, rather than easier, as claimed in the paper. \n- Figure 4 shows the solved rate of the agent on the _training_ levels proposed by the CLUTR and PAIRED adversaries. Since the goal of UED is to produce an adversarial curriculum, it seems that the higher solved rates achieved by the agent under the CLUTR adversary implies the CLUTR adversary is less effective and does a worse job at optimizing for agent's regret. \n\n**The results do not provide evidence for whether the VAE and CLUTR adversary learn anything**\n\n- While the paper argues that CLUTR has the advantage of avoiding sequential credit-assignment, CLUTR must pay the additional cost of a much larger action space\u2014based on the latent-space dimensionality. Thus, CLUTR may in fact face a much more difficult learning problem than PAIRED. Further, the VAE must also learn a useful representation of the task space, the success of which is not shown in the paper.\n- The paper should provide visualizations of how tasks are distributed within the VAE's latent space to provide evidence that meaningful latent structure is being learned via their method. The training curves for the VAE would also be useful to see.\n- The paper should provide evidence that CLUTR's metrics in Figures 3, 4, and 5 are different from that of domain randomization\u2014which would show that CLUTR's adversary is learning to exploit the structure in the latent space, rather than resorting to a randomized policy.\n\nThese figures should compare to domain randomization (DR), and importantly, show that CLUTR is doing something meaningfully different from randomizing over the latent space (which would be case if the adversary policy has difficulty learning to design in this space).\n\n**Curriculum analysis is lacking**\n\n- The main emergent complexity and curriculum results can benefit from thorough analysis. In particular, the authors should plot the solution path length and number of blocks in training maze tasks proposed by the CLUTR adversary, in comparison to other methods. \n- Since the CLUTR tasks are generated by decoding latent representations using the pre-trained VAE, it is possible that CLUTR produces mazes with more obstacle blocks than PAIRED, which is strictly limited to at most 50 blocks. Prior works (Jiang et al, 2021) show that going from 25 to 50 blocks improves the OOD transfer performance of all methods compared, so it seems that this factor is a confounder.\n- Importantly, this curriculum analysis should compare to DR to show that CLUTR learns a meaningful curriculum.\n\n**Missing several key details**\n\nIn addition to the lack of clarity around CLUTR's regret objective, a few other important details seem to be missing. (See the Clarity section for details.)\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThe paper does a good job of explaining the high-level details of the method, as well as providing background on related topics and the experimental setup. However, there are several important details that are missing:\n- As previously mentioned, it is not clear whether the main results are based on running CLUTR with the flexible regret objective or the standard PAIRED regret objective.\n- The maze navigation results should report the number of training seeds used.\n- It is not clear why the CarRacing experiments compare to the additional Robust PLR and ACCEL baselines, but the navigation experiments do not.\n- In Section 5.4, the method of fine-tuning the decoder throughout training on the agent's regret is not provided anywhere in the paper. Could the authors provide details on how this fine-tuning is performed? It is unclear, as it seems the VAE was not trained to predict the regret values\u2014which are not available during pre-training.\n\n### Quality\n- Overall, the paper has several rough edges and could benefit from copy editing.\n- There are several claims that are incorrect or unjustified by experimental evidence, which are detailed in the Weaknesses section of this review.\n\n### Novelty\n- The idea of latent-space task design has been explored in other works such as Florensa et al, 2017, which the authors should also cite. \n- The idea of learning a task representation for UED has not been previously explored in detail.\n\n### Reproducibility\n- Due to the lack of clarity around key details described in this review, and the lack of experimental code from the authors, the results of this study are not reproducible based on the information available.\n\n**References**\nFlorensa, Carlos, et al. \"Automatic goal generation for reinforcement learning agents.\" International conference on machine learning. PMLR, 2018.",
            "summary_of_the_review": "This paper provides an interesting extension of UED, in which a PAIRED adversary, which generates regret-maximizing tasks for an RL agent, searches for such tasks in a learned latent space. The idea in itself is interesting, but the current paper lacks important experimental details and makes several ambiguous or unjustified claims. Therefore, I cannot recommend this paper for acceptance.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3908/Reviewer_NPYm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3908/Reviewer_NPYm"
        ]
    }
]