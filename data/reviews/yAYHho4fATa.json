[
    {
        "id": "vLAzJtg_1lf",
        "original": null,
        "number": 1,
        "cdate": 1666315497030,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666315497030,
        "tmdate": 1668869844808,
        "tddate": null,
        "forum": "yAYHho4fATa",
        "replyto": "yAYHho4fATa",
        "invitation": "ICLR.cc/2023/Conference/Paper782/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "A variants of GFlowNets with continuous state and action spaces is proposed, based the flow-matching conditions and importance sampling to approximate the sums over children and parents of each state. Low-dimenional control experiments are performed showing that a greater diversity of trajectories are obtained compared to a number of standard RL baselines, as would have been expected.",
            "strength_and_weaknesses": "The main strength is that this is the first published paper exploring a continuous state-and-action with GFlowNets. A fixable weakness is that they claim that GFlowNets as previously published cannot handle continuous states and actions, which is not true (they were proposed in the Bengio et al 2021b paper, section 6). The authors also seem to have misunderstood the proposal in that paper. Nonetheless, Bengio et al 2021b was a purely theoretical paper and did not report experiments with continuous GFlowNets, so this is an important validation. In addition, the proposed approach to handle continuous variables is different from the one proposed by Bengio et al 2021b.\n\nThe mathematical derivations in page 3 and Theorem 1 are not really novel: they are identical or continuous versions of the equations in the GFlowNet papers, but replacing sums by integrals. This should be clarified to give a false impression of novelty. The real novelty is in how the authors propose to approximate the integrals and that is worth explaining in more detail. One concern I have is that importance sampling (used to approximate the integrals) is notoriously problematic in high dimension, due to rapidly increase variance of the estimator. Experiments where different numbers of dimensions are compared could be useful in this regard.\n\nThe authors present a series of experiments on control tasks in a low-dimensional state-space and a terminal reward, where the proposed formalism can be directly applied. This is a first and very interesting, along with the results showing greater diversity with GFlowNets (like earlier GFlowNet papers, but now in a control task with continuous variables), and even better rewards in two of the three tasks.\n\nDetailed comments:\n\n* In several places, e.g. par. 2 of sec. 1, 1st two lines of page 2, 1st par. of sec 2.2, the authors make false claims about GFlowNets for continuous states and actions. Bengio et al 2021b state very clearly that all the math shown in earlier sections can be applied to continous variables by replacing sums by integrals, and they propose using integrable densities and the detailed balance criterion to obtain a tractable training objective (with no need to approximate integrals by sums).\n\n* The statement linking DAGs and MDPs (first sentence of 2.2) seems wrong: in general, the seequences of states associated with an MDP do not form a DAG (we can visit a state several times, i.e., form a cycle, in an MDP).\n\n* The term \"particle\" is used on page 3 but not defined (I understand it comes from the GFlowNet paper but the readers of this paper may not have read them).\n\n* The statement made on page 7 (end of 1st par. of sec. 5) about the proposal regarding continuous GFlowNets in Bengio et al 2021b is incorrect. That proposal does NOT require decomposing the continuous state into bins or discretizing it with continuous residuals. The math that is described can be applied with a purely continuous state. Hence there is no issue of exponential explosion as incorrectly claimed. I copy here some relevant sentences from that paper:\n\"However, for the most part one can replace these sums by integrals in case the states or actions are either continuous or hybrid (with some discrete components and some continuous components).\"  \n\"The challenge is to represent continuous densities on the output, with the need to both being able to compute the density of a particular value (say P (sxt+1 | sit+1, st)) and to be able to sample from it. Computing categorical probabilities and sampling from a conditional categorical is standard fare, so we only discuss the continuous conditional. One possibility is to parametrize sxt+1 | sit+1,st with a density for which the normalization constant is a known tractable integral, like the Gaussian.\"\n\"Other approaches include modeling the conditional density with an autogressive or normalizing flow model\".\n",
            "clarity,_quality,_novelty_and_reproducibility": "The first paragraph of page 2 is not very clear. The authors should simply say that they use important sampling to approximate the integrals over in-flows and out-flows in the flow-matching GFlowNet constraint. The writing and clarifty could generally be improved.\n\nThere is interesting novelty in the paper, as explained above, somewhat in terms of the algorithm and significantly in terms of experiments (a first for continuous GFlowNets).\n",
            "summary_of_the_review": "If the authors fix the false claims about previous work on GFlowNets with continuous valued states and actions (which is localized in a few sentences and should be easy to do without changing the main messages of the paper), the paper should be published. \n\n** update: ** given the changes promised by the authors, I have increased my recommendation from 6 to 8.\n\nThe experiments on control tasks are interesting, and a first for continuous GFlowNets, so quite interesting as experimental proof that GFlowNets can be applied in continuous domains.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper782/Reviewer_Rdez"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper782/Reviewer_Rdez"
        ]
    },
    {
        "id": "3PspzcEJsIl",
        "original": null,
        "number": 2,
        "cdate": 1666367610261,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666367610261,
        "tmdate": 1670254990220,
        "tddate": null,
        "forum": "yAYHho4fATa",
        "replyto": "yAYHho4fATa",
        "invitation": "ICLR.cc/2023/Conference/Paper782/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a continuous formulation of GFlowNet, for both the action and state spaces, by converting those spaces into continuous spaces and converting the sums of flow-matching into integrals. This yields a continuous flow matching objective which in practice is estimated by Monte Carlo integration.\nThe authors show an error bound on the accuracy of the MC flow estimation which decreases loglinearly in the number of samples, provided the real flow function is Lipschitz continuous.\nThis method is then benchmarked against three standard continuous control tasks, where it beats standard RL methods on 2 of them, and in all 3 cases appears to cover much more of the state space than any baseline.\n",
            "strength_and_weaknesses": "### Strengths\n- The paper proposes a sensible, if expensive, approach to making GFlowNets compatible with continuous state spaces\n- The experimental results are encouraging, and compatible with previous work suggesting that GFNs are capable of covering states spaces much better than RL\n- The paper is generally well written, and although some sections could be improved, the overall message is clear\n\n### Weaknesses\n- GFlowNet is a new framework, and isn't deeply understood, mathematically or empirically. There isn't much, if anything, in the paper which suggests that the agents are doing better _because of GFN_.\n  - It's not demonstrated that the learned flow distributions $F_\\theta$ really model the theoretically proposed flows\n  - It's not demonstrated that the current formulation converges\n  - The proposed bound is an interesting result but little is known of Lipschitz constants of flow functions (and that's without going into how large DNN Lipschitz constants can get), which makes it hard to relate the bound to empirical quantities.\n- Some of the evaluations done in the paper seem partially incorrect (see below)\n\n\n### Specifics\n\nEq (11) is a bit weird, the theorem prompt asks us to consider $\\hat F (s,a)$, yet the equations refer to $\\hat F(s \\to s')$ and $\\hat F(s)$. It would be good to clarify exactly what is what.\n\nThe indicator function in Eq (13) seems unnecessary. $R(s_t)$ is by definition 0 for all $s_t \\neq s_f$, and ${\\cal C}(s_f)$ is the empty set.\n\n> For continuous tasks, it impossible to access all state-action pairs to calculate the continuous inflows and outflows.\n\nIt is impossible to enumerate all the children of a state, but it is possible in some cases to take integrals over well defined things like Gaussian distributions. It would be nice if this were addressed in the paper.\n\n> we sample an action probability buffer based on the forward-propagation of CFlowNets, from which we can sample an action with probability proportional to the reward.\n\nThis isn't quite the correct wording, and this formulation is repeated multiple times in the paper. GFlowNets (and actually CFlowNets as well, presumably) sample _terminal states_ with probability proportional to their reward. They do not sample _actions_ with probability proportional to any reward (unless in the bandit setting).\n\nSec 4.3 is a bit confusing. \n\n>  we should find the parent state first.\n\nparent _states_? There should be many.\n\n> a transaction deep neural network\n\nI'm not sure what a _transaction_ DNN is. Do you mean a transition DNN? Transient?\n\n> [$G$ has] $(s_{t+1}, a_t)$ as the input while $s_t$ as the output, and train this network based on $B$ with the MSE loss.\n\nThis assumes that taking action $a_t$ in $s_t$ is the _only_ way to get to $s_{t+1}$. This is not a totally unreasonable assumption, but it seems relatively easy to break: imagine a single joint at $\\theta\\in[0,180]$ degrees which moves an arm by $a\\in[-180,180]$ degrees, now imagine that there is a wall such that moving the joint past $\\theta>110$ degrees makes the arm block against the wall. Then for the state $s_{t+1}=\\theta=110$, there are many possible parents with the same action, e.g. $(\\theta=90, a=20)$ and $(\\theta=100, a=20)$ are both valid parents with the same $a_t$.\n\nEither way this assumption should be clearly stated.\n\n\n> All of these improved policy gradient methods can be classified as aiming at maximizing reward\n\nYes, although to be fair most modern PG implementations include some form of entropy regularization (inducing entropy on the trajectory distribution). This is in some sense the basis of control-as-inference/MaxEnt methods, by maximizing the entropy some cover of the state space is induced.\n\n> what SAC learns is not the true distribution of strictly proportional return, [..] This is different from being directly proportional to reward.\n\nThe wording here isn't quite correct. In the gaussian approximation, no, SAC doesn't exactly learn to be proportional to return, but in the general case, yes. But that is not what differentiates SAC and GFN: SAC (or control as inference) learns something like $p(\\tau)\\propto G(\\tau)$ while GFN learns $p(x) \\propto G(\\tau)$ when $\\tau=(s_1,...,s_T)$ ends at $x$, or to relate the two, learns $\\sum_{\\tau:s_T=x} p(\\tau) = G(\\tau_{s_T=x})$. GFN considers all possible trajectories that lead to a terminal state, while SAC (and PG algorithms in general) are trajectory and return-centric: they do not \"care\" about the specific terminal state which the agent reaches.\n\n\n> Figure 2: Reward distributions on Point-Robot-Sparse Task\n\nI'm not sure I understand Figure 2. If I understand correctly, in Point-Robot, an agent is contained within a 2D surface and must navigate using continuous actions to reach some goal, here (10,5) or (5,10). \"Each time the agent can choose to take a step from any angle to the upper right.\" This suggests to me that the action is the angle, or more accurately, the agent moves in the direction of the vector (1,1) rotated by $a \\pi/4$ radians. After checking the supplementary material this seems roughly correct, the agent moves by $(\\cos(\\theta), \\sin(\\theta))$ where $\\theta = (a + 1)\\pi / 4$.\n\nWhat Figure 2 seems to show is $F(s,a)$, or $V(s')$ for the RL methods, and $R(s')$ for the \"True Reward\" _if the agent were to terminate_ (since we are in the sparse setting, I'm assuming this means the agents only get terminal rewards). This seems incorrect, and I'm not sure why the y axis is called \"Estimated Reward Value\".\n\nI see two problems here:\n- I see no reason for flow matching (even continuous) to predict $F(s,a) = R(s')$  when $s'$ is not terminal, this is because with FM there is no notion of preference over paths (see Figure 10 of Bengio et al. 2021a). If this plot was obtained by setting `cnt_step` to 11, this should be explicit.\n   - Similarly for the RL baselines, I see no reason for RL agents to prefer going up or right at (7,7) proportionally to $R(s')$, in fact, the solution found by TD3 might be just fine, it wants to go up to get closer to (5, 10).\n   - Again for the RL baselines, it's not clear why $V(s')$ should be equal to $R(s')$ unless we're terminating.\n   - The straight line for DDPG is honestly suspect. Looks like a bug?\n- As far as I can tell in the current code the agent never sees the current timestep. This is a problem since, as I point out, the plot of Figure 2 only makes sense if (7,7,`cnt_step=11`) is the current state, and more generally, any state has multiple paths which lead to it. Depending on the current timestep the flow predictions for a state _must_ be different (think about the (7,7) example, if there is only 1 action left, then the flow F(s,a) is equal to the reward of the next state, but if there are 2 actions left, then there are many more accessible states and rewards, and so F(s,a) will be larger). As suggested by Bengio et al 2021b S3.3.1, augmenting the state with the current timestep automatically induces a DAG and would make much more sense here.\n\nIf this was truly a terminal reward plot, then it would be a bit reassuring, but it's not obvious to me that this implies that the flow function is fit well in the rest of the state space.\n\n\n\nSome typos: theoritical -> theoretical, \"a cyclic will occur\" -> a cycle will occur, \"need to sum\" -> needs to sum, \"the set contains all\" -> the set that contains all, \"that starting in s0 and ending in s\" -> that starts in s0 and ends in s, \"to form a cyclic\" -> to form a cycle, \"we can modified equation 19\" -> we can modify equation 19, \"has good sample-efficient property\" -> has good sample efficiency.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is fairly clear, and this is as far as I know the first attempt to make GFlowNets continuous. I am confident that I could reproduce these results.",
            "summary_of_the_review": "I am mitigated, on one hand this is a cool and necessary extension of the GFlowNet framework towards continuous domains, on the other hand there is little evidence that the method does what is suggested. Perhaps it does fit flows and learn the right thing, but since relevant quantities are not measured empirically it is impossible to know.\n\n~As is I think the paper needs improvements in order to be accepted, improvements that I think could be addressed in a rebuttal time-length.~\n\nUpdate: The updated revision of the paper is an improvement and makes many things clearer, and so I am changing my score from 5 to 8.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper782/Reviewer_T9R9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper782/Reviewer_T9R9"
        ]
    },
    {
        "id": "B-h_Pw9TS61",
        "original": null,
        "number": 3,
        "cdate": 1666537795548,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666537795548,
        "tmdate": 1670884030847,
        "tddate": null,
        "forum": "yAYHho4fATa",
        "replyto": "yAYHho4fATa",
        "invitation": "ICLR.cc/2023/Conference/Paper782/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper extends the work of (Bengio et al., 2021a) on Generative Flow Networks (GFlowNets), which was limited to discrete state and actions spaces, to continuous control tasks. The authors introduce the notion of continuous flow $F$, and they formulate the conditions required by this function $F$ to correspond to a properly defined Markovian flow, via a generalization of the flow-matching condition of (Bengio et al., 2021b) to continuous states and actions. This new framework is called CFlowNets. Similar to GFlowNets, CFlowNets  While this provides theoretical guarantees, this condition involves integrating over the whole action space, which is typically impractical; fortunately, the authors devise a practical approximation of the flow-matching condition that only involves sample actions only to estimate the outflow, as well as a separate neural network, called the transaction network, that is responsible for estimating the inflow. The authors also provide theoretical guarantees on the soundness of their approximation. Finally, CFlowNets have been applied successfully to 3 different continuous control problems, both in terms of estimating the reward distribution accurately, and in terms of raw performance and exploration on sparse reward environments.\n\n---\n\n*Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation, 2021a.*\n\n*Yoshua Bengio, Tristan Deleu, Edward J. Hu, Salem Lahlou, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations, 2021b.*",
            "strength_and_weaknesses": "**Strengths**: The generalization to continuous state and action spaces proposed here is a very important step towards making GFlowNets applicable to a broader setting beyond discrete states. These generalizations are for the most part natural extensions of prior results, replacing summation by integration anywhere applicable, which makes this new formulation very appealing. And while some extensions to hybrid states and actions have been proposed in the past (Bengio et al., 2021b), CFlowNet is the first work to show empirical evidence of the effectiveness of flow networks in continuous settings. The practical implementation of CFlowNets via sample actions and the transaction network is also well thought, and the theoretical guarantees about the approximation error (Theorem 2) is appreciated to ensure that these approximations are valid.\n\n**Weaknesses**: Unfortunately, this paper falls short in big ways theoretically, with many approximations in their formulation of continuous flows. The authors are too often ignoring some technical aspects, as well as making implicit assumptions that are crucially missing.\n\n 1. The very first definition (Definition 1) of the continuous state flow as $F(s) = \\int_{\\tau:s\\in\\tau} F(\\tau)d\\tau$ is ill defined: what guarantees do we have that this integral is properly defined and finite? I guess you would have to assume that $F$ defines a finite measure over the space of complete trajectories $\\tau$. This is only one example, but none of the integrals used throughout the paper have any guarantee to exist.\n 2. Related to 1., the forward transition probability and backward transition probability in equations (5) & (6) are only properly defined if all the terms involved are well defined and finite.\n 3. The formulation of CFlowNet requires a generalization of the notion of parents and children to trajectories over continuous states (as opposed to parents and children in a DAG for (discrete) GFlowNets). However the definition of parents of a state, preserving the acyclicity constraint, is incorrect (and similarly for children). In the paper (p.3), the parents $\\mathcal{P}(s_{t})$ of $s_{t}$ is defined as the set of states $s$ such that (1) $T(s, a\\in A) = s_{t}$ an action $a$ could make a transition from $s$ to $s_{t}$, and (2) $T(s, a\\in A) \\notin \\mathrm{set}(s_{0}, \\ldots, s_{t-1})$ the state cannot be transferred to a preexisting state, otherwise a cyclic would occur. This implicitly assumes that we are taking a specific (partial) trajectory from $s_{0}$ to $s_{t}$, and we don't want to \"wrap around\" that particular trajectory with any action. However, this condition should be satisfied **for any** trajectory from $s_{0}$ to $s_{t}$, not just a single trajectory. The definition of children is also incorrect and should be defined **for any** trajectory from $s_{0}$ to $s_{t}$, and this has very serious implications later in the paper. I am including an **Example** below to explain why this criterion is incorrect.\n 4. A big consequence of 3. is that Remark 2 in the paper is incorrect, and you must handle the acyclicity carefully, even in practice (this is not simply for \"theoretical convenience\" as claimed in this Remark. See the **Example** below (point 2). A similar claim is made in Section 4.4 too (the measure of $\\mathcal{A}$ is constant).\n 5. In Assumption 1, you assume that $a \\mapsto F(s, a)$ is a Lipschitz continuous, with Lipschitz constant $L_{s}$. However since the state space is itself continuous, how can we guarantee that $\\sup_{s\\in\\mathcal{S}}L_{s}$ is finite?\n 6. You implicitly make the assumption that the state $\\mathcal{A}$ has finite measure (using $\\mu(\\mathcal{A})$), and has finite support (using $\\mathcal{diam}(\\mathcal{A})$). Moreover, the action space $\\mathcal{A}$ must be a measured space defined ahead of time, so that $\\mu(\\mathcal{A})$ is a fixed constant, and therefore it is incorrect to claim that $K/\\mu(\\mathcal{A})$ may be considered as a hyperparameter $\\lambda$ (Section 4.4). Otherwise, if $\\lambda$ is truly a free hyperparameter, what prevents you to set it to $1$?\n 7. According to (Bengio et al., 2021b), a GFlowNet defines a distribution proportional to the rewards **over terminating states** (to borrow their naming conventions, i.e. the parent states of the terminal states $s_{f}$). This is not what is claimed in this paper. For example Section 1: \"*In contrast, the training goal of GFlowNets is to approximately sample candidate actions with probability proportional to a given reward function*\", Section 4.1: \"*we can sample an action with probability proportional to the reward*\", Section 5: \"*generating policies that sample objects through discrete action sequences with probabilities proportional to a predefined reward function*\", Section 5: \"*for CFlowNets [...] the probability of an action being sampled is proportional to the corresponding reward*\". There seems to be some confusion between the distribution proportional to the rewards found by GFlowNets/CFlowNets and the policy (which, according to equation (5), is proportional to the continuous flow). While the flow itself depends on the reward, it is misleading to claim that the policy samples actions proportionally to the rewards.\n\n---\n\n**Example**: Consider the 2D point-mass environment, where the agent moves on a square of size 2 around it with a certain angle. The state space is $\\mathcal{S} = \\mathbb{R}^{2}$ is any point on the plane, and the action space $\\mathcal{A} = [0, 2\\pi]$ (the direction in which the agent moves). Given a state $s = [x, y]$, and an action $a = \\theta$, the environment transitions to a new state $T(s, a) = s' = [x', y']$, where $x' = x + \\mathrm{clip}(\\sqrt{2}\\cos \\theta, -1, +1)$ and $y' = y + \\mathrm{clip}(\\sqrt{2}\\sin \\theta, -1, +1)$. We assume that we start at the initial state $s_{0} = [0, 0]$.\n\n - Since the actions are completely reversible (we can apply action $\\theta$ and then $\\theta + \\pi \\mod 2\\pi$ to get back to the same state), the set of parents $\\mathcal{P}(s_{t}) = \\emptyset$ for any state $s_{t}$ is empty (except all the states on the square of size 2 around $s_{0}$, whose only parent would be $s_{0}$), because if $s\\in\\mathcal{P}(s_{t})$ such that $T(s, a) = s_{t}$ and there exists $s'$ such that $T(s', \\theta) = s$ ($s' \\rightarrow s$ along the trajectory), then we have $T(s, \\theta + \\pi \\mod 2\\pi) = s'$, which violates the condition of $s\\in\\mathcal{P}(s_{t})$. This is a very restrictive notion on parents; but let's imagine for the remainder of this example that we allow situations where the set of such states is of measure $0$ instead.\n\n - Consider the state $s_{t} = [2, 0]$. We have many ways to arrive in $s_{t}$ with a trajectory of length 2; for example $[0, 0] \\rightarrow [1, 0] \\rightarrow [2, 0]$, or $[0, 0] \\rightarrow [1, 1] \\rightarrow [2, 0]$, etc... Effectively, this means that the set of parents $\\mathcal{P}(s_{t}) \\subset \\mathrm{set}([1, y] : y \\in [-1, 1])$. In particular, since for obvious acyclicity reasons the set of children of $s_{t}$ must be distinct from the set of its parents, the set of valid actions satisfies $[3\\pi/4, 5\\pi/4] \\not\\subset \\mathrm{set}(a\\in\\mathcal{A} : T(s_{t}, a) \\in \\mathcal{C}(s_{t}))$. In other words, the whole interval $[3\\pi/4, 5\\pi/4]$ are invalid actions from $s_{t}$, and this does not have measure $0$ in $\\mathcal{A}$, and therefore we can't ignore invalid actions contrary to what is claimed in Remark 2. The situation here is even worse, because we only considered trajectories of length 2 so far; in fact, we can find trajectories so that the parents of $s_{t}$ are all the states on the square of size 2 around $s_{t}$, meaning that $\\mathcal{C}(s_{t}) = \\emptyset$.\n\n - Consider the state $s_{3} = [3, 0]$. If we follow the definition of $P(s_{3})$ in the paper, we need to find states $s_{2}$ such that $T(s_{2}, a\\in \\mathcal{A}) = s_{3}$ and $T(s_{2}, a\\in\\mathcal{A}) = \\mathrm{set}(s_{0}, s_{1}, s_{2})$. For similar reasons as above, a parent state will have the form $s_{2} = [2, y]$, with $y \\in [-1, 1]$. If we only consider a specific trajectory $(s_{0}, s_{1}, s_{2})$ (as is again implied by the argument on the set of measure 0 in Remark 2), then we can take the trajectory $[0, 0] \\rightarrow [1, y] \\rightarrow [2, y]$; modulo the first remark above about sets of measure $0$, state $s_{2}$ satisfies the criterion for being a parent of $s_{3}$. However, we have other trajectories $[0, 0] \\rightarrow [1, y'] \\rightarrow [2, y]$, for any $y' \\in [0, y]$ that are also trajectories, and are distinct from the single trajectory we had to consider before to satisfy the criterion for $\\mathcal{P}(s_{3})$, and in those case, we can reach $[1, y']$ from $[2, y]$. This would violate the acyclicity condition, because we can find for example a cycle of the form $[1, y] \\rightarrow [2, y] \\rightarrow [1, 0] \\rightarrow [2, 0] \\rightarrow [1, y]$. Moreover, the set of such cycles is not of measure 0. That's why the condition for parents should be at least over **all possible trajectories** leading to $s_{3}$.\n\nNote that this example probably suggests that some conditions on the MDP are necessary for the framework of CFlowNets to be valid (similar to how the DAG assumption was necessary in GFlowNets).\n\n---\n\n*Yoshua Bengio, Tristan Deleu, Edward J. Hu, Salem Lahlou, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations, 2021b.*",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, and very clear. The authors provided the source code (in the supplementary material), along with all the hyperparameters used in the Appendix.\n\nSome extensions of GFlowNets to hybrid state and action spaces have been introduced in (Bengio et al., 2021b), including a method to handle continuous states and actions via a (normalized) distribution over actions. Here, CFlowNets are novel in the sense that they do not require an explicit normalized distribution as a policy, but is defined similarly to GFlowNets in terms of continuous flows. Moreover, to the best of my knowledge, this work is the first to empirically show the effectiveness of flow networks on continuous control tasks.\n\n---\n\n*Yoshua Bengio, Tristan Deleu, Edward J. Hu, Salem Lahlou, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations, 2021b.*",
            "summary_of_the_review": "I am very hopeful with this submission, and I believe this could be a very important step toward applying flow networks to continuous control problems. The fact that the authors showed empirically that CFlowNets offer an advantage on exploratory tasks is also an important evidence. I have unfortunately not been able to thoroughly review the proofs in the Appendix, as well as the source code provided, due to the short period of time for the reviews.\n\nDespite my enthusiasm, I strongly believe that in its current state the paper cannot be accepted. The paper is making too many approximations on the theory, and those missing details are crucial. The list I provided is certainly not exhaustive, and most of the theory part of this submission must be reworked with care. That's why I can't say that the paper \"has minor issues that only require small changes\" (in **Correctness**), because this requires major changes, albeit technical.\n\nBecause of this, I am currently recommending borderline reject.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No Ethics Concerns",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper782/Reviewer_Y6ub"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper782/Reviewer_Y6ub"
        ]
    },
    {
        "id": "rTiVd6--XMK",
        "original": null,
        "number": 4,
        "cdate": 1666630223278,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630223278,
        "tmdate": 1669825058236,
        "tddate": null,
        "forum": "yAYHho4fATa",
        "replyto": "yAYHho4fATa",
        "invitation": "ICLR.cc/2023/Conference/Paper782/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a generalization of GFlowNets to environments with continuous action spaces. The continuous version of the flow matching condition, which involves integrals rather than sums, is approximately enforced by using a Monte Carlo estimator of the in- and out-flows. Experiments are performed on three continuous control domains, where the proposed algorithms show strong performance.",
            "strength_and_weaknesses": "Strengths:\n- (+++) This is the first paper to consider GFlowNets with continuous action spaces, which is an important methodological step.\n- (+) It is also the first time that GFlowNets are applied to reinforcement learning environments (discrete or continuous).\n- (+) Clear and (mostly) theoretically sound exposition.\n  - But see my suggestions / comments below.\n- (+) **Update, 10 November:** A strength that I neglected to point out in the initial review is that this paper considers environments where inversion of actions cannot be done analytically -- the \"transaction network\" has to be learned to find the parent (state,action) pairs. This is an interesting idea and the first time such a method is combined with GFlowNets.\n\nWeaknesses / questions:\n- (--) It is hard to interpret and contextualize the experiments, at least for a reader who is closely familiar with GFlowNets but not with these particular RL environments. It would be very helpful to have some figures showing some sampled trajectories in each environment.\n  - Even better would be a demonstration on a toy environment, like a continuous generalization of the 2D hypergrid from [Bengio et al., 2021a].\n- (-) The proposed approach to GFlowNets with continuous action spaces has some weaknesses compared with alternatives. (Ideally, these alternatives would be compared with in the experiments, but at least they should be discussed in the text.)\n  - [Bengio et al., 2021b] writes that the detailed balance (DB) objective, which requires estimates of a state flow and parametric forward and backward policies, can be applied to continuous action spaces: the policy likelihoods in this objective is simply replaced by probability densities. The trajectory balance (TB) objective from [Malkin et al., 2022] can be applied in this setting as well.\n  - DB and TB have an advantage over flow matching in that they do not require evaluating the flow model on multiple parents of a state, and this advantage becomes more prominent in terms of computation cost when the number of parents is large (for discrete spaces). In the setting of this paper, the computation cost grows with the number of samples used for the estimation of inflow, and thus DB or TB could have a strong computational advantage, in addition to the known benefits of TB for faster convergence.\n  - The issue of the variance of the in- and out-flow estimators should receive more attention. (Is the bound involving the Lipschitz constant practically useful?) Also, it should be mentioned that even if the the in-flow and out-flow estimators are unbiased, the estimators of **log**-in- and out-flows, which are used in the loss of equation (20), are not unbiased.\n- (-) On the mathematical formulation: The definition of child set on p.3 uses the trajectory history, which makes it impossible for the Markovian condition on the density of the forward policy to hold (though in many settings it holds almost everywhere).\n  - In addition, this definition looks tailored to the case when the sets of states reachable in any given number of steps are open sets in a fixed \"universe\", which is natural for modeling motion in space, but is not very general. For instance, it does not make sense to talk about new states equalling past states in settings where the dimensionality of the reachable state space grows with the number of steps, such as the obvious continuous generalization of [Zhang et al., 2022b].\n  - I wonder if a better approach to acyclicity would be to remove the constraint in the definition of child set and instead to say something like: \"in our experiment domains, **the set of trajectories in which states repeat has zero measure under the induced trajectory flow** if all policies are absolutely continuous with respect to the standard measure on A, so such trajectories do not affect the analysis\", similar to current Remark 2.\n  - On a related note, I did not find in the text the answer to whether the time step (number of actions from the initial state) was used as an input to the policy model. If the time step is part of the state, then acyclicity is guaranteed. It does not change the math in this case, but could be necessary in RL problems with discrete action spaces, where not appending the time step would result in cycles having nonzero measure.\n  - (A **very** simple way to deal with the cycle issue without having to talk about zero-measure sets: say that the state includes the information about the time step, so cycles are impossible, but the policy ignores the time step part of the state.)\n- (-) Related work could use some improvements:\n  - \"In Malkin et al. (2022), the trajectory balance loss is proposed for GFlowNets to explore the capabilities of previously used objectives\" -- this isn't accurate. In fact, that paper proposed and tested a new objective (TB) and tested another objective that had not been used before (DB).\n  - \"In Bengio et al. (2021b), an idea based on hybrid state is pre\u0002sented to make GFlowNets suitable for continuous tasks\" -- this is also inaccurate (see the comment on DB and TB for continuous spaces above -- the proposal in that paper does not require hybrid states).\n  - The differences between CFlowNets and continuous SAC (pp.7-8) are hard to understand and could be reworded. To my understanding, (1) means that a Gaussian policy in SAC is less expressive than one that uses a general unnormalized action p.d.f. F(s,a), and (2) means that SAC wants each trajectory leading to a state to have the same likelihood (which should be proportional to the reward), while CFlowNets want the sum of likelihoods of trajectories leading to a state to be proportional to its reward.\n\nQuestions and minor comments:\n- How were hyperparameters chosen for CFlowNets and baselines?\n- Equation (3) seems to have a bug. As written, the integral on the right is usually 0, since it is taken over both $s$ and $a$, but restricted to a lower-dimensional subspace of ${\\cal S}\\times{\\cal A}$. I think it should instead be this: $\\int_s\\left(\\sum_{a:T(s,a)=s_t}F(s,a)\\right) ds$.\n  - Should there also be a compactness or similar assumption on $A$, so that any function on it defined by a neural net is integrable (and, less importantly, has finite Lipschitz constant)?\n- Please use curve markers or line styles, and not just colours, to distinguish algorithms in Figure 3.\n- Some of the citations to GFlowNet work are incomplete.\n  - [Bengio et al., 2021a] was published in NeurIPS 2021.\n  - [Zhang et al., 2022b] was published in ICML 2022.\n  - [Deleu et al., 2022] was published in UAI 2022.\n  - [Malkin et al., 2022] will be published in NeurIPS 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "Quality, clarity: Good. No major writing issues; the exposition will be easy to follow for a reader who is familiar with GFlowNet basics.\n\nOriginality: Very good. However, there could be a better discussion of the relationship to other recent work on GFlowNets, specifically, possible limitations in comparison with DB [Bengio et al., 2022b] and TB [Malkin et al., 2022] objectives.\n\nReproducibility: Good. Simple and illustrative code is provided.",
            "summary_of_the_review": "My main reason for leaning positive for this paper is the novelty of the problem studied in two respects: using GFlowNets for RL tasks to encourage exploration and providing the first experimental validation of continuous-time GFlowNets, albeit with a different approach that what has been suggested -- but not empirically tested -- in past work. ~There are some issues with the mathematical foundations, but I believe they are fixable by introducing some mild assumptions.~ Post-rebuttal update: The response have adequately addressed my questions and concerns. I would like to see this paper published because it contributes several important GFlowNet firsts (continuous action spaces, application to RL problems, learnable inversion of actions).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper782/Reviewer_y8C2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper782/Reviewer_y8C2"
        ]
    }
]