[
    {
        "id": "uLo6wMdJtFj",
        "original": null,
        "number": 1,
        "cdate": 1666527366010,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666527366010,
        "tmdate": 1669199109443,
        "tddate": null,
        "forum": "QcA9iGaLpH4",
        "replyto": "QcA9iGaLpH4",
        "invitation": "ICLR.cc/2023/Conference/Paper5712/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper discusses memorisation of training examples in deep neural networks. \n* In particular, the paper shows how the memorisation profile of an architecture varies according to the depth of the networks.  The paper also shows what kind of examples are more or less memorised in deeper and shallower models.\n* The paper discusses how this is impacted by distillation and carries out a similar study, as above, for distilled models.\n* Finally, the paper studies how computationally efficiently computable metrics of memorisation vary from the exact but intractable one.",
            "strength_and_weaknesses": "## Strengths\n* Understanding the impact and diversity of memorisation in modern deep neural networks is an important topic for both understanding neural networks as well as for various notions of reliability of neural networks.\n* The paper is easy to read and does a good job of balancing the mathematical notations and textual verbosity. \n* The authors ask specific questions and designs experiments to validate or negate their hypothesis. I find this approach of empirical validation or falsification quite well done.\n\n## Weaknesses\nWhile the paper provides an interesting empirical study on a fine-grained understanding of memorisation in deep neural networks,  I think the paper lacks in two ways (I illustrate them further in the summary below)\n * Several works have now linked memorisation to other properties like sparsity, robustness, and privacy. I think it would be good to see how the outcomes of this study relate to this. If the authors think a comprehensive study is outside the focus, I think, there should at least be a discussion with some preliminary results. This will not only inspire future works but also increase the relevance of this work.\n* An empirical study where depth and architecture are pain axes of variation, the authors should discuss something more than ResNet and MobileNet. Even all the Mobilenet results have been relegated to the appendix. I think the main paper should include those but in addition, another relevant architecture type should be added.\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\nThe clarity of the paper is very good. The figures are usually clear but in some cases (pointed in the summary below) has confusing line styles and no legends.  \n\n## Novelty and Originality. \n\nThe paper builds of prior work, which is not a drawback but rather a plus. However, I think the paper can improve its novelty and originality by also discussing its impact on other topics like sparsity, privacy and robustness as discussed below.",
            "summary_of_the_review": "## Main points\n\n* The argument that _\"While the study of memorisation in large neural models is fascinating, its practical relevance is stymied by a basic fact: such models are typically inadmissible for real-world settings with constraints on latency and memory. Instead, such models are typically compressed via distillation (Bucila et al., 2006; Hinton et al., 2015)\"_ is rather weird and does not help with the motivation of the paper. Further, the citations provided are also not very relevant for models that are deployed in the real world today.\n\n\n* One of the motivations of the model is the work of Feldman et. al. 2019 , whose main argument is that in certain distributions memorisation is necessary for generalisation. However, this paper also mentioned _\"Consequently, a natural hypothesis is that memorisation of individual training samples might initially increase (as a weak model may have insufficient capacity to memorise), but should eventually saturate or reduce (as more expressive models generalise better).\"_ is a contradiction of that argument and is thus, in some way, self contradicting.\n\n* The observation that memorisation score decreases for larger models is an interesting observation. I find Figure 2(a) and 2(b) to be an excellent illustration of this. \nDoes this possibly explain why adversarial robustness of deeper models is better ? For example, this is observed in Appendix B in Madry et. al. and Paleka et. al and the references therein argues that memorisation of (noisy) training data provably leads to higher adversarial error. I think this and other kinds of similar study on the impact of this observation would make a very nice addition to this paper.\n\n* Figure 1(a) is also a good illustration of the fact that there are different kinds of examples that are memorised. Do the _increasing_ examples correspond to the human noisy labels as observed in Wei et. al ?  If that is the case, that deeper models do not need to memorise (or not as much) noisy labels, then it must also have impact on private learning as well as learning with compressed models with them ? For example, Sanyal et. al. show that private models are more inaccurate examples with higher memorisation score and similarly Hooker et. al. for compressed models but not talk about what kind of \"high memorisation\" scores they are inaccurate on whether _increasing_ or _decreasing_. I think this would be a great addition to this study. Similar questions also apply for adversarial training as observed in Xu et. al 2021.\n\n* All the mathematical metrics here are expectations of random variables where the expectation is over the randomness of the training algorithm. I would like to see what is the empirical variance as well. The colour map, if I understand correctly, shows the variance across examples but not the variance due to the randomness in the algorithm.\n\n* As this is an empirical study, I find it to be limiting to only discuss ResNet and MobileNet. It would be nice if the paper should also discuss pure convolutional architectures at least if not transformer architectures to make the study more comprehensive.\n\n* That distillation inhibits overall memorisation and particularly those that display a higher memorisation score in non-distilled score is not very surprising but a nice thing to know. Perhaps a connection can be drawn to why distillation helps in adversarial robustness.\n\nI find the paper interesting and would like to see the above changes made or at least argued why they should not be. Upon that, I am willing to increase my score.\n \n## Minor points\n\n* I am a bit confused my the line styles employed in Fig 3(a). Is there a particular reason for the way it is ?\n* Figure 6 is missing the colour bar.\n* More generally, I think the figures could be a bit smaller and more interesting results from the appendix could be brought to the main text. For example Figure 14\n\nMadry et al. \"Towards deep learning models resistant to adversarial attacks.\" (2017).  https://openreview.net/pdf?id=rJzIBfZAb\n\nPaleka et. al. \"A law of adversarial risk, interpolation, and label noise.\"  (2022).  https://arxiv.org/abs/2207.03933 \n\nWei et al. \"Learning with noisy labels revisited: A study using real-world human annotations.\" (2021).  https://arxiv.org/pdf/2110.12088v2.pdf\n\nSanyal et. al. \" How unfair is private learning?\" (2022) https://proceedings.mlr.press/v180/sanyal22a.html.\n\nHooker et al. \"What do compressed deep neural networks forget?.\" (2019). https://arxiv.org/pdf/1911.05248.pdf\n\nXu, Han, et al. \"To be robust or to be fair: Towards fairness in adversarial training.\" (2021). https://proceedings.mlr.press/v139/xu21b.html\n\nPapernot et al. \"Distillation as a defense to adversarial perturbations against deep neural networks.\" (2016) . https://ieeexplore.ieee.org/document/7546524\n\n\n================== post rebuttal ==================\n\n\nThe author's answers were satisfactory. The paper provides very useful insights and I lean towards acceptance at this stage.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5712/Reviewer_E5os"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5712/Reviewer_E5os"
        ]
    },
    {
        "id": "fTDTb1TF9yo",
        "original": null,
        "number": 2,
        "cdate": 1666600139245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600139245,
        "tmdate": 1666600139245,
        "tddate": null,
        "forum": "QcA9iGaLpH4",
        "replyto": "QcA9iGaLpH4",
        "invitation": "ICLR.cc/2023/Conference/Paper5712/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies how and to what degree deep networks perform memorization of samples in the clean label setting. The authors build upon previous work and employ the stability-based memorization score developped in [1] which measures how much the prediction of a\nmodel on a given sample changes if the sample is present versus not present in the training dataset. Using this measure, the authors\nperform a in-depth study on how memorization is affected as a function of depth of the model as well as the effects of distillation. The \nauthors find that larger depth generally decreases memorization on average, which is in-line with the fact that deeper models generalize\nbetter. Interestingly, a more local look reveals a more fine-grained picture: There are samples that are less memorized as depth increases\nbut at the same time, there are examples that are memorized more. These examples often correspond to mislabeled samples. Moreover,\nthe authors find that distillation of a larger teacher into a smaller student reduces memorization on average.",
            "strength_and_weaknesses": "**Strengths**\n\n1. This work provides a very broad empirical study regarding memorization in the clean label setting, which has not been performed to this degree in prior work. The results obtained in this paper might help in guiding theory to better understand the phenomenon.\n\n2. It is very interesting that other, simpler memorization measures cannot capture the trends that the stability-inspired metric exhibits. The observed trends might serve as sanity checks and thus faciliate the design of new, computationally lighter memorization measures.\n\n**Weaknesses**\n\n1. While this work makes very interesting observations, a large part of the findings and the methodology have already been presented in prior works [1, 2]. The metric used in this work has been developped by [1] and initial empirical results have already been obtained on large\n\ufeffnetworks such as ResNets in [2]. [2] already establishes that samples with the highest memorization scores are typically ambiguous or mislabeled, while this work builds on top of this and shows that this effect is exacerbated with depth. While some novel findings are made (i.e. that some samples are memorized less under higher capacity), large parts seem to re-confirm what has been observed in the literature.\n\n2. A novel aspect in this work is the study of knowledge distillation, which was not considered in previous works. While certainly useful in practical settings, I am not convinced whether the understanding of memorization in this specific setting is of particular relevance. Given the fact that distilled models usually generalize better than standard ones, it does not seem so surprising that less memorization happens. However, it would be interesting to understand how memorization is reduced, but the authors  do not address this question. \n\n[1] Does learning require memorization? A short tale about a long tail. CoRR, abs/1906.05271, 2019.\n[2] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation",
            "clarity,_quality,_novelty_and_reproducibility": "**Quality**\n\nThe work is very well-written and easy to follow. Plots are of a high quality and simple to understand. The Appendix contains even more numerical evaluations, making this a very complete empirical study.\n\n**Clarity**\nI am listing a few points in the following which need further clarification:\n1. The authors compare the stability-based metric against other, faster memorization estimators and conclude that those metrics are not precise enough to capture the trends found by the former. The authors implicitly assume that the stability-based metric is the superior estimator (other metrics should exhibit the same trends) without giving a complete reasoning as to why this is the case. A better explanation would be helpful for the reader to understand why it is not the case that the other metrics \"might be right\u201d instead.\n2. In Figure 2.a), we can see the in-sample accuracy plotted as a function of model depth. I am a bit surprised that the ResNets models of depth 8, 14 and 20 cannot reach in-sample accuracy 1, which basically is the training accuracy right? Is this standard, especially since the training set is of smaller size here, if I understood correctly?\n\n**Originality**\n\nAs already mentioned in the \"Weaknesses\" section, this work largely builds on top of [1,2] but scales the empirical experiments to a larger scale. Moreover, similar findings are obtained but now re-confirmed in a broader setting. Some aspects are new such as the distillation setting and the fact that some samples are memorized less with increasing capacity. ",
            "summary_of_the_review": "While some novel results are obtained in this work, it still feels more like a larger empirical study that mostly re-confirms what prior work has identified. I am not convinced of the relevance of the distillation experiments since no real insight into the workings of the method are obtained (i.e. why does a distilled network memorize less). The result that simpler memorization metrics cannot capture the same trends is interesting and certainly relevant to the community. Still, in total the novel contributions of this work are not enough in my opinion and I hence recommend the rejection of this work. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5712/Reviewer_wKAi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5712/Reviewer_wKAi"
        ]
    },
    {
        "id": "pcaOww9A2h",
        "original": null,
        "number": 3,
        "cdate": 1666600972804,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600972804,
        "tmdate": 1666600972804,
        "tddate": null,
        "forum": "QcA9iGaLpH4",
        "replyto": "QcA9iGaLpH4",
        "invitation": "ICLR.cc/2023/Conference/Paper5712/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a systematic empirical analysis of the memorization effect of DNNs w.r.t. model size and distillation. It bases the studies on the existing memorization measure 'memorisation score', and performs extensive experiments. The findings are 4-fold: (1) increasing the model complexity tends to make memorization more bi-modal where some hard examples get lower memorization scores while some ambigous examples get higher memorization scores; (2) distillation aids to inhibit memorisation; (3) some other memorization measures can strongly correlate with the used stability-based memorisation measure, but they do not capture key trends seen in the latter.",
            "strength_and_weaknesses": "# Strength\n- The writing and presentation are nice. It is a pleasure to read this paper.\n- The empirical studies are very thorough. I appreciate the authors' efforts in doing so.\n- The paper is very relevant to ICLR community. The paper makes a step toward understanding the memorization and generalization of deep learning.\n- The findings in this paper may be useful to spark new applications in scenarios such as data selection/reweighting when learning under noise and active learning.\n\n# Weaknesses\n\nGenerally, in the aspects of displaying findings and drawing conclusions, the paper does not have significant issues. However, I would like to see improvements regarding the following points to make the paper a better contribution to the community.\n- In the caption of figure 2, you state \"however for architectures starting at depth 32, only a small number of points remain with\nhigh memorisation score values\". Is the used \"small\" correct? I see a relatively large number of points have high scores when the depth is no less than 32.\n- \"More interestingly, this bi-modality is exaggerated with model depth\". However, as I notice, figure 2(b) cannot reflect this finding clearly. The exaggeration is very marginal. Can this be shown more confidently in some other scenarios? E.g., on mini-imagenet.\n- On page 7, by inspecting figure 1(b), you state \"Interestingly, none of the examples with small memorisation score from the one-hot model obtain a significant increase in memorisation from distillation.\" At first, it is hard for me to clearly understand figure 1(b). Are the plotted quantities the number of data? Besides, is the argument \"none of\" correct?\n- Why does distillation have such impacts on memorization scores? Is the found phenomenon generalizable to other settings, architectures, or modalities? More explanations are desired.\n- Do the different results between the stability-based score and C-score come from that C-score omits the first term in Eq 2? But as said, the first term can be ignored when the model capacity is enough. So does the conclusion in section 5 still hold when you used larger architectures like vision transformers? I want to see a clarification on this point. \n- Can you provide some extra experiments on adversarial examples? I want to know what kind of data will neural networks view the adversarial examples as\uff1fAmbigous or hard?\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is mostly good except for some minor issues. The quality, empirical novelty and reproducibility are good.",
            "summary_of_the_review": "This is a good paper with extensive empirical findings yet flaws in aspects of clarity and explanation. Currently, I am learning to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5712/Reviewer_bHhw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5712/Reviewer_bHhw"
        ]
    }
]