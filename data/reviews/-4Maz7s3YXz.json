[
    {
        "id": "Syz26Mljht",
        "original": null,
        "number": 1,
        "cdate": 1665690026200,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665690026200,
        "tmdate": 1665690026200,
        "tddate": null,
        "forum": "-4Maz7s3YXz",
        "replyto": "-4Maz7s3YXz",
        "invitation": "ICLR.cc/2023/Conference/Paper2785/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the robust memorization problem in adversarial training, and explains that why adversarial training suffers from severe over-fitting problem.\n",
            "strength_and_weaknesses": "Strength:\nIt is an interesting idea to connect the adversarial training to some local estimate to explain over-fitting.\n\nWeakness:\n\nMain concern:\n\n[1] The authors need to improve the clarity of the mathematical notations and formulas. For example \n\n(1) Definition of f_adv is confusing. The second part of f_adv is reasonable, but why we take a summation on nearby samples? Is it a summation or an average? Also Sometimes it uses S and sometimes is uses \\mathcal{S}.\n\n(2) In Proposition 3.1, the same equation repeats?\n\n(3) In Proposition 3.2, what is \"local constant label\"?\n\n(4) In Corollary 3.4, if we want to claim that \"a neural network with (some number of) weights can achieve robust memorization\", should we prove that every network reaching a small robust training error has a poor robust test error? If we have \"there exists a classifier\", then how can we make sure this is the one the training is converging?\n\n(5) In Theorem 3.5, the notation N_d is used but not defined. There is a definition in Theorem C.1 that N_d is the number of parameters of the nueral network. Please move the definition to the main text.\n\n(6) In Theorem 5.1, how to understand the sentence \"...when only w0 is activated and zero initialized, deriving a parameter sequence...\"? In Theorem 5.2, again, what is the sequence (k)?\n\n(7) Theorem 6.3 provides an upper bound, and Proposition 6.4 provides a lower bound. Please add some descriptions about these two results in the paper. Also, Theorem 6.2 appears suddenly without any connection.\n\nIt is acceptable to have some minor typos, but there are so many typos in the paper and it prevents me from understanding the logic of the paper.\n\n[2] The authors need more evidence to show why their proposed analysis is prefered than other analysis, or why it is essential to consider the robust memorization.\n\n[3] Most papers in the reference list are published before 2021. Could the authors supplement the introduction with some more updated literature?\n\n========================\n\n\nMinor issues:\n\n[1] About the contribution: is the robust memorization an existing implicit bias in adversarial training? If this is the case, then why the contribution is that \"we propose an implicit bias called robust memorization in adversarial training...\" in the abstract as well as the introduction? Should it be that the authors figure out a source of the implicit bias in adversarial training and identify it as robust memorization? \n\n[2] Spelling or issue. \n\n(1) Page 2: \"is the (samll) margin part of training dataset\"\n\n(2) Theorem 5.1, 6.2: \"over (the) sampled set\", missing \"the\"\n\n(3) Theorem 6.3: \"Let D (is) the underlying\", \"is\" should be \"be\"\n\n(4) Section 5, paragraph 2: \"for small large data\"",
            "clarity,_quality,_novelty_and_reproducibility": "The paper needs improvement in its writing to have a better clarity, quality, and reproducibility. From the current presentation, it is difficult to understand the theorems in the paper. If the authors could make the theorems more clear, it could be an interesting paper.\n\nFor writing, I would suggest the authors:\n\n[1] Add a brief connection before each theorem and have detailed explanation after the theorem (with intuitions, not math notations).\n\n[2] Be more accurate on the wordings, and try to fix typos. ",
            "summary_of_the_review": "The paper has many writing issues. As listed in the weakness part, the big amount of typos/grammars in theorems preventing readers from understanding the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2785/Reviewer_z1zg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2785/Reviewer_z1zg"
        ]
    },
    {
        "id": "g7U34dlkHg7",
        "original": null,
        "number": 2,
        "cdate": 1666131557158,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666131557158,
        "tmdate": 1666212753700,
        "tddate": null,
        "forum": "-4Maz7s3YXz",
        "replyto": "-4Maz7s3YXz",
        "invitation": "ICLR.cc/2023/Conference/Paper2785/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper theoretically analysed the complexity caused by robust memorization in adversarial training. The paper first proposed the concept of memorization then derived the complexity of neural networks required for robust memorization and for robust generalization. The the paper discussed the sample complexity of adversarial training.",
            "strength_and_weaknesses": "Strength:\n* This paper thoroughly analyzed the robust memorization in adversarial training, from both representation complexity and sample complexity.\n* The paper is well-written and easy to follow.\n\nWeaknesses:\n* The paper lacks a large group of necessary references, e.g. [1][2][3][4][6].\n* The concept of robust memorization has been introduced and explored by many papers [1] [2] [3].\n* The generalization bound of adversarial training has been proved by [6] which is part of the last contribution of this paper.\n* In Section 3.2, the paper compares the representation complexity of robust memorization with robust generalization, where it states that the representation complexity of robust generalization is exponential to the dimension. This result is known in [5]. Although the authors have cited this in the introduction, it is not made clear here.\n\n[1] Exploring Memorization in Adversarial Training.\n\n[2] Adversarial Training Can Hurt Generalization.\n\n[3] Towards the Memorization Effect of Neural Networking Adversarial Training\n\n[4] Understanding Robust Overfitting of Adversarial Training and Beyond\n\n[5] Why robust generalizationin deep learning is difficult: Perspective of expressive power\n\n[6] Adversarially Robust Generalization Requires More Data",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written well and clear. But it lacks some necessary references and some important theorems derived in the paper have been proposed in existing works. Therefore I am concerned with the novelty of this paper.",
            "summary_of_the_review": "In all, I think the paper is written well and has a clear structure. However, I recommend the authors to include some necessary discussion on their differences and connection between existing works on robust memorization. I listed some related papers in the review but there can be more related papers.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2785/Reviewer_W3fx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2785/Reviewer_W3fx"
        ]
    },
    {
        "id": "fhGddA9MTbP",
        "original": null,
        "number": 3,
        "cdate": 1666717812929,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717812929,
        "tmdate": 1666728367569,
        "tddate": null,
        "forum": "-4Maz7s3YXz",
        "replyto": "-4Maz7s3YXz",
        "invitation": "ICLR.cc/2023/Conference/Paper2785/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors proposed an implicit bias called robust memorization in adversarial training under the realistic data assumption. By function approximation theory, the authors proved that ReLU nets with efficient size have the ability to achieve robust memorization, while robust generalization requires exponentially large models. The authors also demonstrate robust memorization in adversarial training from both empirical and theoretical perspectives.  \n",
            "strength_and_weaknesses": "Strength\n1. Propose a new implicit bias called robust memorization to explain the empirical behavior of adversarial training\n2. Provide both empirical and theoretical results showing evidence from different perspectives\n\n\nWeakness\n1. This new implicit bias still remains a conjecture in my opinion. There lacks strong evidence to show that it is actually what happens.\n2. The empirical results is somewhat known, and do not provide many new information\n3. The authors did not compare their theoretical analysis on robust memorialization with previous theoretical studies in adversarial training\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. Not sure what Figure 1 is trying to express in the intro part. ",
            "summary_of_the_review": "1. It seems that f_adv in page 2 is constructed, not trained. By \u201cadversarial training error\u201d did the author mean an adversarial loss of training data? If so, why does it implies \u201cglobal convergence of adversarial training\u201d?\n\n2. This new implicit bias still remains a conjecture in my opinion. First, a network that is able to represent the target functions f_adv does not mean it will lean towards f_adv. Second, the empirical study that the maximum gradient norm is sharp outside the perturbation region only suggests on training data it is robust. It cannot show anything related to the outside data points.\n\n3. Dividing data into small-margin and large-margin ones could fail in modern deep learning situations where each data point can easily find a small margin to any other classes (targeted adversarial examples). In other words, $\\mu$ could be just 1. \n\n4. For simple data distributions, previous works have shown that adversarial training can even achieve robust generalization, such as Dan et al. 2020 and\n\n    \"Precise statistical analysis of classification accuracies for adversarial training.\" arXiv preprint arXiv:2010.11213 (2020).\n\n    \"Benign Overfitting in Adversarially Robust Linear Classification.\" arXiv preprint arXiv:2112.15250 (2021).\n\n     Do the authors' results contradictory to theirs? The authors may want to comment on this.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2785/Reviewer_59uS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2785/Reviewer_59uS"
        ]
    }
]