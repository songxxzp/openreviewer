[
    {
        "id": "zQhORIRWDy",
        "original": null,
        "number": 1,
        "cdate": 1665785575028,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665785575028,
        "tmdate": 1669048628466,
        "tddate": null,
        "forum": "1w_Amtk67X",
        "replyto": "1w_Amtk67X",
        "invitation": "ICLR.cc/2023/Conference/Paper3259/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes the use of L-length binary codes (TAC) for the use of multi-class classification. Instead of having a one-hot representation, the authors' approach is to produce a binary value from each sliced-up section of the neural network's activations (Figure 1). Models can be trained to output TAC from scratch or via post-hoc add-ons to existing classifiers. ",
            "strength_and_weaknesses": "Thanks for the authors for the hard work on this paper. \n\nStrengths\n------------\n- Appealingly simple idea\n- Clear writing\n- Strong performance\n- Seems quite generally applicable\n\nWeaknesses\n----------------\n- If I understand correctly, each of the binary codes is associated with a slice of the activations. However, activations in the earlier layers are generally worse at predicting the class (and perhaps one code in a vector of them). As such, shouldn't latter (in terms of neural network topology) codes be upweighted (in terms of loss computation) during training, during the distance computation (for OOD detection), and during inference? To assess whether the premises of my question is correct, it would be great to see a plot of (a) training error and (b) OOD distribution per layer: are earlier codes harder to learn than later ones and are they worse at OOD detection? You have something like this in Figure 2, but these are per-sample: how bad is the error vs depth overall? I think this statement in the appendix supports my concern: \"Moreover, at testing time, we used only the deepest of the 13 layers to compute TAC\u2019s confidence scores since that resulted in improved performance\" - what fraction of the code is this? Did you have to do this kind of trimming for any other datasets? This is something important to understand and should be lifted out of the appendix.\n\n- Sec 2.4 - but why *not* use Hadamard matrices? Is there any advantage in using random binary codes specifically?\n\n- How easy is your method to adapt to multi-label classification tasks? How about regression? Would be nice to have a few sentences about this, but I'm not asking for any new experiments here.\n\n- Table 2 - L1 is used for HWU64 and CLINIC150 but cos is used for ImageNet. It's noted in the appendix that you chose cos for ImageNet, Did you choose it on a held-out validation set? Is that the procedure you generally recommend? If so, please make explicit.\n\n- The related work discusses error-correcting output codes. Why not compare to 1-2 of these methods that can be applied to neural networks? I understand they are not as efficient, but it's important to understand how good your performance is compared to previous similar ideas and trade that off with performance gains.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity/quality - discussed above in the strengths and weaknesses.\n\nOriginality - as the authors point out, this is not too different from prior work on error-correcting output codes, but the specific way to chop up the activations seems new to me.",
            "summary_of_the_review": "This is clear and interesting work. It needs to spend more time exploring how depth is related to code quality and utility for out-of-distribution detection, and needs to compare to other ECOC work. As is, the work is a 6. If both of my concerns are addressed, it would turn into an 8.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3259/Reviewer_rL3k"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3259/Reviewer_rL3k"
        ]
    },
    {
        "id": "WK2GOZaggZ",
        "original": null,
        "number": 2,
        "cdate": 1666549530677,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666549530677,
        "tmdate": 1669005517556,
        "tddate": null,
        "forum": "1w_Amtk67X",
        "replyto": "1w_Amtk67X",
        "invitation": "ICLR.cc/2023/Conference/Paper3259/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the robustness of deep classification models, noting that out-of-the-box models can yield arbitrarily high-confidence predictions on out-of-distribution data. As a proposed remedy, \u201ctotal activation classifiers\u201d (TAC) are introduced to capture \u201cclass-dependent patterns\u201d which are claimed to offer a reliable way to identify test data that do not conform to such patterns. Notably, the approach does not require access to out-of-distribution data at training time.\n\nIn more detail, TAC consists of a predictor mapping data onto the unit cube. The features are extracted from the neural network by taking \u201cslices\u201d from all intermediate activations which are then reduced (via summation) to 2-dimensional objects (e.g., spatial dimensions in convolutional architectures for images). The goal is to \u201cmake it so that groups of high-level features fire-up more strongly depending on the class of the input.\u201d The sequence of all such slices is called the \u201cactivation profile\u201d whose distance is measured to the classes. The class labels are represented as distinct binary codes; the set of binary codes is chosen in this way to maximise the \u201cdiscriminability of its elements.\u201d To train TAC, the binary-cross entropy loss is used to obtain class activation profiles which minimise the coordinate-wise distance to the corresponding class; variations on this loss are explored to improve training stability.\n\nThe overall TAC procedure can be applied to both \u201cfrozen\u201d pre-trained models to obtain a secondary confidence score as well as trained \u201cfrom scratch,\u201d at a slight drop in accuracy (A.1). Thus, the recommended approach is to apply TAC to frozen models.",
            "strength_and_weaknesses": "Overall, this paper tackles an important problem and the experimental evaluations on robustness to adversarial attacks, ability to abstain, and out-of-distribution detection seem fairly convincing overall. However, I have a few concerns about motivation for the proposed approach as well as some lacking discussion (and perhaps experiments) of related prior work.\n\nThe approach is not well-motivated. In the introduction, the question is posed, \u201ccan we enforce this property rather than hope it emerges?\u201d, but the reader is left wondering what property is actually under discussion. It would be helpful to include further discussion of the property of interest to motivate why it\u2019s sufficient to target this to obtain \u201cmodels that know that they don\u2019t know.\u201d \n\nThe use of binary codes is motivated by the need to have well-separated targets for optimization. However, in S2.4 it is stated that the codes are randomly selected with coordinates being independent binary random variables. Do the codes actually need to be binary? ",
            "clarity,_quality,_novelty_and_reproducibility": "Although there are notable differences that make the proposed approach unique and interesting, there is more than a passing similarity to https://arxiv.org/pdf/2102.12967.pdf from ICLR 2022. This prior approach also involves extracting summary statistics from all layers of the neural network, and additionally has the benefit of proposing a principled statistical hypothesis testing framework for OOD detection.",
            "summary_of_the_review": "Overall, this paper tackles an important problem and the experimental evaluations on robustness to adversarial attacks, ability to abstain, and out-of-distribution detection seem fairly convincing overall. However, I have a few concerns about motivation for the proposed approach and there is an important omission in the discussion of related work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3259/Reviewer_eB98"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3259/Reviewer_eB98"
        ]
    },
    {
        "id": "iRWP_hNJhhF",
        "original": null,
        "number": 3,
        "cdate": 1666675969849,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675969849,
        "tmdate": 1666677407748,
        "tddate": null,
        "forum": "1w_Amtk67X",
        "replyto": "1w_Amtk67X",
        "invitation": "ICLR.cc/2023/Conference/Paper3259/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThis paper presents a robust multiclass classification \"add on\" scheme for deep neural networks called TAC (total activation classifiers) in which multiple hidden layers of a network are trained to output error correcting codes (ECOCs) that are used to identify the proper class of an example and guard against spurious predictions in the case of out-of-distribution (OOD) examples and adversarial examples designed to trick the underlying model. Because ECOCs require more bits to be correctly predicted when determining the class label, TAC-augmented models are claimed to be more robust. Specifically, with TAC, the hamming (or cosine) distance to the closest code in the code book can used as a measure of model confidence, and predictions with at least a certain distance can be flagged for review or rejected.\n\nToy experiments on CIFAR-10 show that TAC-augmented neural networks can be trained to output ECOCs at multiple network layers, and that the distances of predicted codes is discriminative at identifying adversarial examples (though not perfectly so).\n\nLanguage model experiments on 3 intent-classification tasks show that TAC improves accuracy over and above the original pretrained RoBERTA and BERT++ models. And experiments on ImageNet data with a pretrained ViT model shows that TAC better identifies missed predictions than other confidence detection measures like the max-logit-score (MLS), maximum-softmax-prediction (MPS), and a recently published approach called \"DOCTOR\".\n\nFinally, on an adversarial benchmark dataset (RobustBench), TAC performs well at identifying attacks in a gray-box scenario where the attackers know the inner workings of the base neural network, but do not have access to the classification codebook. In the full white-box scenario where attackers have the code book as well, TAC is not competitive in providing protection against adversarial attacks.\n",
            "strength_and_weaknesses": "Strengths:\n- TAC is a simple extension that allows one to add safety and robustness to new and existing neural network models.\n- Experiments with TAC show promise in terms of classification accuracy and mis-prediction detection across a range of datasets and misprediction penalty ranges.\n- In gray-box adversarial settings, experiments show that TAC is effective at detecting adversarial examples.\n\nWeaknesses:\n- TAC is an empirical method with intuition, but not theory, justifying the use of ECOCs output by multiple network layers for preventing against spurious predictions. The paper's experiments are suggestive, but are not extensive enough to determine whether TAC provides robust and superior decision support across a wide range of tasks, and compared to other commonly used methods like temperature calibration.\n- While the value-of-classification curve (VOC) was cool and something I have not seen before, many of the experimental results regarding misclassification detection rates are presented in a manner that is rather opaque and hard to interpret in real terms (see comments on paper clarity below).",
            "clarity,_quality,_novelty_and_reproducibility": "The main themes and methods of the paper are very clear. However, the presentation of experimental results can be improved. For instance, metrics like baseline model accuracy and TAC model accuracy are reported for the intent classification tasks, but not for the vision tasks (e.g., CIFAR-10). Or if they are presented, they are presented in a different format such as the plots of figures 6 and 11.\n\nIn the mis-prediction detection experiments, the \"Det. AUCROC\" metric does not seem to be described, nor does the rationale for reporting the detection rate at the point where the \"true positive rate (TPR) equals the false negative rate (FNR)\". Why not use the point at which the TPR and FPR are equal?\n\nSimilarly, the results for the RobustBench leaderboard in table 4 are not well described and hard to make sense of. Why are all the cells for the baseline methods left blank? And what does the multicolumn column header \"Individual attacks from from Auto-Attack (%)\" mean?\n",
            "summary_of_the_review": "Overall, this paper presents a simple and promising extension for adding robustness to both new and existing deep learning models. Theoretical justification is lacking, and some of the experimental results are hard to interpret in real terms. At the same time, some of the experimental results are compelling and show real improvements above commonly used SOTA baselines for misprediction and out-of-distribution identification. Therefore, I am in favor of accepting this paper to ICLR, but not strongly so.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3259/Reviewer_sw7u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3259/Reviewer_sw7u"
        ]
    }
]