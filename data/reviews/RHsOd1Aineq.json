[
    {
        "id": "ksAskKF1pqf",
        "original": null,
        "number": 1,
        "cdate": 1666697017975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697017975,
        "tmdate": 1666697017975,
        "tddate": null,
        "forum": "RHsOd1Aineq",
        "replyto": "RHsOd1Aineq",
        "invitation": "ICLR.cc/2023/Conference/Paper3828/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Current GNN models rely on strong features for good performance. However, they struggle in the absence of features and when only the topology is present. This is true particularly in the case of making network resilient to all forms of attack. The paper proposes to make network resilient even in the absence of availability of features. In addition, contrast to earlier approaches, this paper proposes a learning-based method. The key idea is to create a series of subgraph by removing the node with the highest degree and the learn to aggregate node embeddings from such subgraphs.",
            "strength_and_weaknesses": "# Strengths:\n1. The paper is well-written.\n2. The experiment results show significant gain\n***\n# Weaknesses:\n1. The algorithm can be hard to follow. It would be good to add a pseudocode in the Appendix.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite clear, however, adding a pseudocode in the appendix would be good. The idea in the paper is quite novel. The code is open sourced, so it is reproducible.",
            "summary_of_the_review": "The paper proposes an approach to make network resilient even when the features are unavailable. The method seems to bear some resemblance to [A] but the goals were quite different. Regardless, the ideas are quite novel. The gains shown in the experiment section are pretty impressive. I am incline to accept the paper.\n***\n# References:\n[A] GraphOpt: learning optimization models of graph formation, Rakshit Trivedi, Jiachen Yang, Hongyuan Zha, ICML 2020\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3828/Reviewer_sxfB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3828/Reviewer_sxfB"
        ]
    },
    {
        "id": "kV0KtO8y4dm",
        "original": null,
        "number": 2,
        "cdate": 1666739003316,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666739003316,
        "tmdate": 1666739003316,
        "tddate": null,
        "forum": "RHsOd1Aineq",
        "replyto": "RHsOd1Aineq",
        "invitation": "ICLR.cc/2023/Conference/Paper3828/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work aims to study the problem of improving the resilience of a network while trading off between network utility. The authors point out that though edge rewiring can be a promising direction for network resilience improving, existing learning-free methods are not enough due to their limitations of transduction, local optimality, and utility loss issue; and existing learning-based GNN models are also not enough as they may fail the network resilience optimization. Therefore, the authors provide the solution ResiNet (which includes the newly developed FireGNN model for feature-less graphs), and evaluate it on rich benchmark datasets.",
            "strength_and_weaknesses": "Strength:\n\n1. The problem of considering resilience tasks with utility balancing is very interesting, and doing it under an inductive setting makes this work novel.\n2. The experimental results are very impressive.\n\nWeakness (and questions):\n1. The writing is unclear and very hard to follow.\n1) I think the authors abuse the appendix a little bit. I think usually people put the supplementary stuff (like experiments on extra datasets, related background information, details for the baseline models, etc) in the appendix, but would always keep the main paper self-contained. However, in this paper, the authors put some important details that would affect the reading in the appendix, i.e.the main paper itself is not self-contained and can not be fully understood without the appendix. \n2) Sometimes the audience may need to check the many different sections to understand a single definition. For example, to understand what is a resilience metric in section 3, the audience needs to first go to section 5.1, then go to Appendix B.1, and then, the definition is still not very clear, because the authors use the term \"certain attack strategy\" without giving concrete examples or explaining the requirements of the attack strategy.\n3) A small question for figure 1, why will AB and CD be selected at the t+2k+1 step?\n\n2. The experimental results are not well-interpreted. \n1) For table 1, in datasets BA-50 and BA-100, why do almost all the learning-based methods get 0(1)?\n2) why is ResiNet capable of obtaining such a good performance boost on the EU dataset (>100% performance improvement on the second best)? what is so special about this dataset?  \n3) why are so many \"x\" in table 1?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Unclear\n- please refer to the weakness section for details.\n\nQuality: Fair\n- Though the experimental results are surprisingly good, there is not enough interpretation to explain why we can get this kind of result, which makes the quality fair. \n\nNovelty: Good\n- The studied topic of considering resilience tasks under an inductive setting is novel and interesting. \n\nReproducibility: Fine (between Fair to Good)\n- The experimental setups for the proposed method are given in appendix C2, and experimental setups for baselines are in appendix C4.  So the hyper-parameter setting is fine for reproduction. But it would be better if the example code (not necessary to be the full code) on at least one dataset can be provided.",
            "summary_of_the_review": "Though the novelty of this work is fine to me and the experimental results are impressive, I think the writing is very unclear, and the experimental results are not well-explained, therefore, I think it is not ready to get published.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3828/Reviewer_aMne"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3828/Reviewer_aMne"
        ]
    },
    {
        "id": "-r20EWMJab",
        "original": null,
        "number": 3,
        "cdate": 1666755379009,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666755379009,
        "tmdate": 1666755379009,
        "tddate": null,
        "forum": "RHsOd1Aineq",
        "replyto": "RHsOd1Aineq",
        "invitation": "ICLR.cc/2023/Conference/Paper3828/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the problem of learning to improve network resilience and unity with reinforcement learning on graphs, which is a important combinatorial optimization problem giving its application in power system and other robust networks. The author designs a reinforcement learning framework with modeling the decision of edge rewiring sequentially with MDP. Policy network is modeled with improved graph neural network as many GNNs failed to model edge selection. The author demonstrate the effectiveness of designed RL based policy over simulation datasets and real-world datasets, on transductive setting and inductive setting. ",
            "strength_and_weaknesses": "**Strength**:\n1. Improving network resilience is an important real-world problem. And the method is the first learning based method for finding policies to optimize resilience.\n2. The writing is easy to follow, with many visualization to help understanding the pipeline.\n3. Experimental setup is solid for the new problem, with many traditional baselines, which should be considered as a contribution.\n\n**Weakness**:\n1. The author claimed the designing of FireGNN is effective for modeling the edge selection. The node deleting idea is already explored in [Cotta et al. 21] and [Bevilacqua et al. 21] for improving GNN expressivity. Also deleting by node degree is inherently give a specific ordering of the graph, and it's really strange to see that all other GNNs failed to do edge selection but only the proposed method can do. Even more strange, the biggest improvement is combing from K from 0 to 1, which is hard to understand. Notice that K increasing from 0 to 1 just uses one additional graph for each step, which is similar to using 2-tuple representation. So my intuition is that you need 2-tuple based GNN for edge selection like 2-WL or 2-FWL, and I recommend the author to test with PPGN [Maron et al. 19]. \n2. Similar to above comments, the GNN baselines picked are kind of out-of-date, and I suggest the author to consider include many recent more expressive GNNs. \n3. Perhaps the biggest question is the setup of transductive setting and inductive setting. From my understanding, what we really care about should be the inductive setting. However the author only report partial result with the designed method only, and there is no comparison with non-learning based baselines. Hence I suspect whether the proposed method can really work under inductive setting, or whether comparable with non-learning baselines. \n4. For transductive setting, it seems a bit unfair to compare learning based method and traditional non-learning based method, as the the learning based method needs extremely higher time to train/learn. So the result in Table 1 seems questionable. To give a fair comparison, the author should also report the computational time for all methods. Also I would like to ask whether transductive setting is meaningful in real-world, and I wish the author can give some example about when transductive setting can be used.\n5. Again, the current inductive setting only uses simulation dataset, I wish to see its performance on real-world problem, giving that the problem itself is valuable because of its real-world application.\n\n\n\n\n\n**Reference**:\n[Cotta et al. 21] Reconstruction for Powerful Graph Representations    \n[Bevilacqua et al. 21] Equivariant subgraph aggregation networks    \n[Maron et al. 19] Provably Powerful Graph Networks\n",
            "clarity,_quality,_novelty_and_reproducibility": "The problem studied is not appeared yet and it's novel. The writing is in general good but may missing some details as asked before. ",
            "summary_of_the_review": "The author studies the resilience optimization problem with edge rewiring through reinforcement learning and graph neural network. The problem itself is interesting and the pipeline designed is reasonable. My concerns are mainly about experimental settings and evaluation baselines. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3828/Reviewer_6mo7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3828/Reviewer_6mo7"
        ]
    },
    {
        "id": "Y0NhiLRcaE9",
        "original": null,
        "number": 4,
        "cdate": 1667352161728,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667352161728,
        "tmdate": 1667352161728,
        "tddate": null,
        "forum": "RHsOd1Aineq",
        "replyto": "RHsOd1Aineq",
        "invitation": "ICLR.cc/2023/Conference/Paper3828/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The resilience of complex networks is a critical structural characteristic in network science, measures the network\u2019s ability to withstand noise corruption and structural changes. The authors propose a framework, ResiNet, combining a variant of GNN called FireGNN to model the structural features of networks and reinforcement learning, to enhance the resilience of network with moderate loss of network utility via neural edge rewiring, which selects two directional edges step by step and rearranges them among four involved nodes, connecting two original head nodes and two original tail nodes. Extensive experiments demonstrate the validity of ResiNet in many kinds of networks and generalization on optimizing different utility and resilience metrics.",
            "strength_and_weaknesses": "Strength:\n1. The authors propose the first (claimed by themselves) inductive learning-based approach, ResiNet, to boost the resilience of complex networks, and the design of FireGNN learns meaningful representations via the proposed filtration process.\n2. The experimental results are detailed and plentiful, which provide many perspectives to understand and prove the effectiveness of model.\n3. The authors provide in-depth analysis for their approach and the resilience task itself, further illustrating the reason and the validity of their proposed designs.\n\nWeakness:\n1. There are small writing mistakes in the manuscript, for example, the footnote in the 1st page containing repeating \u201cnetwork resilience\u201d, which makes me confused. In Section 4.2 (5th page) there are wrong spelling \u201cFirGNN\u201d.\n\n2. In Table 1, the authors show that many learning-free methods can also achieve competitive performance, however their computational cost are relatively low. The authors also need to show the computational cost, such as the number of parameters or the training time for their reinforcement learning approach to\tdemonstrate if it is worth trading the cost using the performance.\n\n3. The generalization of reinforcement learning method is important. However, \u201cthe generalization on optimizing different utility and resilience metrics\u201d does not seem to be surprising for a learning-based model. What I am concerned about is the performance of the model which trains on synthetic networks but tests on real-world networks, further indicating its generalization and real-world applications. After all, different sizes of\tBA networks\talso\tshare similar characteristics and experiments on them are not convincing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of this paper is relatively good.\n\nThe authors propose the first (claimed by themselves) inductive learning-based approach, ResiNet, to boost the resilience of complex networks, and the design of FireGNN learns meaningful representations via the proposed filtration process. The novelty of this paper is also guaranteed.\n\nThe authors claim that their implementation has already been open sourced, but I find no external links for their codes. The reproducibility of the paper needs to be further confirmed.\n",
            "summary_of_the_review": "The authors propose a framework, ResiNet, combining a variant of GNN called FireGNN to model the structural features of networks and reinforcement learning, to enhance the resilience of network with moderate loss of network utility via neural edge rewiring. The experimental results and analysis are also comprehensive. However, due to the weaknesses and concerns above, I think further experiment and revision are needed, so I recommend to reject.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3828/Reviewer_CVfp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3828/Reviewer_CVfp"
        ]
    }
]