[
    {
        "id": "H4KvCvUsiTa",
        "original": null,
        "number": 1,
        "cdate": 1666648717656,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648717656,
        "tmdate": 1666648717656,
        "tddate": null,
        "forum": "Jw5ivmKS2C",
        "replyto": "Jw5ivmKS2C",
        "invitation": "ICLR.cc/2023/Conference/Paper3224/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a novel framework for providing post-hoc privacy for queries on models trained using Adversarial Representation Learning (or representation learning in general). The authors draw inspiration on traditional differential privacy (DP) based approaches and are able to provide formal post-hoc privacy guarantees under certain very strong assumptions. ",
            "strength_and_weaknesses": "Strengths: \n1) The paper presents a novel theoretically motivated approach to provide a privacy guarantee for queries on pre-trained representation learning models. \n2) The proposed framework is very similar to DP based approaches and has some properties similar to DP. \n3) The mathematical claims of the paper seem correct (based on my reading). \n\nWeakness: \n1) Definition of privacy: In Definition 1, semantic neighborhood privacy is also a function of $\\theta$. So, it should be included in the definition. This unfortunately significantly weakens this notion of privacy. \n2) Identifying appropriate value of R: The authors state \"To assess the level of indistinguishability, we look at Fig 3 where\nwe project the original images into embedding space and sample points from the boundary of\nneighborhoods of different R. \". This simply isn't a reasonable approach to select a privacy parameter especially since it relies on humans perception of similarity. \n3) The authors don't really propose a practical way to select $\\Delta_{LS}^p$. \n4) The authors state that the runtime exponentially increases as a function of embedding dimension, which is problematic. \n5) Related to weakness 4, the authors evaluate their runtimes on relatively small embedding dimensions. However, there are many use cases when the embedding dimensions are much higher (>1000). What is the feasibility of this approach for such embedding dimensions? I also have significant concerns about how meaningful the privacy guarantees are in the case of high dimensional embeddings. \n6) I would be interested in a privacy/utility comparison with DP based model training. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite easy to read and the proposed approach is original. ",
            "summary_of_the_review": "While the proposed approach is somewhat well motivated, I think there are many potential issues with the proposed framework that significantly limit the utility of the framework to some very specific situations (low dimensional embeddings where the embedding space is sufficiently smooth). To their credit, the authors are quite upfront about the limitations of their framework but they overstate the utility in my opinion. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3224/Reviewer_KPwN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3224/Reviewer_KPwN"
        ]
    },
    {
        "id": "5DIjQylcqJ",
        "original": null,
        "number": 2,
        "cdate": 1666676704244,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676704244,
        "tmdate": 1666676704244,
        "tddate": null,
        "forum": "Jw5ivmKS2C",
        "replyto": "Jw5ivmKS2C",
        "invitation": "ICLR.cc/2023/Conference/Paper3224/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a formal way to capture obfuscation of an image using adversarial representation. The paper does so by using a metric-based differential privacy notion. Since the metric is difficult to compute, the paper proposes a way to estimate Lipschitz constant using an estimation technique from DP. The paper then evaluates several obfuscation techniques on their utility.",
            "strength_and_weaknesses": "Strength:\n- formalising privacy guarantees of obfuscators is an important research problem to avoid erroneous methods\n- metric DP and PTR framework build on existing work\n\nWeaknesses:\n- adversarial model is not given\n- the method relies on knowledge of the classifier which seems at odds with the usefulness of the approach (i.e., why call the classifier)\n- metric-DP may not capture all privacy vulnerabilities (i.e., the privacy guarantee is as good as the metric that is used)",
            "clarity,_quality,_novelty_and_reproducibility": "The contribution and the method is well explained and the choices behind the approach are justified.\nThe adversarial model is not stated which makes it difficult to understand on who the paper is trying to protect against and at what stage. Who are the parties involved?\nIt seems that to compute the sensitivity, the obfuscator (i.e., the user) needs to know the classifier. However if it knows it, why would it then send its data for classification to the server in the first place?\n\nIt is not clear that hamming, l1 or l2 distance will guarantee privacy. Reconstruction of an image is not the only goal of an attacker.",
            "summary_of_the_review": "Given that there were unsuccessful attempts before on adhoc techniques to obtain image privacy by changing/randomizing the enbedding, this paper makes an attempt to provide formal guarantees to capture it. However, the method may not protect against all attacks and seems to rely on the knowledge of the classifier defeating the purpose of sending data for classification.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3224/Reviewer_yTGh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3224/Reviewer_yTGh"
        ]
    },
    {
        "id": "sZ3TZEvwlJ7",
        "original": null,
        "number": 3,
        "cdate": 1666830560195,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666830560195,
        "tmdate": 1666830560195,
        "tddate": null,
        "forum": "Jw5ivmKS2C",
        "replyto": "Jw5ivmKS2C",
        "invitation": "ICLR.cc/2023/Conference/Paper3224/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper utilizes a method called Adversarial Representation Learning (ARL) as a base. ARL \nlearns a privacy-preserving encoding of sensitive user data before it is shared, but lacks formal guarantees. The authors link the local Lipschitz constant of a neural network in ARL (obfuscation layer) with its local sensitivity. Local sensitivity was used by the Propose-Test-\nRelease (PTR) framework to provide privacy guarantee, which the authors modify to work with the Lipschitz constant.\nExperimentally on datasets verify the role of ARL in improving the privacy-utility tradeoff.",
            "strength_and_weaknesses": "Overall, I think this is a good paper with decent results, though the core of paper is a rather simple idea of relating Lipschitz constant to local sensitivity, other techniques are mostly borrowed from other papers.\nThere are terms used in Alg 1, which are defined after the algorithm, as as N. I feel the part after the algorithm can be stated first and then the algorithm.\nFig. 2 could have better legend for the colors for R.\nI do not understand what the epsilon values are on the very first heading in Table 1\nWhat is the encoder row?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing in the experiments part is not good, the Tables and Figures are not explained enough.\nThe method seems novel, though simple.",
            "summary_of_the_review": "Good work, though simple extension. The writing can be better. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3224/Reviewer_82bi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3224/Reviewer_82bi"
        ]
    }
]