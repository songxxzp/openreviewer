[
    {
        "id": "ZEzljidVbrQ",
        "original": null,
        "number": 1,
        "cdate": 1666150702748,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666150702748,
        "tmdate": 1669400028546,
        "tddate": null,
        "forum": "QrdSiDAv5ek",
        "replyto": "QrdSiDAv5ek",
        "invitation": "ICLR.cc/2023/Conference/Paper3684/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose to reduce the computation cost of deep learning model by reformulating the channel squeezing operation. Specifically, the 1x1 conv operation is replaced by firstly channel-wise average pooling the input feature, then calculated the called 'fusion possibilty'. Finally, the input channels are squeezed to be target size with the output 'fusion possibilty'.",
            "strength_and_weaknesses": "Weaknesses:\n\n1. The reference style is problematic.\n\n2. In Sec2.1, the comparison on number of training epoch dose not equal to the comparison of training efficiency, since the compared models are different.\n\n3. Most of the 1x1 conv layers in ResNet are used for increasing the non-linearity while preserving the resolution of feature, it cannot be treated as channel squeezing.\n\n4. The 'Global Context Aggregation' operation collapse the representation into single dimension, which must leads to spatial information loss.\n\n5. In Sec 3.3.1, why the lower possibilities correspond to the average pooling while the higer ones correspond to max pooling? Any analysis and experiments for this operation?\n\n6. In Sec3.3, the proposed combination manner is totally different from the existing one, since the combination of max and mean pooling should cover only a small region of the previous representation combination manner. However, there is no analysis and explanation to reason the proposed manner, which is hard to follow and understand.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The writing is not good.\n\n2. Lacks of analysis and explanation.\n\n3. Limited novelty.",
            "summary_of_the_review": "The proposed method is far different from the exiting deep learning working scheme, and lacks of corresponding explanation and analysis, which is hard to believe the reported performance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3684/Reviewer_ZY5e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3684/Reviewer_ZY5e"
        ]
    },
    {
        "id": "tS_iKRAl0n",
        "original": null,
        "number": 3,
        "cdate": 1666509939406,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666509939406,
        "tmdate": 1666509939406,
        "tddate": null,
        "forum": "QrdSiDAv5ek",
        "replyto": "QrdSiDAv5ek",
        "invitation": "ICLR.cc/2023/Conference/Paper3684/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a FACS network to reduce the computation cost of deep learning models by redefining the module in channel squeezing. The original channel squeezing module (1x1 convolution) is amended by adding the global pooling and designing the channel fusion strategies. ",
            "strength_and_weaknesses": "Pros:\n\nThe proposed FACS method is able to reduce the computation cost and improve performance.\n\nCons:\n\n(1) In my opinion, the global pooled feature will have the same effect on the input feature (with respect to performance), compared with the baseline squeezing. So the performance improvement should be caused by the channel fusion strategies. The authors are suggested to add one ablation study to show the effect of channel fusion strategies (e.g., a single type of pooling).\n\n(2) The operations in the FACS block are very similar to that in SEnet (1 x 1 Conv on c x 1 x 1 feature is the same with the Fc layer).\n\n(3) For Sec 3.3.1 and Sec 3.3.2, why output channel from the channel squeezing can be aligned with the channel-wise avg/max pooling?\n\n(4) Lack of theoretical analysis of why the FACS and channel fusion module are useful.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The reference style is wrong. The write-up is not very clear. \n\nThe novelty is limited \n\nThe authors provided the code in supplemental materials",
            "summary_of_the_review": "As mentioned in 'Strength And Weaknesses', the proposed method novelty is not sufficient. Additionally, it also lacks of theoretical analysis. So I tend to give the 'marginally below the acceptance threshold'.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3684/Reviewer_Ywmp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3684/Reviewer_Ywmp"
        ]
    },
    {
        "id": "eom5Zy1jV_C",
        "original": null,
        "number": 4,
        "cdate": 1666627569789,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627569789,
        "tmdate": 1666627569789,
        "tddate": null,
        "forum": "QrdSiDAv5ek",
        "replyto": "QrdSiDAv5ek",
        "invitation": "ICLR.cc/2023/Conference/Paper3684/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In order to speed up the channel squeezing task, this paper proposes the Fast Adaptive Channel Squeezing (FACS) module to replace the conventional 1x1 convolution operator used in channel reduction. The key benefit of FACS is that it neither alters the number of parameters nor affects the accuracy of a given network. Experiments show that FACS reduces FLOPs yet keeps model performance.  ",
            "strength_and_weaknesses": "[Strengths] \n+ This paper is well-written and easy to follow. Most operations are illustrated with corresponding figures and formulas.\n+ The manuscript conducts several experiments.\n\n[Weaknesses] \n- As the authors sensed, the reader is intuitively aware that the proposed FACS indeed similar to the existing SE-Net [8] and CBAM [25]. The FACS employs the squeeze and channel reduction steps from SE-Net and then excites each channel with the spatial attention as CBAM. Therefore, a supported theory to further explain why composing these operations in the FACS is sufficient and what operations cause its benefits are needed to clarify the work's contribution and novelty.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and hence has good clarity and reproducibility.\n\nThe technical novelty is limited, as shown in [Weaknesses].",
            "summary_of_the_review": "The primary concern of this paper is its technical novelty. Several experiments support the proposed FACS, yet no theoretical explanation or invention of new operations. Therefore, it is doubtful if the proposed method could reach the high standard bar of ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3684/Reviewer_9T37"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3684/Reviewer_9T37"
        ]
    },
    {
        "id": "2r6nn6o9UC",
        "original": null,
        "number": 5,
        "cdate": 1666726492187,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666726492187,
        "tmdate": 1668814456329,
        "tddate": null,
        "forum": "QrdSiDAv5ek",
        "replyto": "QrdSiDAv5ek",
        "invitation": "ICLR.cc/2023/Conference/Paper3684/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a run-time channel squeezing technique. The proposed technique mainly works on the convolutional block and can serve as a plug and play component to many CNNs. The proposed methods does not keeps all possible parameters but can reduce the number of FLOPs to enable fast inference on low-power devices.",
            "strength_and_weaknesses": "## Strength\n\n- The proposed run-time squeezing scheme, although similar to dynamic pruning techniques, is used as a plug-and-play component. This seamless integration can directly improve the runtime performance of many existing CNNs.\n- The authors reported latency numbers on a range of hardware platforms.\n- The design choices are well justified in Section 4.1\n\n## Weakness\n\nThe major weakness of this paper is on its evaluation, it is not clear to me how this proposed method compares to other published methods.\n\n- This paper does not provide a direct comparison to many existing techniques in the field of dynamic pruning [1, 2, 3]. These dynamic channel pruning techniques are closely related to this work.\n- In all the results presented, the authors are mainly comparing to baselines. For example, Table 2 mainly compares a modified ResNet50 with FACS-ResNet50. This does not really provide me with a full picture, what if the ResNet50 is modified further to have the same number of FLOPs? What is the top-1 accuracy gap in this case?\n- The proposed technique performs a lot better on \u2018old-school\u2019 CNNs like VGG and ResNet. On more trimmed model such as MobileNet, the proposed method seems to have diminishing returns. Also models like MobileNet-V3 and EfficientNet might need to be considered.\n\n[1] Dynamic Channel Pruning: Feature Boosting and Suppression\n\n[2] Channel gating neural networks\n\n[3] Boosting the performance of cnn accelerators with dynamic fine-grained channel gating",
            "clarity,_quality,_novelty_and_reproducibility": "## Quality\n\nThis paper provides an interesting idea about squeezing the channels at run-time in a dynamic fashion. This dynamic operation helps the networks to preserve its full capacity but can use a smaller number of FLOPs for the given task. The evaluation of this proposed method is concerning, it only makes a comparison to baseline models, this paper has ignored the entire field of dynamic pruning. These flaws in evaluation have a great impact of the quality of this paper.\n\n## Clarity\n\n- This paper has used some non-standard reference styles, and also its cross reference style looks a bit abnormal to me.\n- Figure 4 is very confusing. I would prefer you label the input and output dimension of each block, and give the block a name and say the weights (eg. the same style in Figure 1) are having these shapes. The illustration is connecting weights together, which is fairly strange.\n\n## Novelty and Reproducibility\n\nIt is hard to judge the novelty of the proposed paper without the authors giving me a full explanation of how it is different from other dynamic pruning techniques.",
            "summary_of_the_review": "This paper proposed an intersting idea, the fact that the proposed method can serve as a plug-and-play component is interesting. However, I do not think the authors have presented a detailed comparison to many existing works in this domain, and this generally affects the quality of this paper and makes it hard to gauge the real contribution. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3684/Reviewer_LDvY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3684/Reviewer_LDvY"
        ]
    }
]