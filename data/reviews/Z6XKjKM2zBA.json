[
    {
        "id": "DfBCtIOVhln",
        "original": null,
        "number": 1,
        "cdate": 1666161101506,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666161101506,
        "tmdate": 1666161140771,
        "tddate": null,
        "forum": "Z6XKjKM2zBA",
        "replyto": "Z6XKjKM2zBA",
        "invitation": "ICLR.cc/2023/Conference/Paper1736/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates video color style transfer problem, and proposes a self-supervised style transfer framework. The framework first stylizes the image (style removal) and then reconstruct the original image with the stylized image\u2019s content features and the original image\u2019s style features (style restoration). By doing so, the structure can be better preserved. This paper also proposes decoupled AdaIN for feature fusion and use ConvLSTM and optical flow for temporal consistency.",
            "strength_and_weaknesses": "**Strengths:**\n+ The idea of transforming a style transfer task to a style restoration task is interesting to me. This makes the unsupervised task becomes a self-supervised task, which makes the output\u2019s structure less distorted. \n\n**Weaknesses:**\n+ I think the choice of the baseline methods for comparison is not very appropriate. The baselines in this paper (photoWCT, WCT2 and photoNAS) are focusing on improve the photorealism and (or) speed of the style tranfer method of WCT. WCT is known to have strong style effects and high computational cost. Therefore, the starting point of the baselines are to maintain the strong style effects while eliminate structure distortion. But this paper, to me, is more like a pure color transfer method (filter style transfer), and is based on a modified AdaIN, which is known to have less distortion and fast speed. Therefore, in speed comparison in Table 1, the proposed method achieves the fast speed since other baselines all use WCT operation. I recommend some baselines [1-3] that better match the topic of this paper (In [3], the backbone CNNMRF can be replaced with AdaIN). \n+ Another problem is the contribution of decoupled AdaIN. First, in Eq. (1), the whitening is to make the feature maps uncorrelated. However, this paper seems only to normalize the feature maps channel-wisely as in AdaIN rather than adjusting the inter-channel covariance. Therefore, why it is called whitening? Second, the decoupled AdaIN seems only add two conv before the addition and multiplication. The novelty is limited. From the visual results, I can hardly tell the difference between AdaIN and the proposed decoupled AdaIN in Fig .4, Fig. 8 and Fig. 13, which weakens the second claimed contribution.\n+ For the video part, the optical flow and LSTM are also common components in video style transfer.\n\n**Some small issues:**\n+ \u2019\u2019conv1_1\u2019\u2019 could be revised to ``conv1_1''\n+ Fig. 6 is referenced before Fig. 5 \n\n[1] 2020 ECCV Filter Style Transfer between Photos\n\n[2] 2020 ECCV Joint Bilateral Learning for Real-time Universal Photorealistic Style Transfer\n\n[3] 2017 BMVC Photorealistic Style Transfer with Screened Poisson Equation\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and the main idea is easy to understand. The reproducibility is not hard if given all the network details. The originality of the work is limited considering the ineffectiveness of decoupled AdaIN.  ",
            "summary_of_the_review": "This paper proposes an interesting idea of style restoration framework for style transfer, which improve the structure preservation. However, the baselines this paper compare do not well matches the topic of this paper. More appropriate baselines could be compared to make the evaluation fairer and more convincing. And the proposed decoupled AdaIN is less novel.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1736/Reviewer_fibA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1736/Reviewer_fibA"
        ]
    },
    {
        "id": "TH_JWOBvHdP",
        "original": null,
        "number": 2,
        "cdate": 1666590948657,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666590948657,
        "tmdate": 1666590948657,
        "tddate": null,
        "forum": "Z6XKjKM2zBA",
        "replyto": "Z6XKjKM2zBA",
        "invitation": "ICLR.cc/2023/Conference/Paper1736/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper thinks that the summary statistics matching scheme in existing algorithms leads to unrealistic stylization and it proposes a self-supervised style transfer framework to keep photorealism. This frame work contains a style removal part and style restoration part. Besides, it proposes decoupled instance normalization to decompose feature transformation into style whitening and restylization. And it conducts extensive experiments to validate the stylization effecrtiveness of the proposed algorithm.",
            "strength_and_weaknesses": "Strength:\n1. This work notice that a lot of previous algorithms will cause unpleasant artifacts or unrealistic results. And it propose a framework which remove style for content image at first and then restore style with the guidance of given style image.\n2. This paper conduct extensive experiments, and the visual resutls looks great.\n\nWeakness:\n1. From my point of view, almost all components in this work are exsiting models or algorithms proposed by previous work, which makes the novelty of this paper weak.\n2. The pipeline in Figure.3 looks complex and ugly. In fact, you can only draw two time step, the repeated others are meaningless for the illusration.\n3. the decoupled instantce normalization seems too simple as a main contribution.\n4. The paper compares their results with works before 2020, but there are a lot of works[1,2,3] for image style transfering in 2021 and 2022. For fair comparison, I think the results compared with latest methods should be given.\n5. There are some problems in writting of this paper, such as 'docouple' in abstract.\n\n[1] Deng, Yingying, et al. \"StyTr2: Image Style Transfer with Transformers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n[2] Kim, Sunwoo, Soohyun Kim, and Seungryong Kim. \"Deep translation prior: Test-time training for photorealistic style transfer.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 1. 2022.\n[3] Wang, Pei, Yijun Li, and Nuno Vasconcelos. \"Rethinking and improving the robustness of image style transfer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the clarity is well but there are some problems in writing.\nThe reproducibility is well. it conduct extensive experiments and show a lot of experimental results to validate the effectiveness.\nHowever, as described in weakness, the novelty is not enough, and it lacks the comparison with the latest work.\n",
            "summary_of_the_review": "This work proposes a self-supervised style transfer framework to keep photorealism and decoupled instance normalization to decompose feature transformation into style whitening and restylization. Overall, the paper conducts a lot of experiments to research, which is great. But the designs of the framework and the algorithms lack novelty. And it also lacks the fair comparison with new state-of-the-art methods. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1736/Reviewer_K1SY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1736/Reviewer_K1SY"
        ]
    },
    {
        "id": "zk1nvKMBalq",
        "original": null,
        "number": 3,
        "cdate": 1666813254820,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666813254820,
        "tmdate": 1666813254820,
        "tddate": null,
        "forum": "Z6XKjKM2zBA",
        "replyto": "Z6XKjKM2zBA",
        "invitation": "ICLR.cc/2023/Conference/Paper1736/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a novel method for photorealistic video style transfer that conducts color style transfer in videos without undesirable painterly spatial distortions and temporally inconsistent flickering artifacts. The proposed style removal-and-restoration framework, namely ColoristaNet, is capable of learning stylization in a self-supervised fashion. The authors propose to leverage docoupled instance normalization and ConvSLTM units to achieve arbitrary style transfer while keeping the photorealism and temporal conherency. Through a number of comparisons and ablation study, the authors demonstrate ColoristaNet's superior quality to previous state-of-the-art photorealistic style transfer algorithms.",
            "strength_and_weaknesses": "Strengths:\n\nThe paper is well written and the exposition is clear.\n- Inspired by AdaIN that applied adaptive affine transformations, the authors propose decoupled instance normalization (DecoupledIN) that uses linear transforms in both feature whitening and stylization, capable of conducting photorealistic style transfer effectively.\n- To maintain temporal consistency and avoid flickering artifacts, a novel architecture employing optical flow estimation and contextual information with ConvLSTM units is introduced to capture temporal dependencies.\n- Sufficient amount of qualitative and quantitative comparisons with prior work and ablation studies are conducted to show the efficiency and quality improvement of the proposed photorealistic style transfer framework.\n\nWeaknesses:\n\n- It seems there is no explicit control, i.e., hyperparameters, to balance the content and style in the stylization output. It is known that more stylized results typically tend to present painterly distortions instead of maintaining the salient image structure. Still, it would be nice to present such extensions or simply show a figure to display results with different level of stylization.\n- Show some failure cases. For example, what happen when optical flow estimation is inaccurate? What happen when content video and style exemplar does not share any similarity in high-level contents?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, clear and easy to follow. I would suggest the authors to release the reference implementation if accepted to faciliate reproducibility. \n\nSome limitations:\nAs the authors pointed out, current framework relies on FlowNet to compute optical flow which is computationally expensive. Along with some other modules, the inference speed prevents ColoristaNet to be a real-time style transfer approach. Would be interesting to explore how the runtime performance can be improved, possibly revisiting some of the modules.\n\nAnother limitation, as mentioned above, is that there seems to have no stylization strength control. This is understable since the method aims for photorealistic style transfer and needs to avoid painterly distortions. That said, it would be interesting to explore how to extend the existiing pipeline to support different stylization levels, while maintaining the photorealism and image structure of the scene content.",
            "summary_of_the_review": "I am leaning towards acceptance. The paper is a technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1736/Reviewer_AmBd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1736/Reviewer_AmBd"
        ]
    }
]