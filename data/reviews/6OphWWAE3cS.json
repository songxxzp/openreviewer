[
    {
        "id": "phJv_r1dK1K",
        "original": null,
        "number": 1,
        "cdate": 1665919655619,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665919655619,
        "tmdate": 1665933148833,
        "tddate": null,
        "forum": "6OphWWAE3cS",
        "replyto": "6OphWWAE3cS",
        "invitation": "ICLR.cc/2023/Conference/Paper1218/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a new partical-based variational inference method by minimization the functional gradient of KL with a regularization where preconditioning seems to be crucial for the performance. The efficiency compared to SVGD is illustrated on several standard benchmark data sets. Relations to previous work were also discussed.",
            "strength_and_weaknesses": "Strength:\n\n1. The precondition functional gradient flow for particle-based VI seems to be new.\n2. The method has been well motivated with clear theoretical justification.\n3. The advantages over SVGD has been clearly stated and illustrated in the experiments.\n\n\nWeaknesses:\n\n1. The preconditioning seems to be crucial for performance, lack of ablation study except for the toy examples.\n2. No theoretical analysis for the precondition matrix H.\n3. Discretization error of ODE is not explicitly considered.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well written. However, learning functional gradient flows for particle-based VI is not new, and similar ideas (Hu et al., 2018; Grathwohl et al., 2020) have been exploited eariler. More specifically, this paper is highly related to a previous workshop paper: neural variational gradient descent (Lauro Langosco et al., 2021). Despite this, the theoretical analysis part and precondition trick is new.",
            "summary_of_the_review": "The paper proposed a new precondition functional gradient flow approach for particle-based VI and demonstrated its efficiency on several benchmark problems against SVGD. The method can be viewed as a score matching approach that learns the discrepancy of the target score and current particle scores, and update the particles according to the learned functional gradient flows. In that sense, the paper is highly related to a recent paper called neural variational gradient descent by Lauro Langosco et al., 2021. Despite this, the paper do provide some theoretical analysis regarding convergence given well trained functional gradients and introduce the precondition trick which seems to be crucial for performance. I, therefore, recommend it for a weak acceptance.\n\n\nminor issue: there is a mistake in the upper bound of dKL/dt term in Theorem 1.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1218/Reviewer_vc1L"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1218/Reviewer_vc1L"
        ]
    },
    {
        "id": "zLJD7NSNIt",
        "original": null,
        "number": 2,
        "cdate": 1666708103186,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666708103186,
        "tmdate": 1669913557231,
        "tddate": null,
        "forum": "6OphWWAE3cS",
        "replyto": "6OphWWAE3cS",
        "invitation": "ICLR.cc/2023/Conference/Paper1218/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new variational inference method for sampling from the distribution. The proposed method is a sort of extension of the SVGD method inspired by functional gradient boosting. That is, the drift term is modeled by the neural networks and is optimized to maximize the discrepancy under an appropriate regularization. This work also provides a convergence guarantee under the assumptions regarding the approximation error to the preconditioned Wasserstein gradient. The effectiveness of the method is empirically verified on several tasks: sampling from Gaussian mixture, ill-conditioned Gaussian distribution, and Bayesian and hierarchical logistic regressions.",
            "strength_and_weaknesses": "**Strengths**:\n\nThe proposed method can be seen as a proper extension of Stein variational gradient descent (SVGD). Indeed, SVGD is a special case of the proposed method through the explicit feature representation of the kernel. In this sense, this work makes a certain contribution to the context by proposing a general form of SVGD. \n\n**Weaknesses**:\n\nThe motivation for using neural networks for representing the drift term is somewhat unclear. Indeed, Langevin dynamics can be directly applied for sampling from Gibbs distribution $p_*$ and this dynamics does not require the approximation. (Note that Langevin dynamics also solves Fokker-Planck equation Eq. (1) with the Wasserstein gradient.) Thus, I think the standard Langevin dynamics is sufficiently powerful in terms of optimization. Moreover, there are also preconditioned variants of Langevin dynamics:\n\n1. Max Welling and Yee Whye Teh. Bayesian Learning via Stochastic Gradient Langevin Dynamics.\n2. Ga\u00e9tan Marceau-Caron and Yann Ollivier. Natural Langevin Dynamics for Neural Networks.\n3. C. Li et al., Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks.\n4. A. Wibisono. Proximal Langevin Algorithm: Rapid Convergence Under Isoperimetry\n\nThese preconditioned Langevin dynamics can be competitors to the proposed method. Thus, the discussion of these methods would be helpful. In particular,  [WIbisono] (see Section 4.1) considered the dynamics which optimizes Eq. (7) in the submission. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-organized and easy to read. The proof technique is rather straightforward.\n\nQuestion: \n\n- Could you elaborate on why $-c \\nabla^2 \\log p_*$ is better preconditioning? A bit detailed explanation of this would be helpful.\n- Is there a connection between Assumption A3 and weak learning conditions for boosting methods?",
            "summary_of_the_review": "The proposed method is interesting because it is a proper extension of SVGD. But, the motivation for using neural networks as the drift term is rather weak. More discussion to motivate the use of the method will make the paper stronger.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1218/Reviewer_m3yE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1218/Reviewer_m3yE"
        ]
    },
    {
        "id": "srHIMRkvTon",
        "original": null,
        "number": 3,
        "cdate": 1667436417868,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667436417868,
        "tmdate": 1669227771740,
        "tddate": null,
        "forum": "6OphWWAE3cS",
        "replyto": "6OphWWAE3cS",
        "invitation": "ICLR.cc/2023/Conference/Paper1218/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies the general gradient flow of KL divergences. The authors consider a generalized metric space and formulate the gradient descent as a least square problem. The metric spaces include L^2, RKHS, and covariance-weighted norms. Some linear and nonlinear families of functions, including neural networks, are used in approximating generalized metric spaces and gradients. Numerical examples are shown the effectiveness of the proposed methods. ",
            "strength_and_weaknesses": "Strength: The authors propose general metric space-induced gradient descent to design sampling algorithms. The generality of metrics (Q) may improve the sampling algorithm.  \n\nWeakness: \n\n1. No convincing examples are provided analytically. I suggest that the authors study a Gaussian target distribution. In this case, the author may illustrate the advantage of general metrics and gradients. \n\n2. The authors miss many crucial works in this field.  Please illustrate them in the context.\n\nA. Garbuno-Inigo, F. Hoffmann, W.C. Li, A.M. Stuart. Interacting Langevin Diffusions: Gradient Structure And Ensemble Kalman Sampler. SIAM Journal on applied dynamical system, 2019. \n\nW.C. Li, L.X. Ying. Hessian transport gradient flows. Research in the Mathematical Sciences, 2019. \n\nY.F. Wang, W.C. Li. Information Newton flow: second-order optimization method in probability space. 2020. \n\nY.F. Wang, W.C. Li. Accelerated Information Gradient flow. \n\nY.F. Wang, P. Chen, W.C. Li. Projected Wasserstein gradient descent for high-dimensional Bayesian inference. \n\nY.F. Wang, P. Chen, M. Pilanci, W.C. Li. Optimal Neural Network Approximation of Wasserstein Gradient Direction via Convex Optimization. \n\nW.C. Li, A.T. Lin, G. Montufar. Affine Natural Proximal Learning. GSI, 2019.\n\nA.T. Lin, W.C. Li, S. Osher, G. Montufar. Wasserstein Proximal of GANs. GSI, 2021. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this paper is good. ",
            "summary_of_the_review": "The paper is written well with clear mathematics. Some analytical examples and important literature are missed.  It will take effect my ratings. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1218/Reviewer_9msK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1218/Reviewer_9msK"
        ]
    }
]