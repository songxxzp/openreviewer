[
    {
        "id": "wkGO9MeRXHQ",
        "original": null,
        "number": 1,
        "cdate": 1666332996541,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666332996541,
        "tmdate": 1666332996541,
        "tddate": null,
        "forum": "_Mic8V96Voy",
        "replyto": "_Mic8V96Voy",
        "invitation": "ICLR.cc/2023/Conference/Paper1516/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents an optimizer in the K-FAC family. K-FAC approximates the Fisher information matrix as the Kronecker product of two smaller matrices, by assuming independence --- they approximate E((a_{l\u22121} \u2297 g_l)(a_{l-1} \u2297 g_l)^T] as E[a_{l-1}a_{l-1}^T] \u2297 E[g_l g_l^T ]. The proposed algorithm, Eva, goes one step further, by approximating E[a_{l-1}a_{l-1}^T] as E[a_{l-1}] \u2297 E[a_{l-1}]^T and E[g_l g_l^T] as\nE[g_l]  \u2297 E[g_l]^T. The result is a rank one approximation to the Kronecker factors, and the authors use damping and the Sherman-Morrison-Woodbury formula to compute the preconditioner. Note that Eva approximates the empirical Fisher information matrix, as opposed to the true Fisher recommended by the K-FAC authors.\n\nThe algorithm uses significantly less memory than K-FAC, in fact uses less memory than Adam or Adagrad --- its memory consumption is similar to Adafactor or SM3. The paper has several comparisons on image datasets, showing that Eva is competitive with K-FAC and Shampoo in terms of accuracy, while achieving performance similar to SGD.",
            "strength_and_weaknesses": "Strengths: The paper is clearly written, and the proposed algorithm is very simple and easy to implement. The update rule is very fast to compute, and the algorithm uses very little memory, so is very fast. The experiments for CIFAR-10 are quite comprehensive. \n\nWeaknesses: The approximations made in the algorithm are not motivated in the paper. Why is it acceptable to approximate E[a_{l-1}a_{l-1}^T] as E[a_{l-1}] \u2297 E[a_{l-1}]^T? K-FAC's approximations are justified by independence assumptions, but Eva's further approximations are not justified anywhere. There is no convergence analysis to show that this update is mathematically well grounded.\n\nFor Imagenet, please include a table showing the number of epochs needed to achieve target accuracy. Also for K-FAC and Shampoo, please include results of increasing the update interval further, since their authors say that larger intervals do not degrade accuracy very much. Also for the autoencoders, please include train error and test accuracy for the 3 common datasets --- MNIST, Faces and Curves.\n\nIt is strange that you claim that \"K-FAC and Shampoo are not time-and-memory efficient,  which severely hinders their feasibility in practise (sic)\" --- these optimizers are used to train some of the largest networks today, with billions of parameters.\n\nK-FAC and Eva are both presented for fully connected and convolutional networks --- does Eva extend easily to other networks such as transformers, and to networks with features like batch norm or layer norm?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written. While rank 1 approximations to large matrices are quite common, I am not aware of this approximation to K-FAC before.",
            "summary_of_the_review": "The paper presents an interesting and fast approximation to K-FAC. I would like to see some theoretical justification for the approximation, and also some experimental results as detailed above for a higher score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1516/Reviewer_1mYD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1516/Reviewer_1mYD"
        ]
    },
    {
        "id": "-RvgKGH8L-3",
        "original": null,
        "number": 2,
        "cdate": 1666534947094,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666534947094,
        "tmdate": 1666534947094,
        "tddate": null,
        "forum": "_Mic8V96Voy",
        "replyto": "_Mic8V96Voy",
        "invitation": "ICLR.cc/2023/Conference/Paper1516/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This study proposes Eva, a second-order optimization method for training deep neural networks that approximates the Kronecker factor of K-FAC using empirical FIM with a rank one matrix. Averaging batch activations or errors construct the rank one matrix. Experimental results on various computer vision tasks show that Eva achieves similar convergence (number of steps vs. accuracy) as K-FAC, Shampoo, and M-FAC. Since Eva's computational cost per step is considerably smaller than other second-order optimization methods and is close to SGD\u2019s cost, Eva achieves relatively fast training times in wall-clock time.\n",
            "strength_and_weaknesses": "Strengthes\n- The paper clearly describes the literature, the differences between the proposed and existing methods, and the experimental details, which are easy to follow.\n- The proposed Eva method is straightforward (in a good sense). It boosts the practicality of second-order optimization in deep learning regarding computational and memory cost and ease of implementation and integration (as long as the fast convergence holds for other tasks). Thanks to the low computational cost, there is no need to use stale second-order information or to tune the second-order information update interval.\n- Experiments are well designed to validate the effectiveness of second-order optimization methods.\n    - (There are still missing points as I will describe in \u201cWeaknesses\u201d, though.) \n    - Several numbers of epochs are considered. A longer training (200 epochs) gives the baseline first-order optimization methods a chance to achieve the same accuracy as the second-order methods. Yet, second-order optimization methods (K-FAC, Eva) show a faster convergence (It would be better if the same comparison (with 50, 100, and 200 epochs) is made for Shampoo and M-FAC, though.)\n    - Hyperparameters specific to second-order optimization are explored. Since damping, second-order information update interval, and running average coefficient largely affect the convergence of mini-batch second-order optimization, a comparison with various values of them is valuable in verifying Eva\u2019s robustness.\n\nWeaknesses\n- As the authors mention, there is no theoretical analysis in this study, and the effectiveness of Eva is supported only by the experimental results. \n    - Eva has shown good experimental performance (as convergent as K-FAC). I appreciate the detailed experiments and analysis in this work. However, there is still no guarantee that good performance will hold for other tasks due to the algorithm's simplicity.\n    - (Here, I am not requesting additional experiments but pointing out that the \u201cno guarantee\u201d is almost inevitable in deep learning, which involves many \u2018unpredictable factors.\u2019)\n    - Therefore, I do not think comparing only the computation and memory costs of existing methods and Eva (as in Table1) is fair. The amount of information (e.g., number of matrix elements and matrix rank) that each method uses for preconditioning gradient should be included as a comparison item. Eva\u2019s computational and memory cost is lower than K-FAC simply because it calculates/stores much less amount of information \u2014- it approximates the Kronecker factor matrices with rank-1 matrices (outer products of the Kronecker vectors). I think readers would appreciate it if this work also shows the trade-off between the amount of information and the computational/memory cost (a \u2018predictable factor\u2019) to decide which algorithm to use.\n- Comparison between K-FAC and Eva with different mini-batch sizes is missing. \n    - (Here, I do request for additional experiments)\n    - The sum of the outer product of average gradient (OPAG), aka mini-batch empirical FIM, is known to have less second-order information when the mini-batch size is larger (https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/readings/L05_normalization.pdf, equation 4) unlike the (per-example) empirical FIM.  \n    - And again, since Eva approximates the Kronecker factor matrices with rank one, it should miss lots of information in large-batch settings (whether and how much the information is beneficial for training convergence and generalization is unpredictable, though).\n    - Therefore, Eva, which uses OPAG or mini-batch empirical FIM, might underperforms K-FAC w/ empirical FIM in large-batch settings so I would appreciate more empirical comparison with different mini-batch sizes.\n    - Regardless of the empirical results, it would be valuable to indicate the amount of information Eva captures/misses clearly.\n- Comparison with several existing studies in the literature are missing.\n    - SKFAC (https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_SKFAC_Training_Neural_Networks_With_Faster_Kronecker-Factored_Approximate_Curvature_CVPR_2021_paper.pdf) and SENG (https://link.springer.com/article/10.1007/s10915-022-01911-x) also apply Sherman-Morrison-Woodbury (SMW) formula to invert the Kronecker factors. Both approaches approximate layer-wise Fisher matrix with low-rank matrix. So one can think of them as higher-rank counterpart of Eva.\n    - KPSVD (https://hal.archives-ouvertes.fr/hal-03541459/document) also consider rank-one approximation of the Kronecker factor. Unlike Eva, it tries to find the optimal rank-one matrices that minimizes the Frobenius norm of the difference between the layer-wise Fisher matrix and the rank-one approximation. Compared to Eva, it might have more computational costs, but it sounds more theoretically-grounded. Again, whether this makes a change is unpredictable in practice, but showing the information-cost trade-off is useful.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is written clearly. The experimental details and code are available. Since there have already been many attempts at low-rank and rank-1 approximations of layer-wise Fisher for natural gradient, the technical novelty of Eva (a rank-1 approximation that averages a batch of activations/errors) is limited. However, there is an empirical novelty in that such a simple method can achieve the same convergence as K-FAC and other second-order optimization methods for various tasks, including ImageNet classification.",
            "summary_of_the_review": "Although there are no solid theoretical guarantees, a simple, low-cost method like Eva can achieve the same level of convergence as a high-cost method like K-FAC is a critical data point for future research on efficient training methods in deep learning. However, due to its simplicity, it is unpredictable that Eva is effective in a wide range of tasks. I believe that comparisons and discussions focusing on predictable aspects such as trade-offs between the amount of information (e.g., number of matrix elements, matrix rank to be used for gradient preconditioning) and computational/memory cost would enhance the value of this study.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1516/Reviewer_j8t4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1516/Reviewer_j8t4"
        ]
    },
    {
        "id": "_p5ImLJQSU",
        "original": null,
        "number": 3,
        "cdate": 1666607670691,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666607670691,
        "tmdate": 1666607670691,
        "tddate": null,
        "forum": "_Mic8V96Voy",
        "replyto": "_Mic8V96Voy",
        "invitation": "ICLR.cc/2023/Conference/Paper1516/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper describes what I would call a (simple but clever) evolution of K-FAC style fisher information matrix approximation (FIM) by approximating the FIM with  kronecker products of activations and gradients (i.e. an outer product over vectors) where K-FAC uses a kronecker product of *matrices* derived from the activation and gradient. While this is an additional step of approximation, it appears to work well in practice and the vector parametrization of the FIM allows efficient inversion via the Sherman-Morrison formula, leading to sizeable speedups, evaluated on real world architectures.",
            "strength_and_weaknesses": "Strengths:\n\n- the explanation of related work, background etc. is well done, I think this paper can serve as a good entrypoint for readers\n- the method is conceptually simple, but clever. I think it is non-obvious that the OPAG approximation would work as well as the K-FAC approximation a priori, and the exploitation of Sherman-Morrison and the rescaling to gradient norm in order to make the method drop-in are nice touches\n- the experimental evaluation is top notch, using the related work and real world datasets and architectures\n- the limitations section is an actual, honest self assessment\n\nWeaknesses\n\n- the authors state most weaknessses in the related work\n- truly a nitpick, but statistical significance tests between results using bonferonni correction would have been a cherry on top",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, quality and reproducibiltiy are all top notch, code is included and seems clean on a brief skim, as I noted, clarity is a particularl strength and quality of evaluation is high as well.\n\nThe only challenge I could make to this paper is that I see very little *theoretical* difference between this and K-FAC (it could be argued it's taking K-FACs approximation to a logical extreme) *but* the implementation details (Sherman-Morrison, rescaling, the exact manner of sampling) which follows the *practical* use of these methods compensates for this in my opinion.",
            "summary_of_the_review": "I think this is a strong paper that has the potential to help bring 2nd order optimisation to the masses, especially for industrial use cases where it is not about individual percentages on benchmarks but about tradeoffs, and the 50 epoch accuracy of Eva seems comparable to the 200 epoch accuracy of SGD, meaing it actually achieves real world speedups (especially for WRN).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1516/Reviewer_p2Vi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1516/Reviewer_p2Vi"
        ]
    },
    {
        "id": "K1Zbavv66Bl",
        "original": null,
        "number": 4,
        "cdate": 1666625079101,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625079101,
        "tmdate": 1666625377443,
        "tddate": null,
        "forum": "_Mic8V96Voy",
        "replyto": "_Mic8V96Voy",
        "invitation": "ICLR.cc/2023/Conference/Paper1516/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, a novel second-order technique is proposed for deep neural networks training. ",
            "strength_and_weaknesses": "strength\n+ it is a good try to further reduce the computational and storage burden for second-order method for training DNNs.\n\n+ the method is not complicated and has shown great advantage in numerical experiments\n\nweaknesses\n-  there are two modifications. One is to use a mini-batch and the other is to use Sherman-Morrison formula. It is not clear how they two can coordinate each other and to achieve very good performance, while only one of them does not work.\n\n- the authors have conducted experiments on ImageNet, however, for the most important comparison on test accuracy, the results on ImageNet are missing.\n\n- one question is about the learning rate. I saw still Eva needs a delicate rate schedule but in theory , second-order methods are less sensitive to learning rate. Did Eva demonstrate advantages on this aspect? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the proposed method is novel. For reproducibility, the method is not complicated and it seems that the results could be achieved by readers easily.  ",
            "summary_of_the_review": "Overall, it is a good try for a cheap second-order method for deep neural network training. I overall give a positive score and if the author could give convincing discussion on the currently weak points, I would like to rise my score.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "not applicable",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1516/Reviewer_2rcV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1516/Reviewer_2rcV"
        ]
    }
]