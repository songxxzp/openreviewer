[
    {
        "id": "bPPwqB3PjY",
        "original": null,
        "number": 1,
        "cdate": 1666649812404,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649812404,
        "tmdate": 1669594520415,
        "tddate": null,
        "forum": "_DYi95e8CAe",
        "replyto": "_DYi95e8CAe",
        "invitation": "ICLR.cc/2023/Conference/Paper4611/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a network architecture search for a multi-task network, while specifying the search space to tree structures. It jointly trains the model structure as well as the parameters. It starts from the set of single networks and gradually groups layers across tasks if their output features are sufficiently similar. Experiments show that the proposed method outperforms the existing multi-task NAS approaches (BMTAS, LTB) without re-training.",
            "strength_and_weaknesses": "Strength\n\n- It proposes a novel method for multi-task NAS while constraining the search space to the tree-structured architectures, similarly to BMTAS and LTB. Compared to BMTAS and NAS, it utilized CKA to guide task streams to be similar to each other and compute similarity between layers, which is reasonable.\n- The paper provides an extensive analysis of the sensitivity of several factors, including similarity measures and hyper-parameters. It is also good that the performance under several corruptions is evaluated.\n\nWeakness\n\n- I am not sure about the advantage of this method over other existing methods, BMTAS and LTB.\nGiven the same search space of tree structures, I would like to see the accuracy comparison with BMTAS and LTB after re-initialization and re-training to evaluate the optimality of the found architecture itself. The paper provides accuracy after re-training in Table 8, but a comparison with BMTAS and LTB is unavailable.\n\n- I am not convinced about the value of simultaneous training of the architecture and its parameters, which is claimed as one contribution of this work. Since this method requires training of single-task models for all tasks for initialization, additional one-time re-intialization and re-training of the found architecture will not be a huge burden, roughly spending a similar training cost for one single task network training. For this reason, I also think the comparison with existing methods (BMTAS, LTB) would more reasonable after the re-training.\n\n- The method needs several hyper-parameters, including the task learning epochs, structure learning epochs. How will they affect the performance? Sensitivity analysis seems missing.\n\n- Scalability can be another issue of this method. It needs to initialize the model with N single-task models for N tasks, needs N^2 pairwise similarity, and find optimal groupings during training.\n\n- In Table 1, for fair comparison, BMTAS can be trained with larger lambda (resource regularizer) to match the similar compute cost.\n- The details of the evaluation setting of Table 2 is unclear. Is it a cross-dataset accuracy, e.g., training on NYUv2 and testing on CityScapes?\n- It would be nice to include the comparison with BMTAS and LTB (after re-training) in Table 3, to compare the robustness against the corruptions.\n\n\nRelated work\n\n- [Controllable Dynamic Multi-Task Architectures, CVPR 2022] looks pretty related in the sense that it also uses task similarity to guide branching. It also searches for the tree-structured multi-task architecture but also allows dynamic control of the total compute cost.\n\n\nMinor\n\n- The purpose of explicitly evaluating the multi-task performance for semantic segmentation and depth (SD) is unclear to me.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper proposes a novel method and it is clearly written.",
            "summary_of_the_review": "In summary, I think this paper proposes a new method for multi-task NAS. However, I could not find an advantage over other existing multi-task NAS approaches in the method and the experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4611/Reviewer_wjTd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4611/Reviewer_wjTd"
        ]
    },
    {
        "id": "GP31MinXURz",
        "original": null,
        "number": 2,
        "cdate": 1666846929410,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666846929410,
        "tmdate": 1666846929410,
        "tddate": null,
        "forum": "_DYi95e8CAe",
        "replyto": "_DYi95e8CAe",
        "invitation": "ICLR.cc/2023/Conference/Paper4611/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents Multi-Task Structural Learning (MTSL) framework to learn a network architecture and its parameters in a multi-task learning setting. MTSL consists of two phases, a structural learning phase and a fine-tuning phase. The former includes repetition of two alternating steps, network training followed by task grouping. Two datasets in computer vision: Cityscape and NYUv2 were used to evaluate the proposed method. ",
            "strength_and_weaknesses": "Strength\n+ The development of the method tried to mimic how human brains works. It may be conceptual advantage over existing ones from the artificial intelligence perspective. \n+ Large efforts made to empirical study from multiple aspect, including robustness and contribution of different components of the method.\n\nWeakness\n- The calculation of CKA only directly involves kernel, which is the pairwise similarity between examples based on activation. I do not see this helps align learned filters in two separate networks. \n- The calculation of CKA is quite complicated. It may be challenging to effectively optimize the overall objective in Eq. (1).\n- CKA determines the similarity among tasks. Its calculation should be detailed, for example the (number) examples involved.\n- After the first round of structural learning, not all task nodes are at the same layer of the network. I do not think the way that they described for grouping is appropriate. That way likely leads to a very complicated grouping structure (not like the one that they show in Figure 2, which has nice tree structure) that is difficult to interpret, making little to no sense at all. \n- It would be interesting to report the task grouping/branching learned by other methods and have a comparison with that by the proposed method.\n- The empirical results (Table 1) are weak and not much supportive to the value/advantage of the proposed method.\n- The manuscript is readable on overall, but with large room to improve. There are inappropriate choices of wording in many places.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of the paper needs improvement. Novelty is acceptable. No code is provided. Based on the description in the paper, reproducibility is questionable. ",
            "summary_of_the_review": "The method has conceptual merit and is innovative. However, the weaknesses in the implementation and their empirical results dampen my enthusiasm of the work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4611/Reviewer_hhES"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4611/Reviewer_hhES"
        ]
    },
    {
        "id": "9jJw1xtuwq",
        "original": null,
        "number": 3,
        "cdate": 1667611027218,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667611027218,
        "tmdate": 1667611027218,
        "tddate": null,
        "forum": "_DYi95e8CAe",
        "replyto": "_DYi95e8CAe",
        "invitation": "ICLR.cc/2023/Conference/Paper4611/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present a method that \"merges\" N single task networks by grouping similar neurons. The training process\nalternates between \"task learning\" during which the network weights are optimized for task performance, and \"structure\nlearning\", during which neurons are group nodes are optimized with a knowledge distillation-like approach.\nThe method is evaluated on two five-task datasets (cityscapes, NYUv2) and compared with four state-of-the-art methods, with\nfavorable results. Generalization and robustness to corruptions are compared to a baseline single-network.",
            "strength_and_weaknesses": "The proposed work describes an effective methodology for combining N single-task networks into a single, branched network\nthat can perform well at multiple tasks. This enables trading-off between task performance and computational efficiency. The\nexperiments appear to be well done, and the authors include an informative ablation study. The results make me confident that\nthe method can perform well and as described.\n\nThe main weakness of the paper is the lack of clarity in parts when describing the method - I give some detailed examples in the\nnext section. \n\nSome minor weaknesses:\nThe paper would be improved if the authors interpret the results for the reader more. For example, the text only very briefly\ndescribes the differences between LTB-R and LTB, which is an important and interesting one. As well - why does cross-stich\nperform so well, and how much has to do with the number of parameters?\n\nI find the analogy with biological neural circuits to be unconvincing, but more concerning was the statement:\n\"The improvement generalization can be attributed to the brain-inspired aspects of the MTSL algorithm.\" I am very\nskeptical of this claim, and couldn't find \u00a0evidence in the work about WHY MTSL's performance is good. \nMore care in separating speculation (which is fine when framed as such) from conclusions (and \"can be attributed to\" sounds\nconclusive) would be appropriate.",
            "clarity,_quality,_novelty_and_reproducibility": "The work appears to be methodologically sound. Experiments are effective overall, and the method \nperforms favorably to the methods compared. The method combines some existing approaches in a new way, so appears moderately\nnovel, though I am not confident in that assessment. The writing is good overall, but there are some methodological\ndetails that are are not clear and seem not sufficiently explained. Without available code, I am not confident a researcher\ncould reproduce the method. I have included some questions below that the authors might consider clarifying in the text and I hope might improve the manuscript.\n\nFrom the abstract \"In each of the structural learning phases, starting from the earliest layer, ...\", but its not clear to \nme from the text when the method \"moves on\" from the earliest to layer layers.\n\nHow does the method determine when to \"stop\" merging task nodes? \u00a0The grouping threshold gamma will clearly play a role.\nI appreciate section D.2 describing the authors' experiences varying gamma.\n\nIt is unclear how or why in Figure 2, the first 6 layers are shared between the two tasks. Is it possible that MTSL could have\ndetermined that one task shares no information / nodes with the other four tasks? \u00a0Is it always the case that early layers are\nshared and later layers are split? One could argue that properties of natural images make that likely, but I wonder if it is\npossible for the algorithm to produce a graph that splits and merges.\n\nTables [1,2,3] What is the difference between \\Delta^SD_MTL and \\Delta_MTL? Does SD use only the S and D tasks?\n\nEq 1 is not very useful, the authors might consider adding definitions for L_MTL and L_CKA for completeness.\n\nEq (2) What does the 'star F' notation refer to? I could not find similar notation in the associated reference Ye 2019.\n\n\"locally similar task layers\" what is \"locality\" with respect to here?",
            "summary_of_the_review": "This paper presents a method that performs well, as evidenced by solid experiments, but suffers from an incomplete / unclear description of the methodology. I am unsure regarding the novelty of this work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4611/Reviewer_2dhv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4611/Reviewer_2dhv"
        ]
    },
    {
        "id": "fd6LGFrYkL",
        "original": null,
        "number": 4,
        "cdate": 1668010178232,
        "mdate": null,
        "ddate": null,
        "tcdate": 1668010178232,
        "tmdate": 1668010178232,
        "tddate": null,
        "forum": "_DYi95e8CAe",
        "replyto": "_DYi95e8CAe",
        "invitation": "ICLR.cc/2023/Conference/Paper4611/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a structural learning algorithm for concurrently learning the multi-task learning architecture and its parameters. The multitask learning is performed by the creation and removal of neurons based on local similarity. The proposed method is validated on the Cityscapes and NYUv2 datasets for five different dense prediction tasks. Experimental results include SOTA comparison, generalization and inference efficiency, robustness to natural corruptions, and ablation study.",
            "strength_and_weaknesses": "Pros:\n+  The paper is well-written and it\u2019s quite well-organized. It was a pleasure reading it.\n+ Task-relatedness is extremely crucial for multi-task learning. Grouping tasks based on local task representations and transferring knowledge to a new group layer is very interesting.\n+  Exposition of the results is good; particularly the ablation study reporting the effect of alignment, average initialization, and attention based knowledge amalgamation.\n\nCons:\n- The generalization experiment reported in 4.2 is not quite clear to me. Did the authors perform cross-dataset evaluation (trained on CS, tested on NYU, and vice-versa) or cross-task evaluation?\n- How would the MTSL work if there are one or multiple tasks requiring varying architectures (e.g., classification from the encoder only and segmentation from the encoder-decoder )?\n- What's the reason behind choosing the particular encoder and decoder networks? Why the decoder is changed from DeepLabv3 to ResNet blocks?   \n-  The authors mentioned that the multitask loss is a weighted sum of all individual task losses. It should be clarified how the task losses are weighted. Which task is to prioritize and when?\n- What value did the authors assign to the balancing factor \\lambda in eq.(1)?\n- Some of the terms are not defined such as, GMac, etc. Table captions could be revised clarifying all the terms. All the performance metrics should be discussed before using them in the tables.\n- it looks like MTSL is consistently poorer than One-Net in the case of noise corruption in Table 3. Why so?\n- Automatic transitioning between the learning phases would be more appropriate as the model is supposed to learn the architecture as well as its parameters automatically.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is fairly written and well-organized.\nThe work seems to be original.\nThe authors indicated that they would make their code available upon acceptance. Except few minor training details, there should not be any issue of reproducibility. ",
            "summary_of_the_review": "I found the idea of multi-task structural learning with task similarity and neuron creation/removal quite interesting. The authors presented results from a number of experiments for validating their proposed method. Although the results are not super convincing, I believe there is the true potential of MTSL in learning similar tasks within a single model.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4611/Reviewer_mKd4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4611/Reviewer_mKd4"
        ]
    }
]