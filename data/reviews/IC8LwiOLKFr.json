[
    {
        "id": "pzTmW5zvIck",
        "original": null,
        "number": 1,
        "cdate": 1666425465123,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666425465123,
        "tmdate": 1669040047615,
        "tddate": null,
        "forum": "IC8LwiOLKFr",
        "replyto": "IC8LwiOLKFr",
        "invitation": "ICLR.cc/2023/Conference/Paper6128/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to solve the suboptimal problems caused by the pretraining models in industrial systems. And the authors analyze the problem with four aspects( the gap of uniform convergence for analyzing pretrained representations, their stochastic nature under gradient descent optimization, what model convergence means to them, and how they might interact with downstream tasks). Then they propose a method to refine the features of pretraining models in various ways to improve the stability of the downstream task model. Experiments on six tasks prove the effectiveness of the proposed method.\n\n",
            "strength_and_weaknesses": "Pros:\n\n-The problem addressed has high practical value: it tries to make pre-trained model more accessible to a range of industrial systems. The \"Featurizing Pretrained Representations\" idea will promote the performance of downstream tasks compared to other approaches.\n\n-This paper verifies its thought with a detailed formula. By analyzing the features of the pre-training models, the author proves that the features from the pre-training models are related to the stability of the performance of the downstream tasks. In detail, the variance of the pre-training features h (x) plays an essential role in the learning process of downstream tasks, and the variance will increase with the increase of random variables in the pre-training model. \n\n-The proposed method (Featurizing Pretrained Representations) is intuitive and effective, which uses a more stable h (x), e.g., the representation of weights in the NW estimator in Section 4.3, in downstream tasks. Moreover, experiments on six downstream tasks indicate that the method has superior performance and stability.\n\n\nCons:\n\n-It may have a more suitable h(x). The authors think the optimal h(x) can be obtained through Fourier Transform. But it is not easy to get the representation of the sample by the inverse Fourier transform. So they replaced this method by increasing the number of samples and the dimension of features. Nevertheless, this method may not be the most suitable one in industrial systems due to the limited sample and computing resources. Maybe the authors can seek a better method from the perspective of math or Fourier Transform.\n-The analysis is not comprehensive enough. Although the paper proves that the features of pre-training model do affect the stability of downstream tasks, it ignores the features of downstream tasks, which are more essential for downstream tasks. Have the authors tried to analyze the downstream tasks, and what does the result look like?\n\n-The essence of h(x) is still not clear. The paper discussed the impact of the variance of h(x) on downstream tasks, and then the authors improved the performance with the optimization of h(x). However, they did not give any description of the reason why the pretraining models generate such features h(x). Maybe it is more important.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Have met the standards.",
            "summary_of_the_review": "Overall, the paper proposed an interesting idea and showed strong empirical results, but it should have a more comprehensive analysis of both pretraining and finetuning. Hence, I would suggest the author complete more related works to give more convincing conclusions. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6128/Reviewer_BWrq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6128/Reviewer_BWrq"
        ]
    },
    {
        "id": "cF3PRERFHjR",
        "original": null,
        "number": 2,
        "cdate": 1666495260476,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666495260476,
        "tmdate": 1669676636965,
        "tddate": null,
        "forum": "IC8LwiOLKFr",
        "replyto": "IC8LwiOLKFr",
        "invitation": "ICLR.cc/2023/Conference/Paper6128/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper observed an instability issue of downstream prediction based on features from pretrained models (i.e., the accuracy of a downstream task is varing, which depends on the pretrained representation trained on the same data, model configuration, training setup, and stopping criteria). This empirical observation is theoretically analyzed via a novel uniform convergence bound for pretrained representations, and eventually proposes a solution, called featurizing pretrained representation, which is empirically justified to outperform a baseline (i.e., a downstream task with usual pretrained features) using the MovieLens1m dataset and an in-house real-world production dataset. \n",
            "strength_and_weaknesses": "**Strengths**:\n* Provide theoretical interpretations on empirical observations. \n* A solution to address the claimed instability issue is simple.\n\n**Weaknesses**:\n* The main empirical observations (Figure 1) are made in weak setups\n* Proposition 1 is not necessarily true. \n* The experiment result is weak\n\n(Weakness 1)\n\nThe paper first made empirical observations on MovieLens-1m along with the traditional Doc2vec model as a pretrained representation, along with logistic regression for a downstream task. Considering a huge advance on a large language model, this experiment setup does not represent the current state of representation learning. If this observation is made with IMDB (or any larger dataset) along with BERT (or RoBERTa), the story would be much stronger. \n\nMoreover, the following statement needs to be more carefully explained: \n\u201cSince we use logistic regression as the downstream model, the fluctuation can only be caused by the instability of pretrained representations.\u201d\nWhy logistic regression cannot be the source of the instability? In particular, do you mean the optimization for the logistic regression is done until it converges to the global optimum?\n\n\n(Weakness 2)\n\nProposition 1 implies that the uniform convergence bound for pretrained representation has an irreducible term, contributing to the instability of the downstream task performance. However, I think Proposition 1 is not necessarily true. In particular, (A.1) decomposes the generalization bound, and the proof claims some terms are non-negative, which is not always true. \nConsider the second term in (A.1), where the proof said \u201cby definition, the second term is non-positive \u201c. First, I think \"non-positive\" is a typo as if it is non-positive, it can cancel out the irreducible term, thus the claim does not make sense; so I assume that the term is \"non-negative\". Here, h* is the optimal representation w.r.t. the expected loss R(h, f) instead of the empirical loss, thus the second term can be negative. In other words, if $h^* = argmin_h \\sum_i \\ell(f \\circ h(X_i), Y_i)$, the second term cannot be negative; but the if statement does not hold. Additionally, the non-negative argument on the last term is the same issue. \n\nThis proof and the affected arguments in the following sections need to be fixed and adjusted. \n\n\n(Weakness 3)\n\nI liked the experiments on real-world production experiments, which demonstrates how knowledge in academia is actually used in practice. But, to justify the efficacy of the proposed approach, it is always good to evaluate multiple public datasets, but currently only one is used. \n\nFor the ML-1m, I\u2019m not sure if this is a widely acceptable dataset in representation learning; the main reason is that its input dimension looks not large enough (e.g., based on the paper, saying \u201ceach movie is provided with its title and genre, in the form of English words or sentences. There are 18 genres in total.\u201d). Based on this limited information, classifying five ratings sounds very challenging (contrast to this, a human rates each movie based on richer information). I personally think this unusual property of the dataset may lead to the unstable results in Figure 1. Appendix F1 includes some results on IMDB, but I cannot see the downstream accuracy results as in the figure in page 2. \n\nIn short, evaluating the proposed approach at least 2 widely acceptable datasets would make the experiment results stronger. I guess IMDB's downstream accuracy results need to be presented to strongly show the observed instability issue. Moreover, I\u2019d recommend using features from BERT or RoBERTa for pretrained model features. Based on Table 1, the featurting pretrained representation looks promising, and hoping that the same trend holds for other datasets and representations. \n",
            "clarity,_quality,_novelty_and_reproducibility": "(clarity) I think the paper is well written. It would be better if details on the data is provided (e.g., the dataset size), as the stability issues may be related to the small number of samples. \n\n(quality) If my concerns are addressed, the paper quality is good as the main claim is supported by both theories and empirical results.\n\n(novelty) I think the claim could be novel if my concerns on the proof are addressed. \n",
            "summary_of_the_review": "I like the way of this paper to attack the observed representation instability; but the paper does not follow the actively changing trends \u2014 large language models are widely used, but didn\u2019t account for this line of work (including in the related work section). Moreover, as mentioned in the weaknesses, I have concerns on the proof that supports the paper\u2019s main contribution. Thus, I vote for rejection for now, but I\u2019m willing to adjust my understanding. \n\n\n=== After the rebuttal\n\nThanks for providing details.\n\nFor the Proposition 1 proof, it needs to provide that the slack term does not disappear for some f and h. In particular, the generalization error in Proposition 1 is decomposed in three terms in the proof. And each term is *independently* bounded by a uniform convergence bound. However, this does not mean that there exist f and h such that the slack term is not canceled by other terms. Moreover, the argument on the existence of an irreducible slack term may not be achievable via upper-bounding the generalization error since I can take any upper bound and then artificially add some slack term to claim that there is an irreducible slack term. \n\nI think having a correct proof Proposition 1 is necessary to generalize the paper\u2019s intriguing observation \u2014 \u201cIn real-world production, however, we have encountered key problems that cannot be justified by existing knowledge. They raise concerns that the naive use of pretrained representation as feature vector could lead to unwarranted and suboptimal solution.\u201d I agree that Proposition 1 is an intermediate step, but this is the basis of the paper\u2019s argument. Otherwise, authors\u2019 need to empirically justify their observations with a large number of experiments with various models and datasets.\n\nFor these reasons, I still lean toward rejection, but I would see the value of the proposed algorithms (as heuristic) and empirical results.  \n\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6128/Reviewer_VPk5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6128/Reviewer_VPk5"
        ]
    },
    {
        "id": "uP818A5ErD",
        "original": null,
        "number": 3,
        "cdate": 1667010980096,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667010980096,
        "tmdate": 1667010980096,
        "tddate": null,
        "forum": "IC8LwiOLKFr",
        "replyto": "IC8LwiOLKFr",
        "invitation": "ICLR.cc/2023/Conference/Paper6128/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The Author's investigation reveals critical insights into the gap of uniform convergence for analyzing pre-trained representations, their stochastic nature under gradient descent optimization, what model convergence means to them, and how they might interact with downstream tasks. Authors propose a simple approach which contributes to both applied and theoretical research of representation learning. ",
            "strength_and_weaknesses": "Strengths:\n1) Authors explain concerns with existing methods clearly.\n2) Authors divide each of their theoretical results and give a takeaway summary for better understanding for readers \n3) Authors back their theoretical claims with empirical results \n\nWeaknesses:\n1) Setting considered in the paper is too simplistic to be broadly used in the industry. \n2) More connection between theoretical results and empirical results is not discussed well enough. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is clearly written and the contribution is novel. ",
            "summary_of_the_review": "\n1) In section 3, why is \\Theta d x k when y is just a scalar? How to make sense of loss in this case?\n\n2) Section 4.2 \"instability of \u02c6h(x): the exact position of \u02c6h(x) in Rd is stochastic, depending on the initialization and the order of the pretraining data that is fed to SGD\": What if the initializations are same every time? \n\n3) \"In addition to improved stability and performance\": How is stability measured in Table 1?\n\n4) How was hyperparameter tuning done for the main results? \n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6128/Reviewer_vPVs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6128/Reviewer_vPVs"
        ]
    }
]