[
    {
        "id": "9KWNK4Sv69",
        "original": null,
        "number": 1,
        "cdate": 1666538974378,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666538974378,
        "tmdate": 1670861415128,
        "tddate": null,
        "forum": "gwizseh-Iam",
        "replyto": "gwizseh-Iam",
        "invitation": "ICLR.cc/2023/Conference/Paper506/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a method for learning fully-grown binary decision trees with axis-aligned splits by gradient descent.\nFor that, it first describes a formulation for determining a point route through the tree (based on an integer formulation of logical-ands and logical-ors) given the set of decision splits, and its output. The splits are parametrized by an index vector (that selects the features to evaluate at each branch node) and a threshold vector (that determines whether the point should go left or right), and the leaf nodes are each associated by a probability distribution over possible outputs.\n\nThen, the paper describes how to optimize the tree parameters. As the overall decision function is piece-wise constant, the authors make use of the straight-through operator to update the discrete parameters by gradient descent. \nAn empirical analysis assesses how the proposed method compares with CART and a genetic tree search method in terms of test accuracies, generalization gap, running times, prediction confidence, robustness to hyper-parameter changes.",
            "strength_and_weaknesses": "## Strengths\n\nThe formulation and method are described in great level of detail. The contributions are clearly stated from the beginning and the empirical analysis studies several aspects of the method's performance. Of particular interest is the study of the performance of all methods using the default values for the hyper-parameters, which shows that all studied methods are quite robust to non-optimal choices of hyper-parameters.\n\nTo the best of my knowledge, the proposed formulation of tree routing is novel. It is also easier to read and understand than those of existing methods, in particular of the cited work by [Bertsimas and Dunn, 2017] and of [3], and more practical, as it does not require constraints reflecting the hierarchy of the tree. \n\n\n## Weaknesses\n1. **The motivation for learning axis-aligned splits (as opposed to oblique) is weak**. In the paper it is said that axis-aligned trees are more interpretable than oblique ones. However one could argue that learning oblique splits allow to obtain trees shallower than with axis-aligned ones, hence with fewer decision rules. For this, trees with oblique splits could be easier to read and to interpret. Moreover, the proposed approach learns fully-grown trees, which are considered less interpretable than sparse ones (e.g., see discussion in [5]).\n\nIt would be valuable to develop this discussion in the paper, especially because:\n- the argument that axis-aligned trees are more interpretable is used to justify the choice of restricting the empirical comparison only to axis-aligned tree baselines;\n- the actual interpretability of the learned trees is not studied in the paper.\n\n 2. **The work should be better placed within the relevant literature**. \n\n- Some references are missing from the literature on optimal decision trees and the one on soft-trees, e.g. [2], [3], [4]. In particular, [3] also optimizes deterministic tree routings by gradient descent, hence it is possibly the closest related work. It would be valuable to discuss the differences between the two methods' formulations (e.g., see \"Strengths\") and of optimization choices (e.g., in which operations the biases are introduced, output nature, loss options). The current work could also take inspiration from the pruning procedure of [3] for extending the method to learning of sparse trees.\n\n- Several times in the main text the optimization procedure is described as \"an adjusted gradient flow\". However,  the updates are discrete so it is not clear why the procedure is described as a \"flow\". Also, the term \"straight-through operator\" should be preferred over \"adjusted\" because it is more specific and references the rich literature that leverages it, starting from the seminal work [1] that should be cited in the paper. Finally, it should be highlighted in the paper that the \"straight-through operator\" introduces a mismatch between forward and backward passes, hence the **claim that \"GDTs directly optimize the loss function\" (page 2) is not technically true**.\n\n- In page 6 an algorithm for performing gradient descent with momentum is mentioned but its name or reference is not provided.\n\n- The paper should be self-contained. However in Section 4 the results of four experiments are commented in the main text but reported in the appendix. I suggest to choose the most significant experiment to report and comment in the main text, and defer the remaining in the appendix. To gain some space, Sections 3.3 and 3.4 could be considerably compacted (e.g., Eq 7 and 8 are not needed once the term straight-through operator is used).\n\n3. To judge the potential impact of the method, the performance of existing differentiable tree learning methods and, when feasible, of optimal tree learning methods should be reported as well. The current empirical evaluation reports the results of baselines that are not always state-of-the-art in terms of accuracy, and it would be valuable to check whether the proposed method improve over the running times of these lines of works, even though it does not over greedy approaches.\n\n## Minors\n- page 3: $d$ is not defined\n\n[1] Yoshua Bengio, Nicholas L\u00e9onard, Aaron C. Courville: Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation, 2013\n\n[2] Ryutaro Tanno, Kai Arulkumaran, Daniel C. Alexander, Antonio Criminisi, Aditya V. Nori: Adaptive Neural Trees. ICML 2019\n\n[3] Valentina Zantedeschi, Matt J. Kusner, Vlad Niculae: Learning Binary Decision Trees by Argmin Differentiation. ICML 2021\n\n[4] Emir Demirovic, Anna Lukina, Emmanuel Hebrard, Jeffrey Chan, James Bailey, Christopher Leckie, Kotagiri Ramamohanarao, Peter J. Stuckey: MurTree: Optimal Decision Trees via Dynamic Programming and Search. J. Mach. Learn. Res., 2022.\n\n[5] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, Chudi Zhong: Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges, Statistics Surveys, 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "The **clarity** and **quality** of the work should be improved. The method is described in great detail and the contributions are clearly stated, but some concepts are badly named/referenced, the comparison is restricted to very few baselines without a strong motivation.\n\nThe tree routing formulation is **novel** to the best of my knowledge. To highlight this contribution, a discussion on the difference with existing formulations should be reported. The optimization procedure itself is not novel: it boils down to a well-known trick in discrete optimization.\n\nThe **reproducibility** of the work is very high. The implementation and scripts for running the experiments were submitted as supplementary material, the method is described in great level of detail and the hyper-parameter values are reported for all datasets and methods.",
            "summary_of_the_review": "Overall the paper proposes a novel formulation for learning fully-grown axis-aligned decision trees. The presentation should be improved, starting from better placing the work within the relevant literature, providing a thorough discussion of the benefits of learning axis-aligned splits and including (at least) an optimal tree baseline in the empirical comparison. Also the misleading claim of page 2 (see Weaknesses) should be rectified. I would be happy to increase my score accordingly.\n\n**update after rebuttal**\nThe authors thoroughly revised the paper based on the feedback, and for this I raised my score.\n\nThe only remaining issue from my side is the motivation for learning axis-aligned split functions and for restricting the empirical comparison only to this type of trees. The interpretability of the learned trees (which is used to justify axis-aligned functions) should have been analyzed, for instance by comparing the number of decision functions needed by each method to achieve similar accuracies. Without such analysis, it is not clear why a practitioner would prefer this new method to existing ones, in particular as it learns only fully-grown trees.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper506/Reviewer_ePjz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper506/Reviewer_ePjz"
        ]
    },
    {
        "id": "4oNHE4ePrv",
        "original": null,
        "number": 2,
        "cdate": 1666655907943,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666655907943,
        "tmdate": 1666655998057,
        "tddate": null,
        "forum": "gwizseh-Iam",
        "replyto": "gwizseh-Iam",
        "invitation": "ICLR.cc/2023/Conference/Paper506/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a method for constructing decision tree classifiers that does not rely on performing greedy splits. Instead, the method constructs a soft decision tree approximation to conventional hard split axis-aligned decision trees, and then trains this soft tree with backpropagation and gradient-based optimisation algorithms. The performance of the algorithm is compared with conventional CART trees and an evolutionary optimisation induction algorithm on a variety of small tabular datasets. The proposed method had comparable performance with CART.",
            "strength_and_weaknesses": "### Strengths\n* Investigating alternatives to greedy splitting for constructing decision trees is an interesting problem, and the paper does a good job of highlighting why this is the case by including an example of where greedy splitting is not optimal.\n* The method is quite simple and the description is relatively easy to follow.\n\n## Weaknesses\n* This paper is not the first to use gradient-based optimisation for axis-aligned decision tree induction. For example, see work by Gouk et al. (2019), Yang et al. (2018), and Suarez and Lutsko (1999). The paper should include a more thorough discussion of relevant related work.\n* The authors assume that gradient descent performs global optimisation, but this is in general only true for some classes of functions. The authors should demonstrate that their decision tree representation belongs to one of these function classes if they want to make that claim.\n* Experimental results are quite limited. A comparison should be performed with more recent prior work (e.g., those listed above), and a neural network baseline should also be included. It would also be interesting if the paper included experiments on something beyond just classification accuracy on UCI-style tabular datasets; e.g., investigation of performance on regression datasets, or other tasks where the ability to specify an arbitrary loss function would be useful. The authors may also consider evaluating how well these decision trees are able to leverage standard ensembling techniques---something that standard decision trees are excellent candidates for.\n\n1. Gouk et al. Stochastic Gradient Trees. Asian Conference on Machine Learning, 2019.\n2. Yang et al. Deep Neural Decision Trees. ICML Workshop on Human Interpretability in Machine Learning, 2018.\n3. Suarez and Lutsko. Globally Optimal Fuzzy Decision Trees for Classificaiton and Regression. IEEE TPAMI, 1999.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I have concerns regarding the novelty with respect to previous approaches that also aim to train trees with gradient-based optimisation. There is an existing body of work that follows the same high-level strategy as this paper (i.e., constructing a tree with soft splits), with perhaps the most relevant piece of work being Yang et al. (2018). In addition, I think the quality of the paper could be improved by running some more experiments that back up some of the claims.",
            "summary_of_the_review": "Given the lack of acknowledgement and comparison with very related prior work, missing baselines, and minimally informative experimental results, I cannot recommend acceptance. I would also argue that since there is no representation learning in this paper, that ICLR is perhaps the wrong venue.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper506/Reviewer_tioQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper506/Reviewer_tioQ"
        ]
    },
    {
        "id": "hZf1kkCRpA",
        "original": null,
        "number": 3,
        "cdate": 1666656123813,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656123813,
        "tmdate": 1670773857457,
        "tddate": null,
        "forum": "gwizseh-Iam",
        "replyto": "gwizseh-Iam",
        "invitation": "ICLR.cc/2023/Conference/Paper506/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a new approach for computing interpretable, axis-aligned decision trees using gradient descent by applying backpropagation with adjusted gradient flow on dense representation of decision trees. Experimental evaluation finds improvement over two baselines (CART, GeneticTree) in binary classification datasets and competitive performance on multi-class datasets.",
            "strength_and_weaknesses": "Strengths:\n* Interesting approach for computing interpretable, axis-aligned decision trees using gradient descent\n* Experimental evaluation show the computed trees outperform decision trees computed by baselines in several binary classification benchmarks and are obtain competitive results on multi-class benchmarks. \n\nWeaknesses:\n* The main weakness is that the paper is missing several highly-related works on gradient-based decision trees. Several recent examples are [1], [2], [3]. While they are not identical to the propose approach (e.g., some focus on ensemble models), they all deal with computing decision trees using gradient descent algorithms. In particular, these approaches have considered similar ideas such as representing cuts as (relaxed) one-hot encoding (e.g., using sparse softmax variants in [1] [2]), computing membership of sample in a leaf using multiplication of logistic-based split functions, etc. In particular, [2] presents approach for temperature annealing that gradually turns the splits to one-hot (i.e., axis-aligned). These papers should be cited and discussed, the similarities and differences should be highlighted, and the novelty should be demonstrated compared to these approach.\n* Baselines: The papers mentioned above (specifically the computing of decision trees in these papers) represent natural baselines that should be compared to in the experimental evaluation.\n* The approach demonstrates relatively modest performance gain is in binary classification and underperforms compared to CART on multi-class datasaets.\n\nMinor question: The Adult dataset has many categorical variables. How were they encoded?\n\n\n[1] Popov, Sergei, Stanislav Morozov, and Artem Babenko. \"Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data.\" International Conference on Learning Representations. 2019.\n\n[2] Chang, Chun-Hao, Rich Caruana, and Anna Goldenberg. \"NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning.\" International Conference on Learning Representations. 2021.\n\n[3] Blanquero, Rafael, et al. \"Sparsity in optimal randomized classification trees.\" European Journal of Operational Research 284.1 (2020): 255-272.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is clear and well-written.\n\n- Novelty: The novelty of the approach is not clear due to missing discussion on related works as noted above. In particular the claim that this is the first approach that attempts to learn univariate, axis-aligned decision trees using gradient descent seems to contradict the results of papers [1] and [2]. [1] experimented with gumbel-softmax as well as sparse softmax variants that can become univariate/one-hot depending on their sparsity hyper-parameter and [2] learned decision trees that are univariate and axis-aligned due to the temperature annealing procedure (they use oblivious trees, but this can similarly work for non-oblivious trees).\n\n- Reproducibility: I did not find important missing details in the paper (except for my question on the handling of categorical variables). The authors have attached their implementation and evaluation code.\n",
            "summary_of_the_review": "The paper presents an interesting approach for computing interpretable, axis-aligned decision trees using gradient descent. The main weaknesses are the missing highly-related literature, the limited novelty, and the limited performance gains.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper506/Reviewer_zziu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper506/Reviewer_zziu"
        ]
    },
    {
        "id": "KAmJ7uj6V7w",
        "original": null,
        "number": 4,
        "cdate": 1667478135202,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667478135202,
        "tmdate": 1667478135202,
        "tddate": null,
        "forum": "gwizseh-Iam",
        "replyto": "gwizseh-Iam",
        "invitation": "ICLR.cc/2023/Conference/Paper506/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel training algorithm for axis-aligned decision trees based on gradient descent. The tree is represented via three vectors: the first one is responsible for storing indices of nodes' split features, the second one responds the split value, and the third one stores leaf values. These vectors are represented in a contiguous form (in particular, a probability distribution for each index value) which can be further used for joint gradient descent optimization. The experimental results demonstrate comparable quality for binary classification tasks. It is highly debatable whether it is reasonable to apply the proposed method to multiclass problems.",
            "strength_and_weaknesses": "+ Interesting direction and ideas\n\n- Much work to do, the profit is questionable. According to Tavle 1, the proposed method does not outperform classical baseline CART. This is not consistent whit the claims in Introduction. ",
            "clarity,_quality,_novelty_and_reproducibility": "The problem of greedy decision tree growth is well-known. Although there were several attempts to get rid of standard greedy procedures in the previous literature, the method prosed by the authors is novel and interesting. However, this is not the first attempt to construct a differentiable axis-aligned tree as stated in the paper (see, e.g., Yang Y. et al. Deep neural decision trees); the significance of the results is arguable.",
            "summary_of_the_review": "I like the direction of the proposed method, most of the ideas are sound and interesting. However, and I think there is still much work to do.\n\n- The statement \"this is the first approach that attempts to learn univariate, axis-aligned decision trees with gradient descent\" is too loud since several papers are solving the same task.\n\n- The paper lacks theoretical justification. It would be interesting to see under what conditions one can find a global optimum and examples of synthetic data on which this method will work better than greedy construction. In particular, is this model capable of solving the \"xor problem\"? I lacked intuition about what the solution to the optimization problem converges to before applying hardmax, and this aspect could be discussed more. For example, it seems that if GDT is used for a regression problem with MSE loss, then the solution should converge to hard splits (but not necessarily optimal) with mean values in the leaves. \n\n- I do not fully understand the trick described by Equation 7. Can it be interpreted as \"we do not care about soft and contiguous nature of \\iota and S during training but apply hard max at the end\"? If so, I do not see any problems with it; it could be written explicitly for more clarity.\n\n- The choice of baseline algorithms is not clear. Why were Evolutionary Decision Trees used but not some other differentiable trees? Results show that it is quite a weak baseline, even compared to classic CART trees.\n\n- The experimental part would be more solid if it consisted of examples of surfaces found by greedy tree and GDT and the plot demonstrating quality vs cardinality dependence.\n\n- I do not understand why potentially more expressive GDT is less prone to overfitting and, at the same time, has comparable or even lower quality. It can be due to the validation set used for GDT training. Other algorithms do not use them, but many pruning techniques could make them more robust, and at the same time, reducing the train set may be crucial for your model (especially with small datasets).\n\n* Questions\n\n- Unfortunately, the results are not so good for some datasets. What can you state about the limitations of applying the proposed method to real-world problems (how to decide whether to use it or not)?\n\n- Experience shows that using more expressive trees in ensembles (such as GBDT, for example) can lead to a decrease in the quality of such models due to overfitting on the level of individual trees. Have you tried using GDT in ensembles? Does it make sense, especially in the context of much longer training time?",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper506/Reviewer_Dpk7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper506/Reviewer_Dpk7"
        ]
    }
]