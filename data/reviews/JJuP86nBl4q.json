[
    {
        "id": "RiVQ-qaO4u",
        "original": null,
        "number": 1,
        "cdate": 1665781833194,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665781833194,
        "tmdate": 1669332230525,
        "tddate": null,
        "forum": "JJuP86nBl4q",
        "replyto": "JJuP86nBl4q",
        "invitation": "ICLR.cc/2023/Conference/Paper857/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a learning-agnostic data valuation framework based on optimal transport (OT). The authors suggest using a Class-wise Wasserstein distance between two datasets as a surrogate for validation performance, thus alleviating the need to pre-specifying the learning algorithm in advance. The authors then propose a new data valuation metric based on the gradient of the OT distance with respect to the perturbation on the probability mass associated with each data point. Even though this data valuation metric is difficult to compute, the paper points out that the difference can be calculated efficiently, allowing for an efficient ranking of the data points. The paper then presents empirical results to demonstrate the efficiency of the proposed methods on several different problems.",
            "strength_and_weaknesses": "Strengths: \n\n(+) The paper is sufficiently novel and the approach is interesting.\n\nWeaknesses: \n\n(-) The proposed approach lacks justification.\n\n(-) The experiment setups are unclear.\n\n(-) The presentation can be improved.\n\nDetailed comments:\n\n- (main concern - soundness) Theorem 1 resembles a statistical learning generalization bound, where we have an out-of-sample error (in this case, the validation set\u2019s error) upper bounded by the training set\u2019s error and some surplus term (the OT distance). By arguing that the training set\u2019s error is close to 0 for sufficiently complex models, the authors justify the use of OT distance as a surrogate for the training set\u2019s error by this theorem. However, one can apply the same logic to other generalization bounds (e.g., one based on VC dimension) to justify the use of VC dimension as a surrogate for the validation error. I find that this theorem does not satisfactorily justify the use of OT distance as a surrogate for the validation error.\n\n- (main concern - soundness) Theorem 1 is vacuous and follows almost immediately from the assumptions that $\\mathcal{L}$ is $k$-Lipschitz and the norm of $f$ is bounded by V. Could the authors elaborate on the challenges/difficulty in the proof of Theorem 1.\n\n- The authors did not explain the experiment setups (not even in the appendix). What is the validation dataset here? And in the mislabeled or noisy feature experiments, are the validation data corrupted too? It is unclear how the proposed framework is applied in these experiments.\n\n- (presentation) In the paragraph \u201cRadius for accurate predictions\u2026\u201d, can the authors be more specific about the results in Bertsimas & Tsitsiklis (1997). I find this paragraph a bit hand-wavy.\n\n- (presentation) I do not follow the argument in this sentence (at the beginning of Section 3.1): \u201cOT distance is known to be insensitive to small differences while also being not robust to large deviations\u201d. I think the phrase \u201cnot robust\u201d is not appropriate here. Also, I\u2019m not sure why that is a desirable property in data valuation.\n\n- (presentation) Please use more equation numbering: Throughout the paper, the authors refer to various quantities/definitions via section number. This is very challenging for readers to follow. Could you please put important quantities/definitions in equation environments with numbers and refer to them?\n\n- (minor aesthetic comment - presentation) The results in Figures 2 and 3 are blurry. Although I can still understand the results, it would be nicer if the authors can include clearer versions of these plots.",
            "clarity,_quality,_novelty_and_reproducibility": "Significance: The problem of data valuation is an important and contemporary problem in machine learning. The paper proposes a method to decouple the data valuation process from the learning algorithm. This is a meaningful contribution and is suitable to be presented at ICLR 2023.\n\nSoundness: The paper proposes new solutions to the data valuation problem. However, I feel that these solutions are not satisfactorily justified. The main theoretical result (Theorem 1) seems to be very vacuous and follows almost immediately from the Lipchitzness assumption. Please refer to the section below for more detailed comments.\n\nNovelty: The paper incorporates concepts from optimal transport to the problem of data valuation, offering an unconventional alternative to the common Shapley value (or more broadly cooperative game theory) approaches to data valuation.\n\nPresentation: The paper is generally well-written and easy to follow. However, the authors occasionally make some arguments that are hard to follow or not well justified by the evidence. Please refer to the section below for more detailed comments.",
            "summary_of_the_review": "The paper offers a fresh perspective on the problem of data valuation using ideas from optimal transport. However, the proposed framework is not sufficiently justified, both theoretically and empirically. The presentation of the paper can be improved.\n\n======= Post Rebuttal =======\n\nThe authors address some of my concerns and I have increased the score accordingly.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper857/Reviewer_5BsF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper857/Reviewer_5BsF"
        ]
    },
    {
        "id": "hx3gHPbI2xG",
        "original": null,
        "number": 2,
        "cdate": 1666521127424,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666521127424,
        "tmdate": 1668762579998,
        "tddate": null,
        "forum": "JJuP86nBl4q",
        "replyto": "JJuP86nBl4q",
        "invitation": "ICLR.cc/2023/Conference/Paper857/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a datapoint valuation method, LAVA, that does not require a pre-defined learning algorithm, which is a common assumption in the existing literature. It utilizes the Wasserstein distance between the training set and the validation set with respect to a hybrid cost that considers both feature and label distances. The authors prove that the distance has theoretical connections with the validation performance (i.e., the utility) and show that the distance can be efficiently calculated using existing solvers. With that, the authors propose to use calibrated gradient to account for a datapoint\u2019s contribution to the distance and thus its value. The paper has done extensive empirical experiments on 5 important application scenarios and demonstrated superior performances. Ablation studies are also done to better understand the behaviors of the proposed method. ",
            "strength_and_weaknesses": "Advantages: \n1. The paper is well-written and easy to follow. The authors additionally give lots of interpretations after results or formulations to help with the understanding. \n2. Label information is incorporated into the dataset distance design.\n3. The theoretical part of the paper is sound. Also, the connection from OT distance sensitivity to individual datapoint value is neat and natural.\n4. The author provides rather extensive empirical validations of the method and showed state-of-the-art performance. An interesting new application on \u201cirrelevant data detection\u201d is also included, which was originally often discussed as mislabeled data detection.\n5. Discussions on the limitations of LAVA in the conclusion are valid and indeed insightful.\n\nDisadvantages:\n1. The motivation for developing a learning-agnostic valuation method, especially in the context of the existing works with similar dataset distance approaches could be improved.\n2. Some small parts of the experiment details require clarification.\nFor more details, see below.",
            "clarity,_quality,_novelty_and_reproducibility": "1. In the introduction, one motivation for a learning-agnostic method is that the retrainings of the models are too expensive in LOO and CGT. However, I do not think LAVA directly addresses this problem, at least about CGT, because LAVA is more like a perturbation-based contribution measure essentially similar to influence function (INF). LAVA performs a LOO-like computation and does not consider combinations (like in CGT). Also, please give formal justifications with references that the LOO and CGT methods \u201cremain expensive\u201d given the approximations.\n\n2. This is related to the novelty of the work, and how insight it has contributed to the community. There have been attempts to use distribution divergence or dataset differences for data valuation. One reason for choosing OT is that OT is computationally tractable from finite samples: As far as I know, MMD used in [1] has similar properties. It is also essential to draw connections between distribution divergence to learning performance: Another work [2] also partially uses MMD to bound generalization performance. Discussions might be needed to examine the significance of LAVA in the context of the existing works mentioned above.\n\n3. The paper has extensive applications and great performance in detecting \u201cbad\u201d data. However, we care about \u201cgood\u201d data as well. Can LAVA perform data summarization tasks (another common application of data valuation) as well? I know that Appendix B.4 is relevant to this question, but I am looking at a much smaller subset size for summarization. For example, can you select 1K points to train a network? This is essentially about the effectiveness of LAVA in picking representative and highly valued datapoints. I would expect a faster increase in model performance when including the highest value points first. \n\n4. In Section 3.1, it is unclear to me how to calculate the actual change in the OT distance when we perturb the datapoint probability mass by a given amount, if we calculate OT distance based on finite samples?\n\n5. Can you also clarify why is the difference in groundtruth values of the calibrated gradients in Section 3.2 enough to rank all data points? Do you calculate this difference with respect to a common datapoint selected and then rank them accordingly?\n\n6. Why is the feature extractor trained with the validation dataset? Will this introduce any form of bias since the validation dataset will be used for the data valuation step? Will an extractor trained on the overall training dataset work?\n\n7. In Figure 3, it really confuses me that most of the baseline methods perform even worse than random guessing in terms of the detection rate. Are the baselines implemented correctly? Or do you have an explanation for this abnormal behavior? Also, it is weird that some attack/model accuracies do not start from the same point when you start to throw away data (bottom plots).\n\n8. Regarding the Poisoning Attack Detection experiments, you mentioned that this task is \u201cespecially hard to detect\u201d. However, the baseline methods actually work better than those in Backdoor Detection. In contrast, LAVA does not perform here as well as that in Backdoor Detection. Any explanation?\n\nMinor: \n\n9. Referring to the discussion on Figure 1, it would be helpful to briefly explain how do you select (or manipulate) the datasets to create different Wasserstein distances to the validation set when drawing the curves? Or alternatively, do you change the validation set?\n\n10. For Appendix B.2.4, I am interested to know how does the model directly trained on the validation dataset detect bad data?\n\nReferences: \n\n[1] Incentivizing Collaboration in Machine Learning via Synthetic Data Rewards. AAAI 2022.\n\n[2] DAVINZ: Data Valuation using Deep Neural Networks at Initialization. ICML 2022.",
            "summary_of_the_review": "Overall, I like the idea of a label-aware dataset distance measure as a data valuation metric despite borrowed from the OT literature. The method is backed by theoretical connections to validation performance, which is of interest in the topic of data valuation. The datapoint\u2019s sensitivity to OT distance also naturally fits the marginal contribution concepts often used in data valuation. If my concerns above can be addressed, I think this work could contribute value to the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper857/Reviewer_BSJ2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper857/Reviewer_BSJ2"
        ]
    },
    {
        "id": "dMKT5vhmjrk",
        "original": null,
        "number": 3,
        "cdate": 1666671606129,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671606129,
        "tmdate": 1666671606129,
        "tddate": null,
        "forum": "JJuP86nBl4q",
        "replyto": "JJuP86nBl4q",
        "invitation": "ICLR.cc/2023/Conference/Paper857/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a data valuation method of individual training data points that does not requires learning algorithms via the gradient of the Wasserstein distance between the training set and validation set w.r.t. the perturbations on the probability mass of a data point. While it is based on existing work on the hierarchically-defined Wasserstein distance between datasets, the paper provides new insight into the connection between this distance and the validation performance which is crucial for data validation. Furthermore, it proposes an efficient data valuation method of individual training data points based on the sensitivity of this distance, which has a wide range of use cases as demonstrated in the experiments.",
            "strength_and_weaknesses": "The strength of the paper is in the application of the existing Wasserstein distance between datasets to the data valuation problem with the new insights on the connection between the distance and the validation performance. Additionally, it demonstrates many use cases of the proposed data valuation method in the experiments. The paper is also well-written so that readers can understand the main idea easily.\n\nThe main weakness of the paper is probably due to the fact that the proposed data valuation only works for individual data points. I have several questions as follows.\n\n1. The proposed approach requires a validation dataset with labels. How would the Wasserstein distance be modified to measure the distance between a training set and a 'reference dataset' without labels?\n\n2. While the paper motivates the problem of data valuation as a central method to the emerging data economy (in data exchange), the proposed method cannot value data sources (which consist of multiple data points). Hence, I am wondering how this approach can be applied in data exchange (between multiple organizations/companies with different datasets).\n\n3. It is often the case that there are duplicates of data points (or redundant data points that are very similar to one another) in the real-world dataset. For simplicity, let us consider a dataset consisting of 3 data points x1,x2,y where x1 and x2 are duplicates of each other; and y is different from x1,x2. Intuitively, removing x1 does not affect the value of the dataset (since x2 contains the same information as x1), so the value of x1 is low. Hence, point-wise values of x1 and x2 are low. However, if we remove both x1 and x2 (since they have low valuation), then it is problematic because y does not contain information in x1 and x2. How does the proposed pointwise data valuation method work in this case?\n\n4. Like the work of Koh & Liang (2017), can the proposed method be used to explain a particular prediction of the model by finding the training data point that is responsible for the prediction? In this case, the analogous notion of a validation set contains only a single data point (the prediction to be explained), so I am wondering if the approach can still work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "While the paper applies the existing Wasserstein distance between datasets to the data valuation problem, it proposes novel perspectives through the new insight into the connection between the distance and the validation performance and the use of gradients in constructing an efficient valuation method. The paper is well-written and easy to follow. The significance of the proposed approach is demonstrated through many use cases in the experiments.\n",
            "summary_of_the_review": "While limited to pointwise data valuation, this work proposes a new perspective in constructing a data valuation method without a learning algorithm and empirically evaluates the performance in various use cases. Thus, I believe the contribution and quality of the paper are sufficient.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper857/Reviewer_uRtv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper857/Reviewer_uRtv"
        ]
    },
    {
        "id": "P7zQpPxFXQ2",
        "original": null,
        "number": 4,
        "cdate": 1666723023186,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666723023186,
        "tmdate": 1666723023186,
        "tddate": null,
        "forum": "JJuP86nBl4q",
        "replyto": "JJuP86nBl4q",
        "invitation": "ICLR.cc/2023/Conference/Paper857/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a data valuation method that is oblivious to the downstream learning algorithm. The main idea is to evaluate the training data by a class-wise Wasserstein distance between the training and the validation set. They prove that the class-wise Wasserstein distance approximates the performance for any given model under certain Lipschitz conditions. They also propose a method to evaluate individual data by the sensitivity analysis of this class-wise Wasserstein distance. Finally, the paper empirically evaluates the performance and efficiency of their methods. They show that their method improves the state-of-the-art performance while being orders of magnitude faster.\n",
            "strength_and_weaknesses": " The paper studies an interesting problem and proposes a novel data valuation method using a natural measure based on the class-wise Wasserstein distance. They provide solid theoretical justification for their approach by proving that their measure characterizes the upper bound of the validation performance of any given models. They also propose an interpretable and easily-computable measure for individual data valuation. Finally, they conducted extensive experiments to test the efficacy and efficiency of their approaches in practical use cases.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very organized and well-written. The contribution is novel as far as I know.",
            "summary_of_the_review": "Overall, I think this is a good paper with nice conceptual contribution and solid theoretical analysis and extensive empirical evaluations that show the efficacy of their data valuation approaches in practical use cases.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper857/Reviewer_7VZM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper857/Reviewer_7VZM"
        ]
    }
]