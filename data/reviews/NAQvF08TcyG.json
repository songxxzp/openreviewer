[
    {
        "id": "X4bVpP66eil",
        "original": null,
        "number": 1,
        "cdate": 1666654120609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654120609,
        "tmdate": 1666654120609,
        "tddate": null,
        "forum": "NAQvF08TcyG",
        "replyto": "NAQvF08TcyG",
        "invitation": "ICLR.cc/2023/Conference/Paper317/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a way to personalize large, pre-trained text-to-image diffusion models based on specific objects/styles presented via a small amount (3-5) of images. To achieve this, a new text-embedding is obtained by optimizing it via the reconstruction loss for the small set of training images. After some fine-tuning steps the new text-embedding can be used in normal captions to describe the original object/style in novel setting and surroundings. The experiment show that this approach is more successful than other baselines such as Guided Diffusion or VQGAN+CLIP loss.",
            "strength_and_weaknesses": "The paper proposes a novel approach to personalize pre-trained text-to-image models. This allows users to generate specific objects or styles which were not seen during training. Compared to several baselines the approach performs much better and is able to generalize the new concepts to different surroundings and settings at test time.\n\nSeveral ablation studies also show that encoding a novel concept into a single text embedding (as opposed to several text embeddings or applying specific regularization terms) obtains the best trade-off between semantic reconstruction and text-image alignment.\n\nMy main questions are related to the robustness of the approach and its applicability to other text models.\n\nRegarding robustness: how much is the final results and generalization affected by\n* the chosen caption templates during training\n* the number of fine-tuning steps\n* the number of training images and the diversity in the training images (e.g. background, lighting, etc)\n* learning rates\n\nRegarding applicability to other text models:\n* you mention in the supplementary that this approach doesn't work as well for Stable Diffusion due to its use of CLIP instead of BERT, what does this mean for other models? Do you think this approach would work for DALLE2 which isn't conditioned on a sequence of tokens?\n* what are the characteristics that are needed to make this approach work, generally speaking?\n\nYou also briefly mention Pivotal Tuning in the supplementary which includes fine-tuning the model itself, too. I guess this is somewhat similar to DreamBooth (concurrent work, so I don't expect a comparison). It would be helpful to include more thoughts about the comparison between those two appraoches in general (finetuning a text embeddings vs finetuning a model itself), what the trade-offs between those two appraoches are, etc..",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, novel, and reproducible.",
            "summary_of_the_review": "Overall I like the paper and it seems to obtain good results and be somewhat novel.\nHowever, I believe it would benefit from some more clarity about robustness and generalization capability, as well as how easy it would be to apply it to other models (not based on BERT) and how it compares (generally speaking) to approaches that finetune the model directly instead of finetuning a text embedding.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper317/Reviewer_xRCj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper317/Reviewer_xRCj"
        ]
    },
    {
        "id": "YfSRL7Ye5D",
        "original": null,
        "number": 2,
        "cdate": 1666663316200,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663316200,
        "tmdate": 1666663316200,
        "tddate": null,
        "forum": "NAQvF08TcyG",
        "replyto": "NAQvF08TcyG",
        "invitation": "ICLR.cc/2023/Conference/Paper317/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for utilizing a pre-trained text-to-image diffusion model to generate novel images with a specific concept referenced with few samples.\nBy training an additional text token with the reconstruction loss for reference images, the model gets to generate various images with newly trained concept.\nSince this model does not fine-tune the generator itself, the new concept can softly incorporate with other concepts the original model can generate already.",
            "strength_and_weaknesses": "Strengths\n- By the nature of the model, users only could manipulate a text prompt to get a specific style of image in mind. But it is hard to find such a prompt as can be seen in Fig 3. The proposed method solved the issue by finding a way to use a reference image set as a prompt, which is to train a new token for the concept in the images. Further, since the process does not hurt the original model\u2019s image generation performance, the newly trained token can mingle with another concept, whether it is the one that the model could generate already or the one that was trained with another sample set likewise.\n\nWeaknesses\n- The results are mostly analyzed qualitatively. Considering the stochasticity of diffusion models, more analysis on failure cases and scores on quantitative metric that can measure the failure rate would be needed.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This work tackles the important issue and broadens the applicability of text-to-image diffusion models with simple, novel method.\nThough it is unclear whether this method would be reproducible for more various concepts, the authors clearly demonstrate their method and its efficacy. \n\n",
            "summary_of_the_review": "Even though it is not clear whether this method would be working consistently well, the method is simple and novel, and the results are impressive enough.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper317/Reviewer_W72Q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper317/Reviewer_W72Q"
        ]
    },
    {
        "id": "_CYhN_EFs0",
        "original": null,
        "number": 3,
        "cdate": 1666707820699,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666707820699,
        "tmdate": 1670580873346,
        "tddate": null,
        "forum": "NAQvF08TcyG",
        "replyto": "NAQvF08TcyG",
        "invitation": "ICLR.cc/2023/Conference/Paper317/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to use a small number of image samples in the text-to-image generation process, where these samples can be converted into pseudo-word to enable novel image generations, related to the given image samples.",
            "strength_and_weaknesses": "Strength:\n1. The paper is well written and easy to follow.\n2. Good qualitative results shown in the paper.\n\nWeaknesses:\n1. Although authors claim their method is to invert several image samples into new pseudo-words, it more likes to use some image features extracted from given image samples, and use these features along with a given text to generate new images. There are already couple of works focusing on image+text -> new images. It might be better that author can discuss the differences between them.\n2. I would like see more quantitative evaluation metrics adopted in the paper, and comparison with state-of-the-art text-to-image generation methods.\n3. How is the diversity of proposed method, does the given image samples constrain the diversity of synthetic results?\n4. If authors provide more image samples (> 3), would this further improve the performance? If these samples are unrelated and do not describe the same object, would this degrade the quality of output images?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well presented, and shows sufficient details about the proposed method. ",
            "summary_of_the_review": "See above weaknesses. I am happy to raise my rating based on authors' responses. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper317/Reviewer_ah2h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper317/Reviewer_ah2h"
        ]
    },
    {
        "id": "5pyO0u533i",
        "original": null,
        "number": 4,
        "cdate": 1666859407156,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666859407156,
        "tmdate": 1666859442396,
        "tddate": null,
        "forum": "NAQvF08TcyG",
        "replyto": "NAQvF08TcyG",
        "invitation": "ICLR.cc/2023/Conference/Paper317/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on the task of textual inversion which in essence tries to capture a concept in a set of images (either style, abstract, object, or relations) as a single new \"word\" which can then we used to guide the generation of the generative models based on this new \"word\". An example would be to extract out abstract things in Pablo Picasso's style and apply it to new images. The paper qualitatively shows how superior it is compared to past approaches and does human evaluations to also verify the same.",
            "strength_and_weaknesses": "### Strengths\n- This is a very novel and important task for guided generative models and the results are impressive. \n- The approach is very simple to understand and extend.\n- Improves our knowledge about the text embedding in the generative models and how can we \"interpolate\" in between the embeddings to leverage the true latent space of text embedding by learning specific and new \"concepts\" and \"words\". The title is very appropriate in that sense.\n- The human evaluations are done and show case how well the approach works. \n- The approach also works in comparison to human written prompts which don't capture nuances in the image\n- It is nice how one can further exploit this \"word\" by specifying style of S* to guide the generation in certain directions.\n\n### Weaknesses\n- Lack quantitative analysis and guidance for future work on how to continue working on this task and have proper evaluations. I had expected the authors to create a dataset for quantitatively evaluating there generation using FID score based on the concepts and generations they have. (Though indeed it will require some creativity to build this)\n- It is unclear how this approach works with images that contain multiple objects or COCO-style scenes. Is one word enough to capture complex scenes. \n- The training setups are not super clear.",
            "clarity,_quality,_novelty_and_reproducibility": "- It is unclear if the approach is reproducible as a lot of diffusion models are proprietary \n- The paper is clearly written and easy to follow.\n- The approach is novel and the reader actually learns something from the paper.\n",
            "summary_of_the_review": "The paper's contributions are significant as it allows using existing pretrained generative models to empower a new use case and leverage the latent space of these models from the corners it wasn't accessible before. I am impressed by the results and contributions of this papers towards understanding these models better and thus suggest an accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper317/Reviewer_a4CY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper317/Reviewer_a4CY"
        ]
    }
]