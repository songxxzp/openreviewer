[
    {
        "id": "IvfjIPp71S1",
        "original": null,
        "number": 1,
        "cdate": 1666629049707,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629049707,
        "tmdate": 1666629049707,
        "tddate": null,
        "forum": "pO7KggcbMiP",
        "replyto": "pO7KggcbMiP",
        "invitation": "ICLR.cc/2023/Conference/Paper1803/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a new vertical federated learning algorithm for bilevel optimization, called BAMBI, and a privacy-preserving version, called BAMBI-DP. To speed up computation, the authors utilized a zeroth-order estimator to locally approximate the Jacobian matrix. The theoretical convergence rate of BAMBI is provided as O(1/sqrt(K)), which is comparable with algorithms without a zeroth-order estimator. Experiments with three real datasets are given to support the algorithms. ",
            "strength_and_weaknesses": "Strength: \nThe targeted problem is interesting. The writing of the paper is clear and easy to follow. Convergence analysis is given. Experiments are conducted on three real datasets.\n\nWeakness:\n1. It looks like Algorithm 1 is very communication expensive. In each iteration, two communications between users and servers are required. Is it possible to reduce communication costs?  \n\n2. Synchronous update is also a very strong assumption it seriously hampers the computation resource utilization in real applications. Is it possible to extend the work to an asynchronous algorithm?\n\n3. For BAMBI-DP, convergence analysis is not given. Could you theoretically show the privacy-convergence trade-off by using DP?\n\n4. In Figure 3 (b), it seems when increasing the DP level epsilon, the curves of the training accuracy are almost the same. This result looks suspicious. Usually, larger epsilon requires more iteration rounds to achieve the same accuracy. Is there any reason for this?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well organized, but the presentation has minor details that could be improved. In terms of novelty, this paper contributes some incremental advances.",
            "summary_of_the_review": "The paper proposed a vertical federated learning algorithm for bilevel optimization and a privacy-preserving version, respectively. However, the contribution seems to be incremental.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1803/Reviewer_unLb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1803/Reviewer_unLb"
        ]
    },
    {
        "id": "UDbCDayC_EI",
        "original": null,
        "number": 2,
        "cdate": 1666636832584,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636832584,
        "tmdate": 1666636832584,
        "tddate": null,
        "forum": "pO7KggcbMiP",
        "replyto": "pO7KggcbMiP",
        "invitation": "ICLR.cc/2023/Conference/Paper1803/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a stochastic Bilevel Optimization Method with a desirable JacoBian estImator (BAMBI) to address the vertical federated learning (VFL) problem, where BAMBI constructs a zeroth-order (ZO) estimator to approximate the Jacobian matrix and compute the hypergradient. It also introduces a BAMBI-DP algorithm to mitigate the concern of label privacy, and provides  some theoretical analysis and extensive experiments.",
            "strength_and_weaknesses": "Strengths:\n1. The paper is the first work to solve the VFL problem under the bilevel programming framework.\n2. The zeroth-order estimation is applied to compute the hyper gradient and can preserve the privacy with low computation cost.\n3. BAMBI preserves the privacy of features from computing the gradient and BAMBI-DP provides label privacy preserving guarantees.\n3. Theoretical analysis is solid and matches the current work of bilevel algorithms.\n\nWeaknesses: \n1. Related work is missing. (i.e. The authors mentioned \"ZO estimation has promising properties of preserving privacy and low computation complexity as analyzed in previous work\". It is not clear to the reviewer what properties the authors are talking about.)\n2. Several notations are vague, which are listed below (Hope the authors can provide some clarifications).\n\nDoubts:\n1.Vague notations.\nIn equation(1), $[x_1,...,x_l] \\in \\mathbb{R}^p$ with each $x_i \\in math{R}^p$ seems not proper, and $[x_1,...,x_l] \\in \\mathbb{R}^{l \\times p}$ may be better. It is same for $[y_1,...,x_l]$.\nIn Algorithm1 and equation(5), what is Q standing for?\n2. What if we do not employ the zeroth-order method and consider the first order method since the privacy leakage is from directly computing the second-order derivatives.\n3. In experiments, only 2, 4 parties are shown in figure 2(a), which seems to be impractical.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has some merit as mentioned in the strengths of the paper. But the writing and presentation need to be improved.",
            "summary_of_the_review": "The paper employs the zeroth-order method to address the privacy preserving and the computational cost issues with regard\nto the VFL problem in the bilevel programing framework, which is somewhat novel. However, the writing and the presentation of the paper need improvement.  Many notations are unclear and further clarifications are needed.\nThe score for the paper can be improved after the doubts are solved.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1803/Reviewer_zLs3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1803/Reviewer_zLs3"
        ]
    },
    {
        "id": "urQk1oHth6G",
        "original": null,
        "number": 3,
        "cdate": 1666651869978,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651869978,
        "tmdate": 1666651869978,
        "tddate": null,
        "forum": "pO7KggcbMiP",
        "replyto": "pO7KggcbMiP",
        "invitation": "ICLR.cc/2023/Conference/Paper1803/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers the problem of bilevel optimization for vertical federated learning. Using a zeroth-order estimator to locally approximate the Jacobian matrix, and adopting GradPerturb algorithm for providing label privacy, it shows convergences in the rate of $O({1}{\\sqrt{K}})$ under the nonconvex-strongly\u0002convex setting. The full algorithms BAMBI and its DP version are written, and then experimentally verified.",
            "strength_and_weaknesses": "Strength: The problem of bilevel optimization under the setting of VFL is interesting, and the paper also considers the privacy concern in VFL.\n\nWeaknesses:\n1. The reviewer does not understand the importance of label privacy in vertical federated learning. Could the authors give more clear explanation for this? How does the Hypergradient leak a specific user's label? \n2. Please provide the definition of differential privacy before using it. \n3. How to deal with the case when there are multiple active parties in the VFL framework, like the settings in [1] Zhang, Qingsong, Bin Gu, Cheng Deng, and Heng Huang. \"Secure bilevel asynchronous vertical federated learning with backward updating.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 12, pp. 10896-10904. 2021. Does the method still achieve label privacy? \n4. For Figure 2 and 3, please give the testing accuracy comparisons instead of only testing loss. \n5. Please provide the real computation time comparisons instead of only complexity analysis. \n6. What are the challenges for providing the convergence analysis for BAMBI-DP?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper needs to provide the explanation why a user's label can be leaked via  the Hypergradient, and also give the definition of differential privacy before using it. ",
            "summary_of_the_review": "Please see Strength And Weaknesses for possible improvements.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1803/Reviewer_AuMJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1803/Reviewer_AuMJ"
        ]
    }
]