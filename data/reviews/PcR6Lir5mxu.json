[
    {
        "id": "VEnPKxUBjbJ",
        "original": null,
        "number": 1,
        "cdate": 1666224619198,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666224619198,
        "tmdate": 1666224619198,
        "tddate": null,
        "forum": "PcR6Lir5mxu",
        "replyto": "PcR6Lir5mxu",
        "invitation": "ICLR.cc/2023/Conference/Paper2501/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new extension of the MuZero algorithm (called online planning to explore, or OP2E) that leverages estimates of uncertainty in the algorithm\u2019s predictions of value and reward to generate exploratory behavior. This approach has the advantage of propagating the uncertainty estimates to the policy through the planning tree, rather than through a value function like in other methods. The paper claims that OP2E is better able to achieve deep exploration, than standard MuZero. The claims are supported by experimental results in two different domains.\n",
            "strength_and_weaknesses": "The main strength of this paper is the extension to MCTS that propagates an estimate of uncertainty through the planning tree, and drives exploration in the environment. This idea is potentially very powerful if one has access to good measures of uncertainty.\n\nThe main weakness of this paper is the experimental evaluation of the claim that OP2E provides improved deep exploration over plain MuZero. The results provided in the two domains are not enough to convince me of this.\n\nThe Slide environment appears to be a genuinely hard exploration task, even though it is a tabular MDP. Results in this domain are good, and support the conclusions. The environment is described as giving a positive reward when the agent reaches the rightmost state. What is the magnitude of this reward? (This would help with my understanding of Figure 1.)\n\nI strongly disagree that Mountain Car is a \u201chard-exploration task used to evaluate advanced exploration approaches.\u201d I think evaluating an exploration algorithm on Mountain Car does not provide much insight. Sarsa with a simple linear function approximation scheme and epsilon-greedy exploration can learn a good policy in less than 500 episodes (p. 246, Sutton & Barto, 2018). This is significantly less than ~20,000 episodes required to learn a good policy for OP2E. Mountain Car is at best a validation test to ensure that the components of an algorithm are working correctly, but not as evidence of advanced deep exploration.\n\nI suggest that the authors test their algorithm on a few more hard exploration problems. Ideally on mix of problems that are tabular and that require function approximation. Parisi et al. (2022) describe many such problems, like Deep Sea for example.\n\n\nReferences:\n\nSutton and Barto. (2018). Reinforcement learning: an Introduction. 2nd Ed. MIT press.\n\nParisi et al. (2022).  \"Long-Term Visitation Value for Deep Exploration in Sparse-Reward Reinforcement Learning\" Algorithms 15, no. 3: 81. (https://doi.org/10.3390/a15030081)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear, but there are some definitions that could be clarified. For example, the paper makes reference to a \u201csomewhat on-policy\u201d algorithm (Section 3.3), and it is not clear to me what that is. Usually, an RL algorithm is either on-policy, or off-policy; this is a binary classification.\n\nFurther, the paper defines deep exploration as \u201cexploration that is able to propagate uncertainty from distant areas of the state-action space into local decisions and is consistent over multiple time-steps.\u201d This explanation is confusing to me. Deep exploration could be more simply described as \u201cexploration which is directed over multiple time steps\u201d (Osband et al., 2016), or as exploration that takes \u201c\u200b\u200bseveral coherent actions to explore unknown states instead of just locally choosing the most promising actions\u201d (Parisi et al. 2022).\n\nThe content of the paper appears to be novel, and given the information provided in the paper I suspect that the results would be reproducible.\n\nReferences:\n\nOsband et al. (2016), Deep exploration via Bootstrapped DQN, NIPS 2016 (https://proceedings.neurips.cc/paper/2016/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf)\n\nParisi et al. (2022).  \"Long-Term Visitation Value for Deep Exploration in Sparse-Reward Reinforcement Learning\" Algorithms 15, no. 3: 81. (https://doi.org/10.3390/a15030081)\n",
            "summary_of_the_review": "Even though the proposed algorithm appears to be novel and interesting, there are some flaws in the experimental evaluation. For this reason I suggest rejecting this paper.\n\nThe paper could be improved by expanding the experimental evaluation to include some additional environments that require deep exploration, and/or a larger more complex environment that requires deep exploration.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2501/Reviewer_6utc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2501/Reviewer_6utc"
        ]
    },
    {
        "id": "41w7S1iffH",
        "original": null,
        "number": 2,
        "cdate": 1666566256869,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666566256869,
        "tmdate": 1666566256869,
        "tddate": null,
        "forum": "PcR6Lir5mxu",
        "replyto": "PcR6Lir5mxu",
        "invitation": "ICLR.cc/2023/Conference/Paper2501/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a modified version of MCTS that encourages exploration and incorporates uncertainty into the planning tree. The authors compare the performance of the proposed method against MuZero in two environments: Slide and Mountain Car.",
            "strength_and_weaknesses": "Strength:\n1. This paper is well-written and easy to follow.\n2. This proposed method adopts the classic UCB exploration bonus from RL theory, which is intuitive and the reviewer appreciates the efforts towards connecting RL theory and practice.\n3. The experimental results are strong, among all tested environments.\n\nWeakness:\n1. As an empirical paper, the reviewer thinks the experimental results are not enough to demonstrate the empirical benefits of the proposed method \u2013 the two tested environments are too toy which limits the contribution of this work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. Adding a UCB type of exploration bonus is not a novel idea, but it is good to have some practical implementation of the theoretical results. The reviewer also believes the experiments are reproducible.",
            "summary_of_the_review": "The reviewer highly appreciates this work as it is trying to implement the classic UCB type of exploration bonus to the powerful MuZero framework. But at the current moment, the reviewer is having a hard time accepting this paper based on empirical evidence of two toy environments. The reviewer understands that some of the environments in the MuZero paper require large computational resources, however, perhaps the author can still try the proposed method in some easy games in Atari that require a reasonable amount of computation to make the empirical results stronger and more convincing. The reviewer would like to change the rating once the author can provide more empirical evidence of the proposed method in more challenging tasks (say Montezuma in Atari).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2501/Reviewer_3Uzt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2501/Reviewer_3Uzt"
        ]
    },
    {
        "id": "ss9tcjkqeT",
        "original": null,
        "number": 3,
        "cdate": 1666609328598,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666609328598,
        "tmdate": 1666609328598,
        "tddate": null,
        "forum": "PcR6Lir5mxu",
        "replyto": "PcR6Lir5mxu",
        "invitation": "ICLR.cc/2023/Conference/Paper2501/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper identifies lackluster exploration as a standing problem in model-based deep RL algorithm involving tree search. Hence, the authors propose to estimate epistemic uncertainty in order to drive exploration.  Instead of directly integrating uncertainty bonuses in the value function, their method directly propagates uncertainty through the planning tree, thus decoupling uncertainty and value estimation. The paper brings forth three main contributions: (1) a propagation scheme for uncertainty through a planning tree, (2) a modification of the planning objective that includes epistemic uncertainty and encourages exploration and (3) a decoupled training process for exploratory and exploitative episodes, which is designed to stabilize training. The technique described in (1) enables efficient computation of propagated entropy by using Taylor approximations and independence assumptions between reward  at time step $k$ and n-step returns at time step $k+1$. (2) consists in adding an uncertainty bonus to the action selection criteria in the tree policy in order to encourage global exploration. Finally, the training process from (3) relies on alternating exploration and exploitation episodes. The former compute two planning trees in parallel: actions are selected from a tree with uncertainty bonus, while value bootstrap terms are obtained by a search procedure not involving uncertainty bonuses. Finally, policy and value utilize n-step exploratory targets only when greater than exploitation value estimates. The authors provide a brief experimental evaluation against vanilla MuZero, and an ablation study on toy environments (Slide, Mountain Car).",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is well written, the motivation is clearly states and contributions are thoroughly explained.\n- While the training procedure introduces several modifications (alternating between exploration and exploitation, using exploitation value bootstraps and max targets), an appropriate ablation study is presented in Figure 2.\n\nWeaknesses:\n- The experimental evaluation of OP2E is limited to a selection of problems which is small (2 environments), and only includes toy problems. On the other hand, OP2E is presented as generally applicable in problems for which MuZero was designed. As a result, an evaluation on a larger number of challenging environments (e.g. standard exploration-hard games in ATARI), would be extremely helpful in improving this paper's soundness.\n- The paper argues that naively propagating uncertainty through the value function suffers from several problem (p. 1, last paragraph). While such arguments are well supported, they should also be validated empirically if possible.\n\nMinor comments:\n\n- Section 2.2: the objective of MCTS is conventionally to find $a = \\argmax Q^\\pi(s_t, a)$ rather than a policy maximizing each action-state value.\n- Section 2 is very detailed, and can be shortened significantly to create space for experimental results.\n- Eq. 4: is $\\mathbb{E}_{s_k} [\\mathbb{E}_{s_{k+1}}[s_k | s_k, a_k]]$ supposed to be $\\mathbb{E}_{s_k} [\\mathbb{E}_{s_{k+1}}[s_{k+1} | s_k, a_k]]$ instead?\n- Section 4: restricting the experimental evaluation to a deterministic world model significantly simplifies the uncertainty propagation mechanism. Although this is acknowledged, empirical results could benefit from an evaluation of stochastic forward models.\n- Section 5 could include relevant work on MuZero in stochastic settings [1].\n\nReferences:\n[1] Antonoglou et al. \"Planning in Stochastic Environments with a Learned Model\" in ICLR 2022",
            "clarity,_quality,_novelty_and_reproducibility": "The work represents an interesting extension to tree search methods within MBRL, which directly addresses exploration through uncertainty bonuses. While not entirely novel, it represents an important are to research. The work is clear, but its quality is held back by an insufficient experimental evaluation.",
            "summary_of_the_review": "The paper tackles an important problem by combining two interesting ideas (MBRL with tree search, and uncertainty-driven exploration) in a principled way. The quality of the paper is good, and the main contributions are clearly presented, but they are only validated on a limited number of toy problems. The lack of convincing experiments in challenging domains constitutes the main weakness of this paper, and I recommend rejection in its current state.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2501/Reviewer_YegC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2501/Reviewer_YegC"
        ]
    },
    {
        "id": "TXy4Psn58Fu",
        "original": null,
        "number": 4,
        "cdate": 1666698435744,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666698435744,
        "tmdate": 1666698435744,
        "tddate": null,
        "forum": "PcR6Lir5mxu",
        "replyto": "PcR6Lir5mxu",
        "invitation": "ICLR.cc/2023/Conference/Paper2501/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes planning with uncertainty, which incorporates epistemic uncertainty into planning trees for deep exploration in model-based RL. Built on top of the SOTA model-based RL algorithm MuZero, the proposed method has shown effectiveness of exploration with standard uncertainty estimation methods. ",
            "strength_and_weaknesses": "**Strength**\n\n+ This paper modifies Monte Carlo Tree Search (MCTS) for an exploratory objective, where the best actions are predicted for gathering relevant information rather than for highest expected return. This was done by first propagating uncertainty in a plannig tree, and then optimizing the action selection for exploration using a modified UCT operator with the propagated uncertainty.  \n\n+ Propagating uncertainty in planning tree is novel and interesting. \n\n**Weakness**\n\n+ Need to compare with more exploration baselines.\n\n+ Need to evaluate the proposed method on more diverse and challenging tasks. \n\nQuestions:\n\n+ Can the proposed method extend to other model-based RL methods? \n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\nThe presentation is clear. \n\n**Quality**\nThe quality of the paper is good.\n\n**Novelty**\nThe proposed method is novel.\n\n**Reproducibility**\nNeed more details about the algorithms and experiments to reproduce the results. \n",
            "summary_of_the_review": "Overall, the proposed method is novel and interesting, but need to further strengthen the empirical evaluation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2501/Reviewer_u2v3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2501/Reviewer_u2v3"
        ]
    }
]