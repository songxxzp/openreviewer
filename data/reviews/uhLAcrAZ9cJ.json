[
    {
        "id": "CPc1AujFBmt",
        "original": null,
        "number": 1,
        "cdate": 1666267997178,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666267997178,
        "tmdate": 1666267997178,
        "tddate": null,
        "forum": "uhLAcrAZ9cJ",
        "replyto": "uhLAcrAZ9cJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1835/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper starts with pointing out that the prevalent perspective regarding modeling heterogeneous FL clients by looking at the label distribution is limiting, since, in practice, the data distribution may differ between clients, too. The authors propose to mitigate this concern using a variant of an iterative domain translation model that they adapted to the FL scenario. They evaluate on a one-domain-per-client setting using Rotated MNIST and FashionMNIST.",
            "strength_and_weaknesses": "I am no expert in federated learning, so I cannot comment on how novel the contributions are and how relevant this work is to practical federated learning. But on a high level, the paper seems to be well motivated. I could easily follow the overall story. I did not check 100% of the formalisms, but I could not find any vagueness/clarity issues in the ones I did check. The claims regarding communication efficiency are clearly demonstrated in Fig. 2.\n\nTables 1 and 2 are also clearly demonstrating the benefits regarding domain generalization, but I'm wondering why they do not include a baseline from Liu et al. (2021b) (see Sec. 3). Also, if the advantage over Liu et al. (2021b) is that this work is amenable to other data modalities such as text, it would be nice to evaluate on a text-based task.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written very well and concise. The extensive abstract should contain everything necessary to reproduce their results.",
            "summary_of_the_review": "To a reviewer without a background in federated learning, this looks like a strong submission.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1835/Reviewer_zb7q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1835/Reviewer_zb7q"
        ]
    },
    {
        "id": "4xS5JZc1-BR",
        "original": null,
        "number": 2,
        "cdate": 1666769476975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666769476975,
        "tmdate": 1666769476975,
        "tddate": null,
        "forum": "uhLAcrAZ9cJ",
        "replyto": "uhLAcrAZ9cJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1835/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on a challenging setting in federated learning (FL) where class conditional distribution varies amongst clients. To address this issue, the authors develop a federated domain translation methodology based on an advanced iterative translation model, namely Iterative Naive Barycenter(INB).\nINB is more viable to the FT setting than standard translation methods, e.g. StarGAN. Furthermore, the authors propose several FL-motivated improvements to INB, which significantly reduce communication costs. Empirical results show that the proposed federated translation model not only performs better than FL versions of baseline translation model, but also improves robustness",
            "strength_and_weaknesses": "Strength:\n\n1. Novelty: The authors propose to apply INB in FL setting for the problem where different clients have their own class distribution. In addition, the authors propose several FL-motivated improvements to INB, including the use of variable-bin-width histograms and autoencoder.\n2. Performance: Empirical results on Rotated MNIST and FashionMNIST show that the proposed method outperforms FL versions of standard translation models under practical settings of limited client-server communication capability.\n\nWeakness and Suggestions:\n\nWhile it does propose practical improvement, the work seems incremental in applying INB in federated learning.\n\nThe key point in INB is to learn the transformation which maps each domain distribution to a shared latent distribution. While it could be reasonable or practical for some domains, there could be cases where the mapping is difficult to learn. This may need to be discussed as potential weaknesses.\n\nI am wondering how much the difference is between a INB and a FedINB, where there be a balance/trade-off between communication and the performance?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is basically clear, with solid experiments and demonstration to previous arguments. Its idea is somewhat incremental, but practical improvement are presented to reduce the communication cost of the original method.",
            "summary_of_the_review": "This is a solid paper which applies INB in a Federated Learning setting with practical improvement to reduce the communication cost.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1835/Reviewer_CNpn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1835/Reviewer_CNpn"
        ]
    },
    {
        "id": "-wVV-dz8lA",
        "original": null,
        "number": 3,
        "cdate": 1666854286563,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666854286563,
        "tmdate": 1666854383452,
        "tddate": null,
        "forum": "uhLAcrAZ9cJ",
        "replyto": "uhLAcrAZ9cJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1835/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a federated domain translation approach that can mitigate conditional shift issues in federated learning tasks. The proposed translation model is empirically shown that it performs better than the state of the art FedStarGAN. The paper also shows that by combining FedINB with DIRT, regularization from domain translation model significantly improves the model\u2019s ability to generalize to unseen domains. I am not particularly familiar with the topic to have indepth comments. However I think the work is solid and should be accepted to ICLR.",
            "strength_and_weaknesses": "* Strength:\n\nThe proposed framework sounds to me. I also found the experiments solid, showing the proposed model is more efficient in terms of both communication and computational cost than FedStarGAN. Experiments also show that practical improvements proposed in the paper seem helpful to reduce communication costs.\n\n* Con: \n\nI found some parts of the writing are not clear. However, I also must say that I am not the particularly familiar with the topic, so it also contributes to some difficulty in following the paper to me (see Clarity section).",
            "clarity,_quality,_novelty_and_reproducibility": "Some writings in the paper are not clear to me, to name a few:\n\n- Most existing non-IID FL works assume that the client distributions only exhibit class imbalance, i.e., the marginal class distributions are different (pm(y)  is different to pm\u2032 (y)), but the class-conditional distributions are equal (pm(x|y) = pm\u2032 (x|y)). In contrast, we focus on the case where the class-conditional distributions differ across clients, i.e., pm(x|y)  is differrent to pm\u2032 (x|y). -> Are the marginal class distributions also different in this study? (i.e. pm(y)  is different to pm\u2032 (y))\n\n- algorithm 1: d is never explained ({Server} Randomly initialize \u03b8 \u2208 Rd)\n\n- algorithm 1: [i] should be j to my best understanding\n\n- algorithm 1: I don't understand why the barycenter is  y[i],k instead of y,k\n\n- However, we can show that treating y[i],k as a constant will actually return the same gradient value as if it had been treated as a function of \u03b8 -> I found this part is both interesting and important, thus should be elaborated more in the paper.\n\n",
            "summary_of_the_review": "An well-studied federated domain translation approach that can mitigate conditional shift issues in federated learning tasks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1835/Reviewer_uckp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1835/Reviewer_uckp"
        ]
    }
]