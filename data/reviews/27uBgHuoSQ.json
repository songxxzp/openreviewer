[
    {
        "id": "Jfc15wWOKib",
        "original": null,
        "number": 1,
        "cdate": 1666651914921,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651914921,
        "tmdate": 1670482187047,
        "tddate": null,
        "forum": "27uBgHuoSQ",
        "replyto": "27uBgHuoSQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3637/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The work studies the connection between sequence data continuity with the performance of deep sequential models and proposes a regularizer that adjusts the Lipschitz continuity of the model to improve its performance. It analyzes the property of the regularize from both time and frequency domains. Experiment results conform with the analysis and show the effectiveness of the proposed regularizer.",
            "strength_and_weaknesses": "Strength:\n\n1. The main idea of the work is simple and clear. It is strongly motivated by empirical observations.\n2. The analysis from both the time and frequency perspectives provides deeper insight and a solid basis for the proposed Lipschitz continuity regularizer.\n3. The experiment results are coherent with the motivation and theoretical analysis.\n\n\nWeakness:\n\nThe work only studied S4, informer, and ReLU networks. The theoretical results on the Lipschitz continuity property of S4 and ReLU network which in turn support the use of Lipschitz regularizer are model-specific. I\u2019m concerned about the general applicability of the proposed Lipschitz regularizer to similar sequential models without more general theoretical or experiment results.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and of high writing quality. The perspective of studying Lipschitz continuity property of sequential models is interesting and novel. I have no concerns with the reproducibility of the results.",
            "summary_of_the_review": "The perspective of studying the continuity property of sequential model is interesting and novel. The proposed Lipschitz regularizer is well supported by theoretical analysis and strong performance in experiment results. However, I'm concerned that the set of models studied in the paper, despite being exemplary, are limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3637/Reviewer_L7Qs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3637/Reviewer_L7Qs"
        ]
    },
    {
        "id": "a6GzQO0-y9",
        "original": null,
        "number": 2,
        "cdate": 1666666280702,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666280702,
        "tmdate": 1666666280702,
        "tddate": null,
        "forum": "27uBgHuoSQ",
        "replyto": "27uBgHuoSQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3637/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes the impact of data continuity on deep learning models, and propose Lipschitz regularizer which can improve performance of different models based on their preferred data continuity.",
            "strength_and_weaknesses": "The paper has a good motivation, which is reasonable and straightforward;\n\nThe proposed regularizer is simple and general, can be used to improve performance of different models;\n\nGood experimental results are obtained across different datasets on different models;\n\nSome theoretical analysis and clear explanations are provided;\n\nSome experiments can be added to strengthen the paper. Specifically, instead of training from scratch, I'm curious of whether we can use the proposed method to improve large pre-trained models by fine-tuning its embedding layer and freeze other layers on down-stream tasks, instead of fine-tuning the whole model or several selected layers. If the claims are correct, the proposed method should lead to good results. I think this experiment is very important, given the fact that many pre-trained transformer-like models are very important in real-world applications.\n\nIs the finding in Figure 1 always holds across different dimensionalities?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has good novelty and clarity, code is also provided by the authors.",
            "summary_of_the_review": "Based on  data continuity, the authors analyze the continuity preference of different models, and propose a simple yet effective regularizer to improve performance of different models. I lean towards acceptance of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3637/Reviewer_EzfA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3637/Reviewer_EzfA"
        ]
    },
    {
        "id": "zwbbRGBNu6d",
        "original": null,
        "number": 3,
        "cdate": 1666863486201,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666863486201,
        "tmdate": 1666863486201,
        "tddate": null,
        "forum": "27uBgHuoSQ",
        "replyto": "27uBgHuoSQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3637/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a Lipschitz regulariser to adjust to the continuity of certain type of data.",
            "strength_and_weaknesses": "Strengths:\n- The methodology seems to be novel.\n- Experimental evaluation looks extensive and quite thorough with different models and data types.\n\nWeaknesses:\n- The regulariser seems to be quite simple, it is just the square difference between two sequence values.\n- Adding the regulariser did not always improve results but made them worse. I would have liked to see a detailed analysis on those situations, why that happens. E.g. For Pathfinder in table 1 and the quite a few cases (especially multivariate) in table 2.",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is clearly written. Quality is good. Novelty cannot evaluate as I am not familiar with the type of data used.\nReproducibility: No link is provided to the code or the data.",
            "summary_of_the_review": "Paper seems interesting but I cannot evaluate its relevance to the field.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3637/Reviewer_cBYY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3637/Reviewer_cBYY"
        ]
    },
    {
        "id": "NNcvK3jUQ55",
        "original": null,
        "number": 4,
        "cdate": 1666948891520,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666948891520,
        "tmdate": 1666949272484,
        "tddate": null,
        "forum": "27uBgHuoSQ",
        "replyto": "27uBgHuoSQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3637/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper comprehensively investigates the continuity of sequential data from both theoretical and empirical perspectives. The authors observe that different model structures may prefer data with different continuity. Then, they give a theoretical analysis in both time and frequency domains. To further improve sequence modeling, they propose Lipschitz Regularizer which can flexibly adjusts data continuity based on the model preferences. Experimental results on different model structures and tasks demonstrate the effectiveness of Lipschitz Regularizer for many deep models in sequence modeling.",
            "strength_and_weaknesses": "Strengths:\n\n1) In my view, this paper is both comprehensive and deep. Now, there are many effective model structures such as Transformer and state space models for sequence modeling. But few works try to investigate the reasons why they work well in specific tasks. This paper starts from continuity, which is an intrinsic property of data, and clearly reveals the relationship between task data and model structures with theoretical supports. The authors provide meaningful insights into different tasks and model designs, which may guide the future research of sequence modeling.\n\n2) In addition to analyzing the phenomenon, the authors also propose a simple and effective method by adjusting the data continuity according to model preferences. Empirical results show its superior performance on challenging tasks like LRA, which are convincing for me.\n\n3) This paper is well-organized and easy to follow. I really enjoy reading this paper and learn a lot from it.\n\nWeaknesses:\n\n1) The authors only conduct experiments on LRA and time-series forecasting tasks in the analysis of Section 4. I wonder whether the authors try to use Lipschitz Regularizer in NLP or audio tasks such as language modeling because they are also attractive sequence modeling tasks.\n\n2) Typo: Preformer \u2013> Performer in line 7 of Section 2; Removing ) in line 1 of Section 5.1.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality of this paper is satisfactory. The comprehensive analysis and proposed method are novel. The reproducibility is high since the authors submit codes.",
            "summary_of_the_review": "In my view, this paper is of high quality. I will surely recommend acceptance. But if other reviewers raise serious problems in this paper, I will be still open to change my mind.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3637/Reviewer_Tz2y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3637/Reviewer_Tz2y"
        ]
    }
]