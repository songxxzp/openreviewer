[
    {
        "id": "9Dz30luBKy",
        "original": null,
        "number": 1,
        "cdate": 1665597681158,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665597681158,
        "tmdate": 1669995823144,
        "tddate": null,
        "forum": "Rpo9dvNlEYW",
        "replyto": "Rpo9dvNlEYW",
        "invitation": "ICLR.cc/2023/Conference/Paper5383/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper considers the federated learning problem and proposes to\nminimize the difference between two log determinant functions instead of\nthe cross-entropic loss.\n\n",
            "strength_and_weaknesses": "# Weakness\n1. The work is not solid. My\n   judgment is based on the following:\n   - The background section is unnecessarily long. For example, the\n     review of Section 2.2 is redundant, and the paper can simply say\n     that, as motivated by some rate-distortion theory, Yu et al.\n     (2020) proposed this MCR2 objective.\n   - Theorem 1 is basically a copy-and-paste from a follow-up work\n     (Ryan et al., 2022) of Yu et al. (2020), called ReduNet.\n     Moreover, the copy is wrong: It misses the key assumption that the\n     objective admits a low-rank solution, see Theorem 12 of Ryan et al.\n     (2022). Finally, the paper did not mention that this theorem is\n     due to Ryan et al. (2022).\n\n   - Theorem 2 is a very\n     standard result for smooth optimization. Moreover, \n     footnote 1 of the paper has noted that the MCR2 objective\n     requires some non-convex constraints for itself to be well-posed,\n     but Theorem 2 (and perhaps also the implementation) simply\n     ignored these constraints, which are required for Theorem 1. This\n     creates a great mess.\n   - Figures similar to Figure 2 have already been reported by Ryan et\n     al. (2022). The purpose of such a repeat is not very clear.\n\n2. It is quite confusing why the paper keeps saying that MCR2 is a\n   principle (Similar objectives to MCR2 already exist, cf. Lezama et\n   al., (CVPR 2018), why their objectives are not principles?). Also,\n   the coding rate reduction explanation is only applicable to Gaussian data.\n   And one gets no interpretation when dealing with real data. It is\n   also not very clear to me why diverse representations are\n   practically important, and the paper did not show the performance\n   benefits compared to the cross-entropy loss.\n\n3. Here are the references that I mentioned above:\n   - https://arxiv.org/pdf/2105.10446.pdf (Ryan et al., 2022)\n   - https://arxiv.org/abs/1712.01727 (Lezama et al., CVPR 2018)",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of the work is extremely limited. The writing becomes unclear when it comes to whether the normalization constraints are imposed on the MCR2 objective.",
            "summary_of_the_review": "As detailed above, the paper is clearly below the bar of ICLR in several\naspects. In its current form, the paper is better suited for some less\nselective conferences.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5383/Reviewer_fuw1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5383/Reviewer_fuw1"
        ]
    },
    {
        "id": "KWfxClqW08A",
        "original": null,
        "number": 2,
        "cdate": 1666595168724,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666595168724,
        "tmdate": 1666606823135,
        "tddate": null,
        "forum": "Rpo9dvNlEYW",
        "replyto": "Rpo9dvNlEYW",
        "invitation": "ICLR.cc/2023/Conference/Paper5383/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors proposed to replace the cross-entropy loss with the MCR2 loss in federated learning. They provided a theoretical analysis of learning convergence of the global model using the MCR2 loss. They also empirically show the convergence of the loss and the orthogonality of the class-wise subspaces learnt. ",
            "strength_and_weaknesses": "Strength:\n\nThe idea of separating the backdone network from the project head is an interesting idea.\n\nWeakness:\n\nThe idea proposed seems rather trivial. And the empirical evaluation is not done in comparison with some natural baselines. It is difficult to see the significance of the proposed work. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well-written and very readable. But it lacks novelty.\n",
            "summary_of_the_review": "The paper makes a rather simple change to the standard federated learning setting by using a different loss function. The empirical evaluation does not show the advantage of performing that change.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5383/Reviewer_LtxX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5383/Reviewer_LtxX"
        ]
    },
    {
        "id": "AvVsawhCeF",
        "original": null,
        "number": 3,
        "cdate": 1666706615574,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666706615574,
        "tmdate": 1666706615574,
        "tddate": null,
        "forum": "Rpo9dvNlEYW",
        "replyto": "Rpo9dvNlEYW",
        "invitation": "ICLR.cc/2023/Conference/Paper5383/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an algorithm to learn low-dimensional representations in the Federated Learning setting. The paper borrows idea from a recent work which formulated a loss that could maximize the code rate difference between the entire dataset and summation of individual classes. The work takes a further step by identifying that learning a low-dimensional representation could be a collaborative objective and introduces an algorithm to solve the learning objective in a distributed manner.\n",
            "strength_and_weaknesses": "Strength: The work is overall well-written and easy to read, all the details of the cocepts, setups are clearly stated. The theoretical results demonstrate that in the distributive setting,  the learning objective could still be successfully solved.\n\nWeakness: The novelty of the work is quite limited, although the usage of the MCR loss in federated learning is new, both the loss and the algorithm design are not new. Besides, the experimental part is quite weak as it only demonstrates the results of the MCR loss under Federated Learning setting, no comparison has been done to show the supremacy of the proposed algorithm.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to understand and the code is released for the reproducibility. The novelty of the paper is quite limited.\n",
            "summary_of_the_review": "This is an well-written paper proposing an interesting extension for the MCR loss in the federated learning setting. The experimental parts are a bit weak and the overall novelty is also limited.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5383/Reviewer_bnnY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5383/Reviewer_bnnY"
        ]
    },
    {
        "id": "9BbJ4CDvS2",
        "original": null,
        "number": 4,
        "cdate": 1666797385068,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666797385068,
        "tmdate": 1666797385068,
        "tddate": null,
        "forum": "Rpo9dvNlEYW",
        "replyto": "Rpo9dvNlEYW",
        "invitation": "ICLR.cc/2023/Conference/Paper5383/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The submission proposes a federated methodology to learn low-dimensional representations from a dataset distributed among several clients. Basically, the idea lacks novelty and is with limited contribution as it is a simple application of the MCR2 objective to a federated learning setting. The optimization is still using fedAVG and the proofs are also routine as FedAVG as the embedding parameters are optimized based upon SGD. ",
            "strength_and_weaknesses": "Strength:\n\nS1). The paper is clearly presented. Basically, the paper is well-organized and easy to follow. The authors give sufficient background on both federated learning and MCR2. \n\nS2). The references reviewed are somehow complete. The authors presented both traditional Federated learning and personalized FL.\n\nWeakness:\n\nW1). the work lacks novelty and the contribution is very limited. Basically, it is only a simple adoption of MCR2 to the federated setting. Since the optimization is also SGD and fedavg, there are no new challenges to transferring MCR2 objective to a federated setting including the proofs of convergence and others. \n\nW2). Moreover, since the optimization needs to collectively sum over different labels (K), however, in the personalized FL setting, it is possible that the same sample in different clients is assigned with different labels, as mentioned in paper https://arxiv.org/pdf/1910.01991.pdf\nIn this case, how the proposed obj of MCR2 would work in FL setting?\n\nW3). The representation learning framework MCR2 is still using class labels, why not use cross-entropy loss? what's the extra merits? The authors need to clarify the usage of such supervised embedding learning compared to popularly used cross-entropy loss with information bottleneck guidance. \n\nW4). The experiments are far from sufficient. Basically, the authors only present the convergence and effectiveness of learning a compact representation learning. Basically, what are the usefulness and the quality of the learned representations? What's the superiority of the optimization framework fast converge at the FL setting compared to traditional FedAVG?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is fine\nQuality is low because of insufficient experiments.\nNovelty is very limited.\nReproducibility is fine.",
            "summary_of_the_review": "Since the work lacks novelty and the contribution is very limited, I thus do not champion acceptance. Basically, it is only a simple adoption MCR2 to the federated setting. Since the optimization is also SGD and fedavg, there are no new challenges to transferring MCR2 objective to a federated setting including the proofs of convergence and others. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5383/Reviewer_L6k4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5383/Reviewer_L6k4"
        ]
    }
]