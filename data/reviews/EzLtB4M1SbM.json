[
    {
        "id": "wvsTvvBeNT1",
        "original": null,
        "number": 1,
        "cdate": 1666345141937,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666345141937,
        "tmdate": 1669022152861,
        "tddate": null,
        "forum": "EzLtB4M1SbM",
        "replyto": "EzLtB4M1SbM",
        "invitation": "ICLR.cc/2023/Conference/Paper2622/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new approach for fully test-time adaptation. The approach aims to prevent model collapse due to confirmation bias by using nearest neighbour information for pseudo-labelling. It then adapts an ensemble of adaptation modules that are used instead of the original linear classifier layer, on top of the feature representation. The approach is evaluated on four domain generalization benchmarks and on the common corruptions benchmark on CIFAR-10/100.",
            "strength_and_weaknesses": "Strength:\n * The paper is overall well written and presented\n * The proposed TAST approach is a novel contribution (extending T3A)\n * Figure 1 provides a useful illustration of the proposed procedure\n * The empirical evaluation on domain generalization benchmarks is extensive and strong performance of the proposed TAST is demonstrated.\n * Reasonable ablation studies on TAST are conducted.\n\nWeaknesses:\n * Clarity is insufficient, mostly because there are several inconsistencies in notation and undefined symbols (see below)\n * The ensembling of adaptation modules is not well motivated and also the trade-off such an ensemble poses in terms of increasing inference time is not discussed or quantified. Also, if one employs an ensemble, wouldn't it be natural/preferable to also employ different support sets in conjunction with different adaptation models in order to increase diversity within the ensemble?\n * The method TAST has several additional hyperparameters (N_s, T, M, N_e) and it is not entirely clear how these have been chosen in certain experiments. In particular, they should not be chosen in any way based on the target domains. In general, TTA methods should have few if any free hyperparameters, so it would be preferable to clearly state default values for hyperparameters and use these everywhere except for ablation studies.\n * Experiments on TTA on common corruptions are only conducted on CIFAR10/100-C and not ImageNet-C. Is this because 1000 classes are a challenging for TAST?\n * Runtime of different TTA methods is not reported but a relevant quantity when considering TTA in practical applications. Specifically, would TAST-BN, T3a be slower or faster than TAST? \n * The weak performance of TAST on CIFAR10/100-C is not well explained. A summary sentence like \"T3A and TAST show slight performance changes compared to existing TTA methods that fine-tune the feature extractor.\" does not reflect well that TAST/T3A are more than 20 percent points worse than TENT on CIFAR100-C. I think it is fair to say that CIFAR10/100-C require adapting the entire feature extractor and thus, only TAST-BN is a competitive method here, while TAST is not performing systematically better than \"No Adaptation\".\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed approach builds upon T3A but has sufficient novelty. Quality of the empirical evaluation is good, albeit I would ask the authors to also conduct an evaluation on ImageNet-C to demonstrate that TAST can also deal with many classes (1000). Reproducibility of the approach is an open question to me since no code is provided and clarity of presentation is missing. I list several issues in terms of clarity below:\n - In Equation 2, the same symbol $\\mathcal{S}^k_t$ refers to two different sets on the left-hand and right-hand side of equation (after and before filtering high entropy samples)\n - Which distance function $d$ is used in Equation 4?\n - The set $\\mathcal{S}$ is a set of class-specific sets $\\mathcal{S}^K$, but the authors use in Eq. 3 and later the notation $z \\in \\mathcal{S}$ as if it would be a flat set of feature vectors. This is confusing as it is not clear how this flattened set is obtained (just as the union of al sets $\\mathcal{S}^K$?)\n - The adaptation modules $h_{\\phi_i}$ are not defined properly on the bottom of page 4, e.g. what is their output dimensionality?\n - For the cross-entropy based loss $\\mathcal{L}^{TAST}$ it is not clear if the gradient wrt. $\\phi$ is propagated into both arguments of the cross-entropy or only one of them? In any case, it would be worth investigating which of the two options is preferable.\n\n\nMinor:\n - T3A should be cited again on the page 3\n - The statement \"Although it achieves effective test-time adaptation, it has a limitation that it can be utilized only if there are BN layers in the trained classifier.\" is not quite true, because one can always add affine layer  anywhere in a pretrained classifier, initialize them as identity and then adapt during TTA. \n - I would recommend to move the related works section before Section 2, as it will introduce several baseline methods used in Section 4. ",
            "summary_of_the_review": "Overall, in the current state I find clarity of the exposition lacking and would tend towards rejection. However, I would be willing to reconsider my evaluation if the issues mentioned above would be addressed.\n\n### Update after discussion period ###\nThe authors have devoted considerable work in clarifying open questions and revising the manuscript. Overall, I think the proposed TAST method can be useful in certain settings (image classification problems with few classes, high level semantic shifts) but in others, like (a) image corruptions (where only TAST-BN performs competitive) or (b) with many classes, it remains preferable to adapt the feature extractor/backbone itself. I attribute (a) to lower-level perceptual shifts being handled better by earlier features and (b) to the property that features unlike prototypes are shared over classes and thus adapting them is more data-efficient. I think the remaining \"niche\" for TAST is still useful and relevant but I would like the authors to be more transparent about the limitations and avoid broad statements like \"TAST outperforms the state-of-the-art TTA methods\" in the abstract. Assuming the discussion of strengths and weaknesses becomes more balanced in the final version, I would lean towards acceptance and I increase my score accordingly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2622/Reviewer_tB9n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2622/Reviewer_tB9n"
        ]
    },
    {
        "id": "C-am6Bb5Cr",
        "original": null,
        "number": 2,
        "cdate": 1666585266795,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666585266795,
        "tmdate": 1666760110708,
        "tddate": null,
        "forum": "EzLtB4M1SbM",
        "replyto": "EzLtB4M1SbM",
        "invitation": "ICLR.cc/2023/Conference/Paper2622/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The author proposed a test-time adaptation via a self-training method (TAST). Introducing an adaptation module to generate pseudo-labels through the nearest neighbor approach. Based on the T3A method, the author incorporated a support set created using previous data and a prediction from a classifier. The support set only contains examples with a reliable pseudo label, which is determined by calculating the cosine similarity between the test sample x and its nearest neighbor in the embedding space. The multiple adaptation module is randomly initialized, trained during the test time, and then ensemble the result generated from different adaptation modules to predict the labels for test data. The method outperforms state-of-art results on domain generalization on corrupted image corruption datasets.",
            "strength_and_weaknesses": "Strength:\n+The proposed method's main strength is introducing adaptation module(s) that were randomly initialized and fine-tuned using the nearest neighbor samples from test data. The author also introduced TAST-BN, which fine-tunes the BN layer (if it exists) instead of the adaptation module. \n+ This method outperformed other techniques in image corruption datasets. The empirical results show the benefit of the method, and some ablation studies are provided.\n+The supplementary material provides additional implementation details and experimental results that help support the paper.\n+ Also, since the codes are also provided in supplementary material, there is less concern that the results in this paper would be difficult for a reader to reproduce.  The paper includes information that would make it possible to reproduce the methods and experiments (but not with MIDRC data).\n\n\nWeaknesses:\n+ The proposed method is computationally expensive. Although, the author compares their method with others in terms of computation in an ablation study, and mentions that the proposed method (TAST) was computationally expensive. Still, their method takes 4x more time than the state-of-the-art (T3A). This performance is not ideal, especially when adapting the model on test time.  ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper was well-written and organized. The proposed technique was clearly described and referred to all the papers where it was needed. The novelty lies in using the previous data efficiently during test time such that the unreliable samples were discarded and only the confident examples made it to the support set. ",
            "summary_of_the_review": "This work delivers on its claimed contributions, although the time complexity may limit it application at test time. I anticipate it will be a valuable publication for researchers in test time adaptation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None. ",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2622/Reviewer_Lha6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2622/Reviewer_Lha6"
        ]
    },
    {
        "id": "pUa1rcoO8JA",
        "original": null,
        "number": 3,
        "cdate": 1666603244052,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603244052,
        "tmdate": 1666603244052,
        "tddate": null,
        "forum": "EzLtB4M1SbM",
        "replyto": "EzLtB4M1SbM",
        "invitation": "ICLR.cc/2023/Conference/Paper2622/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a test time adaptation (TTA) method named TAST where a trainable projector is added at test time on top of the feature extractor and optimised via information from the top nearest neighbors.",
            "strength_and_weaknesses": "### Strengths\n\nS1: The method uses \"pseudo-label distributions for test data using the nearest neighbor information\" an interesting idea and novel in a test-time adaptation setting\n\nS2: The method improves results on different benchmarks\n\n### Weaknesses\n\nW1: **Transductive vs non-transducive test setups mixed**. A support set is needed and kept updated at test time  to define (pseudo)-prototypes for the proposed method, similar to T3A but unlike other methods like TENT . This seems like a strong enough assumption since now one could call the test setup \"transductive\" and authors should explicitly mention this eg in Table 1, and separate the methods accordingly.\n\nW2: **Differences with related works are conceptually small**. The proposed approach adds adaptors a projector for TTA, something generally common in many other settings (see N1) and instead of constructing prototypes as in T3A, it uses a neiarest neighbor classifiers. TAST-BN is very similar to other TTA methods that only updated BN parameters. It is unclear what the intuitions behind this specific design is and what insights it offers.\n\n\nW4: **discussion on efficiency missing**.How much is the added compute and parameters vs other methods? \n\n### Notes and questions\n\nQ1: **architecture of adaptors**. What is the architecture of h? this should be more prominently featured\n\nN1: **Discussion on the use of projectors beside TTA** is missing. From SSL to SL projectors are being used during training o help transfer performance. SSL methods like AsimSiam, SImSiam, SimCLR and many others, as well as supervised methods like SL-MLP (Revisiting the Transferability of Supervised Pretraining: an MLP Perspective, CVPR 2022) or t-ReX (Improving the Generalization of Supervised Models, arxiv 2022).\n\nN2: **Missing refs utilizing nearest neighbor information during training**. Some missing references that learn using nearest neighbor information in similar ways and should be discussed/cited - see below. Moreover, \"self-training with nearest neighbor information\" is how most manifold learning methods operate -  some discussion on that should be added. \n\n- Iscen, Ahmet, et al. \"Learning with Neighbor Consistency for Noisy Labels.\" CVPR 2022.\n\n- Almaz\u00e1n, Jon, et al. \"Granularity-aware Adaptation for Image Retrieval over Multiple Tasks.\" ECCV 2022 \n\nNote that although for a different setting, Almaz\u00e1n, Jon, et al. also use multiple adaptors that are trained using pseudolabels, while also fused using nearest neighbor-based pseudo labels or by averaging the multiple adaptors\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clearly written and has novelty for the specific application of TTA. It seems reproducible and hopefully authors will share code",
            "summary_of_the_review": "Although an interesting method with some technical novelty, intuitions on the design choices and motivation are missing, as well as discussions/related work of the use of the same techniques outside TTA. I encourage the authors to respond to the concerns above and I am willing to increase my score accordingly with clear and satisfying answers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2622/Reviewer_rMac"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2622/Reviewer_rMac"
        ]
    },
    {
        "id": "3QnwqRyVhd",
        "original": null,
        "number": 4,
        "cdate": 1666606951451,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666606951451,
        "tmdate": 1666635927969,
        "tddate": null,
        "forum": "EzLtB4M1SbM",
        "replyto": "EzLtB4M1SbM",
        "invitation": "ICLR.cc/2023/Conference/Paper2622/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In the setting of test-time adaptation, an adaptation method tries to update a model given only online access to test data in order to reduce generalization error without altering the training process. The proposed test-time adaptation via self-training with nearest neighbors method (TAST), differs from existing methods in (1) its choice of parameters for adaptation, (2) its use of nearest neighbors in defining the targets for self-training updates, and (3) it's combination of prototype (average) and exemplar (neighbor) predictions. The parameters for adaptation are those of newly-initialized predictors on top of the trained feature extractor, including multiple predictors acting as an ensemble, in order to provide a variety of predictions to average over for robustness. TAST differs from prior prototypical methods like T3A by making use of neighbors to update the representation, while T3A keeps the representation fixed which limits the range of its improvement. Experiments on standard benchmarks for domain generalization (VLCS, PACS, OfficeHome, TerraIncognita) and image corruption (CIFAR-10/100-C) show marginal improvements of +1 point of accuracy (with the exception of a +6 point gain on PACS). This marginal improvement is above the state-of-the-art methods for test-time adaptation including entropy minimization and prototypical approaches.",
            "strength_and_weaknesses": "*strengths*\n\n- TAST only updates parameters of the deepest, final layers so it does not need to backpropagate gradients through the entire model, unlike methods like TENT/MEMO/SHOT that update parameters in all normalization layers/all layers/all but the last layer, respectively. This could improve computational efficiency, if it is not cancelled out by the number of new final layers that TAST adds and the number of updates needed to train them.\n- The domain generalization datasets diversely include images that are real (VLCS, OfficeHome), stylized/synthetic (PACS), and scientific (TerraIncognita) and follow the established choices of prior work on test-time adaptation (T3A). The ablation of including nearest neighbor predictions without updates (\"Effect of nearest neighbor information\") does show a slight (<1%) effect of of neighbors alone, which validates the idea of the proposed method.\n- The image corruption benchmarks are standard, but small. CIFAR-10/100-C are common, but ImageNet-C is preferred, as it is more representative of the scale of current image classifiers. prior work such as Tent and SHOT report on ImageNet-C for this reason.\n- The results are insensitive to batch size across 8-128 and show improvement over ERM at each size. However, the minimum viable batch size is not reported, which would be worth knowing to determine which methods are competitive at small batch sizes and in the limit batch size one.\n\n*weaknesses*\n\n- The effect of TAST is small. generally improves by 1 point or less, with the exception of PACS with +6 points (Table 1), and this gap is only achieved with a higher number of adapters. The ablation of $N_e$ for the number of adapters shows that the improvement with no adapters is only recovered at 5 adapters, which has greater computational cost than none.\n- More memory and computation is needed to maintain a set of neighbors, as done by TAST, than to maintain a set of prototypes, as done by T3A. \n- The results are sensitive to the choice of parameters to update, and which choice is best varies across datasets. In particular neither TAST, with updates to new modules, nor TAST-BN, with updates to normalization layers, is a clear winner. Without a model selection rule to choose between them it is not clear which to apply.\n- The method has hyperparameters to determine the threshold between reliable and unreliable pseudo-labels (eq. 2), like T3A, but there are no ablations to check the sensitivity of results to this reliability rule. If it needs tuning then it would limit the general applicability of the method.\n- There is closely related work on source-free domain adaptation with neighbors, which is cited (Tang et a. 2021 and Yang et al. 2021), but these works do nevertheless reduce the novelty of the proposed TAST.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The clarity is good. The main idea and details of the method are communicated by the text, figures (especially Figure 1), and pseudo-code in Algorithm 1, which provides a variety of formats for comprehension. The notation is clear and consistent throughout, with the right amount to specify the setting and method without obscuring them behind too many symbols.The organization into sections and subsections is sensible and easy to navigate. While there are a few minor points to improve clarity (see Misc. Feedback below), they can all be resolved and do not harm understanding.\n\nQuality: The baselines and comparisons are broad, appropriate, and representative of recent and strong test-time adaptation methods. The experiments cover not only prior methods exactly as they were proposed, but also include variations that are relevant to the parameterization of the proposed TAST, for instance TentBN which adds new parameters and TentClf which only trains the last parameters. While the experimental design is good, the significance of the results is minor, as the combined total effect of the nearest neighbor information and new adaptation layers is barely an improvement.\n\nNovelty: The proposed TAST is a novel assembly of existing choices for test-time adaptation, which in particular draws from entropy minimization methods and prototype methods to define a new hybrid adaptation technique that uses both. The use of prototypes to define targets follows SHOT and T3A, but this work also includes nearest neighbors of target points in the assignment of pseudo-labels. The use of multiple adapter layers as an ensemble is not new to TTA, as that that was done by BACS (Zhou & Levine, NeurIPS'21), though the test-time ensemble parameters differ. There is empirical novelty in comparing more TTA methods and TTA version of SFDA methods (SHOT) side-by-side on domain generalization benchmarks. For more related work on mixed prototype/exemplar inference, the work could consider citing and discussing IMP (Allen et al., ICML'19) and the multi-prototype representation of Mensink et al. PAMI'13.\n\nReproducibility: This work could be reproduced, as the exposition is sufficiently detailed, and the method itself is not too intricate. The method could also be completely specified by releasing the code, but the submission does not indicate whether or not this will be done.\n\nquestions:\n\n- What is the computational cost of TAST compared to T3A and compared to Tent? This could be a wallclock measurement, or an abstract measurement such as the number of forwards and gradients in the network.\n\nmiscellaneous feedback:\n\n- Abstract:\n  - Please consider mentioning the datasets along with the tasks to know to highlight the experimental scope early on.\n- Introduction:\n  - \"two popular categories for TTA\" consider including a third category for normalization or statistics-based methods, such as only updating batch normalization statistics (see Scheider et al. NeurIPS'20).\n- Table 1: Consider marking the best non-TAST result, the prior state-of-the-art, by underlining to make it easier to compare against TAST. As an aside, it is striking that so many test-time adaptation methods harm accuracy on domain generalization benchmarks. This is worth noting in the caption or the corresponding text of Section 4.1.\n- Figure 2: please remind the reader in the caption or text of the meaning of each hyperparameter, because the notation introduced by Algorithm 1 is pages away.\n- Notation: NN could be confused as either \"nearest neighbor\" or \"neural network\" so consider \"N(x; S)\" as a notation for the \"neighborhood\" of x in S instead.\n- Method name: TAST for \"Test-time Adaptive Self Training\" is too generic, because it could describe any number of existing methods like TENT, MEMO, BACS, etc. The nearest neighbor part of the proposed method is more unique, so could an ancronym including \"N\" for \"neighbor\" be more distinctive?\n- Other naming: \"TentBN\" is confusing at first, because Tent already adapts normalization layer. Consider \"TentAdapter\" or \"TentFinal\" to designate Tent with a last, new adapter layer included.\n- Proofreading:\n  - \"Especially, we use [...]\" after Eq. 5 should be \"Specifically, we use [...]\"\n",
            "summary_of_the_review": "TAST is a technically correct and slightly novel test-time adaptation method that combines adaptation by exemplar (neighbor), prototype (cluster), and entropy minimization (optimizing). It achieves a slight improvement in accuracy (around 1 percentage point) for multiple domain generalization and image corruption datasets, but only reaches this improvement by requiring more computation time and memory for inference and adaptation. As such it is a borderline paper, but it can inform the community about the possibility of improving test-time adaptation results by combining clustering based on neighbors with entropy minimization of newly-included adaptation layers without updating the parameters from training.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2622/Reviewer_hpzb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2622/Reviewer_hpzb"
        ]
    }
]