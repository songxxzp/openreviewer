[
    {
        "id": "RjROwoiUvTF",
        "original": null,
        "number": 1,
        "cdate": 1666588960756,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588960756,
        "tmdate": 1669502137138,
        "tddate": null,
        "forum": "7sWLxZBLPO5",
        "replyto": "7sWLxZBLPO5",
        "invitation": "ICLR.cc/2023/Conference/Paper2935/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThe paper studies continual learning with multiple modes. The proposed method MOTA is motivated by mode connectivity and ensembles literature. MOTA aims to train $N$ diverse solutions on the first task, while for the next tasks, it uses the average/joint prediction to minimize the training loss and parameter drift. In addition, at the end of training each task, MOTA uses stored checkpoints to further optimize the drift. The authors show the effectiveness of MOTA on various tasks and instance incremental scenarios.\n\n\n",
            "strength_and_weaknesses": "### Strengths\n\n\n- The paper is easy to follow, and the method is intuitive. \n\n\n- The idea of continual learning with multiple modes and the trade-off analysis is interesting. \n\n\n\n### Weakness:\n\n\n- First, I should note that in terms of novelty and contributions, continual learning with multiple modes is not new. In fact, in [1] (see Alg. 4, 5), the authors build on the same idea of multiple modes motivated by neural network subspaces [2]. While in this work, there is an additional term to encourage diversity, the authors in [1] report the cosine regularization in [2] does not provide substantial additional benefits. Overall, while I suspect authors may not be aware of [1], in terms of novelty, I believe the core ideas of [1] are similar to this work.\n\n\n\n- The proposed method, MOTA, has some limitations. First, it requires knowledge of task boundaries (to discourage drift). In addition, I believe the experiments do not cover the Class-IL setup. Also, I believe several other related baselines, such as [1] and [3], should have been included in benchmarks.\n\n\n\n- [Minor; did not play a role in my decision] I believe the paper needs additional proofreading. More specifically, there are several typos (e.g., section 1, section 2), and on page 2, the paper refers to a figure on page 8.  \n\n\n\n\n**References**:  \n\n\n[1] Doan, Thang et al. \u201cEfficient Continual Learning Ensembles in Neural Network Subspaces.\u201d ArXiv abs/2202.09826 (2022).\n\n\n[2] Wortsman, Mitchell et al. \u201cLearning Neural Network Subspaces.\u201d ICML 2021. \n\n\n[3] Wen, Yeming et al. \u201cBatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning.\u201d ICLR 2020.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n\n**Novelty**: As I explained before, the core ideas and main motivations of this work are already proposed. However, it may be the case that the authors were not simply aware of those works.\n\n\n**Clarity & Quality**: Overall, the paper is clear. However, I believe in terms of organization and minor editorial matters, there is room for improvement. ",
            "summary_of_the_review": "Overall, I find the idea of continual learning with multiple modes interesting, I believe the paper needs improvement regarding the organization/presentation. However, I think this is a good first step.\n\n\n\n\n**Update (Post-Rebuttal)**  \nI would like to thank the authors for their response. Given that the authors have addressed most of my concerns and included new baselines, I would like to increase my score.\n\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2935/Reviewer_ydF1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2935/Reviewer_ydF1"
        ]
    },
    {
        "id": "VglFUmBKNx",
        "original": null,
        "number": 2,
        "cdate": 1667387846753,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667387846753,
        "tmdate": 1667387846753,
        "tddate": null,
        "forum": "7sWLxZBLPO5",
        "replyto": "7sWLxZBLPO5",
        "invitation": "ICLR.cc/2023/Conference/Paper2935/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the usage of more than one models for CL problem, and apply two regularization terms to maximize the difference of all models at 1st task and to minimize the parameter drift.\n\nThe paper claims better model quality when compared to other method.",
            "strength_and_weaknesses": "Strength\n1) Two regularization are proposed. One to max the difference of all models at 1st task. One to reduce param space drift between two tasks.\n2) The MOTA work should be easy to enable and user friendly for many real world tasks.\n\nQuestion\n1) The two regularization both use the weighted factor when added to the loss. How robust would the model metric be when the weighted factor change?\n2) As the results in Table 3 have base line approach from papers before or in 2018, are they the best base line model for now, or should the comparison add newer approach from papers after or in 2019?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality are very nice.\n\nNovelty seems mostly on the practical aspect.",
            "summary_of_the_review": "Recommend accept, marginally over threshold. Major reasons for not higher score would include the extent for novelty and baseline model selection in the comparison, whether more advanced and newer methods are compared to MOTA.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2935/Reviewer_x9Gv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2935/Reviewer_x9Gv"
        ]
    },
    {
        "id": "ciVagSEdfZ",
        "original": null,
        "number": 3,
        "cdate": 1667526908369,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667526908369,
        "tmdate": 1669645395779,
        "tddate": null,
        "forum": "7sWLxZBLPO5",
        "replyto": "7sWLxZBLPO5",
        "invitation": "ICLR.cc/2023/Conference/Paper2935/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a novel method, Mode-Optimized Task Allocation (MOTA), for continual learning with superior backward and forward transfer performance as well as lower average task drift.\n\nThe paper first identifies the key weakness behind prior regularization-based continual learning methods: they anchor all future parameters to the first task observed, even when doing so degrades transfer performance. MOTA instead aims to leverage the global geometry of the parameter/loss space by instead optimizing $N$ different modes, each having a set of tasks allocated to it. It proves that when $N$ and the set of tasks per mode are optimized against parameter capacity $|\\theta|$, the total task drift is lower than when $N=1$.\n\nIt describes the MOTA algorithm which first initializes the $N$ modes such that the pairwise distance among them are maximized, and updates the parameters for each task via checkpointed gradient updates such that the drift per mode is minimized.\n\nThe paper concludes after presenting empirical comparison of the transfer performance of MOTA with other baseline continual learning methods, which shows that its average accuracy is superior to all except multi-task learning.",
            "strength_and_weaknesses": "Strengths:\n- Provides clear motivation to study multi-mode continual learning (Theorem 1).\n- Shows creativity in the construction of its algorithm.\n- Solid experimental result and its discussion.\n\nWeaknesses:\n- Theoretical justification for the specific steps of its algorithm (Section 3) lacking.\n- In particular, motivations behind (1) the choice of parameter prior that maximizes the pairwise distance and (2) using random-weight-interpolated parameter in its loss function are unclear.\n- The second paragraph of Section 3.2 (starting with \"If a task has a high level of certainty, ...\") is confusingly written and may benefit from an illustrative diagram or a figure.\n- The paper claims that the algorithm does not depend on the knowledge of the task boundaries, and yet the specific implementation of the algorithm seems to depend on it (e.g. regularizing distance with parameter of previous task). Further clarification would be helpful.\n- Conclusion is abrupt and does not provide any direction for future research.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is, for the most part, very clear and easy to understand. It seems to be highly original, though \"we are amongst the first to leverage the global geometry of the loss landscape for task adaptation,\" seems to suggest there may be other works with similar insights, and a more detailed comparison and claims of novelty would have been appreciated.",
            "summary_of_the_review": "Overall, the paper clearly identifies a limitation with prior regularization-based continual learning methods and suggests an alternative that is both theoretically and empirically well-supported and convincing. Although parts of its main algorithm lack clear motivation and its conclusion does not offer insights about its potential impact and directions for future research, it is an original and creative work that I strongly recommend.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2935/Reviewer_EVp1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2935/Reviewer_EVp1"
        ]
    },
    {
        "id": "ZVzRjGWCJgS",
        "original": null,
        "number": 4,
        "cdate": 1667782950376,
        "mdate": 1667782950376,
        "ddate": null,
        "tcdate": 1667782950376,
        "tmdate": 1667782950376,
        "tddate": null,
        "forum": "7sWLxZBLPO5",
        "replyto": "7sWLxZBLPO5",
        "invitation": "ICLR.cc/2023/Conference/Paper2935/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new heuristic approach to continual learning in which a set of models (\"modes\") are joinlty updated after each task, as in an ensemble method, but using a novel loss function, which encourages diversity. Some empirical gains (relative to some other heuristic methods) on some task incremental learning image classification benchmarks are shown.",
            "strength_and_weaknesses": "The idea of using a mixture of sub-models (\"modes\"),, instead of a single large parameter model, seems intuitively reasonable, although is not very novel (there is a large prior literature on ensembles and mixture of experts, etc). Furthermore, the paper makes claims that I think are false, and is very hard to read.\n\nOne of the biggest problems is that the authors claim they do not need to know about task boundaries. Yet it is clear from algorothm 1 that the data is actually provided to the algorithm in chunks, with each chunk corresponding to a different task, and the algorithm is allowed to exploit this fact , by storing different parameter vectors for each mode i and task t, in the form theta(i,t). This seems contradictory. Furthermore, it seems the algorithm stores multiple copies of theta(i,t), one per epoch, to get theta(i,t,e). Thus the running time in step 12 is O(t E N) for each task t, resulting in a total complexity of O(T^2 E N). This seems expensive in time and space - more so than a simple replay buffer strategy (which they do not compare to).\nAnother problem is that their predictions are computed by using an unweighted average of the logits from each mode. What is the justification for this? If the modes are specialized on different tasks, why should they all get to vote equally?\n\nThe presentation is very unclear. For example, what does thm 1 even mean? theta_d^MTL is never defined. And why should we care about how much paramters drift or not? What does this have to do with predictive performance? Also, what assumptions are you making on the data stream (data distribution for each tasks)? \n\nMany terms are either not explained (eg \"EWC\" is not defined), or very poorly explained. For example, the distinction between instance-IL, task-IL and class-IL is very unclear. It seems to have something to do with the label (output) space for each task, Y_t ( some partitioning of a fixed, larger (\"fine grained\") label set. ) . It seems the algorithm assumes ahead of time that every task will always be a C-way classification task, but the \"meaning\" of each label may change across tasks (eg for task 1, 0=dog, 1=cat, 2=horse; for task 2, 0=truck, 1=car, 2=airplane), which we can think of as a hierarchical two-part label, (task, class). So it seems once again that you do need to know the task boundaries.\n\nThe experiments are not very clear. \n- In sec 4.0, What does it mean to say \"We utilize resnets-{18,50,152} with {11, 181, 642 || 23, 528, 522 || 58, 164, 298} adjustable parametrs? What does the || notation mean? What are these adjustable parameters, and which ones are fixed? \n- In sec 4.1, you compare to MTL as your \"gold standard\", but what exactly is this? (I assume you train a single big model with T output heads, each with C classes, in an offline way?) \n- In sec 4.2, the meaning of fig 3 is unclear. It seems that the loss does not vary at all in the subspace of  mode 2, suggesting that a single mode may suffice. Also, for mode 1, the low loss region seems constant across tasks, but this might be an artefact of your regularizaton strategy. Once again, why should we care about how the parameter subspaces change? What does this have to do with predictive performance?\n- In table 3, you only compare to somewhat older baseline methods (<=2018) -are these SOTA? And you only compare on 1 dataset (split cifar-100). Are the differenes between methods significant? (No error bars are reported!) Can the diferences be explained away by hyper-parameter tuning?\n- tables 1-3 are all hard to read (bolding the best and second best might help, and/or focus on fewer metrics).\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is very low - see above.\n\nQuality seems low - only compares to one prior method (EWC), which is quite old and probably not SOTA.\n\nNovelty seems small -there are several prior works on online learning with ensembles and sparse models.\n\nReproducibility seems reasonable.",
            "summary_of_the_review": "The paper proposes some ad-hoc heuristics which are poorly explained and seem lacking in theoretical justification. The experimental results seem inconclusive.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2935/Reviewer_MaED"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2935/Reviewer_MaED"
        ]
    }
]