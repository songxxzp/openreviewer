[
    {
        "id": "zVi9ByAs8AD",
        "original": null,
        "number": 1,
        "cdate": 1666125426323,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666125426323,
        "tmdate": 1666125426323,
        "tddate": null,
        "forum": "MjikLUwiB3M",
        "replyto": "MjikLUwiB3M",
        "invitation": "ICLR.cc/2023/Conference/Paper6528/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the landscape of a shallow neural network with a few neurons (in contrast to the popular infinite-width limit)\n\nThe paper showed that in some settings, a one-neuron network has only one critical point, which is its global minimum. The paper also studied the case with more than one neuron",
            "strength_and_weaknesses": "Strength: \n1. the paper is well-written and easy to read. For a theoretical paper like this, such clarity is difficult to achieve\n2. the case of few neurons is novel, and results have some fundamental importance\n\nWeakness:\n1. The title feels unsatisfactory. The paper is only about the loss landscape of neural networks. I feel that the authors need to emphasize that it is a \"landscape theory\" in the title. The paper does not study the theory of dynamics at all and is far from \"complete.\"\n\n2. The main result that \"a one-neuron network has only one critical point\" only applies to a very special setting, and the authors made a lot of overclaims based on the main result\n- the proof of claim only applies to this activation function $\\log(1+e^x)$ (and empirically found to work for sigmoid)\n- this claim certainly does not hold for tanh or a linear activation. To see this, consider the origin where all weights are zero: unless we have a very strange dataset, the origin is always a saddle point, and so the main claim does not apply to these cases\n- thus, the theory is very special, and the related claims needs revision (in fact, I feel that this result can only be proved for activations that are not zero when its argument is zero, which is not the most popular scenario in deep learning)\n\n3. Missing reference and discussion: the landscape of a few neurons is very well understood in the case of linear models, and the authors need to discuss the relevance of these works. \n- For example, the classical result: https://proceedings.neurips.cc/paper/2016/hash/f2fc990265c712c49d51a18a32b39f0c-Abstract.html\n- A recent one: https://arxiv.org/abs/2202.04777\n- the authors might think linear models are too special, but as I argued in point 2, the softplus activation is no less special than a linear one\n\n4. In reality, neural networks are always finite-width, and so I would expect some of the results of this work to imply something for realistic models, but the authors provided no example of how relevant the results are\n- this can especially be a problem because the main theoretical results only apply to a very special setting\n",
            "clarity,_quality,_novelty_and_reproducibility": "Both the clarity and the novelty are good",
            "summary_of_the_review": "The paper has a few major weaknesses, and I recommend rejection based on these:\n1. the main result is too special, and many claims are too strong\n2. the lack of discussion of relevant works\n3. lack of empirical implication\n\nWhile I think, after a major revision, this work could have a lot of potential due to the novelty of the results, its current version does not convince me so",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6528/Reviewer_rakv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6528/Reviewer_rakv"
        ]
    },
    {
        "id": "9JCXv2LpGl5",
        "original": null,
        "number": 2,
        "cdate": 1666364264820,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666364264820,
        "tmdate": 1666685803734,
        "tddate": null,
        "forum": "MjikLUwiB3M",
        "replyto": "MjikLUwiB3M",
        "invitation": "ICLR.cc/2023/Conference/Paper6528/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study the loss landscape of models with a small number of neurons.\nIn a teacher-student setting with a quadratic loss function, they show that the loss landscape of one-neuron networks has only a single critical point, the global minimum. They then show that a class of critical points belonging to models with more neurons descends from how larger models can be created from lower-dimensional models. Specifically, we see a proliferation of critical points, and study their stability.\n\n",
            "strength_and_weaknesses": "\nStrengths:\n- Even though the described theory does not allow to identify all kinds of critical points, it provides a way to understand at least one kind of critical points which one can find in neural networks, and shows that zero modes are present even in underparameterized models.\n\nWeaknesses: \n- The theory does not include common activations such as ReLU, and is focused on a square loss\n- It is not clear to me how much the findings rely on the chosen distribution for the data (gaussian iid)\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe writing is acceptable, though I would avoid the usage of exclamation marks and check the typos (e.g. activativation, is to studies).\nI found the figures hard to interpret, and the labels too small.\nTo my knowledge, the results are novel.\nThe simulation details are given and the proofs are detailed.\n\n",
            "summary_of_the_review": "The work seems relevant and correct.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6528/Reviewer_P9i2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6528/Reviewer_P9i2"
        ]
    },
    {
        "id": "YyC566FW5qC",
        "original": null,
        "number": 3,
        "cdate": 1666549968738,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666549968738,
        "tmdate": 1666549968738,
        "tddate": null,
        "forum": "MjikLUwiB3M",
        "replyto": "MjikLUwiB3M",
        "invitation": "ICLR.cc/2023/Conference/Paper6528/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors analyze the critical points of networks with a small number of neurons. They use a teacher-student setup where the student (initially) starts with a small number of neurons. The student network can then be grown one neuron at a time, and by careful selection of the weights in the larger network, critical points can be found. Moreover, there is a manifold of critical points induced by the smaller network. In the case of increasing by a single neuron, the authors analyze the new line of critical points and show that along the line, the nature of the critical points changes from minima to saddles.",
            "strength_and_weaknesses": "The analysis in the work is clearly presented and, to my ability to check it, correct. The construction of the higher dimensional critical points is simple yet clever.\n\nHowever, it is not clear to me how relevant the analysis are in understanding the overall network dynamics. In particular, there may be other critical-point manifolds which are missed by this approach, particularly as the number of minima increases. While having a characterization of some subset of the critical points can be useful, in most ML scenarios we care about the fixed points which are actually reached by optimization.\n\nIt is also not clear how relevant the low-dimensional, saddle point dominated dynamics is for real neural network optimization. The effects of a large number of neurons and finite step size give rise to convergent dynamics, sometimes provably so (e.g. http://proceedings.mlr.press/v97/du19c.html). This is in contrast to Langevin dynamics in statistical physics where the saddle points dominate (e.g. https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.71.173) - the type of dynamics the authors seem to be focusing on. I am very curious to the authors' response on this point.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is clearly presented; I did not have issues following along with the main results. I believe the analysis is novel, but I am not familiar with the literature on exact solutions of low-dimensional neural networks.",
            "summary_of_the_review": "Overall, though the results are clearly communicated, it is not clear to me that they are significant enough to warrant acceptance into ICLR. In particular my two major concerns are 1. the relevance of this specific family of fixed points, and 2. the general interest in the dynamics of small-neuron learning dynamics.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6528/Reviewer_Atsv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6528/Reviewer_Atsv"
        ]
    },
    {
        "id": "xtbxMzZ7E_",
        "original": null,
        "number": 4,
        "cdate": 1666699914180,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699914180,
        "tmdate": 1666700364928,
        "tddate": null,
        "forum": "MjikLUwiB3M",
        "replyto": "MjikLUwiB3M",
        "invitation": "ICLR.cc/2023/Conference/Paper6528/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the landscape of a 2-layer neural network with $r$ hidden neurons. It adopts a teacher-student network setting where the training data is generated by a 2-layer network (teacher network) with $k$ hidden neurons. This paper establishes the following results:\n\n1)  For $r=1$, prove that if the teacher network has orthogonal input weights and uniform output weights, the training of the student network only has one stationary point.\n2) For an irreducible (no identical input weights) local minimum,  characterize the number of positive, zero, and negative eigenvalues for the Hessian matrix.\n\nThe second result, combined with Fukumizu & Amari (2000), can be used to identify the type of stationary points for a width-($r+1$) network. Specifically, a local minimum of a width-$r$ network may give rise to a line of stationary points of a width-$(r+1)$ network. By counting the number of zero and negative eigenvalues of Hessian, the stationary points on the line can be identified to be non-strict or strict saddle points. ",
            "strength_and_weaknesses": "This paper studies the landscape of a 2-layer neural network. Despite the numerous parameters in practical neural networks, the optimization of 2-layer networks is still not fully understood. This paper tries to shed some light by characterizing the stationary points of 2-layer networks.\n\nThe first results show that 1-neuron student network has only one stationary point. The intuition is that at the stationary point, the incoming vector of the student network should **align with** each input vector of the teacher network **equally**, which is quite insightful. However, the proof only holds for the softplus activation, a positive, increasing, and convex function. The extension to other activations is not straightforward.\n\nThe second result counts the number of positive, zero, and negative eigenvalues of the Hessian matrix. It reduces the eigenvalue characterization of the whole Hessian to that of a smaller matrix (matrix $Y$ in Eq. (15)). This helps to characterize the type of stationary points of a wdth-($r+1$) network.i\n1) For one output dimension, the result does not provide much new insight compared with Fukumizu & Amari (2000). The only new information is that the transition point from local minima to strict saddles is a non-strict saddle. This is not very interesting, and not difficult to infer from the original result of Fukumizu & Amari (2000).\n2) For multiple output dimensions, the result can help to identify strict saddles on the line of stationary points, which is a new result. However, as the authors admitted, it can not help to identify local minima.\n\nSome other comments:\n- The first part of Theorem 4.1 is not correct. \n> Since $\\theta$ is a local minimum, $HL(\\theta)$ is a positive definite matrix which completes the proof of the first part of the statement.\"\n\nThe Hessian of a local minimum may not be positive definite. It can have zero eigenvalues. For example, $x=0$ is a strict local minimum of $f(x)=x^4$, but the Hessian is 0.\n\nFor the same reason, the following statement on page 5 is not correct:\n> Local minimum: A critical point where the Hessian of the loss has only positive values, i.e. the loss increases in any direction of perturbation.\n\n-  In Eq. (12), the notation $r$ is already used to denote the width of the student network (which is set to 1 in this theorem). Please change the notation to avoid confusion.",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "The first result is insightful, but only holds for a 1-neuron network with softplus activation.\n\nThe second result does not provide much new insight compared with Fukumizu & Amari (2000).\n\nTheorem 4.1 has a cavity.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6528/Reviewer_vz8N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6528/Reviewer_vz8N"
        ]
    }
]