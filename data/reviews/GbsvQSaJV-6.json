[
    {
        "id": "0XwUcS-npmB",
        "original": null,
        "number": 1,
        "cdate": 1666285015722,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666285015722,
        "tmdate": 1668766689360,
        "tddate": null,
        "forum": "GbsvQSaJV-6",
        "replyto": "GbsvQSaJV-6",
        "invitation": "ICLR.cc/2023/Conference/Paper1827/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a feasible way to perform curriculum learning in multi-agent reinforcement learning (MARL). It describes well why curriculum learning cannot be applied to MARL seamlessly and proposes sound solutions to these problems. The experimental evaluation is conducted in a satisfactory way, although the results are not outstanding. There are additional theoretical results on the regret analysis of the proposed approach.",
            "strength_and_weaknesses": "Strengths\n-----------\n- The paper considers the important problem of enabling the use of curricula in multi-agent reinforcement learning (MARL).\n- The proposed solution is sound and backed up by satisfactory intuitive motivations.\n- The empirical analysis is conducted on challenging RL experiments comparing with a satisfactory number of baselines. I think it would be a nice addition to have comparisons with more than the single QMIX method for value function factorization (e.g., QTRAN [1], QPLEX [2]), although I do not expect great improvements.\n\nWeaknesses\n--------------\n- The paper is not very well written, and the structure is quite chaotic. Especially Section 3 is split in different subsections, not well connected, and the whole flow is not fluent. \n- The regret bound could be an interesting theoretical results, but it is not complemented by any remark or empirical study, which makes the analysis a bit pointless.\n- The method seems not easy to implement and relies on complex self-supervised mechanisms which may be not easy to use in generic problems. To this end, at least a hint of a computational and time complexity analysis of the proposed method is a needed addition.\n- The ability to transfer seems an important feature of the proposed method, but experiments do not provide clear evidence of effective skills transfer.\n- The paper does not cite important works in automatic curricula generation such as [3], [4], [5].\n\n[1] Son, Kyunghwan, et al. \"Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning.\" International conference on machine learning. PMLR, 2019.\n\n[2] Wang, Jianhao, et al. \"Qplex: Duplex dueling multi-agent q-learning.\" arXiv preprint arXiv:2008.01062 (2020).\n\n[3] Klink, Pascal, et al. \"Self-paced deep reinforcement learning.\" Advances in Neural Information Processing Systems 33 (2020): 9216-9227.\n\n[4] Klink, Pascal, et al. \"Curriculum Reinforcement Learning via Constrained Optimal Transport.\" International Conference on Machine Learning. PMLR, 2022.\n\n[5] Svetlik, Maxwell, et al. \"Automatic curriculum graph generation for reinforcement learning agents.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 31. No. 1. 2017.",
            "clarity,_quality,_novelty_and_reproducibility": "- The novelty of the paper is satisfactory, as it is one of the few in the literature analyzing the use of curricula in MARL.\n- Clarity of the paper could be significantly improved (see above)\n- The source code of the method is included in the submission, which is good for reproducibility.",
            "summary_of_the_review": "This paper is a possibly good contribution to MARL, but has currently some limitations that I described above and that I would like to see addressed in the revision. I am willing to increase my score in case of an improved version of this paper.\n\nPost-rebuttal feedback\n------------------------------\nI thank the authors for the improved version of their submission. Most of my concerns have been addressed and I am happy to increase my score, as previously promised.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1827/Reviewer_qKoe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1827/Reviewer_qKoe"
        ]
    },
    {
        "id": "YQ4TLizxETS",
        "original": null,
        "number": 2,
        "cdate": 1666594676100,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594676100,
        "tmdate": 1666594676100,
        "tddate": null,
        "forum": "GbsvQSaJV-6",
        "replyto": "GbsvQSaJV-6",
        "invitation": "ICLR.cc/2023/Conference/Paper1827/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes Skilled Population Curriculum (SPC), a framework for cooperative multi-agent reinforcement learning based on automatic curriculum learning. In particular, the student is equipped with a hierarchical skill set and designed to be population-invariant. The teacher is a contextual bandit that uses a representation of the student\u2019s policy as its context. To implement population invariance, SPC uses self-attention to combine agent messages. In the experimental evaluation, SPC is compared to QMIX, IPPO, and VACL on Google Research Football and the Multi-agent Particle-world Environment, which they solve from (relatively) sparse rewards.",
            "strength_and_weaknesses": "Strengths:\n- This paper tackles a particularly difficult problem, which is cooperative multi-agent RL from sparse rewards. While other methods struggle on the evaluated tasks, especially Google Research Football, SPC makes significant improvements in terms of performance.\n- The visualizations and ablation study help readers understand the importance of each component of SPC.\n\nWeaknesses:\n- The experiments are only performed on Multi-agent Particle-world Environment and Google Research Football, which is relatively narrow compared to prior works that also study other environments like the StarCraft II benchmark.\n- The method is quite complex. It\u2019s shown, though, that the HRL structure and contextual bandit teacher are important. What about the self-attention channel?\n- It would be useful to understand qualitatively what the different learned skills are doing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity/Quality: The paper is overall written well. However, here are a couple suggested rephrasings:\n- \u201cby the teacher\u2019s giving the sequence of training tasks\u201d -> \u201cthrough the sequence of training tasks given by the teacher\u201d\n- \u201cIt is diagonal to\u201d -> \u201cIt is orthogonal to\u201d\n\nOther comments:\n- Why is only a subset of methods evaluated in each environment? That is, why is QMix missing from MPE and VACL missing from GRF?\n- It\u2019s a good idea to report the performance of baselines even if they are low for completeness.\n- For better clarity, I would recommend smoothing out the training curves in Figure 5 a bit.\n\nNovelty: While the application of HRL to MARL settings is not new, this paper also uses contextual bandits to produce non-uniform task sampling distributions and a self-attention channel to combine agent messages, which are novel as far as I\u2019m aware.\n\nReproducibility: The authors include code for their method and the baselines, which allow readers to reproduce the experiments in the paper.",
            "summary_of_the_review": "The experimental results are very strong, showing the improved performance achieved by SPC. There are also useful ablations and visualizations that illustrate how SPC achieves this performance. I am recommending to accept this paper, but believe it could be strengthened (see Weaknesses).\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1827/Reviewer_cbi7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1827/Reviewer_cbi7"
        ]
    },
    {
        "id": "0iABY8VJgM3",
        "original": null,
        "number": 3,
        "cdate": 1666610686049,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666610686049,
        "tmdate": 1669220539571,
        "tddate": null,
        "forum": "GbsvQSaJV-6",
        "replyto": "GbsvQSaJV-6",
        "invitation": "ICLR.cc/2023/Conference/Paper1827/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the SPC framework that combines automatic curriculum learning (ACL) and hierarchical MARL to learn cooperative behaviors in complex multi-agent domains. Specifically, this paper leverages a multi-armed bandit algorithm for ACL while addressing the following challenges: 1) the varying number of agents across tasks based on the transformer architecture, 2) non-stationarity based on the contextual bandit framework, and 3) reward sparsity based on the skills. Evaluations in MPE and GRF demonstrate the effectiveness of SPC. ",
            "strength_and_weaknesses": "**Strength:**\n1. The paper develops the theoretical contribution: the regret analysis of the contextual bandit problem for solving ACL (Section 3.2.2). \n2. Evaluations are performed using the complex domain of GRF.\n\n**Weaknesses, Questions, and Comment:**\n1. Overall, this paper combines multiple methods (e.g., transformer, skills, contextual bandits) for tackling too many challenges simultaneously, resulting in a relatively complicated framework. \n2. While SPC ablation studies are provided, it is still challenging to understand the effectiveness of SPC. For example, SPC uses the transformer architecture, but baselines do not use the transformer. As such, it is unclear whether SPC performs better than baselines due to ACL and skill or due to the base architecture difference. \n3. The paper's clarity can be improved. In particular, the paper states that multi-armed bandit algorithms are used due to the non-differentiable objective. However, there are other methods, including REINFORCE, and it will be helpful to justify why multi-armed bandit algorithms are selected over other methods. \n4. Regarding hierarchical skills, the paper states that the categorical distribution is used to enable the option-style skill by passing the one-hot embedding to low-level policies. However, each option consists of a tuple of the initiation set, intra-option policy (low-level policy), and termination function. As such, it is unclear whether simply passing the one-hot embedding to the low-level policy enables the option. \n5. Regarding the VACL baseline, the paper states that the centralized critic is removed for a fair comparison. However, my understanding is that the use of a transformer architecture can be effectively viewed as centralized training because my understanding is that gradient is passed across agents during training. If this view is correct, then it is unclear why the centralized critic is removed in VACL. \n6. In Section 5.4, the paper states that Figure 6b shows the shift in student policy representation throughout the training, but this is unclear because Figure 6b does not include the time notion. Could you clarify this? \n7. Minor: There is a typo: \"In Fig. 6, we can see that without the curriculum learning scheme\" -> \"In Fig. 5, we can see that without the curriculum learning scheme\"",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** SPC has a relatively complex framework, so clarity needs improvement.  \n**Quality & Novelty:** SPC combines multiple existing frameworks, so novelty and contributions could be limited.  \n**Reproducibility:** The source code is provided in the supplementary material to reproduce the results.  ",
            "summary_of_the_review": "I initially vote for 5 due to the weaknesses and concerns. I will make a final decision on the recommendation after the authors' response.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1827/Reviewer_WEWH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1827/Reviewer_WEWH"
        ]
    },
    {
        "id": "fcIXgEMZulZ",
        "original": null,
        "number": 4,
        "cdate": 1666661810315,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661810315,
        "tmdate": 1666672410667,
        "tddate": null,
        "forum": "GbsvQSaJV-6",
        "replyto": "GbsvQSaJV-6",
        "invitation": "ICLR.cc/2023/Conference/Paper1827/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a learning framework that aims to address the change in the number of agents in an environment as a student agent is trained through a curriculum of tasks as proposed by a separate teacher agent. The teacher agent is modeled as a contextual bandit that aims to resolve the inherent non-stationarity in student policies. The results show that using the proposed changes, the agents are able to outperform previous work on competitive benchmarks.",
            "strength_and_weaknesses": "The paper does an extensive study on the current issues in Automatic Curriculum Learning framework. The schematic diagram helps in better understanding the contribution and the proposed changes to the ACL framework. The MAB correction to train the teacher also helps in computing an approximate regret bound of its policy.\n\nThe paper misses out on some prior work that also use variable agents in a population as a means to learn diverse strategies [1, 2]. I am curious to know how the does the hierarchical setup affect in training diverse student strategies. Could a setup using meta-RL help in learning more diverse strategies rather than using bandit-based Exp3 algorithm?\n\nBesides, some more ablation studies on analyzing the various student policies would also help in understanding the effect of the population invariant method?\n\n[1] Gupta et al. 2021. Dynamic population-based meta-learning for multi-agent communication with natural language.\n\n[2] Portelas et al. 2020. Meta Automatic Curriculum Learning.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed work is novel and the contribution is clearly highlighted. The paper although is a bit dense but is easy to read and the important relevant concepts are thoroughly explained. Some ablation studies as highlighted above could help bolster the claims of the paper.",
            "summary_of_the_review": "The paper proposes novel changes to the popular ACL framework addressing key challenges such as non-stationarity in student policies and variable agents in the populations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1827/Reviewer_HPXF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1827/Reviewer_HPXF"
        ]
    }
]