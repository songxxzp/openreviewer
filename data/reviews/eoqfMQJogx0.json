[
    {
        "id": "CbGrC46Tuy",
        "original": null,
        "number": 1,
        "cdate": 1666650697275,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650697275,
        "tmdate": 1666650697275,
        "tddate": null,
        "forum": "eoqfMQJogx0",
        "replyto": "eoqfMQJogx0",
        "invitation": "ICLR.cc/2023/Conference/Paper3358/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the limitation that the graph signal smoothing framework only works on continuous values, and proposes the concept of *distributional graph signals* in order to support discrete node labels.\nThe paper generalizes some important notions in graph signal processing (GSP), i.e., smoothness and non-uniformity, to distributional graph signals based on the Wasserstein metric.\nThe paper also proposes a general regularization method based on the proposed concept to enhance graph neural networks (GNNs) for the semi-supervised node classification task.\n",
            "strength_and_weaknesses": "**Strengths**:\n    1. This paper introduces the concept of *distributional graph signals*, generalizes important notions in GSP to the proposed frameworks, and constructs a novel graph regularization method, with a solid mathematical and theoretical foundation.\n    2. The proposed regularization method shows empirical performance improvements compared to the selected base models and outperforms other selected regularization methods.\n\n  - **Weaknesses**:\n1. There is no comprehensive discussion on related work. The paper lacks a section devoted to related work, but beyond this, the embedded discussion (in the introduction and experiments sections, for example) does not provide an adequate introduction to compared methods and does not explain how the proposed approach differs and offers advantages. Example works on regularization that could be included in a discussion of related work are [R1-R4]; pioneering works on graph regularization include [R5-R6]. But beyond this, the paper needs to position the proposed technique within the general landscape of node classification methods. \n\n 2. It appears that many of empirical improvements on the based model and outperformance over the benchmarks are not statistically significant. The paper mentions \u201cnoticeable\u201d improvement, but there is no definition of this. There is no mention of testing to demonstrate a statistically significant improvement. In many cases the improvements appear to lie within 1 std, which doesn't augur well for the outcome of a carefully conducted test. \n\n3.    The majority of the experiments are on Cora, Citeseer, Pubmed and PPI. These are small datasets that have received too much experimental attention \u2013 methods are over-tuned to a limited number of datasets. The limitations of the datasets do not allow one to properly examine the expressive capability of the models. One cannot trust that a technique will provide improvement for general graph datasets if experiments are only provided for these datasets. There are now many graph datasets to choose from and most recent GNN papers perform experiments on a much richer set.\n\n4. The authors only verify the proposed enhancement method on the base models GCN, GAT and GraphSAGE (as well as GIN for graph-classification). All of these models were presented in papers published 4 or 5 years ago. The results would be much more persuasive if experiments were conducted with SOTA models as the base models. \n\n[R1]: Lingxiao Zhao and Leman Akoglu. PairNorm: Tackling Oversmoothing in GNNs. In ICLR 2020.\n[R2]: Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and Relieving the Over-Smoothing Problem for Graph Neural Networks from the Topological View. In Proc. AAAI, 2020.\n[R3]: Yang, Z., Cohen, W. and Salakhudinov, R., . Revisiting semi-supervised learning with graph embeddings. In Proc. ICML, 2016.\n[R4]: Li, Q., Wu, X. M., Liu, H., Zhang, X., & Guan, Z. Label efficient semi-supervised learning via graph filtering. In *CVPR, 2019*.\n[R5]: Zhu, Xiaojin, Zoubin Ghahramani, and John D. Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Proc. ICML, 2003.\n[R6]: Zhou, D., Bousquet, O., Lal, T., Weston, J., & Sch\u00f6lkopf, B. Learning with local and global consistency.  In Proc. NeurIPS, 2003.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The methodology is clear and well written. The experimental design is lacking detail (in particular the details of the compared methods and the experimental settings).\nNovelty: The work is novel and presents a very interesting approach for integrating graph signal processing techniques into graph neural networks.\nReproducibility: Code is not provided (only snippets are available). Based on the paper, it would be very challenging to reproduce the reported results.\n",
            "summary_of_the_review": "This paper provides an interesting and novel idea with good mathematical foundation. However, the work lacks a discussion of related work, and the experiments do not provide compelling evidence of the effectiveness of the proposed framework. The experiments need to be conducted more carefully with proper significance testing. Moreover, the experimentation should include more recent base models and richer datasets.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3358/Reviewer_XoLJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3358/Reviewer_XoLJ"
        ]
    },
    {
        "id": "EKWL6S9TEl",
        "original": null,
        "number": 2,
        "cdate": 1666686498830,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666686498830,
        "tmdate": 1668666025732,
        "tddate": null,
        "forum": "eoqfMQJogx0",
        "replyto": "eoqfMQJogx0",
        "invitation": "ICLR.cc/2023/Conference/Paper3358/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed a regularization term that can be added to traditional GNN training. The regularization term incorporate the defined distributional graph signals, which is a probability measure defined over the node label space, and shows significant improvement when compared to baseline models without regularization in empirical results. ",
            "strength_and_weaknesses": "Pros: \n- Provides a distributional view in the final softmax layer, which open possibility to add different GSP regularizations. \n- Theoretical analysis on the bound of total variations/losses. \n- No need to change the base model, and can apply to almost all existing framework. \n- Works on both Euclidean space and Hyperbolic space models. \n- Extensive empirical experiments with multiple baseline models in different real-world dataset. \n\nCons: \n- Selection/Tuning of \\ita is missing. \n- Does the method works on different input feature space (Euclidean and Hyperbolic), with only one total variance (or the two surrogate T_1, T_2)? Same for transiting from transductive to inductive learning, why would the assumption still be true if the underlying signal are coming from different/unseen space? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with source code also provided for reproducibility. The idea of applying GSP regularization at the output layer is nicely elaborated and can be easily adapted to different methods.   ",
            "summary_of_the_review": "Overall, I think the paper's idea is neat and empirical results show it work nicely with different input data in various learning task. Given the simplicity and versatility of method, I would recommend acceptance of the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3358/Reviewer_34XZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3358/Reviewer_34XZ"
        ]
    },
    {
        "id": "FNSp_mR-uM",
        "original": null,
        "number": 3,
        "cdate": 1666695315187,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695315187,
        "tmdate": 1666695795357,
        "tddate": null,
        "forum": "eoqfMQJogx0",
        "replyto": "eoqfMQJogx0",
        "invitation": "ICLR.cc/2023/Conference/Paper3358/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new regularization for Graph Neural Networks. Signal smoothness in cases if discrete node classification task is hard to define. Instead, the paper proposes to look at the distribution of the class labels at per node level and define smoothness on the distributional graph signals. The proposed regularization additionally ensures that nodes do not end up having uniform probabilities. The paper then proposes to use the regularization at different layers of a GNN model so as to be able to extend it for tasks beyond node classification. ",
            "strength_and_weaknesses": "# Strength:\n1. The regularization of a GNN is an important problem.\n2. The paper has some interesting ideas.\n***\n# Weaknesses:\n1. While the organization of the paper is fairly clear. The arguments are not easy to follow. \n2. The gains in the results are within a single standard deviation of the best baseline. It would be good to do a stat-sig test to see whether the observed gains are statistically significant or not.",
            "clarity,_quality,_novelty_and_reproducibility": "The organization is somewhat clear but the arguments are not easy to follow. For example, it is unclear what Section 3.1 is trying to do. It felt like the paper is going to compute total variation for distributional graph signals, but it ends up simply defining l1 and l2 versions of standard graph regularization, only that it uses class distribution instead of node labels, but that is what is usually done anyway. So what exactly are we getting by defining regularization of distributional graph signals. The key difference seems to be the use of non-uniformity constraint, but the connection between Section 3.2 and 3.3 is not very clear. How does the definition in Section 3.3 follow from the arguments made in Section 3.2?\n\nThe paper does seem to have some original ideas, but the final proposed regularization does not seem that very different from standard regularizers.",
            "summary_of_the_review": "The paper proposes a regularization term for Graph Neural Networks. The paper proposes to define graph distributional signal and define a smoothness on this signal, however, the final proposed form does not seem to be very different from standard regularizers. Secton 3.1 does not seem to actually make any substantial point. Also, it is unclear how Section 3.3 follows from Section 3.2. The final proposed regularizer seems to have come from air, based on very thin justifications. The gains reported in the experiment section are well within a single standard deviation of the best baseline. So it is unclear whether the observed are statistically significant or not.\n\nBased on these, I am inclined to mark this as marginally below acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3358/Reviewer_Eo3M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3358/Reviewer_Eo3M"
        ]
    }
]