[
    {
        "id": "sf0-zWWizS",
        "original": null,
        "number": 1,
        "cdate": 1666453539790,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666453539790,
        "tmdate": 1666453539790,
        "tddate": null,
        "forum": "HumfPzF2yeI",
        "replyto": "HumfPzF2yeI",
        "invitation": "ICLR.cc/2023/Conference/Paper5941/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an architecture for audio visual representations that can be used to embed multi-model representations useful for downstream robot learning tasks.",
            "strength_and_weaknesses": "Strenghs:\n- very relevant problem space: this is an important area of research,\n- the model is well-motivated, and described in sufficient detail to be generally useful,\n- the quantitative results are compelling.\n\nWeaknesses:\n- The whole paper can be summarized as: 'we took VAR, replaced the triplet loss with a contrastive loss, and things worked better'. While a solid result to be publishing, the novelty is limited: the general idea that contrastive losses generally improve over triplets is well documented in the literature.\n- The model is well engineered and reasonable for the setting that's explored, but the paper doesn't provide any unique, novel insight beyond 'here is an architecture that works well for this problem'. This means there is little one will learn from reading this paper beyond this exact setting.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and detailed. The appendix could be enhanced by providing all hyperparameter settings or a link to the open-source codebase.",
            "summary_of_the_review": "Nice contribution to a very specific problem, which is clear and correct, but likely too incremental to meet the bar for a venue like ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5941/Reviewer_hTKz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5941/Reviewer_hTKz"
        ]
    },
    {
        "id": "TyEyhEx3Oc",
        "original": null,
        "number": 2,
        "cdate": 1666526564370,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666526564370,
        "tmdate": 1669736970473,
        "tddate": null,
        "forum": "HumfPzF2yeI",
        "replyto": "HumfPzF2yeI",
        "invitation": "ICLR.cc/2023/Conference/Paper5941/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents VAR++, that is representation that generates intrinsic rewards by associating images and sound commands. This model is used to perform command-following robot tasks. The method builds on top of VAR (Visual-audio representation), is evaluated in 3 different tasks in simulation, and is compared against other 2 baselines in addition to the basic version.",
            "strength_and_weaknesses": "The paper is well structured and written in a clear way. The problem of interest is introduced and motivated in a comprehensive way, mentioning relevant literature in the field. The \"list of contributions\" reads more as the list of steps describing the approach and its advantages, rather than highlighting the novelties of this particular piece of work. The methodology is presented in a clear way and the diagrams are clarifying and well presented as well. The experimental setup is also described fairly well, however there is very little visualisation (only Fig. 4) of the simulated environment and setup. There is also no visualisation of the data and of the results obtained. The tables do report quantitative numbers, however they seem sometimes incomplete: for example why is ASR+NLU+RL not included in Table 2 and 4? Also, what is the impact of the transcription step (which is not part of the ASR+NLU+RL per se) on the performance obtained from applying ASR+NLU+RL? What are the results if you used (correct) text directly?\nThe limitations of the proposed work are not discussed. Can you describe in which cases the proposed method would fail or perform just as good as other methods? Is there an evaluation on time efficiency? Is the proposed method also more efficient with respect to other alternatives?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally written clearly, including the formulation of the problem and of the proposed solution. The novelty is, in my understanding, although interesting, marginal. The method introduced mainly adds on top of an existing method. The ideas proposed to improve the existing method are interesting, although I cannot evaluate their novelty with respect to other existing works.\nThe results are not reproducible, as there are very little details of the implemented models and of the simulated environments used.",
            "summary_of_the_review": "The proposed method is interesting and the paper well written. The novelty and contribution of the paper are incremental. The experimental evaluation does not include enough details to be reproducible, and the quantitative evaluation is missing some results (eg relative to one of the baselines considered) and some discussion about the limitations of the proposed method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5941/Reviewer_Fs4r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5941/Reviewer_Fs4r"
        ]
    },
    {
        "id": "F867pAJLxFN",
        "original": null,
        "number": 3,
        "cdate": 1666589019375,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589019375,
        "tmdate": 1669748453527,
        "tddate": null,
        "forum": "HumfPzF2yeI",
        "replyto": "HumfPzF2yeI",
        "invitation": "ICLR.cc/2023/Conference/Paper5941/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduced a method to learn a visual-audio conditioned control policy. The core idea is to learn a joint visual-audio representation and derive a self-supervised reward for training task policies. To acquire a high-quality visual-audio representation, they improved the existing VAR algorithm by adopting a batch-based supervised constrastive loss to improve the data efficiency. The policy reward is then defined as the difference between the embedding of the current image and the embedding of a sound signal provided at the beginning to indicate the task goal. The method was applied to two control tasks and one navigation-related task.",
            "strength_and_weaknesses": "Strengths:\n- Improvement over the existing VAR representation, which can be potentially helpful beyond the presented setting.\n- Self supervised RL training method appears solid and effective.\n- Enable robots to follow audio command is a useful and essential skill.\n\nWeaknesses:\n- Fine-tuning doesn\u2019t seem very efficient in general: 2400 labels and 1M self-supervised RL training for fine-tuning seems a lot. Does the -user need to provide 2400 labels (image/audio pairs)? Providing the label and collecting 1M samples in the real-world seems infeasible.\n- Not all images have intentions in real environments, which would lead to sparse reward problems and make the method potentially difficult to handle long-range tasks.\n- No examples of handling dynamics gap were presented even though it is claimed to handle that. For robotic control tasks they are in general simple: e.g. the manipulator only needs to reach certain location without interactions like grasping. It's not clear if the method can effectively adapt to like manipulation of unseen objects.\n",
            "clarity,_quality,_novelty_and_reproducibility": "good",
            "summary_of_the_review": "I think the paper is in general interesting and can be a good work for the venue, though I do have a couple concerns as listed in the weaknesses above. \n\nIn addition I also have some clarification questions for the work: \n1) For E2E was it trained with 1M RL steps as well? Or it was terminated after doing a pass through the 2400 new labels?\n2) For the intrinsic reward, was eq 4 or eq 5 used in the final system? \n3) For the training process, how were the sound labels as well as intentions generated?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5941/Reviewer_dfAL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5941/Reviewer_dfAL"
        ]
    }
]