[
    {
        "id": "DWXb_BH6xT",
        "original": null,
        "number": 1,
        "cdate": 1666256950846,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666256950846,
        "tmdate": 1670721728544,
        "tddate": null,
        "forum": "ykql_wKavL",
        "replyto": "ykql_wKavL",
        "invitation": "ICLR.cc/2023/Conference/Paper2318/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a new Federated Averaging algorithm, called z-SignFedAvg, that compresses the model updates to their signs after perturbing them with noise. They show both theoretically and empirically that z-SignFedAvg enjoys a faster convergence rate than existing sign-based compression methods. The authors further exploit the noise injection mechanism and propose a modified algorithm that has differential privacy guarantees. They demonstrate their findings on EMNIST and CIFAR-10 datasets.",
            "strength_and_weaknesses": "Strengths:\n\n- The authors study an important problem and their findings shed light on the success of other stochastic sign-based methods as well.\n- The paper is theoretically sound, and the experimental results are consistent with most of the theoretical findings. \n\nWeaknesses:\n- By looking at Figure 2, it seems that the value of the noise scale should be very carefully adjusted to enjoy both a good convergence rate and a final performance. The paper misses a recipe on how to pick a good noise scale for a particular task with a particular architecture. The plots in Figure 2 also suggest that the performance could be improved with an adaptive strategy to update the noise scale. For instance, $\\sigma=0.5$ has a faster convergence rate at the beginning of the training, but later it increases the loss, while $\\sigma=2$ is slower in the beginning, but it reaches the best objective value at the end. So it would make sense to start with $\\sigma=0.5$ and then gradually increase it to $\\sigma=2$ for the best result. A recipe on how to pick the best noise scale and how to update it throughout training seems necessary to make the algorithm more practical. Perhaps the prior work [1] on how to match the noise and the compression level for the best utility of the data may help authors gain more intuition on this. It's probably hard to apply the idea in [1] (matching the entropy of the injected noise to the target distortion at the compression stage) to this problem directly since sign-based compression is not a \"good\" compressor in the rate-distortion theoretic sense. But it may still give an idea for a heuristic approach. \n\n- It has been shown in many papers that the gradients tend to get sparser during the training, both in federated and centralized training. I believe this is another motivating factor to update the noise scale during training because the compression rate changes through iterations due to increasing sparsity. Again, please refer to [1] to see why the noise amount should be increased as the compression becomes more aggressive, or vice versa. \n\n- It could be interesting to see how the proposed method performs compared to QSGD -- another famous stochastic quantization method. \n \n\n[1] Isik, Berivan, and Tsachy Weissman. \"Learning under Storage and Privacy Constraints.\" arXiv preprint arXiv:2202.02892 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. The authors share sufficient details on the experimental setup and also the code. ",
            "summary_of_the_review": "Overall, I enjoyed reading the paper and found the findings interesting. The proposed algorithm is also very promising, although it could be improved to pick the best noise scale at each iteration. I listed some of my ideas and suggestions above. \n\n**Post rebuttal:** I think the new experiments on additional baselines improved the paper's empirical claims. The discussion on the noise scale also strengthened the paper. And I hope to see further investigation on this point in future work. After reading other reviews and authors' responses, I have decided to keep my original score 6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2318/Reviewer_ExtX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2318/Reviewer_ExtX"
        ]
    },
    {
        "id": "w9Vggcbjqb",
        "original": null,
        "number": 2,
        "cdate": 1666682104416,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682104416,
        "tmdate": 1666683443612,
        "tddate": null,
        "forum": "ykql_wKavL",
        "replyto": "ykql_wKavL",
        "invitation": "ICLR.cc/2023/Conference/Paper2318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies stochastic 1-bit (sign) quantization in federated learning. Particularly, the stochastic quantization is achieved by adding a random noise to the value and then taking the sign. The authors considers a general noise distreibution parameterized by $z$ with Gaussian ($z=1$) and uniform ($z=\\infty$) as special cases. Convergence rates of applying this compressor to FL gradient uploading is provided. Experiments are conducted to show the effectiveness. The authors also discuss combining DP with the proposed algorithm at the end of the paper.",
            "strength_and_weaknesses": "S1. Stochastic 1-bit quantization, though being a popular compression strategy in many applilcation, has not been well investigated in FL. Thus, the paper provides a good application of such compressor to FL.\n\nS2. The unified noise distribution is interesting and provides a general framework.\n\n========================================================\n\nW1. Adding Gaussian noise ($z=1$) leads to biased compressor, which in some sense loses the advantage of stochastic quantization. From the experiments we also see that it is alsmost always worse than uniform noise with $z=\\infty$. This makes the practical implication of Gaussian noise limited.\n\nW2. The theoretical results are not surprising and has been reported in literature. Basically, when the compression is biased ($z<\\infty$), we have a bias term in the rate. If it is unbiased ($z=\\infty$), then the rate can be $O(1/\\sqrt{nTE})$. In the paper the authors stated \"To the best of our knowledge, the previous works have never shown the sign-based method can achieve a linear-speedup convergence rate.\" Yet, the same convergence rate has been reported in literature, for example, \n\n[1] Federated Learning with Compression:Unified Analysis and Sharp Guarantees, Haddadpour et al., AISTATS 2021.\n\nTherefore, this claim is not true. In fact, the algorithm in this paper is also the same as that in the reference [1], except that in this paper stochastic sign is used instead of a general compressor. But since $z=\\infty$ results in unbiased compression, the result in this paper is actually covered by the result in [1].\n\nW3. In the experiments, I think the paper missed QSGD as an important and popular baseline method, since it is another way for unbiased compression. Also, how did you tune the learning rates? Could the slow convergence of sto-SignSGD in Figure 1 due to too large or small learning rate?\n\nW4. Section 4.4 on DP variant seems unnecessary and incomplete. There is no formal analysis of the DP guarantee, nor the choice of noise variance, etc. I feel that this section may distract the main focus and contribution of the paper, plus this section itself is not well justified.\n\n=====================================================\n\nQuestions:\n\nQ1. The noise scale $\\sigma$ in the paper seems important, since different $\\sigma$ gives very different performances. How to properly tune this parameter in practice?\n\nQ2. In Algorithm 1, line 10, why don't you multiply the Sign by $\\eta_z\\sigma$ as in (4)?",
            "clarity,_quality,_novelty_and_reproducibility": "please see above.",
            "summary_of_the_review": "The paper considers an interesting problem of using stochastic unbiased sign in FL. Some new results are developed for the noise distribution in the quantizer. However, the theoretical analysis is existing in literature and same result has been reported before. The experimental evaluation misses an important unbiased quantization baseline, and the 'extra' section of DP approach seems incomplete. Thus, currently I'm biased to rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2318/Reviewer_M8Jc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2318/Reviewer_M8Jc"
        ]
    },
    {
        "id": "s499bKQxJn0",
        "original": null,
        "number": 3,
        "cdate": 1667315202497,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667315202497,
        "tmdate": 1669873414572,
        "tddate": null,
        "forum": "ykql_wKavL",
        "replyto": "ykql_wKavL",
        "invitation": "ICLR.cc/2023/Conference/Paper2318/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers sign-based compression for federated learning (FL). It proposes a stochastic sign-based compressor, where first a random noise from (the proposed) $z$-distribution is added and then a sign operator is applied. The $z$-distribution covers Gaussian and uniform noise as its special cases. The sign-based compression is applied on top of the classical FedAvg, where clients use local SGD to compute model updates and the server computes a weighted average of updates. SignFedAvg can be viewed as an extension of SignSGD in [Bernstein et al. 2018] to FedAvg.",
            "strength_and_weaknesses": "**Strengths:**\n1. Applying sign-based compression on top of model updates is a natural extension of signSGD [Bernstein et al. 2018], where compression is applied on gradients. This is conceptually simple and can be practically useful. \n\n**Weaknesses:**\n1. For the case of $z = +\\infty$, i.e., when the injected noise is uniformly distributed on $[-1,+1]$, Assumption 3 seems very strong. It is also required for the theoretical result that the variance of the noise should be substantially large, i.e., $\\sigma > E(G + Q_{\\infty})$. This large variance in noise, in turn, results in a large variance term of the convergence result (Theorem 3 in Appendix B, formal version of Theorem 2). Remark 2 claims that SignFedAvg has the same order-wise convergence rate as that f uncompressed FedAvg. However, it is crucial that the large variance of Theorem 3 is discussed. \n\n* More specifically, the variance term in Theorem 3 is $\\frac{4\\gamma\\sigma^2\\sum_{j=1}^{d}L_j}{En}$. The theorem requires that $\\sigma > E(G + Q_{\\infty})$, which implies that the variance term is at least $\\frac{4\\gamma G^2 (E + Q_{\\infty})^2 \\sum_{j=1}^{d}L_j}{En} \\geq \\frac{4\\gamma G^2 E \\sum_{j=1}^{d}L_j}{n}$. \n\n    * When $L_j$\u2019s are constant, the variance terms is dimension dependent. Since large neural network have significantly large $d$, the variance would be quite substantial. \n\n   * It is not clear why the variance term behaves as $O((n\\tau)^{-1/2})$ if $E \\leq n^{-3/4}\\tau^{1/4}$. It is important to give further details.\n\n   * The authors compare the convergence result with [Yu et al. 2019], but the variance term in that paper is proportional to the variance of SGD operator. On the other hand, in Theorem 3, the variance term depends on $d$ and $Q_{\\infty}$, which can be really large.\n\n2. The noise variance $\\sigma$ is considered as a hyperparameter, and for EMNIST and CIFAR values of $\\sigma$ chosen via hyperparmater tuning are quite small (0.01 and 0.005). Theorem 3, on the other hand, shows counter-examples that the algorithm cannot converge for small values of $\\sigma$. This seems to indicate a gap between theory and experiments. More discussion on this point would be really helpful. \n\n3. In DP-SignFedAvg (Algorithm 2), the DP noise is added only on the local update. This will give client-level privacy and not item-level privacy. In other words, DP guarantee on the aggregate model at each round against adding or removing a client should. The paper does not mention any of these details, but only has Definition 2, which is generic and does not specify how to apply it to FL. \n",
            "clarity,_quality,_novelty_and_reproducibility": "* The presentation is decent and understandable. I did not go through the detailed proofs to check the correctness, but there does not appear to be any discrepancy in results. On the other hand, the assumptions are quite strong, and the questions mentioned under \u2018Weaknesses\u2019 need to be discussed. \n\n* The extension of SignSGD to SignFedAvg is a logical next step. The key novelty is in terms of analysis of $z$-distribution and convergence analysis of SignFedAvg.\n",
            "summary_of_the_review": "The analysis of sign-based compression with $z$-distribution noise is interesting. At the same time, the convergence analysis relies on very strong assumptions and leaves open several questions. Further, the DP part needs to be fleshed out. Therefore, the paper is not yet ready for publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2318/Reviewer_xh3m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2318/Reviewer_xh3m"
        ]
    }
]