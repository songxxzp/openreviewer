[
    {
        "id": "JhO4VvJYby9",
        "original": null,
        "number": 1,
        "cdate": 1666407921862,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666407921862,
        "tmdate": 1666407921862,
        "tddate": null,
        "forum": "N3kGYG3ZcTi",
        "replyto": "N3kGYG3ZcTi",
        "invitation": "ICLR.cc/2023/Conference/Paper6611/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a type of neural networks that uses lateral inhibition inspired layers. Lateral inhibition in this paper refers to the effect where the contrast of nearby neuron excitation in the lateral direction is increased for recognition. Inspired by this, the paper proposes to use a Gaussian low-pass filter with a zero center weight along with a learnable channel weight to mimic this effect. A flexible alternative would be a depthwise convolution. Then this output is used to subtract from the input tensor to form the final output. This lateral inhibition inspired design is tested with AlexNet and ResNet18 for image classification on ImageNet, and shows improvements at little additional cost.",
            "strength_and_weaknesses": "This paper is motivated by a neurobiological effect named lateral inhibition. It is always fascinating to see methods that are inspired in this way because it may reveal some important angles that the academic community has overlooked.\n\nBut this paper seems to be less prepared for this conference. The method is interesting but very simple: it adds a couple of convolutional layers to existing architectures. The baseline architectures are not from recent works, and the model scales in the test are also small (e.g. ResNet18, AlexNet). I also didn\u2019t find the visualizations mentioned in the paper. I hope that the authors could add more experiments to the paper, show larger experiment settings, and polish the paper a bit more.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear to read. But its quality is below the conference standard. The method is simple and the novelty is limited. The reproducibility seems good.",
            "summary_of_the_review": "This is a paper inspired by the lateral inhibition effect. The motivation is great, but the method is too simple, and the paper does not show the strength of the method through large-scale and comprehensive experiments. Overall, I think the paper is below the conference standard.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6611/Reviewer_KkWZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6611/Reviewer_KkWZ"
        ]
    },
    {
        "id": "uM31Bm53z8",
        "original": null,
        "number": 2,
        "cdate": 1666471907071,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666471907071,
        "tmdate": 1666471907071,
        "tddate": null,
        "forum": "N3kGYG3ZcTi",
        "replyto": "N3kGYG3ZcTi",
        "invitation": "ICLR.cc/2023/Conference/Paper6611/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "\nSummary:\nThis article starts from the perspective of neuroscience, hoping to design a neural network that conforms to human cognition. The original intention of the article is good, but there is nothing special about the method of the article. Regarding the writing, methodology, and experiments, this article is far below the acceptance criteria for a good conference like ICLR. Reviewers put a lot of effort into reviewing each article, so hopefully, authors will write each article well before submitting it.\n\n",
            "strength_and_weaknesses": "\n(Positive) Although this article is very poor, its exploratory spirit, specifically the exploratory spirit of exploring neural networks that conform to human cognition, is worthy of recognition.\n\n\n(Negative) The authors claim some gain in their approach. But as we all know, after introducing attention or dynamic mechanism, a general neural network can have some improvement. In particular, the worse the network, the more obvious the improvement.\n\n(Negative) In fact, filtering is not a new thing in neural networks.\n\n\n(Negative) A very working method should be validated on large-scale tasks. In addition to classification, there is detection, segmentation, and so on.\n\n\n(Negative) The writing of this article is very poor. There are multiple copies of the text in the article. For example, there are multiple repetitions of text in the abstract and introduction. A good article should have a better way of expressing it.\n\n\n(Negative) Gaussian filtering and bilateral filtering are not new in neural networks. The method proposed in the article is equivalent to a bilateral filtering non-local neural network.\n\n(Negative) The article associates its method with depth-wise convolution. This is very imprecise and arrogant. If this is true, is MLP-Mixer also a special case of this article?\n\n(Negative) As I said before, adding attention or dynamics to the neural network, such as senet, sknet, acnet, dynamic nets, will bring gains. There are too many such examples. So the results in Table 2 are not dazzling at all.\n\n(Negative) The numbers in Equation 5 are so intuitive and natural.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe quality, clarity, and the originality are poor.\n\n",
            "summary_of_the_review": "\nSee \"Summary Of The Paper.\" Regarding the writing, methodology, and experiments, this article is far below the acceptance criteria for a good conference like ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "\nThere is no ethics concern.\n",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6611/Reviewer_GVUz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6611/Reviewer_GVUz"
        ]
    },
    {
        "id": "ok-SHSEAckf",
        "original": null,
        "number": 3,
        "cdate": 1666592637956,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666592637956,
        "tmdate": 1669466181398,
        "tddate": null,
        "forum": "N3kGYG3ZcTi",
        "replyto": "N3kGYG3ZcTi",
        "invitation": "ICLR.cc/2023/Conference/Paper6611/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper incorporates the lateral inhibition of neuron into the Deep network for image classification. Results show the effectiveness of the proposed mechanism. The lateral inhibition is not novel but the idea to explore the usability of the lateral inhibition into artificial neural network is interesting.  The key weakness of this paper is that there are no any explanations why their design improves the performance.  \n",
            "strength_and_weaknesses": "The lateral inhibition based idea to explore the usability of the lateral inhibition into artificial neural network is interesting and useful. Such an approach should be strongly supported. \n\nThe key weakness of this paper is that there are no any explanations why their design improves the performance.  ",
            "clarity,_quality,_novelty_and_reproducibility": "1\\ Why the low-pass filter with eliminating the central weight could model the LATERAL INHIBITION is not clear from this paper. The design is not a typical Difference of Gaussian function which is generally adopt in image processing [1].  Eq (5) seems an OFF-receptive field ?\n\n2\\ The inhibition computation by subtracting a computed value from the previous one is commonly used in many previous models [1][2][3]. The traditional models simulated visual mechanisms should be clearly mentioned and discussed [1][2][3]. \n\n3\\ The description of related work is very limited as the reason shown in above. \n\n4\\ The learned weight W plays the similar function of channel attention. Hence, what\u2019s the difference between channel attention and the inhibition computation?  \n\n5\\ Eq (5) seems an OFF-receptive field ? Author should visualize the learned kernels as many examples. \n\n[1] IEEE Transactions on Image Processing. 2015;24(8):2565{2578.\n[2] Brain-inspired weighted normalization for CNN image classification.\n[3] IEEE transactions on pattern analysis and machine intelligence. 2015;37(10):1973{1985.\n\nMinors:\n\nSection 3.3 INHIBITION PLACE\n\nwe place it right after that convolution, as shown in Fig.\n",
            "summary_of_the_review": "It can be seen that the work is not complete with only five pages as the main text. Many important issues such as explaining why the design of lateral inhibition improves the performance. Computationally, what\u2019s the difference between channel attention and the inhibition computation?  In summary, the technique contribution is simple.  \n\nI encourage authors to enhance their work by addressing 1-2 key questions mentioned above, adding content and depth to the paper.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6611/Reviewer_edpQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6611/Reviewer_edpQ"
        ]
    },
    {
        "id": "8pd0pVqmwM",
        "original": null,
        "number": 4,
        "cdate": 1666671272853,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671272853,
        "tmdate": 1666671272853,
        "tddate": null,
        "forum": "N3kGYG3ZcTi",
        "replyto": "N3kGYG3ZcTi",
        "invitation": "ICLR.cc/2023/Conference/Paper6611/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose to add a biologically inspired lateral inhibition mechanism into deep convolutional networks for image recognition. When incorporated into AlexNets and ResNets, LI seems to improve performance on ImageNet classification without increasing trainable parameters. The authors examine the LI filter weights and find a biologically-resemblant center-surround pattern of inhibition.",
            "strength_and_weaknesses": "Strengths:\n1) There are considerable gains on AlexNet and ResNet while using LI, however, I would like to note that these are preliminary results as also highlighted by the authors.\n\nWeaknesses:\n1) The proposed work lacks novelty, several methods in the past have tried to apply a very similar lateral or divisive inhibition mechanism to deep convolutional networks and have reported gains in image classification performance (particularly when added to AlexNet). See [1] [2] and [3] for example. The proposed work is almost exactly similar to [1].\n2) The evaluation is very preliminary and lacks comparison to suitable baselines (other kinds of normalization such as BatchNorm, LayerNorm, etc.) or other normalization techniques such as [1, 2] which are very relevant.\n\nReferences:\n1. Hasani, H., Soleymani, M., & Aghajan, H. (2019). Surround modulation: A bio-inspired connectivity structure for convolutional neural networks. Advances in neural information processing systems, 32.\n2. Miller, M., Chung, S., & Miller, K. D. (2021, September). Divisive Feature Normalization Improves Image Recognition Performance in AlexNet. In International Conference on Learning Representations.\n3. Pan, X., Giraldo, L. G. S., Kartal, E., & Schwartz, O. (2021). Brain-inspired weighted normalization for CNN image classification. bioRxiv.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed work is not novel and evaluations are preliminary. The authors don't discuss sharing code or trained models, which is troublesome to reproduce the presented results.",
            "summary_of_the_review": "The proposed work is preliminary and lacks the novelty and quality of work expected at ICLR. I do not recommend accepting this paper at this stage. I suggest the authors to please consider a suitable workshop for this work and a significant extension of this work could be suitable for the ICLR audience with wider evaluation using suitable baselines and tasks.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6611/Reviewer_Ct44"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6611/Reviewer_Ct44"
        ]
    }
]