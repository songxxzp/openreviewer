[
    {
        "id": "Q4p2mtFo4Xd",
        "original": null,
        "number": 1,
        "cdate": 1666239572130,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666239572130,
        "tmdate": 1666305716730,
        "tddate": null,
        "forum": "ELmZduELxm",
        "replyto": "ELmZduELxm",
        "invitation": "ICLR.cc/2023/Conference/Paper3932/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submission proposes a new CTDE algorithm called UTS. UTS is designed to have two properties that the submission calls policy invariance and full representational capacity. The submission benchmarks UTS in matrix games, predator prey, and SMAC.",
            "strength_and_weaknesses": "### Clarity\n\nI personally found the description of the problems that UTS is trying to solve somewhat obfuscated. Perhaps I am not the target audience and that is why I found it difficult to follow, but I think the submission could be clearer overall. To give a concrete example, I feel like I read through the whole intro without really understanding the main idea.\n\n### Baselines\n\nIt seems like the submission is lacking some relevant baselines, such as MAPPO, IPPO, and HATRPO, given that it is claiming to \"outperform state-of-the-art\".\n\n### Benchmarking\n\nIn deep RL, and perhaps especially deep MARL, there is historical precedent for bad empirical practices, such as reporting mean return over a small number of seeds. It is well documented that this practice leads to unreliable reporting of algorithmic strength -- see Deep Reinforcement Learning at the Edge of the Statistical Precipice (NeurIPS 2021). I think the submission ought report the alternative metrics advocated in the statistical precipice paper if it wants to make big claims like \"outperforming state-of-the-art\".\n\n### Motivation\n\n> In this section, we introduce a shaping function to generate the shaped joint action-value Qf(s, u) to replace the original Q(s, u). The shaping function should have the following properties: 1) policy invariance, i.e., not making the agent deviate from the true goal and keeping the optimal policy unchanged; 2) full representational capacity, i.e., all shaped joint action-values should belong to\na subset of Qmvf.\n\nThis is a really important sentence in the submission. Yet, it doesn't feel clear to me. What does \"not making the agent deviate from the true goal\" mean? Additionally, why are we saying that \"shaped joint action-values should belong to a subset of Qmvf\"? Isn't belonging to a subset of a set the same as belonging to the set itself?\n\n### Dealing with stochasticity\n\nThe approach to determining whether stochasticity is present feels unsatisfactory to me. The state predictor may output zero even if there is stochasticity.\n\n### Confusion\n\n> However, these approaches suffer from instability arising from the non-stationarity of the environment induced by simultaneously\nlearning and exploring agents (Kuba et al., 2021). Centralised learning of joint action-values can solve these problems, but it is challenging to scale in real-world applications due to intractably large joint action space and communication constraints. \n\nKuba et al. (2021) show that MAPPO-style approaches lack a monotonic improvement guarantee. Unless I am misunderstanding, UTS does not have such a guarantee either. Could the authors clarify this point?",
            "clarity,_quality,_novelty_and_reproducibility": "I personally found some aspects of the clarity to be lacking, as discussed above. I also raised some issues regarding the quality of the empirical approach (ie, dealing with stochasticity) and the quality of the baselines and benchmarking. To my knowledge, the approach is novel. The code is not available; I would guess that it would difficult to independently reproduce the results.",
            "summary_of_the_review": "The method feels a little hacky to me, but I think that's not the worst thing if it performs well. \n\nThe submission makes big claims about its method's performance, such as \"outperforming state-of-the-art\". However, I do not feel that it lives up to these claims in the experiments, both because it is lacking in baselines and because it does not follow modern deep RL benchmarking practices.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3932/Reviewer_9h2R"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3932/Reviewer_9h2R"
        ]
    },
    {
        "id": "DutBHniwGV",
        "original": null,
        "number": 2,
        "cdate": 1666607899474,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666607899474,
        "tmdate": 1666607950199,
        "tddate": null,
        "forum": "ELmZduELxm",
        "replyto": "ELmZduELxm",
        "invitation": "ICLR.cc/2023/Conference/Paper3932/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a reward shaping scheme for MARL towards fixing QMIX, an existing algorithm suffering from suboptimality in non-monotonic settings. The method uses target shaping by predicting reward and next state uncertainty. ",
            "strength_and_weaknesses": "1. There are some important works missing in the related works which need to be discussed towards getting a proper context for learning in CTDE settings (instead of focusing only on value based methods) eg. : Tesseract [1] an actor-critic method natively supports decentralization despite environment stochasticity and non monotonicity, it also proves important representation capacity results. Similarly, there have been developments in model based methods [2] for addressing representational issues.\n\n2. How does the proposed method deal with uncertainty arising from neural network approximation? this is a huge component of target noise in practise. There isn't enough discussion on this given that the proposed method relies on uncertainty estimates that too with limited number of environment steps.\n\n3. What is meant by \"convergence to the optimal QMIX for any non-monotonic and stochastic Q\"? is it the best qmix learnable policy?\n\n4. How exactly is eq 7 preventing a bad monotonic projection? There needs to be a guarantee in probabilisitic or worst case sense.\n\n5. Why are stronger baselines not used for comparison? For eg. [3] has better performance on all the SMAC maps considered here. Overall this also affects significance of the work if strictly better methods already exist.\n\n6. The paper claims to be aimed for stochastic domains but SMAC is not adequately stochastic.\n\n7. The paper needs improvement in terms of writing and presentation, especially for conveying the overall problem and method. Currently, I do not find it easily readable.\n\nMinors:\n1. The results used from other works should be cited where they are used: eg. The line in introduction \"However, QMIX can only represent values in the restricted monotonic space\" should cite Maven, Mahajan et al 2019 and Qtran, Son et al 2019, there itself.\n2. Eq 3 has been repeated twice.\n3. Pg.5 ending, \"objection of\", did you mean \"objective of\"?\n\nReferences:\n1. Tesseract: Tensorised Actors for Multi-Agent Reinforcement Learning, Mahajan et al, 2021\n2. Model based Multi-agent Reinforcement Learning with Tensor Decompositions, Van Der Vaart et al 2021\n3. Rode: Learning roles to decompose multi-agent tasks, Wang et al, 2021\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Writing needs improvement as the paper is not very clear. Related works also needs to address MARL approaches more broadly.\n- The paper tries to fix problems in an existing method.\n- There is also need for better empirical evaluation on complex domains with actual stochasticity. The performance compared to existing methods like [3] is also not well. Some of the claims about representability achieved by reward shaping is not well supported with proofs.\n- SMAC is an opensourced domain, the  code for the method and predator prey environment has not been provided.",
            "summary_of_the_review": "Interesting idea but needs improvements (see the 2 sections above).",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3932/Reviewer_gapB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3932/Reviewer_gapB"
        ]
    },
    {
        "id": "aP5aaOoJWM",
        "original": null,
        "number": 3,
        "cdate": 1666903637303,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666903637303,
        "tmdate": 1666903637303,
        "tddate": null,
        "forum": "ELmZduELxm",
        "replyto": "ELmZduELxm",
        "invitation": "ICLR.cc/2023/Conference/Paper3932/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new value factorization method for cooperative MARL. The technique brings out an alternate way of value factorization (to WQMIX and QMIX) when the target values are non-monotonic and stochastic. In particular, prior approaches is the inability to recover the optimal Qtot value when the joint value function is non-monotonic and stochastic. A popular matrix game example variant shows that the previous approaches frequently overestimate or underestimate joint Q values in stochastic and non-monotonic settings. An alternate shaping function is introduced to generate the shaped joint action-value that can consider the stochasticity in the reward and state function and place appropriate weight on the different joint actions. To this end, a neural network is trained to predict standard deviation and error in the reward and the embedding of the next state. Another neural network is trained to estimate the best action value for each agent, assuming the agent gets the cooperation of other agents. Finally, based on the prediction error and the best action values, the shaping function generates monotonic target joint action-values for QMIX.",
            "strength_and_weaknesses": "Strengths\n\u2022\tIncorporates stochasticity in reward and state representation\n\u2022\tGood empirical performance\n\u2022\tLimitations of prior works is explained through theory and the example of matrix game\n\nWeaknesses\n\u2022\tThe overall approach is a bit of a kitchen sink with many hyperparameters to be tuned and models to be trained. QMIX alone is already known for being challenging on this front with larger number of agents.  See, e.g., the below reference.\n\u2022\tGoal of estimating the stochasticity of rewards and next states seems similar in spirit to distributional RL, but there is no comparison to related work from this literature.\n\u2022\tWhile the experiments include some ablations, the role of various parts of the overall approach is not fully explored.  For example, there are two approaches used in combination to detect stochasticity (reward predictor and state predictor).  What if you only include one?  Or what happens if you use different rules about when to apply or not apply the change of target than that in Equation (7) since at the very least it has two different cases.\n\n@article{avalos2021local,\n  title={Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning},\n  author={Avalos, Rapha{\\\"e}l and Reymond, Mathieu and Now{\\'e}, Ann and Roijers, Diederik M},\n  journal={arXiv preprint arXiv:2112.12458},\n  year={2021}\n}\n",
            "clarity,_quality,_novelty_and_reproducibility": "The use of random embedding for estimating stochasticity seems similar to some work on exploration such as the following (although with a different goal).  Is there a relation?\n\n@article{burda2018exploration,\n  title={Exploration by random network distillation},\n  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},\n  journal={arXiv preprint arXiv:1810.12894},\n  year={2018}\n}\n\nThe first sentence of 4.2 is confusing to me.  I thought Q^{mvf} is intended to be the set of things that QMIX can in principle learn.  But the phrasing suggests that there are things in it that QMIX cannot represent.  Is there a typo here or something that needs to be better explained?\n\nThe reason of predicting each agent\u2019s best action value function is not well explained in the main text and required studying the proof of Theorem 2 for me to understand.  My intuition is the goal is quantify the suboptimality of actions because, by definition, the the join action is suboptimal (and the optimal action is unique) then for every agent not playing their part of the optimal action their q_a will be less than the optimal Q value.  \n\nThe statement of Theorem 2 seems to be missing something.  It contains a \u201csuch that,\u201d with no indication what the assumption is.  From the proof I\u2019m guessing it is that the optimal policy is unique and the discussion of this issue in the proof should be discussion and not actually part of the proof?  Relatedly, I don\u2019t know where there is a quantification over joint actions u in the theorem statement.\n\nThe intuition for the decision not to change the target if it is highly stochastic is not clear to me. (Discussion before (8))\n",
            "summary_of_the_review": "A paper with strong empirical performance but room for improvement in the throughness and clarity of the exposition.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3932/Reviewer_kwZB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3932/Reviewer_kwZB"
        ]
    }
]