[
    {
        "id": "FFjDGnx8So",
        "original": null,
        "number": 1,
        "cdate": 1666105469669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666105469669,
        "tmdate": 1666105469669,
        "tddate": null,
        "forum": "KfptQCEKVW4",
        "replyto": "KfptQCEKVW4",
        "invitation": "ICLR.cc/2023/Conference/Paper732/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper covers a new method for approximate nearest neighbour search aimed at very large datasets where hyper-parameter optimisation can be achieved efficiently.  The hyper-parameter optimisation uses some clever approximations that then allow very efficient optimisation.  The performance is measured empirically and shown to give better results than current methods.",
            "strength_and_weaknesses": "This is a well written paper.  The idea is elegant and sophisticated.  The empirical evaluation looks very convincing.\n\nAlthough this is not my research field I see no obvious weaknesses.  Two very minor points that can be easily corrected.  The first is the Pareto should be capitalised as it refers to a proper name (Vilfredo Pareto).  Secondly, I might be wrong but I could not see a reference to Figure 2 in the text.  I think this should be referenced in Section 5.2.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written despite its technical content.  It address a problem that I suspect is of considerable importance, namely approximate nearest neighbour detection.  This is not my field, but this seems to address an important problem of existing techniques is that it is hard to find appropriate hyper-parameters to make these methods work for different types of dataset.  The approach used seems novel.  The results seem reproducible.",
            "summary_of_the_review": "This is a well written paper with a nice idea that looks well motivated and technically sound.  The empirical results back up the claims (there is one approximation about independence that needs empirical justification).  In my view this is a good paper that deserves publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper732/Reviewer_DrxR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper732/Reviewer_DrxR"
        ]
    },
    {
        "id": "JvupWHi_6r",
        "original": null,
        "number": 2,
        "cdate": 1666637800772,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637800772,
        "tmdate": 1666637800772,
        "tddate": null,
        "forum": "KfptQCEKVW4",
        "replyto": "KfptQCEKVW4",
        "invitation": "ICLR.cc/2023/Conference/Paper732/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper looks at the problem of hyperparameter tuning for the Approximate Nearest Neighbor (ANN) search. The authors propose a constrained optimization-based approach for tuning the quantization-based ANN methods. They only require a search cost or recall as input, then the Lagrange multipliers-based approach can provide a configuration such that the performance reaches the speed-recall pareto frontier. Extensive results confirm the superior performance of their proposed method. \n",
            "strength_and_weaknesses": "The paper has the following strengths:\n\nS1. The motivation is clear. As the dataset size increases significantly, tuning the parameters (manually or using traditional methods like grid search and black box optimizer) becomes computationally prohibitive. It is in significant demand to have a theoretically grounded yet computationally lightweight approach that can produce excellent performance.\n\nS2. The proposed method is novel and easy to use. The input is simply the search cost or recall, which is friendly for fresh users unfamiliar with the ANN methods' details. The authors also provide sufficient insights and theoretical analysis when they introduce their idea.\n\nS3. This paper is well written with clear clarity and thus is easy to read.\n\nThe paper has the following weaknesses:\n\nW1. In the last paragraph of the introduction section, they claim their approach is very general and can be extended to distance measures, quantization algorithms, and search paradigms. It would be nice if they could simply illustrate how to extend such a claim, e.g. for different search paradigms.\n\nW2. In Section 5.2, they claim they compare to heuristic, hand-tuned, and black-box optimizer settings. However, I cannot find such settings in Figure 2, and Figure 3 only depicts the comparison with a black-box optimizer Vizier. Moreover, the baselines that are shown in Figure 2 lack explanations. \n\nW3. In Section 5.3 and Figure 4, could you explain what $R^2$ is?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, based on the strength and weaknesses of this paper, the clarity and the quality seem good to me. \n\nConsidering the proposed approach is computationally efficient and has a theoretical guarantee, it seems to me that this paper is novel. \n\n ",
            "summary_of_the_review": "In summary, I think the motivation is clear, the idea is sound, and the paper is written well enough. Thus, I tend to initially give acceptance to this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper732/Reviewer_cFiP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper732/Reviewer_cFiP"
        ]
    },
    {
        "id": "PllT6RwRVkN",
        "original": null,
        "number": 3,
        "cdate": 1666707546175,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666707546175,
        "tmdate": 1666707546175,
        "tddate": null,
        "forum": "KfptQCEKVW4",
        "replyto": "KfptQCEKVW4",
        "invitation": "ICLR.cc/2023/Conference/Paper732/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "ANNS algorithms, especially those involving quantization, have many hyperparameters to tune. Careful tuning can yield significantly better accuracy and query throughput. However, tuning can be expensive, especially for large indices, if done naively (build the index and search). Therefore, the authors design a proxy loss metric to guide the hyperparameter search using a simple linear-scan based algo. This loss is an approximation of the metrics that matter in practice. Using this they show that on billion scale datasets, the tuning can outperform baselines by a margin and improve upon blackbox optimizers. Further, the authors entry won Track 1 of a recent NeurIPS challenge on large scale ANNS.",
            "strength_and_weaknesses": "Strengths:\n1. Good empirical results -- improves significantly on baselines, outperforms blackbox tuning (e.g. Vizier) and on small scale datasets closely matches exhaustive tuning\n2. Results demonstrated on multiple large scale datasets.\n\n\nWeakness:\n1. Not much discussion on applicability to tuning algorithms other than the linear scan in the paper. \n2. Its not clear that the techniques can be applied for interactive query processing.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity** \n- What are the family of ANN algorithms that you are able to tune with your approach? linear scan? IVF? hashing? or just linear scans?\n- If the answer to above question is linear scan, I suppose the algorithm does a full scan over the entire billion scale dataset in 16bytes precision? Is it possible to do so in an interactive mode (one query at a time), or must the entire dataset be batch processed to get reasonable query throughput? Scanning 16GB (1B points X 16byte PQ) for each query seems expensive!\n- In Alg 1 and Section 3.4 and 4, you list S1/X1 as the largest set with lowest bitrate. However, In fig 6, you list X5  as the largest set with highest bitrate. As in X5 has all the elements and has the highest number of bytes in its representation. Is there a mistake in Fig 6? If not, the exposition in 3.4 and 5 seems to contradict what is presented in Fig 6.\n\n**Quality**\n- In section 5.4, why not use Yandex T2I dataset which has out-of-distribution queries?\n- Can you comment on how well your techniques apply to graph algorithms and other algorithm on Track 2/3 of the NeurIPS competition.\n- Can you list the total number of hyperparameter configurations that can be chosen from at billion scale to give the reader a sense of the size of this space.\n- If Google Vizier is allowed to run for more than 6 hours, does it find the pareto-optimal configurations found by your algorithm? Could you also comment a bit more about where Vizier would fail and where it might succeed?\n- Can one sample the dataset (1B->1M), run exhaustive tuning to find 4 levels of quantization and then train the 5th level while going back up to 1B? How much worse would this be compared to one-shot tuning using your proxy loss.\n\n**Reproducibility**\n-- Are the results on the training query set or the validation query set released in the competition?\n\n",
            "summary_of_the_review": "The results presented in the paper are a great improvement over baselines, however the paper could be stronger if it generalizes to interactive query processing or to other algorithms rather than the linear scan. It could be that the techniques are applicable, but couldn't ascertain this in my reading of this paper. I would rate the paper higher if this was the case.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper732/Reviewer_F6KH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper732/Reviewer_F6KH"
        ]
    },
    {
        "id": "g9c57FW1oo",
        "original": null,
        "number": 4,
        "cdate": 1666730230490,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666730230490,
        "tmdate": 1666730230490,
        "tddate": null,
        "forum": "KfptQCEKVW4",
        "replyto": "KfptQCEKVW4",
        "invitation": "ICLR.cc/2023/Conference/Paper732/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper uses the recursive vector quantization ANN method, which can also be seen as a hierarchical VQ approach. The proposed method provides an optimization-based approach to fine-tune this ANN method. The analysis was made on million-scale and billion-scale ANN datasets. ",
            "strength_and_weaknesses": "Strengths:\n\n1) The paper is novel and shows a performance improvement over baselines with negligible optimization cost.\n2) The optimization cost is carefully designed. The method is based on maximizing the geometric mean recall for maximizing the ANN recall. It maximizes the arithmetic mean lower bound. To minimize the ANN search time, it minimizes memory accesses during the query phase. The overall constrained optimization maximizes the recall with the given search time constraints. \n3) The authors have shown the benchmark against the grid search and one black box search method- Vizier.\n\nQuestions and weaknesses:\n1) Overhead of the parameter tuning: How does the parameter tuning time compare with the Black- box optimizers such as Vizier?\n2) What is the rationale behind choosing the recall rate as the objective?:  The objective function is the recall rate, and the constraint is the search cost. Does the opposite, where the objective function is the search cost, and the constraint is the recall rate, results in a similar performance? \n3) Please correct me if I am wrong here: What query set is used for computing the objective function- $\\mathcal{L}_i (t_i)$? As mentioned in 5.4, it looks like the query set is being used for optimization. However, this will not suit well for the real-time near-neighbor search, where the queries are unseen and seen one-by-one or batch-by-batch as a stream. The optimization cost should be negligible to achieve a real-time performance here.\n4) With an assumption of an in-distribution query set (same distribution of index and query points), this can even use a fraction of index data to optimize the parameters.\n5) Are there any other black box optimizers available for the ANN parameter tuning?\n6) Section 5.4: The random split of the query set $Q_1$, and $Q_2$ is not the best way to see the out-of-sample performance. A better split where $Q_1$ and $Q_2$ are itself out of distribution to each other may be a better way to test the out-of-sample performance. Please clarify if I am missing something here.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written well and clearly. The method is novel. No code is provided for reproducibility ",
            "summary_of_the_review": "The constraint optimization formulation is based on the search time and recall is intuitive, however, the applicability of the optimizer is questionable for the unseen query data. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper732/Reviewer_vMnt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper732/Reviewer_vMnt"
        ]
    }
]