[
    {
        "id": "ccx9AQ5uNl",
        "original": null,
        "number": 1,
        "cdate": 1666589584632,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589584632,
        "tmdate": 1666589584632,
        "tddate": null,
        "forum": "lV0fWRDJwR",
        "replyto": "lV0fWRDJwR",
        "invitation": "ICLR.cc/2023/Conference/Paper2223/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers jointly learning a stable controller and neural network represented by the Lyapunov function, which is \"empirically\" robust with respect to the dynamics difference between the training and test environment. Experiments are conducted on Pendulum, Cart Pole, car tracking, and 2-link pendulum. ",
            "strength_and_weaknesses": "Strength:\n* The writing is clear and well-organized. \n* Detailed Description of the experiment setup and results\n\nWeaknesses:\n* The intellectual contribution of this proposed work in comparison to existing neural Lyapunov control seems incremental. The basic framework is based on the Neural Lyapunov Control paper; with the same Lyapunov risk function, the same falsifier, and the same training process, with the addition of state transition dynamics uncertainty.\n\n* In addition, the performance gain of the algorithm compared to Neural Lyapunov Control seems not obvious. For instance, in Fig A-15, the region of attraction of the proposed approach (ARNLC) looks similar to the Neural Lyapunov control (NLC) method. As shown in the Neural Lyapunov control paper, a better Lyapunov function usually leads to a large ROA - so is the learned Lyapunov better than the NLC without considering the disturbances in dynamics?\n\n* On the same vine, not enough visualization about the learned Lyapunov function is provided (like Fig 2 in the Neural Lyapunov Control paper). I am interested in how the perturbation influences the Lyapunov functions. With the perturbation, some states x which satisfy the Lyapunov condition can have negative V(x) or positive V(x)\u2019 when the perturbation is large, it might demonstrate some interesting properties. In particular, a comparison of the learned Lyapunov function shape for ARNLC in comparison to the learned Lyapunov function without adversarial training (NLC) would be interesting, at least for the pendulum example - and comment on the difference.\n\n* Adversarial training usually requires large samples and works for small perturbations. However, in the proposed method, the authors consider the disturbances in the system model. For example, in the Pendulum example, a change of the ball mass, pole length, friction coefficient, and gravity, etc. But then the model disturbances are converted to a state perturbation to be considered in the training, i.e., xk'=M(x_k, a_k)+a_k^v. How do you convert the model discrepancy to a value of a_k^v? What range for the disturbance $a_k^v$ is reasonable to be considered? Also, in many nonlinear control problems, a small change in the system parameters can lead to a significant difference in the state transition, which makes a_k^v sufficiently large. \n\n* Can the authors better describe the difference between PNLC and ARNLC? There seems no proper description of PNLC.\n\n* For figure 1, for example, in (e) and (f), in (e), the curve is increasing, and in (f) the curve is decreasing, it is hard to compare. And some plots do not share the same y limits, which makes the results hard to follow.\n\n* For the training process in the pseudo-code, xk'=M(x_k, a_k)+a_k^v, where M is the learned model, despite that this learned model will introduce error. In the training process, at least for the forward pass, why not use the real simulator to generate data (e.g., like what the authors did in \"Sablas: Learning Safe Control for Black-Box Dynamical Systems\"? Using the learned model may bring unnecessary errors.\n",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "This paper considers jointly learning a stable controller and neural network represented by the Lyapunov function, which is \"empirically\" robust with respect to the dynamics difference between the training and test environment. Experiments are conducted on Pendulum, Cart Pole, car tracking, and 2-link pendulum. Main concern lies in the significance of the proposed approach both intellectually and empirically. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2223/Reviewer_tGgb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2223/Reviewer_tGgb"
        ]
    },
    {
        "id": "HSC0wjK9cQl",
        "original": null,
        "number": 2,
        "cdate": 1666934332127,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666934332127,
        "tmdate": 1666934332127,
        "tddate": null,
        "forum": "lV0fWRDJwR",
        "replyto": "lV0fWRDJwR",
        "invitation": "ICLR.cc/2023/Conference/Paper2223/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies training a neural network based controller under potential\nadversaries that maliciously alter the states of the dynamic system. The\ntechnical approaches the paper uses consist of three parts: (1) train an\napproximate environment model for the system; (2) use typical reinforcement\nlearning algorithm like PPO to train an adversarial on the approximated system\nmodel to find the worst case perturbation; and (3) train a neural network\ncontroller under this adversary using Lyapunov loss. The paper claims to learn\ncontrol policy with theoretical guarantee of stability using this approach.\nExperimentally, the paper evaluates this approaches on a few classic control\nenvironments such as pendulum and 2-link pendulum.",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper studies an important problem of learning stable and robust neural\nnetwork controllers. The approach of combining a RL agent as the adversarial\nwith neural Lyapunov learning is interesting. The use of neural Lyapunov under\nthe context of adversarial robustness is also novel.\n\n2. Empirical results are comprehensive and the visualization of results look\nnice. The proposed approach achieves empirical stability in a few simulation\nruns and it converges better under the attack of the learned adversary.\n\nQuestions and weaknesses:\n\nOne major benefit of using Lyapunov theory in neural controllers is that it can\nformally guarantee stability. However, the paper does not convincingly show\nsuch a formal guarantee.  The paper emphasized a few times that the learned\npolicy enjoys a theoretical guarantee of stability (e.g., in the abstract\n\"learned control policy enjoys a theoretical guarantee of stability\"), however\nmost evaluations were done only empirically, and there is no real stability\nguarantee. The benefits of neural Lyapunov control is that it can provide\nstability guarantee, however training with an empirical Lyapunov loss function\nwith an RL learned non-optimal adversaray based on approximated system dynamics\nseem far away from giving formal guarantees. The algorithm presented in\nAlgorithm 1 has not convergence guarantees, and the paper is also unclear about\nthe details of the falsifier (which might be able to give some formal\nguarantees) used during training.\n\nThe biggest question is, given a fixed system dynamic function and a fixed\ncontroller function, can you *formally* show that the controller is stable\nunder *any bounded adversary* inside a certain region of convergence using Lyapunov theory?  The\nresults presented in this paper do not support this - results show a few\nempirical simulation of the systems only.\n\nIf the controllers do have guaranteed stability, then these critical results\nmust be presented:\n\n1. How large is the region of the attraction?  \n2. How large is the allowed perturbation for the adversary for the guarantee to hold?\nTechnically, the adversary needs to be the optimal adversary to give a meaningful\ntheoretical guarantee. If the guarantee is only valid for a trained adversary it is much\nweaker since we don't know how good the adversary is.\n3. What is the cost of computing such a guarantee (e.g., time to run the falsifier)?\n\nIf these guarantees cannot be obtained, then this paper needs a major revision to\nremove claims on stability guarantees, such as those in abstract. And it will\nbecome less exciting since the use of Lyapunov theory does not provide any\nformal guarantees. In that case, it is unclear if the use of Lyapunov theory is\nnecessary, and a strong baseline would be using normal RL training to replace\nthe Lyapunov loss to guarantee empirical stability. The benefits of using\nLyapunov is unclear here. In addition, since the proposed approach is evaluated\nunder the learned adversary, there is a chance that the controller overfits to this\nadversary so the evaluation is not fair - this must be addressed in experiments.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I am overall excited about the topic this paper is studying. The paper is generally clear and easy to follow and writing quality is good, although some key claims seem\nto be problematic. Novelty is good although there are issues in claims and approaches,\nas discussed in \"questions and weakness\" above.",
            "summary_of_the_review": "I feel this paper definitely studies an intriguing problem and I got excited\nwhen seeing its title and abstract. Unfortunately I feel some key claims in the\npaper are not well supported and the paper is not able to deliver its promise.\nSo I tend to reject the current version of this paper, but I am happy to\ndiscuss with the authors further for clarifications. I am glad to re-evaluate\nthe paper based on responses from the authors.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2223/Reviewer_g1he"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2223/Reviewer_g1he"
        ]
    },
    {
        "id": "oTFGINAP3H",
        "original": null,
        "number": 3,
        "cdate": 1667423065483,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667423065483,
        "tmdate": 1667423065483,
        "tddate": null,
        "forum": "lV0fWRDJwR",
        "replyto": "lV0fWRDJwR",
        "invitation": "ICLR.cc/2023/Conference/Paper2223/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose an adversarially robust neural Lyapunov control (ARNLC) method to improve the robustness and generalization capabilities for Lyapunov theory-based stability control. They claimed that the main contributions are:\n\n[1] propose a perturbed Lyapunov risk for learning the control policy under perturbations.\n\n[2] formulate an optimization problem for adversarially robust controller learning, to learn a policy in face of the worst-case perturbations that are imposed by the RL-trained adversary.\n\n[3] propose an adversarially robust neural Lyapunov control (ARNLC) approach to approximately solve this problem, and demonstrate its performance on several stability control tasks.",
            "strength_and_weaknesses": "Strength:\n\n[1] The LNC robustness stabilization problem studied here is interesting and important, from both theory and application perspective.\n[2] Overall speaking, the paper presentation is clear and I found it straightforward to follow.\n[3] The empirical evaluation results based on several case studies are helpful to demonstrate the advantages of the proposed method, in comparison with other available baseline methods in the literature.\n\nWeakness:\n[1] In the paper abstract, it is claimed by authors that, \"the learned control policy enjoys a theoretical guarantee of stability\". However, based on my reading of the paper, I didn't see any rigorous theorem & proofs about such \"theoretical guarantee of stability\", for the proposed ARNLC algorithm. What's presented in the paper is some \"empirical evaluation based on cases studies\" plus some non-rigorous comment/explanations of the proposed algorithms. So the \"theoretical guarantee of stability\" seems to be a over-claim here and it is inappropriate. Note that, the Theorem 1 about using stability guarantee with Lyapunov function was only for using the exact control policy and Lyapunov function without approximation, and for the proposed ARNLC method with multiple approximations (and could potentially violate Eq2 conditions many times), there is not any rigorous \"theoretical guarantee of stability\" any more. In my opinion, lacking of any rigorous theoretical performance analysis of the proposed ARNLC method (which has multiple approximations) is a main weakness of this paper, and the paper would be significantly improved if the authors are able to add/derive any rigorous stabilization performance analysis of the proposed ARNLC method. And at the very least, the authors should remove their claim about  \"the learned control policy enjoys a theoretical guarantee of stability\" for their proposed ARNLC method.\n\n[2] The system considered in this paper is general continuous-time nonlinear dynamic systems, without other assumptions/conditions (e.g., lipschitz condition on f) on the system dynamics. Is it really the case that the proposed ARNLC method could work for such \"general nonlinear systems\" with \"theoretical guarantees about the stability\", without any other assumptions/conditions on system dynamics? It is well known in nonlinear dynamic system control theory that, for nonlinear dynamic systems, it could even possibly has a nonlinear system finite escape time. For the proposed ARNLC method with multiple approximations,  I'm not convinced that the closed-loop system using the proposed ARNLC method could achieve  \"theoretical guarantee of asymptotical stability\" for such general nonlinear unknown dynamic systems (without other assumptions/conditions). Rigorous mathematical analysis about the closed loop system performance for the proposed ARNLC method would be required if the authors want to justify their claim about theoretical stability guarantee of their proposed method. Without that rigorous mathematical analysis, the results in this paper is still valuable/helpful, but its main contributions is from empirical application side, not theoretical side.\n\n[3] In page 4, it was mentioned that Equation (7) is an empirical unbiased estimator of Equation (6). But I think Equation (7) is an empirical unbiased estimator of Equation (5) rather than Equation (6)? Also, for the notation N in equation (7), it might be good to add some clarification for it, as well as what value of N was used for the approximation and also justify it.\n\n[4] In the paper, it was mentioned that \"if we can train a controller under the worst-case perturbation (which degrades the performance of its policy to the most) in a certain range, the controller then obtains a conservative policy that is robust to any perturbation within that same range\". In my opinion, such a statement is inaccurate and also not sufficiently convincing, and it might possibly cause misunderstanding about the proposed method's performance against generic perturbations within that same range. In particular, considering all the perturbations within that same range, the controller was trained/optimized against the so-called \"worst-case\" perturbation and thus the final obtained controller might possibly perform better against that specific perturbation (since it has be somewhat trained/optimized to adapt to that type of perturbations),  and there is not any rigorous mathematical analysis to support the statement that the obtained controller would perform well for all other perturbations (for which the obtained controlled were not trained/optimized against) within that same range. It might be helpful for the authors to explain/elaborate this in a more accurate/convincing way to avoid potential misunderstandings about the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper seems to be written clear/well and it contains valuable novel results. I'm not convinced by authors' claims regarding the \"theoretical guarantee\" for their proposed ARNLC method, but the proposed method and the empirical evaluations is valuable.",
            "summary_of_the_review": "This paper studied an interesting problem and proposed a novel ARNLC method for robust stabilization control problem of general nonlinear dynamic systems, and it contains some novel valuable results. The advantages of the proposed method are demonstrated via multiple empirical evaluation case studies. The main weakness is lacking of theoretical analysis of the proposed ARNLC method, and I'm not convinced by the author's claim about \"the learned control policy enjoys a theoretical guarantee of stability\".\n\nI think the contribution of this paper from theoretical perspective is weak, but the proposed method and empirical evaluations are still valuable. The paper contribution would be much more significant if the authors are able to add rigorous theoretical analysis to prove that \"the learned control policy indeed achieves theoretical guarantee of stability for the closed-loop system using their proposed ARNLC method\". Without adding that theoretical part, I think this paper's contribution is board-line, and slightly below the acceptance threshold.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2223/Reviewer_6NKw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2223/Reviewer_6NKw"
        ]
    },
    {
        "id": "1qPFjaCrEf",
        "original": null,
        "number": 4,
        "cdate": 1667621542535,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667621542535,
        "tmdate": 1667621622421,
        "tddate": null,
        "forum": "lV0fWRDJwR",
        "replyto": "lV0fWRDJwR",
        "invitation": "ICLR.cc/2023/Conference/Paper2223/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes an extension to Neural Lyapunov Control (NLC) to minimize model mismatch: Adversarially Robust Neural Lyapunov Control(ARNLC). This approach treats model mismatch as an additional control input to the system that an adversary may use to reduce the performance of the system. ARNLC then takes a multi-step training procedure. First the authors train on the perturbed Lyapunov Risk function: an extension to the NLC's Lyapunov risk with the new adversary as an input. Then, they train the adversary using PPO in an RL formulation with reward designed to go against the goal of the nominal controller. Finally the authors demonstrate the performance of their approach in synthetic control tasks namely the Pendulum, Cartpole, Car Trajectory Tracking and 2-link pendulum. ",
            "strength_and_weaknesses": "# Strengths \n1. Thorough description of experimental setup and methodology.\n2. Experiments detailed sufficiently to be reproducible even without code.\n\n# Weaknesses\n1. ***Lack of Stability Guarantees***:\nIn the abstract the authors claim: \"the learned control policy enjoys a theoretical guarantee of stability.\"\nAlthough this may be true for NLCs that achieve zero Lyapunov risk loss, ARNLC significantly changes the setting by the inclusion of adversarial disturbances where it is not clear how the guarantees go through. To make this claim the authors should include a theorem with the specific assumptions on the adversary or modeling error.\n4. ***Lack of Reference to robust control theory***:\nThe topic of robustness with Lyapunov theory has been extensively studied not just in controls but also in the area of controls applied to learning. At least some comparison should be done with standard robust control approaches for this to be accepted. Using black-box environments should not be an obstacle given that the authors are learning an approximation of the dynamics in $\\mathcal{M}_\\eta$. It may be that control methods achieve similar performance without having to learn an adversary or policy. Possible papers to cite include but are not limited to (I highlight these references because they discuss robustness with provable guarantees in the learning context because stability guarantees are mentioned in the abstract):\n    1. Robustness in the context of Lyapunov theory precisely when trying to use learning to overcome model mismatch: Taylor, A. J., Dorobantu, V. D., Krishnamoorthy, M., Le, H. M., Yue, Y., & Ames, A. D. (2019, December). A control lyapunov perspective on episodic learning via projection to state stability.\n   2. Robust  MPC with provable guarantees: Aswani, A., Gonzalez, H., Sastry, S. S., & Tomlin, C. (2013). Provably safe and robust learning-based model predictive control. Automatica, 49(5), 1216-1226.\n1. ***Unclear theoretical setting***:\nThe authors should clearly specify the assumptions on the dynamics function $f$ in particular they should clearly state the assumptions required for uniqueness and existence of solutions on which the Lyapunov stability theorem relies. \n1. ***On the generality of control-affine systems*** In the second paragraph the author states that : \" However, this approach only considers  the control-affine dynamical systems, not the more general nonlinear one\"\nA general nonlinear system can always be placed into control affine form by choice of an integral controller. Furthermore the test systems are mechanical systems which can be placed into control-affine form. It would be interesting to see a case where the generality of ARNLC is required.\n1. ***On Exponential Stability and Robustness***: It can be shown that exponentially stable systems converge to a small area around the equilibrium point rather than the equilibrium point itself under perturbation. This property is commonly referred to as Input-to-State Stability(ISS). Depending on the magnitude of the perturbation from the nominal dynamics, an exponentially stable system may converge close enough to the equilibrium to achieve the results shown. Some experiments showing that the resulting stability cannot be achieved by a sufficiently exponentially stable controller would strengthen the claims of the paper. I would consider citing the following: Sontag, Eduardo D. \"Input to state stability: Basic concepts and results.\" Nonlinear and optimal control theory. Springer, Berlin, Heidelberg, 2008. 163-220 . or Liberzon, D., Sontag, E. D., & Wang, Y. (2002). Universal construction of feedback laws achieving ISS and integral-ISS disturbance attenuation. Systems & Control Letters, 46(2), 111-127.\n1. ***On the generality of additive noise*** Suppose you have a nominal dynamics $f(x,\\hat{w})$ and a true dynamics $f(x,w)$. I can write the true dynamics as the nominal dynamics with additive noise as follows: $\\dot{x} = f(x,\\hat{w}) + \\epsilon$ where the additive noise $\\epsilon = f(x,w) - f(x,\\hat{w})$. The authors mention that some prior work is only restricted to additive disturbances as follows: \"However, it is usually restricted to the additive disturbances (L\u00f6fberg, 2003) \". The paper would be significantly strengthened by showing a case where an additive disturbance model is not sufficient either theoretically or empirically.\n1. ***Unclear performance measurement***:\nThe paper shows curves of controllers successfully stabilizing the desired input. Although this clearly shows that the controllers can stabilize the system, they may still be wildly inefficient at achieving this goal. You may be able to achieve very high robustness while using very high inputs. To evaluate the performance of the controller, the authors should consider showing the norm of the control input as the pendulum converges. Similar robustness results could be achieved by scaling the control input to increase the scale of the system relative to the noise. \n1. ***Unclear Statements about worst-case perturbation***:\n> The intuition behind is that if we can train a controller under the worst-case perturbation (which degrades the performance of its policy to the most) in a certain range, the controller then obtains a conservative policy that is robust to any perturbation within that same range.\n\nIn general, its not safe to assume that robustness to the worst case perturbation results in robustness to any perturbation. Even in a linear case, making the system robust to a single worst-case perturbation does not stop other perturbations from causing instability. There may be a version of this statement that is true but, again, the authors would have to state the assumptions clearly in a theorem and provide proof. \n\nFor the empirical evaluation I am not sure if the ARNLC results are evaluated on a new adversary or the one that was used to train the policy and a lyapunov function. Not retraining the adversary while keeping the weights of the policy constant would seem akin to reporting training accuracies since the policy has been able to adapt to that particular instance of the adversary and may not generalize. An ablation study of a newly trained adversary to which the policy cannot adapt and a uniform random adversary would show empirical evidence of the worst-case perturbation assumption stated previously.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well written and clear. As a minor note, the papers notation for dynamical systems is rather confusing. Instead of using $x_t$ for the state in continuous time, most works in both RL and control use $x(t)$ for states in continuous time and $x_t$ for states in discrete time. This becomes particularly important when the authors train the system using PPO since they use an Euler discretization of the continuous dynamics and thus have a series of discrete states and actions. ",
            "summary_of_the_review": "Overall this paper tackles the interesting and topical subject of model mismatch and sim-to-real in a fresh way. The use of a trained adversary to in an actor-critic fashion to fit a lyapunov function is certainly novel and has future potential. Unfortunately, this particular instantiation of the idea is lacking a few critical and minor points. Although the experimental section is written clearly, it does not state explicitly that the adversarial policy used for evaluation is new see point [8] above which weakens significantly the empirical claims of the paper. Similarly as stated in point[1] the theoretical guarantees promised do not have a theorem or similar statement in the paper. Finally, the paper does not mention the extensive literature in Robust control on achieving robust stability with Lyapunov-like conditions see [2,5]. Because of these problems I have to recommend the paper for rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2223/Reviewer_1FTL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2223/Reviewer_1FTL"
        ]
    }
]