[
    {
        "id": "td_Y3MsPQS",
        "original": null,
        "number": 1,
        "cdate": 1666351153476,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666351153476,
        "tmdate": 1666351153476,
        "tddate": null,
        "forum": "qmV_tOHp7B9",
        "replyto": "qmV_tOHp7B9",
        "invitation": "ICLR.cc/2023/Conference/Paper341/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a visual representation learning framework combining contrastive learning, masked autoencoders, and diffusion noise prediction. It is interesting to introduce noise prediction for the unmasked tokens. The models pre-trained by the framework achieve competitive results on the image classification task.",
            "strength_and_weaknesses": "Strength:  \nInteresting idea to add the diffusion noise prediction.  \nGood results for the linear probe.  \nClear written.  \n\nWeaknesses:  \n(1) The Title of the paper does not cover the meaning of \u201cnoise\u201d prediction accurately since it\u2019s a synthesis of contrastive learning, masked autoencoders, and noise prediction.   \n(2) Choosing SimCLR as the primary baseline in Table 2 and the following Figures is actually not convincing. SimCLR is an early contrastive learning-based method and its performance has lagged behind other contrastive learning methods. How about DINO and MOCO v3?  \n(3) In Figure 6, masking 50% of patches is not the best choice, why the paper still uses 50%? The authors should better provide more explanations and quantitative analysis.  \n(4) How about the ablation study between contrastive learning loss, reconstruction loss, and denoise loss? (That\u2019s the result of a pairwise combination)  \n(5) Lack of downstream experiments on Object detection and Semantic segmentation.  \nTypos:  \nPage 4: \u201csee Table 1\u201d --> \u201csee Figure 1\u201d  \n",
            "clarity,_quality,_novelty_and_reproducibility": "Well-written and part of the idea is interesting.  \n However, the Title and some of the details (e.g., limitations, training costs, etc.) need to be reorganized and discussed.",
            "summary_of_the_review": "I am inclined to accept the paper if the author can dispel my concerns well.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper341/Reviewer_y8Yc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper341/Reviewer_y8Yc"
        ]
    },
    {
        "id": "Pdi4j8qX3IF",
        "original": null,
        "number": 2,
        "cdate": 1666531487957,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666531487957,
        "tmdate": 1666531487957,
        "tddate": null,
        "forum": "qmV_tOHp7B9",
        "replyto": "qmV_tOHp7B9",
        "invitation": "ICLR.cc/2023/Conference/Paper341/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes CAN, a simple, efficient, and scalable method for self-supervised learning of visual representations, which combines contrastive learning, masked autoencoders, and noise prediction approach used in diffusion models. The learning mechanisms are complementary to one another. Extensive empirical studies on linear evaluation, finetuning, transfer learning, and robustness demonstrate that the proposed method achieves strong downstream performance.",
            "strength_and_weaknesses": "Strength:\n+ Combining contrastive learning, masked autoencoders, and noise prediction is novel.\n+ Extensive empirical studies on linear evaluation, fine-tuning, transfer learning, and robustness demonstrate the effectiveness of the proposed method.\n\nWeaknesses:\n+ JFT-300M is not accessible to everyone. Thus, ImageNet-1K or ImageNet-22K is more suitable for researchers to conduct fair comparisons. It seems the performance of CAN is not superior to concurrent work CAE or even MAE in terms of fine-tuning accuracy.\n+ Lack of full ablation studies for each component.\n+ For the denoising part, how is MLP designed? Why is the target noise? How could you ensure the network could learn high-frequency information as claimed?",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is novel with extensive empirical studies. There is no code for reproduction.",
            "summary_of_the_review": "See Strength And Weaknesses",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper341/Reviewer_JaaX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper341/Reviewer_JaaX"
        ]
    },
    {
        "id": "NsicvJXyyF",
        "original": null,
        "number": 3,
        "cdate": 1667556988441,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667556988441,
        "tmdate": 1671114018371,
        "tddate": null,
        "forum": "qmV_tOHp7B9",
        "replyto": "qmV_tOHp7B9",
        "invitation": "ICLR.cc/2023/Conference/Paper341/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a new approach called CAN that combines contrastive learning and masked autoencoders, and also adds denoising diffusion-inspired loss to learn visual representations. Experiments are performed over Imagenet where the model is compared and surpassing leading self-supervised approaches like MAE and SimCLR. The approach is also much more computationally efficient than SimCLR.",
            "strength_and_weaknesses": "**Strengths**: \n* **Combining complementary approaches**: the paper combines contrastive learning with masked auto-encoding to achieve complementary benefits of global and local signals. In high-level it\u2019s similar to BERT two losses at the sentence level and at the masked-word level. That\u2019s a great simple idea that that paper shows to work well. The usage of both noise prediction and auto-encoding also intuitively seem to play well together, where the former captures high-frequency features while the latter captures lower ones. \n\n* **Model Technical details**: I also like the specific technical manner in which the ideas are combined and find it an conceptually appealing approach (like a form of function composition): two masks are produced and then the two partial views are compared using contrastive learning. If the representations are good, then two partial views of the same image should be close to each other compared to other images.\n  \n* **Novelty**: The ideas explored in the paper are novel and simple. It combines existing general methods (contrastive learning, masked autoencoding) in an original manner making them complement each other. See more details below.\n\n* **Simplicity**: The way the two approaches are combined is simple and elegant, and so given the good empirical results, more likely to be broadly adopted by the community. In such cases, simple ideas are preferable in my opinion than more complex combinations as is the case with other works that explore manners to combine these approaches. \n\n* **Extensive set of suitable experiments**: multiple experiments are presented over both ImageNet and larger-scale JFT datasets, and ablations are also explored. The contribution of each loss is being explored and it looks like each of them contributes to a comparable degree. The subject of efficiency is also explored in its own section, although extending it further with additional experiments could be useful. Finally, the paper also explores the model\u2019s robustness under distribution shifts. Additional experiments in the appendix explores additional transfer learning experiments. \n\n* **Strong results and suitable baselines**:  strong results on the imagenet linear probing compared to leading approaches such as MAE and SimCLR. It also had a lot better computational efficiency than SimCLR.\n\n**Weaknesses**: \n\nI don\u2019t identify fundamental weaknesses. See some suggestions for writing and presentation improvement below. Smaller improvements:\n* **Limitations and future directions discussion**: Would be good if the authors could discuss any limitations they recognize of the work and potential directions for next steps! \n* **Qualitative results and visualizations**: could be good to show any sort of qualitative results and some image samples, or empirical visualizations of the model. \n* **More baselines**: Comparing to additional baselines will also be useful to corroborate the paper findings. \n",
            "clarity,_quality,_novelty_and_reproducibility": "**Quality**: The paper has concrete motivation. I would suggest changing the introduction though a bit and starting directly from the benefits of combining the self-supervised approaches rather than from the properties of the datasets (curated vs uncurated).  I think that could help getting to the key idea and setting the relevant context from the beginning.\n\n**Clarity**: The paper is very well-written, the idea is clearly presented, especially the modeling section in terms of technical descriptions, and it is accompanied with useful diagrams and visualizations. The experiments section is straightforward and easy to follow. I think in the presentation it will be good to make some of the expressions in paragraphs such as the first and second, and the first of the 5th page, centered in their own line rather than packed with the text, to make it easier to follow the modeling section.\n\n**Novelty**: Combining the approaches of contrastive learning and masked auto-encoding is simple but novel, and seems to reflect parallel discoveries in the NLP domain (with BERT). The diffusion-inspired noise-prediction loss is also novel. The related work section of the paper is also good and covers the pillars this works builds upon: masked autoencoding, contrastive learning and denoising diffusion models.\n\n**Reproducibility**: The approach is described to a sufficient degree, and the hyperparameters impact is analyzed in the paper, covering  masking rates, noise level and loss weights, and the selected hyperparameters are detailed in the appendix. \n",
            "summary_of_the_review": "That\u2019s a great paper with a novel and simple idea that combines proven modern approaches and shows strong results across multiple empirical dimensions (accuracy, efficiency, few-shot learning, robustness under distribution shifts) compared to leading methods on an important task of self-supervised visual learning. I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper341/Reviewer_sdad"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper341/Reviewer_sdad"
        ]
    },
    {
        "id": "jEAQl7xNaWw",
        "original": null,
        "number": 4,
        "cdate": 1667604063643,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667604063643,
        "tmdate": 1668521630481,
        "tddate": null,
        "forum": "qmV_tOHp7B9",
        "replyto": "qmV_tOHp7B9",
        "invitation": "ICLR.cc/2023/Conference/Paper341/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "CAN is a method of self-supervised learning for visual representations. The method builds on three families of self-supervised learning methods: Contrastive learning, Masked Autoencoding, and Noise prediction. The paper claims that the learning mechanisms of each family are complementary to one another, and includes some experiments with comparisons to selected methods in the masked autoencoding family (MAE) and the contrastive learning family (SimCLR). The paper claims empirical improvements over MAE and SimCLR on ImageNet, and when pre-training on JFT-300M and fine-tuning on ImageNet they achieve an 85.9% top-1 accuracy. \n",
            "strength_and_weaknesses": "Strengths\n- Combining the denoising diffusion objective with masked reconstruction in order to improve the prediction of high-frequency features without incurring significant additional computation cost is a good idea. This reviewer would be curious to see how much this approach benefits other SSL methods independently of the CAN method. \n- The experimental infrastructure is impressive -- the experiments required a lot of hardware time to run. \n\nWeaknesses\n- In my opinion, the primary weakness of this paper lies with its comparison to baselines. It\u2019s unclear why the authors' implementation of MAE appears to differ so significantly from the original method (e.g., different masking ratio,  different image augmentations, different learning rate). For example, the MAE paper explicitly mentions that their method does not use color jitter since it degrades performance, yet in this paper the authors apply color jitter during pretraining. \n- It seems odd to report only the linear probe accuracy in Figure 1, without the finetuning performance, when including that information seems like it would paint a different (and more complex) picture. Further, as mentioned in [1], linear probing performance does not necessarily correlate well with transfer learning performance (i.e., finetuning well on a downstream task of interest is usually more valuable than linear separability of the learned features). And in addition, Figure 5 in the MAE paper [2] shows a significant difference in the linear probe performance vs the finetuning performance when masking rate is changed. They observed that linear probe accuracy varied quite significantly, while the finetuning performance was more robust to choice of hyperparameters. This reviewer is open to the possibility that their understanding is incorrect, but in that case phrases in the reviewed paper such as \u201cIn experiments our default masking rate is m = 50% unless explicitly stated otherwise.\u201d are misleading. \n- The authors do not provide the source code (though they promise they will release it). This would have been useful to clear up any misunderstandings about their implementation of the MAE.\n- On page 8, the authors claim that \u201cCAN performs particularly well under JFT-300M pre-training: it improves over SimCLR and MAE baselines in all 7 cases, often by significant margins\u201d. However, their results in Figure 5 do not support this claim. MAE appears to have outperformed CAN in 3 of the 7 cases when pre-trained on JFT-300M and finetuned on IN-Real, IN-Sketch, and IN-V2. \n- Table 2 and 3 seem to be a bit misleading, or at least the work is lacking some citations. While the comparison to exemplar methods of these SSL families is important, acknowledgement of the other methods is also important, especially if comparing to the \u201cstate of the art\u201d on ImageNet linear probe. For example, [3] is an ICLR 2022 paper  that shows several methods with comparable FLOP and parameter counts to CAN, but that achieves better linear probe performance. More specifically, EsVIT with the Swin-B architecture achieved over 80% on the ImageNet linear probe task, and the EsViT with Swin-T has only 28 M parameters and achieved 78.1% linear top-1 acc on ImageNet, whereas the best reported method in Table 3 achieved 78.2%, and CAN achieved 76.2%.\n\n[1] Exploring Simple Siamese Representation Learning https://arxiv.org/pdf/2011.10566.pdf\n[2] Masked Autoencoders Are Scalable Vision Learners https://arxiv.org/pdf/2111.06377v2.pdf\n[3] EFFICIENT SELF-SUPERVISED VISION TRANSFORMERS FOR REPRESENTATION LEARNING https://arxiv.org/pdf/2106.09785.pdf\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing was clear, but some of the results do not seem reproducible (e.g. the MAE results). Also, I had a few questions:\n\n- What is the virtue of symmetry between the two backbones? It seems that breaking symmetry between the encoders can have benefits.\n- Why 50% masking ratio? MAE showed that higher ratios achieved better performance.\n- Are you comparing against an MAE with a 50% masking ratio in table 2?\n- Why is there no FLOP comparison with MAE when the abstract reports the FLOP improvement over SimCLR? I suspect this method is not more FLOP efficient, but the authors need to at least report that result and explain why the results are that way.",
            "summary_of_the_review": "The method seems promising and the topic is important and timely. However, the paper failed to make convincing comparisons (e.g., to EsViT, or to a strong MAE baseline with a fair masking ratio and FLOP comparison). In addition, the paper did not present experiments that illuminated the effects of combining masking, denoising, and contrastive learning methods in general, but only in the context of their method, CAN, and when comparing to their own MAE implementation. Further, the paper lacked several important details, and as a result this reviewer lost confidence in the efficacy of the proposed method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper341/Reviewer_Jkgq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper341/Reviewer_Jkgq"
        ]
    }
]