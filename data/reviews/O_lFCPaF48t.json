[
    {
        "id": "tzPgSPdfX1J",
        "original": null,
        "number": 1,
        "cdate": 1666628360866,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628360866,
        "tmdate": 1666628360866,
        "tddate": null,
        "forum": "O_lFCPaF48t",
        "replyto": "O_lFCPaF48t",
        "invitation": "ICLR.cc/2023/Conference/Paper4223/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an autoencoder architecture for structured representation learning. In particular, the proposed model incorporates latent variables at different layers of the convolutional decoder through Structural Transform (StrTfm) layers (inspired by FiLM (Perez et al., 2018) and Ada-In (Karras et al., 2019) layers). In essence, an StrTfm layer applies an affine transformation to the feature maps from the preceding convolutional layer. The latent variable associated with an StrTfm layer determines the magnitude and bias of the affine transformation corresponding to the layer. The latent variables are not regularized by a prior distribution and can be sampled independently for generation based on their training distribution. The proposed model is trained on several datasets and performs favorably when compared to various baselines and benchmarks (different versions of VAEs) based on several evaluation metrics. The evaluation metrics include reconstruction quality (measured by reconstruction loss), visual quality of generated samples (measured by FID), and disentanglement of latent features (measured by DCI, MIG, IRS, Mod/Exp). Additional qualitative analysis shows that the latent space of the structured decoder exhibits some hierarchy where latent codes incorporated at the bottom layers of the decoder (farthest from reconstruction) represent high-level information (such as shape) while latent codes at the top layers of the decoder represent more lower-level information (such as color). ",
            "strength_and_weaknesses": "**Strengths**\n\n- The proposed structured decoder which works with a hierarchy of latent variables affecting reconstruction at different scales through StrTfm layers and without regularizing the latent variables to have a pre-set prior distribution is novel.\n\n- The proposed structured decoder is compared to various baselines and shows favorable performance in terms of reconstruction quality, FID, disentanglement, and hierarchy of information encoded in the latent representations.\n\n- The authors provide implementation of their model in the supplementary materials (although I didn\u2019t have a chance to check it closely.)\n\n**Room for improvement**\n\nPlease find below clarification questions regarding some of the statements and methodology in the paper.\n\n- *Statistical significance*: Are confidence intervals available for the results in Figure 2 (comparing reconstruction quality for the different models)? \n\n- *Fairness of comparison*: Is it accurate to say that the SAE model has more parameters than the VAE models it is compared to due to the presence of MLP modules in StrTfm layers? If so, could the favorable performance of SAE models in terms of reconstruction quality and FID be partly due to higher capacity of the structured decoder?\n\n- *Initialization of the affine transformation*: Can the authors please elaborate on how the \u201caffine transform for each Str-Tfm layer is initialized to identity\u201d if the hidden layers (I assume the MLPs) are \u201cindependently initialized\u201d? I am probably missing some details but are not $\\alpha_i$ and $\\beta_i$ computed as the outputs of passing $U_i$ through MLP$_i$?\n\n- *Latent traversal*: For the visualizations in Figure 4, when traversing values for one latent dimension, what are the remaining latent dimensions set to? Can the authors elaborate on why there does not seem to be a lot of variation in the generated samples in the top two rows of Figure 4(a)?\n\n- *Information bottleneck acts as regularization*: The latent representations have much lower dimensionality than the inputs ($d\\ll  D$) by design which introduces an informational bottleneck that acts as regularization to avoid trivial solutions. How do authors expect the SAE\u2019s performance will be affected as $d$ increases in terms of the reported metrics?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally written well. There are sufficient implementation details for reproducibility purposes. The proposed model is related to others existing in the related literature but, to the best of my knowledge, the exact setup is novel. ",
            "summary_of_the_review": "This paper proposes a way to learn a hierarchy of latent representations for generation with an autoencoder. The latent variables are not regularized by a fixed prior distribution and can be sampled independently to generate new samples with relatively high visual fidelity when compared to other models. I have some clarifying questions regarding the methodology and assumptions in the paper and tentatively propose a weak accept. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4223/Reviewer_ECfH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4223/Reviewer_ECfH"
        ]
    },
    {
        "id": "qxbkDRcpjBU",
        "original": null,
        "number": 2,
        "cdate": 1666640774415,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640774415,
        "tmdate": 1666640774415,
        "tddate": null,
        "forum": "O_lFCPaF48t",
        "replyto": "O_lFCPaF48t",
        "invitation": "ICLR.cc/2023/Conference/Paper4223/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "An architecture of autoencoders for images that enable disentanglement of the latent variable is proposed. It is contrasted with disentanglement approaches based on regularization. The authors empirically investigate the properties of the proposed method and other disentanglement approaches. One of the conclusions is that the specific architecture of the proposed autoencoder allows the disentanglement of latent representation without any regularization.",
            "strength_and_weaknesses": "### Strengths\n\n- Although the method seems simple, it seems to disentangle the representation as nicely reported with intensive experiments.\n- Not only the proposed method but also other popular approaches for disentanglement are investigated in different aspects.\n\n### Weaknesses\n\n1. The method's applicability is limited to data for which CNNs are meaningful. This is not a serious weakness, but a brief discussion about the (im)possibility of application to non-image data types would make the paper more complete.\n\n2. Related to the above, the discussion starts by assuming that the data are images without mentioning so at all. This might be natural in the disentanglement community, but for a broader range of audience, limiting the data type first, even just for the sake of discussion, would be helpful.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and the empirical investigation attains a high quality. The technical novelty may not be noteworthy, but the empirical investigation would be valuable for the community. Although I did not try it by myself, the results seem to be reproducible using the attached codes.",
            "summary_of_the_review": "This is a solid paper with an intensive empirical study on the disentanglement happening in different types of autoencoders.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4223/Reviewer_HNCN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4223/Reviewer_HNCN"
        ]
    },
    {
        "id": "U4WkdpztQTN",
        "original": null,
        "number": 3,
        "cdate": 1666667497607,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667497607,
        "tmdate": 1669417028823,
        "tddate": null,
        "forum": "O_lFCPaF48t",
        "replyto": "O_lFCPaF48t",
        "invitation": "ICLR.cc/2023/Conference/Paper4223/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed SAE, which uses structural decoder that infuses latent information one variable at a time to induce an intuitive ordering of information, and provided a sampling method called hybrid sampling which replies only on independence between latent variables without imposing a prior latent distribution. \n\nThe paper compared the metrics with many other baselines, and show improved metrics.",
            "strength_and_weaknesses": "Strength\n1. The paper showed SAE has better metrics when compared with 6 baselines, and the authors even compared the model performance when hybrid sampling are used in the baseline models.\n\nWeaknesses\n1. The hybrid sampling method assumes the independence between the latent variables. The model structure uses one latent variable at a time, but given that all the latent variables are trained at the same time. Not sure how independent are these latent states.\n2. Another baseline missing from the paper is autoencoder trained without prior, and sampling based on the proposed hybrid sampling. Because hybrid sampling removed the need of prior distribution for the latent variables, the VAE model can also remove the prior.\n3. Base on the generated images Figure4 and Figure 6, the diversity of generated images seems poor.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite easy to read and understand, the hybrid sampling method seems quite interesting. The result should be easy to reproduce.",
            "summary_of_the_review": "The paper proposed a new model architecture and a new sampling method at the same time, it's difficult to see which gives the real contribution. And it's not clear about the diversity of the generated images. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4223/Reviewer_hCAA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4223/Reviewer_hCAA"
        ]
    },
    {
        "id": "pb_PX6RL4Pm",
        "original": null,
        "number": 4,
        "cdate": 1666716326222,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666716326222,
        "tmdate": 1669654351530,
        "tddate": null,
        "forum": "O_lFCPaF48t",
        "replyto": "O_lFCPaF48t",
        "invitation": "ICLR.cc/2023/Conference/Paper4223/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers an autoencoder architecture where the structure in the latent space is induced by architecture of the decoder, which uses latent variables sequentially to generate samples from the target distribution. Rather than imposing a prior distribution on the latent space authors use a \u201chybrid sampling\u201d approach to generate samples that involves picking components of latent vectors from the cross product of a set of randomly chosen examples from the dataset. Authors study the effectiveness of their approach using FID scores, disentanglement scores and a series of visual inspections on several datasets including (MPI3D, Celeb-A, 3D-Shapes). Authors find that their models perform favorably with respect to other baselines, especially on the disentanglement metrics.\n",
            "strength_and_weaknesses": "Strengths:\n* Carries out experiments on several datasets.\n* Paper suggests a simple and clearly explained approach\n\nWeaknesses:\n* Model is evaluated on rather simple test problems\n* Some natural baselines like VQ-VAE are not represented\n* Limitations/caveats arising from the choice of hybrid sampling are not clearly addressed/stated/investigated. (e.g. effect on FID score etc.)\n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is clearly written and references some of the key works in this area. Appendix provides sufficient details to reconstruct the training setup and all of the model parameters which should be sufficient to reproduce the results. Some figures (e.g. Fig 3 is missing a legend for the type of sampling used).\n",
            "summary_of_the_review": "While the paper presents a new and interesting approach to adding structure to a latent representation by modifying the decoder architecture and how the latent vectors are converted to samples from the data distributions, I\u2019m not convinced that current results offer evidence to support the claim that this is a principled step in a right direction for representation learning. I might have misunderstood parts of the paper in which case I\u2019d love to be convinced otherwise.\n\nMy main concerns are:\n(1) IIUC objective scores like log likelihood of the test dataset are not possible to calculate with hybrid type sampling, while scores like FID are easily fooled by memorization.\n(2) Other generative models have not been considered (e.g. VQ-VAE-2 [1], RG-Flow[2] etc.)\n\n\n\n\n[1] https://arxiv.org/pdf/1906.00446.pdf\n[2] https://iopscience.iop.org/article/10.1088/2632-2153/ac8393/pdf\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4223/Reviewer_X8MV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4223/Reviewer_X8MV"
        ]
    }
]