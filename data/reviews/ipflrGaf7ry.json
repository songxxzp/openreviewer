[
    {
        "id": "2-JecRLTVqO",
        "original": null,
        "number": 1,
        "cdate": 1666538490617,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666538490617,
        "tmdate": 1666539151128,
        "tddate": null,
        "forum": "ipflrGaf7ry",
        "replyto": "ipflrGaf7ry",
        "invitation": "ICLR.cc/2023/Conference/Paper6011/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes \u201crelay-evaluation\u201d a method to asses the generalization capabilities of RL agents when being forced to start from a state that is out of the distribution defined by the agent\u2019s policy. The paper also introduces \u201cSelf-Trajectory Augmentation\u201d (STA) a training method to improve generalization in which the agent is randomly set to start from a state that is sampled from one of the agent\u2019s old policies.",
            "strength_and_weaknesses": "### Strengths\nThis paper addresses an important problem in RL that I believe hasn\u2019t been explored sufficiently. Generalization in RL is normally regarded as the ability of an agent to perform well on similar tasks/environments.  However, this paper evaluates generalization to out-of-distribution trajectories, showing that, even when the environment stays the same agents may fail to perform well if states are sampled from a different policy. The paper is clearly written, for the most part, and the experiments are thorough.\n\n### Weaknesses\nThe relay-evaluation metric is computationally heavy it requires training multiple other agents to ensure that there is sufficient variety in the policies of the strangers. If the number of stranger policies is small, the method may incorrectly determine that an agent is able to generalize well just because the algorithms used for training the test and stranger agents are very similar.\n\nThe definition of \u201ccontrollable state\u201d is somewhat loose (i.e. what does \u201chigh return\u201d mean and how large does L need to be?).\n\nI would have liked to see experiments on other types of environments. The environments used are all of the same nature. Agents need to learn to walk/move and keep their balance. These require a lot of coordination and thus it is natural for the agents to overfit to a particular trajectory.\n\nCan the STA training scheme negatively impact the agent\u2019s performance? As explained in Section 3.1 STA is equivalent to training in an augmented MDP with a larger initial state set. Is it possible that the optimal policy for this augmented MDP is different from that of the original MDP? \n\nAs far as I can see, a simulator that can be reset to any state is needed for STA.\n\nI found Section 3.2 particularly unclear: \n\n  * The score used by STA to evaluate if a state is controllable depends on the policy being followed. Hence at the beginning of training when policies perform badly, the inclusion of states in the initial set seems arbitrary. That is, some states may not be included just because these bad policies can't perform well when starting from them.\n\n * Is the score of a state averaged over multiple runs? Or is it just a single sample estimate? The latter is a poor estimate of the true score if the policy and/or the environment are stochastic. Why don\u2019t you use the Q network to estimate the score?\n\n * I am not sure why the states need to be controllable to be added to the initial set. What happens if you just add all states to the starting set regardless of whether they are controllable? Wouldn\u2019t that also improve generalization?\n\n * Why is $Q(s, \\pi(s))$ a good metric to determine if the agent is familiar (not familiar) with a particular state? A high (low) Q value may be due to the agent being overconfident (underconfident) about that particular state.\n\n * The use of a parameter $\\lambda$ to truncate the return is a bit hacky. Can't you just divide the score by the number of remaining timesteps?\n\n * How is $\\eta_{STA}$ computed?",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThe paper is generally clear although it could benefit from proofreading. The second half of the paper where STA is introduced is hard to follow. In particular, Section 3.2 is quite convoluted. It is difficult to parse how the method works especially given that STA uses so many new hyperparameters.  Perhaps adding bullet points or pseudocode explaining step-by-step how states are added to the initial set may improve clarity.\n\nTypos/grammar mistakes:\n\nAbstract \u201cgeneralizatin failure\u201d -> \u201cgeneralization failure\u201d\n\n\u201cpretraining additional models are time-consuming\u201d -> \u201cpretraining additional models is time-consuming\u201d\n\n\u201cbased the experiment of Table 2\u201d -> \u201cbased ON the experiment IN table 2\u201d? \n\n\u201cwhich are very slow\u201d -> \u201cwhich are very low\u201d?\n\n\n### Novelty \nI found the paper novel, although I must admit I am not very familiar with the literature on generalization in RL.\n\nThe idea of \u201crelay-evaluation\u201d seems similar in nature to that of \u201cTandem RL\u201d (Ostrovski et al., 2021). Could the authors comment on how these two papers are related?\n\nOstrovski et al., 2021: The Difficulty of Passive Learning in Deep Reinforcement Learning.\n\n### Reproducibility\n\nSource code is not provided. However, the method should be relatively easy to implement.\n",
            "summary_of_the_review": "I am overall in favor of accepting this paper. I really liked the first part of the paper where the problem of generalization to out-of-distribution trajectories is evaluated using the relay method. I am a bit less excited about STA since the method makes lots of choices that seem rather arbitrary and very domain dependent. I am tentatively giving the paper a score of 6 but I am willing to increase it if the authors can clarify some of my concerns in their rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6011/Reviewer_7NUq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6011/Reviewer_7NUq"
        ]
    },
    {
        "id": "b3oMiwHDTrG",
        "original": null,
        "number": 2,
        "cdate": 1667137855806,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667137855806,
        "tmdate": 1667137855806,
        "tddate": null,
        "forum": "ipflrGaf7ry",
        "replyto": "ipflrGaf7ry",
        "invitation": "ICLR.cc/2023/Conference/Paper6011/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose \"relay-evaluation\", a method for evaluating the generalization performance of RL agents. Specifically, it forces the agent under test to start from the state s\\_0, which is sampled from the trajectory generated by stranger agents. A sharp degradation of performance is observed under relay-evaluation, which proves the limitations of existing methods. Correspondingly, the authors proposes STA, a novel method that can be applied in any modern RL algorithm, to improve the generalization ability.\n\nFurthermore, there is no code or any other supporting material for this paper, and the reproducibility of the paper is unconvincing.\n",
            "strength_and_weaknesses": "#### Strength\n\nOverall, generalization is a popular and essential topic in RL, and many methods have been explored to evaluate or improve the generalization of RL agents from different aspects. It is quite interesting that the authors studied the generalization from a brand-new point of view and proposed a concise but effective method. The experiments are compelling and well-rounded. I think the paper did contribute some new ideas and open up a new direction worth being studied.\n\n#### Weakness\n\nDespite of the outstanding performance illustrated in the experiments, a provable improvement might can not be guaranteed as many components applied in STA are more likely intuitive.   \n\nIn relay-evaluation, authors sampled states from trajectories generated by stranger agents, which are trained with only different random seeds. PBT[1] might be of help to find more \"controllable\" states where test agent fails and thus evaluating the generalization more accurately.\n\nNot really a weakness, more a suggestion. A pseudocode that describes the process of STA overall might help authors understand it more clearly. Would you consider adding it to the paper in the next version, please?\n\nThere is no code or any other supporting material for this paper, and the reproducibility of the paper is unconvincing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "#### Clarity\n\nThe paper is mainly well-written and is generally easy to follow. The phenomenon that existing methods fail in relay-evaluation is motivating, and the analysis is persuasive. \n\n#### Quality\n\nThe quality of the paper is high, and the experiments are compelling, as far as I can see.\n\n#### Novelty\n\nTo my knowledge, the paper is the first to propose a new method for evaluating the generalization of RL agents on \"controllable\" states and improve it via STA, a new training paradigm.\n\n#### Reproducibility\n\nA/N\n",
            "summary_of_the_review": "Overall, the paper is very interesting as it studies the generalization of RL agents from a novel view, despite some minor defects like the lack of provable improvement. The paper is well organized, and the analysis of the experiments is convincing. The proposed method seems concise but effective enough without introducing extra sample complexity.  \n\n[1] Derek, K., & Isola, P. (2021). Adaptable Agent Populations via a Generative Model of Policies. *Advances in Neural Information Processing Systems*, *34*, 3902-3913.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6011/Reviewer_9fm8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6011/Reviewer_9fm8"
        ]
    },
    {
        "id": "2QfM826Yt8",
        "original": null,
        "number": 3,
        "cdate": 1667182863920,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667182863920,
        "tmdate": 1668561839623,
        "tddate": null,
        "forum": "ipflrGaf7ry",
        "replyto": "ipflrGaf7ry",
        "invitation": "ICLR.cc/2023/Conference/Paper6011/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper discusses the challenge of \"relay generalization\", where a policy trained with RL is evaluated when starting from initial states observed under an independently trained policy.  Relay generalization is intended to measure the out-of-distribution generalization error, by testing how well the policy performs starting from the \"reasonable\" states found by another good policy.  They demonstrate in some of the MuJoCo environments that policies that perform well from from the environment's default initial state distribution can fail catastrophically when they are required to take over control from other, equally effective policies.\n\nThey determine that the issue stems at least in part from the fact that independently trained policies tend to settle into different regions of the state space, and so never learn how to take successfully control the agent from the states that other policies concentrate on.  The propose a simple method to mitigate this issue, in which the current policy is, with some tunable probability, rolled-out from a state encountered previously during training, rather than an initial state of the environment.  They demonstrate that this significantly improves relay generalization when applied to the soft actor-critic algorithm.",
            "strength_and_weaknesses": "The main contribution of the paper is the idea of \"relay generalization\", and empirical demonstration that this issue actually exists.  While it is not entirely surprising that on-policy methods would fail when asked to take control from a state they would not typically encounter during training, this work provides a means of directly measuring this type of out-of-distribution error.  They also provide some qualitative insight into exactly why these failures occur in the MuJoCo environments.\n\nThe main weakness of the paper is that the empirical results are limited to four MuJoCo environments.  The failure mode, at least in the humanoid environment, seems to be somewhat specific to this setting, and so it is unclear how large a problem this would be in general.  It is also unclear how much of an issue this would be with fully off-policy methods with large replay buffers, which might have a similar mitigating effect to the initial state sampling method they propose here.  Additionally, the solution they present (STA) is relatively simple, and it is unclear how much it can improve generalization performance overall, for example, from states that no policy would be likely to find during training.",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of the work comes mainly from the explication of relay generalization as a specific form of generalization error in RL.  The article appears to be technically correct, and is clearly for the most part.  A few suggestions for improvements, however:\n1. The related works section could be more organized, as it is currently just a long block of text.\n2. Table 1 (or an additional table) could show actual returns in addition to failure rates.  Table two provides this, but divided out between \"failure\" and \"success\" trajectories, so the actual impact of relay generalization of quantitative performance is not immediately clear.\n3. \"Failure\" in this case is determined by each environments termination criteria, these should be described for each environment, as it is possible the termination criteria in the MuJoCo environments are too strict, and terminate policies that might eventually succeed.\n4. Definition 2.1 is not entirely clear, as it does not seem to require that a trajectory be \"feasible\" given the action space available to the agent (seems to admit trajectories that arbitrarily jump between states).\n5. While we assume this is the case from the context, it is not explicitly stated that the \"stranger\" and \"test\" policies are trained using the same configuration, for the same number of steps.\n5. There are a number of typos, for example \"very slow\" rather than \"very low\" in the first paragraph of page 4.",
            "summary_of_the_review": "Overall this work empirically demonstrates an interesting phenomenon that is important for RL researchers to be aware of.  It provides a heuristic solution to this specific issue, but does not tackle the problem of out-of-distribution error more generally.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6011/Reviewer_kkAe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6011/Reviewer_kkAe"
        ]
    },
    {
        "id": "wwBQqdr7ZPx",
        "original": null,
        "number": 4,
        "cdate": 1667286989969,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667286989969,
        "tmdate": 1669010627118,
        "tddate": null,
        "forum": "ipflrGaf7ry",
        "replyto": "ipflrGaf7ry",
        "invitation": "ICLR.cc/2023/Conference/Paper6011/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a new experimental setting for RL called relay evaluation, which tests the ability of the RL agent to handle the trajectory of other agents (strangers) in the mid-way. Such a situation may be helpful in the application of RL when the agent needs to take control from humans and is seldom considered in the literature. It turns out that current RL algorithms can easily fail in the relay evaluation. Therefore, the authors propose a naive framework and a self-trajectory augmentation framework to improve the generalization ability of the RL agent, with any RL algorithm in theory. They use the trajectories, either trajectories from other stranger agents or historical trajectories to enhance the RL agent. The results on multiple datasets with different RL algorithms show the performance of the proposed frameworks.",
            "strength_and_weaknesses": "Strength:\n\n1. To the best of my knowledge, the authors are the first to consider the relay evaluation setting in the field of RL, which does have real-world usage such as in the self-driving system. I think this evaluation could be used as an important check before the application of RL to the real world.\n2. The results of the relay evaluation show that existing RL algorithms can easily fail to handle this situation. Figure 1 illustrates the results well and shows a possible explanation for the failure.\n3. The authors propose two frameworks to improve existing RL algorithms, both of which better utilize the trajectories, either trajectories from other stranger agents or historical trajectories. In theory, these frameworks can be combined with any RL algorithms and are easy to modify.\n\nWeakness:\n\n1. In my view, the motivation of this work is unquestionable but I have some concerns. First, the authors provide an example situation that \"a self-driving system may need to take over the control from humans in the middle of driving and must continue to drive the car safely.\"  Under such a situation, the algorithm used in the RL agent is known by the users, right? However, in the experiments, agents using different RL algorithms are considered the main part of the experiments (A-B pairs in Table1). I wonder does there exist a gap between the experiments and the claimed application scenarios.\n2. If we consider the adaptation of different trajectories, I think the works of meta-RL may be very relevant to this work. In my view, meta RL can handle different distributions of the environment, which could be seen as different trajectories as well. Therefore, could the authors compare their works with several popular meta-RL frameworks?\n3. After looking at the results of Table 2 and Table 3, I think the performance of STA is kind of unsatisfactory compared to the naive algorithm. So, I begin to question the performance of STA, maybe the simple way in the naive algorithm can already handle relay evaluation?",
            "clarity,_quality,_novelty_and_reproducibility": "In my view, the novelty of this work is unquestionable since the relay evaluation is hardly considered in the literature. But the quality of the work is not so satisfactory due to the weakness I find. As for reproducibility, I can not make a judgment since the authors do not plan to open-source their code.\n",
            "summary_of_the_review": "This paper aims to solve the ray evaluation issue which is hardly considered in the literature. However, except for this point, the other parts of this paper look not good enough to me. As I mentioned in the weakness part, the authors need to clarify their motivation and enhance their experiments. Therefore, I consider this paper is not yet ready for ICLR. Admittedly, I am not an expert in this area, if there are some fatal mistakes, please feel free to tell me.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6011/Reviewer_vBmJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6011/Reviewer_vBmJ"
        ]
    }
]