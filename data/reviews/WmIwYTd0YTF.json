[
    {
        "id": "RZNmV6Jvvs",
        "original": null,
        "number": 1,
        "cdate": 1666641896463,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641896463,
        "tmdate": 1666641896463,
        "tddate": null,
        "forum": "WmIwYTd0YTF",
        "replyto": "WmIwYTd0YTF",
        "invitation": "ICLR.cc/2023/Conference/Paper2088/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method to reduce the sample variance for\ntraining score-matching models. The method is based on using multiple\nreference samples and weighted importance sampling to compute more\nstable targets. Small-scale experiments show improvement when using\nthe proposed scheme.\n",
            "strength_and_weaknesses": "**Strengths**\n- I found the paper interesting to read, with the teorethical\n  part well  explained.\n- The paper demonstrates in simple examples the high target variance\n  of vanilla score-matching, and how it is reduced when using the\n  proposed approach.\n- The approach improves the sampling of the baseline methods, without\n  incurring in extra training time. In the case of CIFAR-10, it\n  significantly improves the training efficiency.\n\n**Weaknesses**\n\n- The empirical evidence in favor of the method is rather weak, with\n  experiments showing some improvements only for the small CIFAR-10\n  benchmark.\n- In Figures 6 and 7, FID is computed on only 1K samples, yet the\n  Inception features are typically of higher dimension, so the Gaussian\n  approximation for the FID would be ill-conditioned. I wonder if the\n  authors can comment on that (Curves show scores of ~40, while ~2 is\n  reported in Table 1.)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, has good writing quality and novelty. Source code was provided for reproducibility.",
            "summary_of_the_review": "The paper proposes an interesting idea for improving score-matching training. On the flip side the paper presents limited\nexperimental evidence in favor of the proposed approach.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2088/Reviewer_Qtdt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2088/Reviewer_Qtdt"
        ]
    },
    {
        "id": "_cGmo6hcmy1",
        "original": null,
        "number": 2,
        "cdate": 1666775537885,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666775537885,
        "tmdate": 1666775537885,
        "tddate": null,
        "forum": "WmIwYTd0YTF",
        "replyto": "WmIwYTd0YTF",
        "invitation": "ICLR.cc/2023/Conference/Paper2088/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Score-based generative models are learned via denoising score matching at many time steps during a diffusion process. This paper identifies an important problem with such loss function---estimating this loss via simple monte carlo could have high variance in the middle stage of the diffusion. The authors then propose to use self-normalized importance sampling to reduce the variance, which leads to a new objective function called stable target field. Empirical results show that this new objective consistently improves the performance of score-based generative models. ",
            "strength_and_weaknesses": "## Strengths\n\n* I find the paper very interesting, address a relevant problem in score-based model training, and potentially have high impact. Simply observing the variance problem in the middle stage of training is nontrivial and requires a deep understanding of the denoising score matching idea and the training of score-based models.\n\n* Moreover, the proposed fix is simple and very practical. Both the theory and empirical results suggest the modified loss has lower variance.\n\n## Weaknesses\n\nI do not see much weakness except a few minor points below:\n\n* It would be nice to give a derivation of eq. (7) in appendix.\n* It would be nice to see if there a connection between the modified denoising score matching loss and an importance-weighted version of the ELBO. See, e.g., \n\nBornschein, J., & Bengio, Y. (2014). Reweighted wake-sleep. arXiv preprint arXiv:1406.2751.\nBurda, Y., Grosse, R., & Salakhutdinov, R. (2015). Importance weighted autoencoders. arXiv preprint arXiv:1509.00519.\n\n* since this paper focus on reducing variance of score matching loss, the following paper that proposes a control variate for DSM could be relevant\n\nWang, Z., Cheng, S., Yueru, L., Zhu, J., & Zhang, B. (2020, June). A wasserstein minimum velocity approach to learning unnormalized models. In International Conference on Artificial Intelligence and Statistics (pp. 3728-3738). PMLR.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is of high quality with an original observation, theoretically sound fix, and sufficient theoretical and empirical evidence to support the claim. \n\nThe technical part of this paper is very well-written. It clearly states the variance problem, and introduces all necessary backgrounds to understand the fix. Nevertheless, I feel the introduction part can be improved. It frequently mentions terminology like stable targets, direction of inverse path, reference batch, etc. It is hard to get the idea by just looking at the first two pages. I suggest to revise them and point out the variance problem more directly. \n\nMinor:\n\n* At the beginning of section 4: \"The vanilla denoising score-matching approach (Equation 3) can viewed as\", missing a verb\n",
            "summary_of_the_review": "I find this paper identify an important problem, propose a nice solution, and is sufficiently practical to have large impact. Therefore, I recommend acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2088/Reviewer_B7Es"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2088/Reviewer_B7Es"
        ]
    },
    {
        "id": "T0SK1qMjw4H",
        "original": null,
        "number": 3,
        "cdate": 1667084024899,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667084024899,
        "tmdate": 1669107078268,
        "tddate": null,
        "forum": "WmIwYTd0YTF",
        "replyto": "WmIwYTd0YTF",
        "invitation": "ICLR.cc/2023/Conference/Paper2088/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper analyzes the denoising score matching objective used to train diffusion models, which have recently become the dominating class of deep generative models. The paper argues that at intermediate perturbation levels the denoising score matching objective is particularly noisy and \"unstable\", because one perturbed data sample could correspond to many different original clean data samples (the paper calls this phase 2; at the same time, we aren't yet in the regime where every data point is almost entirely destroyed and the ground truth score is effectively almost the same standard normal score for all data points -- phase 3 in the language of the paper). Standard denoising score matching uses only a single data sample among the plausible ones to estimate the score in each iteration. This effectively leads to high variance training (although unbiased). The paper therefore proposes to include a reference batch of additional samples to calculate a more stable score estimate. This corresponds to a form of importance sampling, introducing an unknown normalization factor. This problem is circumvented with self-normalized importance sampling, which, however, introduces bias. Ultimately, the paper proposes \"Stable Target Field\" (STF), a biased but lower variance score estimator for training score-based generative models.\n\nExperimentally, the paper shows superior performance of \"Variance-Exploding\" SDE models when trained with STF compared to standard denoising score matching training. Also, training tends to converge quicker. These advantages primarily manifest when using ODE samplers to generate data.",
            "strength_and_weaknesses": "**Strengths:**\n\n- The idea to use an additional reference batch to create a more stable and lower variance target score estimate for denoising score matching is novel. It is an interesting, well-motivated and smart idea.\n\n- I like how the paper derives the STF approach and constructs its main method. The analysis of the original denoising score matching objective, followed by the introduction of the reference batch leveraging an importance sampling framework, and then resorting to self-normalized importance sampling to avoid the intractable normalization factor, is elegant.\n\n- The work includes explicit technical analyses on the bias and variance of the proposed STF objective.\n\n- The variance reduction analyses (Figure 3) in the different phases are quite interesting and insightful.\n\n**Weaknesses:**\n\n- It is nice that the paper provides some analytical analyses on the bias and variance of the STF approach. However, how tight is the bound on the STF variance in Theorem 2? I agree that when $p_{0|t}(x|x(t))\\approx p_0(x)$, then the bound is meaningful. But this only really happens in \"phase 3\" (essentially, phase 3 is defined by $p_{0|t}(x|x(t))\\approx p_0(x)$). But the variance reduction also works well in the more relevant \"phase 2\", as empirically shown. But in this relevant regime, where $D_f$ is not yet 0, the bound is very hard to interpret and provides little insight.\n\n- The paper states that Variance Exploding SDEs (VESDE) are the state-of-the-art SDEs used for training score-based generative diffusion models. It is correct that Song et al. [1] achieved strong results with the VESDE when combined with predictor-corrector SDE sampling. However, almost the entire literature on diffusion models uses Variance Preserving SDEs (VPSDE). Moreover, it has recently been shown how to subsume all different common diffusion model SDEs under one umbrella in Karras et al. [2]. This paper then shows how a VESDE-based model can be improved significantly simply by using appropriate model and network parametrizations and preconditionings (their \"EDM\" scheme). Consequently, I would consider this EDM the state-of-the-art approach. Moreover, the main question is, does the STF approach also help when applied to more modern and widely used diffusion models, this is, (a) VPSDE and (b) Karras et al.'s EDM approach? The paper focuses on the vanilla VESDE with ODE-based sampling, even though it is well-known that the vanilla VESDE does not play well with ODE solvers and isn't widely used like that in the literature. In fact, already when sampling stochastically with predictor-corrector sampling, the benefit by STF is very marginal and almost not significant (Table 1, PC sampler (SDE)). Also note that the Karras et al. paper is not cited or discussed at all.\n\n- Note that Karras et al. [2] also discuss different perturbation regimes (see paragraph \"Loss weighting and sampling\" in their Section 5), similar to the different phases discussed in this work. The fact that there are effectively different phases during SGM training is known. This should be acknowledged.\n\n- In Section 3, the paper points to the multimodality of the distribution $p_{0|t}(x|x(t))$. Note that this is extensively discussed also in Xiao et al. [3]. It would make sense to acknowledge that work.\n\n- Regarding \"The STF objective stabilizes the model performance at convergence.\": I think this is overclaiming. The model only shows a significantly lower variance on the evaluation metric for the setup VESDE + RK45 (already when using the SDE framework with a PC sampler, the variance isn't much lower; Table 1). To verify this claim more broadly, again, experiments on other diffusion models should be run (VPSDE, EDM).\n\n- The paper mentions the computational overhead induced by the reference batch. How small or big is this overhead actually? My understanding is that it is small, because the samples from the reference batch are only used to estimate the target score, but we do not have to call the expensive neural network on them. However, what about memory? Some more details would be helpful here. Note that this comment did not influence my rating significantly.\n\n[1] Song et al., Score-Based Generative Modeling through Stochastic Differential Equations, 2021.\n\n[2] Karras et al., Elucidating the Design Space of Diffusion-Based Generative Models, 2022.\n\n[3] Xiao et al., Tackling the Generative Learning Trilemma with Denoising Diffusion GANs, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** I think the paper is clearly written and was easy to follow for me, despite some fairly technical content.\n\n**Quality:** Overall, the quality of the paper is okay. The paper is well written and motivated with interesting ideas and insights. Most experiments are appropriate, although insufficient. There are some concerns regarding the thoroughness of the experiments. Also, there is very relevant work that is not properly discussed and taken into account (see above).\n\n**Novelty:** Technically, the method clearly is novel, to the best of my knowledge. It is creative and original work. I do have some concerns regarding the relevance and significance, though (see above; benefits only shown clearly for VESDE + ODE sampling).\n\n**Reproducibility:** I think the approach is reproducible. The method is described clearly, experiment details seem to be provided in the appendix, and the submission also includes code.",
            "summary_of_the_review": "In summary, I think the main idea and the proposed method of the paper, the STF for training diffusion models, is interesting, well-motivated, and also novel. I do not have any concerns with respect to the technical and methodological novelty.\n\nHowever, my concern is that the method has been primarily validated on experiments using a standard VESDE scheme, together with ODE solvers for sampling. This setup is known to be challenging and in practice, almost all diffusion models actually rely on a different setup, either the VPSDE or the recent EDM scheme. It isn't clear whether there is any benefit when applied to these methods. If there isn't, then the new method is not particularly important or significant. Consequently, I cannot recommend the paper for publication at this point. I would recommend the authors to run experiments with different SDEs and setups and demonstrate the benefit of STF more broadly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2088/Reviewer_M5mL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2088/Reviewer_M5mL"
        ]
    }
]