[
    {
        "id": "IUqg9oeU3ub",
        "original": null,
        "number": 1,
        "cdate": 1665852556800,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665852556800,
        "tmdate": 1665852556800,
        "tddate": null,
        "forum": "H71l8_zALJ",
        "replyto": "H71l8_zALJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5038/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposed a batch sampling method aimed at generating harder negative samples for contrastive learning methods that use other samples in a batch as negatives, for instance SimCLR.\n\nBrief description of the method: the approach builds a graph whose nodes are training datapoints, and each node is connected to the $k$ nearest neighbors (distances measured using stale embeddings) of a uniformly subsampled graph (the subsampling stochastically ensures there is some connections between less similar samples). A batch of size $B$ is generated by selecting a random starting node, and running a random walk until $B$ distinct nodes have been visited. In order to ensure the sampling process doesn't drift too far from the starting point, each random walk step has a fixed probability of returning to the starting node. \n\n ",
            "strength_and_weaknesses": "**Strengths**\n\n- A good, broad set of experiments show that the approach, called ProSampler, lead to better downstream performance on many tasks (images, text, graphs). They also show that ProSampler can be successfully combined with existing in-batch hard negative sampling methods such as that of Robinson et al. \n\n- Methodological studies show that ProSampler behaves as expected in a number of ways: for instance it samples negatives that are harder (higher cosine similarity) than uniform sampling, but less similar than the $k$ nearest neighbors (hardest negatives possible). \n\n- My personal main strength is the proposal to use graphs based on input similarities for batch sampling. Many other alternative such methods could be tried too.\n\n**Weaknesses**\n\n- the two additional hyperparameters will probably be a bother to users. At least they appear to be reasonably robust, so decent default choices can be used. \n\n- the paper probably could have been made more enlightening by focusing more on proposing a more general underlying principle (graphs build from input similarities) and talking more about how this graph should be built. At the moment, the proposed method is simply given, and it is very believable. But more work is left to the reader than is necessary terms of  \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty lies in the problem framing: there are many hard negative sampling methods for contrastive leanring, but many either 1) work by mixing, reweighing etc. samples from a uniformly sampled mini-batch, or 2) add some additional mechanism such as a memory bank. The proposed method is well distinguished from these approaches, by keeping the SimCLR-style in-batch negative sampling, but more intelligently sampling the batch itself to ensure harder samples.\n\nApproach comes with adequate implementation details. I **strongly encourage** the authors to release their code, which they do not currently state that they plan to do. \n\nClarity is also good, but can always be improved. Take some care to make sure all details are explained sequentially and clearly. For instance one silly thing that tripped me up for a bit was the line \"for each instance $v_i$ by randomly picking $M(M < N)$ neighbor candidates.\" I wasn't clear how these $M$ samples were being picked - was it uniformly? Or using some other sampling algorithm? After searching and finding no further description of how this sampling is done I concluded it was uniformly. But you can save the reader this time by being precise and stating that it is uniform.",
            "summary_of_the_review": "Congratulations on a nice paper. The approach is:\n\n- well motivated: global methods for generating hard negatives are likely to work better that in-batch methods.\n- sensibly executed: the proposed algorithm, based on constructing a similarity graph and running a random walk, is a natural approach building on core graph algorithms. There will be many many other possible implementations people could test too. For instance, simply define the weight $w_{ij}$ of edge $(i,j)$ to be the similarity score $e_i \\cdot e_j$ between the embeddings of points $i$ and $j$. Then run a random walk where at. node i, the next node is sampled from the distribution Softmax($w_{ij}$) (softmax over all $j$'s). Probably keeping the chance of returning to the starting node at each step. My point here is that the proposed ProSampler is just a single example of many similar algorithms based on random walks on similarity graphs, many of whom will probably work well too. This is good in my books since it means that the paper hasn't just proposed a single fiddly algorithm that they managed to get to work, but that there is a core underlying idea, which is robust to many different possible implementations. \n- thoroughly evaluated: downstream performance improves. on lots of tasks across three modalities. Visualizations (e.g., Fig 12) and comparisons of negative similarities (e.g., Fig 5) give good sanity checks that the method is working as hoped. \n\n\nIn all this is a solid piece of work and I can't see any major flaw that could justify rejection.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5038/Reviewer_uJHq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5038/Reviewer_uJHq"
        ]
    },
    {
        "id": "LHGTUc8ePl",
        "original": null,
        "number": 2,
        "cdate": 1666713902068,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666713902068,
        "tmdate": 1666728950995,
        "tddate": null,
        "forum": "H71l8_zALJ",
        "replyto": "H71l8_zALJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5038/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a new sampling strategy for the construction of batches containing hard-negatives (avoiding false negatives in the batch) for contrastive learning methods. According to previous studies, it is legitimate to assume that hard negative pairs contribute the most in training the network. The method is based on a proximity graph and the sampling is performed by short random walks (-with restart). Results show improvements over a set of different tasks and datasets.",
            "strength_and_weaknesses": "**Strength**\n- creating more informative mini-batches.\n- selecting hard negatives examples \"globally\".\n- time complexity and time cost of mini-batch sampling and proximity graph construction (*)\n\n\n**Weaknesses**\n\n- what is the space complexity? can a graph representing imagenet-20K be kept in memory?\n\n- \"Sampling on the proximity graph can better exploit the hard negatives \"globally\" by bridging in similar instances from the entire dataset.\"\n\n1. **Proximity Graph Construction**: it is not clear if the construction of the graph is based on all the examples of the dataset or if there is a random selection of the examples.\n \n2. Looks like \"that the candidate set for each instance is formed by randomly picking a number of neighbor candidates\"\n  - if the candidates are drawn at random how is it possible to say that the method can \"exploit the hard negatives globally\"?\n  - if the neighbors are given, it means that there's an additional cost about knowing which neighbors of a certain node are? how are neighbors found?\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, well written and easy to read. It introduces a novelty in terms of creating more informative and effective mini-bacthes for training contrastive learning methods. ",
            "summary_of_the_review": "The authors propose a new solution to the problem of selecting hard-negatives from a dataset without including high percentages of false negatives (as it happens in the KNN-sampler). The solution seems to be not expensive in computational terms, some details are missing that could clarify any doubts about the cost required for the construction of the sampler (graph-based) and memory required.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5038/Reviewer_nXV4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5038/Reviewer_nXV4"
        ]
    },
    {
        "id": "LxT0PjeyzN",
        "original": null,
        "number": 3,
        "cdate": 1666767514095,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666767514095,
        "tmdate": 1670201294887,
        "tddate": null,
        "forum": "H71l8_zALJ",
        "replyto": "H71l8_zALJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5038/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on how to sample hard negative pairs in every batch, which is an important topic in contrastive learning. The paper motivates by the defects of uniform and KNN strategies, and then the authors propose to construct a proximity graph and employ the random walk to generate each batch. According to the construction of the proximity graph (selecting samples randomly and then choosing the k nearest ones), the sampling method can be regarded as a combination of uniform sampling and KNN.",
            "strength_and_weaknesses": "### Strength \n- The paper is well-organized and easy to follow. The motivation is clarified and the sampling of hard negative pairs is a promising topic. \n- The experiments are sufficient for me. \n\n### Weakness\nMy main concern is the limitation of the novelty of the sampling strategy. The proximity graph, the core of the proposed sampling, is constructed through two steps: \n- Randomly (Uniformly) select multiple points,\n- Choose the k nearest ones.\nIt seems like the method to introduce randomness to KNN. In other words, it seems like a combination of the two existing strategies. The subsequent random walk step is another step to introduce further stochasticity.\n\nAlthough some theoretical analyses are provided, they are not attractive to me. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is quite well-written and well-organized. The motivation is clear and convincing. \n\n- The novelty, however, is limited for me. \n\n- The source code is provided in supplementary. ",
            "summary_of_the_review": "The paper is well-written and I admit that the topic is meaningful. However, I concern whether the novelty of the sampling is significant enough for ICLR. I would like to update my score during the discussion period. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5038/Reviewer_4epo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5038/Reviewer_4epo"
        ]
    },
    {
        "id": "tcHYl5OUSp7",
        "original": null,
        "number": 4,
        "cdate": 1666882626387,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666882626387,
        "tmdate": 1666883146852,
        "tddate": null,
        "forum": "H71l8_zALJ",
        "replyto": "H71l8_zALJ",
        "invitation": "ICLR.cc/2023/Conference/Paper5038/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a negative sampling strategy for contrastive learning, which many contrastive learning-related frameworks can incorporate to improve performance. Instead of the uniform sampler and kNN sampler, the proposed proximity graph could well capture the similarity relationships among instances. Random walks among the proximity graph can \ufb02exibly explore the negatives. Experiments on several datasets demonstrate the superiority.",
            "strength_and_weaknesses": "Strengths:\n\n(1) The proposed method is simple and effective.\n\n(2) The authors provide a detailed theoretical analysis.\n\n(3) The authors conduct extensive experiments for validation, including several large datasets and different modalities.\n\nWeaknesses:\n\n(1) The proposed method is very simple, which is similar to searching hard negative samples from a top knn graph constructed by different similarity computation strategies. The overall contribution is incremental from this point. I wonder why the authors pick M neighbors by distance and select K nearest ones by inner product operation. And why do not you directly select the K nearest neighbors?\n\n(2) To a certain extent, the construction of a mini-batch with hard negative samples is similar to filtering out the positive samples from the original mini-batch. There are already several methods [1,2] that adopt affinity graphs to select positive pairs from the original mini-batch and achieve much better results on contrastive feature learning and clustering tasks. In this case, the novelty of this paper is unsatisfying. \n\n(3) The main motivation of this paper lies in the influence of batch size on existing contrastive learning methods. According to MoCo v2 and MoCo V2+, the influence of batch size is marginal. I wonder whether the proposed sampling strategy still works for such a situation and please give more explanations.\n\n(4) According to Table 1, the authors only present results of 100 and 400 epochs. I wonder whether the final results can be improved by the ProSampler, such as the results after 800 or 1,000 epochs. Besides, compared with SimCLR, SwAV, and BYOL, the improvement on ImageNet by [1] is much more significant than the proposed method. Please compare the results with [1] in detail.\n\n(5) According to Table 9, the improvement is marginal on these small datasets.\n\n\n[1] Weakly Supervised Contrastive Learning, ICCV 2021\n[2] Graph Contrastive Clustering, ICCV 2021\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to my detailed comments above.",
            "summary_of_the_review": "The proposed method is quite simple and is highly related to existing graph-based positive sample selection methods. The overall novelty and contribution are incremental. Besides, the experimental improvement is also marginal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5038/Reviewer_FNA5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5038/Reviewer_FNA5"
        ]
    }
]