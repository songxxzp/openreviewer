[
    {
        "id": "94t93Gs9e3m",
        "original": null,
        "number": 1,
        "cdate": 1666045226051,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666045226051,
        "tmdate": 1666045226051,
        "tddate": null,
        "forum": "gVSJ83n47IT",
        "replyto": "gVSJ83n47IT",
        "invitation": "ICLR.cc/2023/Conference/Paper1234/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a technique to preserve physical conservation laws in neural network based dynamical systems. The authors propose to solve two tasks concurrently: (i) learning variables that remain constant for a given trajectory, and (ii) a dynamics function that respects the constant variables and that fits input trajectories. The first objective is achieved by adopting well-known contrastive learning objectives so that the new objective \"pulls the latent factors\" (constant) within a trajectory whereas it \"pushes them away\" if they belong to different sequences. The second goal is realized by a gradient-matching loss. The model is experimentally demonstrated to preserve constant terms oftentimes and hence leads to non-divergent trajectories.",
            "strength_and_weaknesses": "Strengths:\n- The model surely learns _some_ constants that remain the same throughout integration.\n- The framework is straightforward: Similar contrastive objectives are commonly used and well understood. The dynamics function is also easy to understand as it is built to preserve the constant variables.\n\nWeaknesses:\n- The paper suffers from clarity issues as well as grammatical mistakes. I believe it should undergo significant updates. See my comments at the end of this question.\n- Methodology should be motivated. The authors argue that \"this is the first work that studies discovery of conservation laws for general dynamical systems through contrastive learning.\" What if we had a simpler method in which certain global parameters remain constant throughout integration? Would the proposed approach still be preferable? If so, why? Furthermore, among all other techniques, why is contrastive learning chosen? It does come with a complicated and costly loss term; so what is the catch here instead of a simpler approach?\n - The gradient matching term (eq. 7) should be carefully analyzed. It is well known that this objective becomes problematic with increasing noise levels and bigger time differences as the differential function is directly fitted to the empirical gradients $\\frac{x_{i+1}-x_{i}}{t_{i+1}-t_{i}}$. This is in contrast to learning a generative process, such as GP or neural ODEs, and is the very motivation of these approaches. \n- The paper lacks a whole discussion on causality. In my understanding, the method discovers certain factors on which the trajectory (forward simulation) is conditioned. This is the very definition of causal learning while the paper does not touch upon this connection. Similarly, the connections between invariance and causality could be included (in addition to the short discussion on invariance that is already included).\n- The method should be compared against SOTA baselines. I do acknowledge that the literature on \"black-box conservation learning\" is quite limited but at least Lagrangian NNs or some adaptation of vanilla neural ODE systems could be considered here. This would help us judge the level of difficulty of the problem. Similarly, we should see how the performance would change if we consider higher dimensional latent spaces ($m>2$) in Sec 4.2. \n\n\nWriting issues:\n - \"deploying the knowledge towards the new occurrence\" is unclear.\n - \"conservation law were\" is wrong\n - The connection between the sentence starting as \"On the other hand, data-driven\" and the prior sentences is missing.\n - \"one or more distinguishing features of physics-based system trajectories\" This sounds mysterious, i.e., what are the examples of such features?\n - \"By comparing the latent space distance of the system state observation and assigning them into their trajectory\" Both parts of the sentence is unclear. What is latent space distance? How does \"assignment\" take place?\n - What does \"ConCerNet\" stand for?\n - \"of the simulation in the longer term\" Longer compared to what?\n - Naming the function approximations already on the first page would help follow the text.\n - \"projecting the dynamical neural network output towards the\" Perhaps \"on the\"?\n - \"conservation manifold from the first one\" is unclear. Perhaps something like \"the manifold learned by the first module\"? \n - \"guaranteeing\" instead of \"guarantying\"\n - \"Unlike discriminative models that explicitly learn the data mappings\" What mapping is referred to here?\n - Sec2.1 could be organized chronologically (by moving the paragraph starting with \"Despite the recent surge of\" to the beginning)\n - Neural ODE paper (Chen, Ricky TQ, et al. \"Neural ordinary differential equations.\" Advances in neural information processing systems 31 (2018).) should be cited in Sec 2.2.\n - \"however, the soft Lagrangian treatment does not guarantee the model performance during testing\" This should be elaborated.\n - \"to propose a contrastive learning framework in a more generic form that is compatible with arbitrary conservation.\" What is meant by \"arbitrary conservation\"?\n - \"total time step T\" is not clear.\n - \"We let the trajectory set ... be the simulation history starting\". I'm not sure what this means: Do $C_k$'s refer to sequence ids?\n - \"the initial conditions have various conservation values.\" What does this exactly mean?\n - \"where the probability of point ... **belonging** to the trajectory\"\n - Similar to a previously raised concern: What is the motivation for eq.2? What does it imply?\n - \\log and \\exp should be used instead of log and exp.\n - What does $x\\sim C \\sim D$ mean?\n - A short discussion on NCA would be nice as it is referred to multiple times.\n - \"Softmax like function, ensuring the nice property of probabilistic distribution\" What does this mean? What is nice here?\n - \"learning of the m conservation terms\" Perhaps an m-dim vector is referred to?\n - \"by eliminating its parallel component\" Parallel to what?\n - Eq,1 uses $f_\\theta(x)$ whereas it later becomes $\\tilde{f}_{\\theta_d}(x)$. What is the difference?\n - Eq. 5-6 are unclear. A (verbal), more intuitive explanation should be given. Projection(.) function should be defined. \n - \"the orthonormalized set of vectors from\" is unclear.\n - \"conservation term in learned\" \n - x and y axes names and figure titles should be added/revisited.\n - What are the system states in Sec 4.1?\n - Why to use $x[1]$ instead of $x_1$?\n - \"the learned conservation here is approximately the exact conservation\" is unclear.\n - \"coordinate mean square loss to the ground truth\" is unclear.\n - Are the equations in the first paragraph of 4.2 really difficult to learn, given the recent success of neural nets on many tasks?\n - Why does the issue that is resolved by eq 8 take place in the first place? Why only on this experiment? Why does eq.8 help at all?\n - Better to use $m=2$ instead of $dim(H_{...})=2$.\n - Table1 could be given at the very beginning of Sec4 as it nicely summarizes all results.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: See above.\n- Quality: The writing requires significantly more effort. Similarly the experiments require justifications and comparisons with baselines. I believe these considerably diminish the quality of the presented work.\n- Novelty: The approach brings two rather known objectives together. Yet, the idea of explicitly encoding the conservation as well as the projection (of the dynamics) are timely and would be beneficial for the community.\n- Reproducibility: None of the experiment details is visible in the main text.",
            "summary_of_the_review": "I believe encoding state-invariant variables is a nice idea that comes with immediate real-world applications. A contrastive objective to learn such variables is shown to be effective. However, we are unable to tell whether the benchmarks are sufficiently challenging (so that we can see the limits of the framework). We are also not in a position to decide whether simpler ideas or existing methods would perform any better. Moreover, the text requires significant updates, which is why I recommend a reject. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1234/Reviewer_qJvt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1234/Reviewer_qJvt"
        ]
    },
    {
        "id": "eneicaBgfW",
        "original": null,
        "number": 2,
        "cdate": 1666676355645,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676355645,
        "tmdate": 1671074617235,
        "tddate": null,
        "forum": "gVSJ83n47IT",
        "replyto": "gVSJ83n47IT",
        "invitation": "ICLR.cc/2023/Conference/Paper1234/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a contrastive learning framework to model conservative dynamical systems with hidden conservative laws. More specifically, the proposed framework consists of two networks; an invariant representation network that maps elements of a certain trajectory to an identical latent features $H_{\\theta_c}(x) \\in \\mathbb{R}^m$ which mimic $m$ invariant quantities, and a dynamics network  $\\hat{f_{\\theta_d}}(x) = \\text{Projection}({f_{\\theta_d}}(x), f: \\langle f, \\nabla_x H_{\\theta}(x) = 0 \\rangle)$ that models the targeted conservative dynamics by projecting a neural ODE vector field ${f_{\\theta_d}}(x)$ to the orthogonal direction of  $\\nabla_x H_{\\theta}(x)$. The former and latter are trained via a neightborhood conservative loss and a standard MSE loss, respectively. The authors validate their proposed method for two simple dynamical systems (two variables with one conservative law), Kepler system (four variables with two conservative laws), and heat equation on a 1D rod (101 mesh points and one conservative law).",
            "strength_and_weaknesses": "Strength:\n1. This paper tackles an important and challenging problem of physics (can machine learning models learn conservative dynamics and extract invariant quantities from observed trajectories without using prior knoweldge (e.g., conservation of mechanical energy) or assuming underlying structures (e.g., Hamiltonian)?).\n\n2. To solve the above important problem, the authors introduce a constrative learning framework that can learn invariant quantities via a representation learning way. To me, this idea is novel.\n\n3. The paper is generally easy-to-follow.\n\nWeakness ( and Questions):\n1. I am not sure whether the contrastively-learned invariant latent features, i.e., the modeled invariant quantities, can be valid for unseen initial conditions. I think the authors should report generalization performance of the proposed method to ensure it.\n\n2. $m$, the latent feature dimension, seems to be very important hyper-parameter because it directly determines the number of invariant quantities. The authors do not explicitly state how one can tune such a important hyper-parameter when one does not know the exact value of $m$ for an unknown physical system, thus I am not sure the robustness of the proposed method in this paper.\n\n3. Espeically, for a heat equation problem, the authors use an auto-encoder that maps an observation to $d$-dimensional laten space. It means that the modeled dynamics is $d - m$ dimensional sub-manifold and it seems that both $d$ and $m$ seem to be very sensitive hyper-parameters when learning the dynamics and invariant quantities accurately.\n\n4. Experimental results seem to be weak. The proposed method fails to accurately learn the 4-variable Kepler system. Also, the authors only compare their model with a simple vanila network. It would be nice if the authors compared their model with some advanced models, e.g., HNN or [1].\n\n5. Can the proposed model learn a non-conservative systems such as a dissipative one?\n\n6. The core idea of this paper is that a certain state should have an invariant representation with respect to its time evolution. While the contrastive learning is one of good candidates to finding such a representation, there are various other methods that can enforce the invariance, e.g., consistency regularization [2]. What motivated the authors to use the contrastive learning?\n\n***\n[1] Liu, Z., & Tegmark, M. (2021). Machine learning conservation laws from trajectories. Physical Review Letters, 126(18), 180604.\n\n[2] Sinha, S., & Dieng, A. B. (2021). Consistency regularization for variational auto-encoders. Advances in Neural Information Processing Systems, 34, 12943-12954.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe paper is well-written and easy-to-follow.\n\nQuality:\n\nAs I mentioned in the main review, I have some concerns on (1) generalization performance, (2) hyper-parameter tuning, and (3) weak experimental results with limited comparisons.\n\nNovely:\n\nThe idea of using the contrastive loss to model and extract invariant quantities is novel for me.\n\nReproducibility:\n\nThe paper contains an incomplete set of used model architectures and hyper-parameters. Code is not made publicly available.\n\n",
            "summary_of_the_review": "Although the paper is interesting, I think there is still much room for improvement, e.g., see Weakness (and Questions). My current evaluation on this paper is borderline reject.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1234/Reviewer_vHDM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1234/Reviewer_vHDM"
        ]
    },
    {
        "id": "JsOK7oDTrgs",
        "original": null,
        "number": 3,
        "cdate": 1666761396053,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666761396053,
        "tmdate": 1666761396053,
        "tddate": null,
        "forum": "gVSJ83n47IT",
        "replyto": "gVSJ83n47IT",
        "invitation": "ICLR.cc/2023/Conference/Paper1234/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors propose a contrastive learning approach to learn conserved quantities of a dynamical system using an auxiliary neural network. The neural network learns through positive samples from the same trajectory of the dynamical system and negative samples from different trajectories. The conservation is enforced during prediction by projecting the outputs from a forecasting neural network onto the conservation manifold. ",
            "strength_and_weaknesses": "Learning conservation laws from observed trajectories is an important task to study a dynamical system and can also help in generalization. To my knowledge, the contrastive learning approach is novel and interesting. \n\n\n\n**Weaknesses**\n\n1. The paper does not adequately showcase the utility of learning conserved quantities this way. Since conserved quantities here are learnt as a NN representation, they are not interpretable. As mentioned by the authors, the learnt quantities may be a non-linear function of the (true) relevant conserved quantities. Additionally, as seen in Figure 4, the approach is not always able to enforce the learnt conservation law properly to the forecasting network.\n2. Paper does not empirically compare with any other baselines that learn conservation laws. Only comparison provided is a standard neural network which does not learn the conservation law, as expected.\n3. Writing in Section 3 is rushed (possibly with mistakes) and can be improved with more details about the equations. Section 4 (and Appendix) does not provide adequate details about the experiments & training/testing methodology, and a few plots are hard to parse due to no labels. \n\n\n\n**Questions**\n\n1. Except the ideal spring mass experiment, every other experiment considers energy conservation where Hamiltonian/Lagrangian networks can be used, potentially with additional rotational symmetry [1, 2] for angular momentum conservation. I think these methods should be added as baselines in these experiments. Further, the paper would be much stronger if there were more experiments like spring mass experiment that conserve quantities other than energy. \n2. The experiment section should include comparison with other baselines that learn conservation laws from trajectories (e.g., [3, 4] cited in the paper). \n3. In Equation (2), $t_1$ is not defined, should this be $t$? Can $k = i$ in this equation? \n4. In Equation (3), the expectation is defined over $x^+$ and then the summation inside is also over $x^+$. There is the same issue for $x^-$. In Equation (4), there seem to be a lot more negative samples compared to positive samples, does this not introduce some bias in learning?\n5. This framework does not seem to allow for the case when two different trajectories have the same conservation value. For example, trajectories with same energy but phase-shifted. Is this true?\n6. Please provide details of the experiments (in Section 4 or Appendix), for example, what are the time steps the trajectories were observed during training and the same during testing. Specifically, are the methods asked to predict beyond the times seen during training?\n7. Figure 3: Please provide y-labels for all plots. In Mass/Energy conservation plots, are the true values of mass/energy learnt by the neural network H?\n8. Figure 4: Please provide x-labels for plots of top row; I am assuming it is time(s) but then there is discrepancy in x-axis between top and bottom rows. \n9. In Section 4.2, could you please elaborate on what type of trivial solution does the model learn. It is also not clear to me why Equation (3) will be biased toward \u201csimilarity\u201d within trajectories, and why the proposed solution solves the issue. \n\n\n**References**\n\n[1] Finzi, Marc, et al. \"Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data.\" International Conference on Machine Learning. PMLR, 2020.\n\n[2] Finzi, Marc, Max Welling, and Andrew Gordon Wilson. \"A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Ha, Seungwoong, and Hawoong Jeong. \"Discovering conservation laws from trajectories via machine learning.\" arXiv preprint arXiv:2102.04008 (2021).\n\n[4] Liu, Ziming, and Max Tegmark. \"Machine learning conservation laws from trajectories.\" Physical Review Letters 126.18 (2021): 180604.",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is clearly written but quality can be improved with more details in Section 3 & 4. Learning conserved quantities using contrastive learning is new. ",
            "summary_of_the_review": "My main concerns are regarding the utility of learning conservation laws this way as these NN representations are not interpretable and may not be easy to enforce onto the forecasting network. Also, paper does not adequately compare with any other baselines that learn conservation laws from trajectories.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1234/Reviewer_m8A3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1234/Reviewer_m8A3"
        ]
    },
    {
        "id": "VibZD658aMi",
        "original": null,
        "number": 4,
        "cdate": 1666913803539,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666913803539,
        "tmdate": 1666913847385,
        "tddate": null,
        "forum": "gVSJ83n47IT",
        "replyto": "gVSJ83n47IT",
        "invitation": "ICLR.cc/2023/Conference/Paper1234/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a method to impose an acquired conservation law on a learned time-evolving model such as a DNN.\nThe proposed method consists of a two-step approach. The first step is invariant learning, in which underlying conservation laws are estimated from trajectory data, and the second step is dynamic learning, in which the time evolution model is subjected to preservation of conservation laws. Numerical experiments show that the proposed method consistently outperforms the baseline method in both coordinate error and conservation measures and can be further extended to complex, large-scale dynamics by leveraging autoencoders. The proposed method may also be useful in discovering conservation laws in unknown dynamical systems.",
            "strength_and_weaknesses": "Strengths\nThe framework of estimating hidden conservation laws from data and then improving simulation accuracy based on the estimated conservation laws is highly novel.\n\n\nWeaknesses\n\nA weakness of the research is that the effectiveness of the proposed method has not been verified because there is no theoretical analysis to guarantee the effectiveness of the method and the numerical experiments are limited to simple systems for which the conservation laws are known.\nConcrete weaknesses are that the following two points have not been validated.\n\n\n1. the effectiveness of the proposed method for learning the energy function\n\nThe denominator of the \"invariant learning\" loss function (Eq. (3)) seems to be based on the good properties of the energy terrain.\nFor example, it is questionable whether it would work effectively in the case of a complex energy function with multiple peaks, such that starting from different initial conditions would result in the same energy value.\nIn addition, the behavior of the model when the number of data is small relative to the degrees of freedom the system has is unclear.\nNumerical experiments with more complex energy landscapes and a discussion of the relationship between the number of data and learning performance would be needed. \n\n\n2. effectiveness of the proposed method as a method for estimating unknown conservation laws\nThe authors mention the application of the proposed method to the estimation of unknown conservation laws, but its effectiveness is questionable in terms of interpretability and feasibility of estimating complex conservation laws.\n\n\n2-1. Interpretability\nAll of the previous studies cited in section \"2.3 LEARNING WITH CONSERVED PROPERTIES\" seem to achieve interpretable conservation law estimation.\nOn the other hand, the proposed method does not seem to be able to achieve interpretable conservation law estimation.\nMethods for estimating interpretable conservation laws from DNNs trained on dynamical system data have been proposed [1,2].\nI think that an additional discussion based on such previous studies seems necessary.\n\n\n2-2. Possibility of Estimating Complex Conservation Laws\nThe authors claim that the proposed method can estimate complex conservation laws.\nAs an example, they demonstrate the estimation of an angular momentum conservation law for the Kepler system.\nHowever, this task is not so difficult.\nConservation law estimation for the Kepler system has been realized in many studies [1,2].\nIn order to claim the effectiveness of the proposed method as a conservation law estimation method, it is necessary to at least realize conservation law estimation corresponding to symmetries for nonlinear transformations such as Runge-Lenz vectors.\nNon-linear conservation estimation is also mentioned in [1][2]. \nAlso, [3] achieves it using Hamiltonian Neural Networks by estimating the mass tensor.\n\n[1]Ziming Liu and Max Tegmark, \"Machine Learning Hidden Symmetries,\" PRL, 128, 180201, 2022.\n\n[2]Yoh-ichi Mototake, \"Interpretable conservation law estimation by deriving the symmetries of dynamics from trained deep neural networks,\" PRE, 103, 033303, 2021.\n\n[3]Nate Gruver, Marc Finzi, Samuel Stanton, Andrew Gordon Wilson, \"Deconstructing the indirect biases of hamiltonian neural networks,\" ICLR 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "quality:\n\nThe manuscript was of high quality with no typographical errors or omissions.\n\n\nclarity:\n\nThe paper was clear except for the following two points\nThe meaning of x\u223cC\u223cD in equation (3) was unclear.\nIn section 4.3, I did not understand why the reduced space was created.\nIf we bring all possible initial states, does the system still have 9 dimensions of freedom?\n\n\noriginality:\n\nOriginality is considered to be high enough to be published.\n\n\nreproducibility:\n\nPublication of the source code for the numerical experiments is desired.",
            "summary_of_the_review": "The approach of estimating hidden conservation laws from data and improving the accuracy of simulations based on these conservation laws is an innovative research, and the novelty of this research fully satisfies the conditions for acceptance for presentation at the conference.\nOn the other hand, the evaluation of the effectiveness of the proposed method is weak, and the validity of the study is questionable.\nI believe that additional discussion and numerical experiments to improve these points would be a condition for the conference to accept the study.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1234/Reviewer_kRTH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1234/Reviewer_kRTH"
        ]
    }
]