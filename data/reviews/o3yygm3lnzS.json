[
    {
        "id": "WEbq2gvRMCI",
        "original": null,
        "number": 1,
        "cdate": 1666473043447,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666473043447,
        "tmdate": 1672734727889,
        "tddate": null,
        "forum": "o3yygm3lnzS",
        "replyto": "o3yygm3lnzS",
        "invitation": "ICLR.cc/2023/Conference/Paper2228/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a generative framework for synthesizing multi-view consistent portrait videos. Towards this, the authors employed EG3D architecture in NeRF and made the network conditioned on motion features. 2 discriminators are used to maintain spatial and temporal consistency. While it\u2019s claimed in the paper that this is the first work for the task, I have noticed the existing method [1] doing the same task and they are able to generate 3D-aware portrait videos with motion controllability.\n\n[1] P. Zhuang et al., Controllable Radiance Fields for Dynamic Face Synthesis, 3DV 2022.\n",
            "strength_and_weaknesses": "Strength:\n- The motivation of the proposed paper is interesting. The paper is well-written. The video quality looks reasonable.\n\nWeaknesses:\n- It\u2019s unclear in the paper how is this work different from [1]. Please clarify.\n- I doubt if the video discriminator that only takes paired frames w/ the corresponding time delta is sufficient to maintain spatio-temporal consistency, especially for texture and albedo. One example is the man on the left in the first video. His mustache flickered over time, i.e., it looks faded sometimes. Also, from other generated portrait videos, I can see that the head shape deformation over time as it dynamically became bigger and smaller. Similar inconsistency happens to hair.\n- Following the last question w.r.t. spatio-temporal consistency, could the authors generate some videos from multiple views at one time for the same person, e.g., rendering 3 videos with the fixed left, middle, and right views, simultaneously?\n- It's unclear how the same motion features work on various face identities. To verify the constant effectiveness of motion conditioning, could the authors show multiple image/video results (i.e., different face identities) with the same motion features?\n- Can this work generate face videos with complex expressions, which the earlier work [1] can handle?\n- I\u2019m worried about data processing. The authors used alignment which is commonly used in image preprocessing but it might not be proper for videos. Usually, people crop videos with a fixed cropping window. If doing alignment, will the processed videos become jittering? Did the authors observe this issue and fix it? Please explain.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear, and the results are of good quality. While the method uses existing EG3D architecture and two discriminators that are commonly used in video generation, I believe the work has its contributions to the community. Given that EG3D code is released, I don\u2019t worry about reproducibility.\n",
            "summary_of_the_review": "The paper is well-structured and the topic is interesting. The video quality is good. I\u2019ll wait for the authors to reply to the questions.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2228/Reviewer_YSuv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2228/Reviewer_YSuv"
        ]
    },
    {
        "id": "VtaOI_nJt22",
        "original": null,
        "number": 2,
        "cdate": 1666692273279,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692273279,
        "tmdate": 1666693091460,
        "tddate": null,
        "forum": "o3yygm3lnzS",
        "replyto": "o3yygm3lnzS",
        "invitation": "ICLR.cc/2023/Conference/Paper2228/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tackles the problem of training 3D generative models for portrait video generation with only 2D videos. In order to enforce temporal consistency and diverse motion, PV3D utilizes disentangled appearance and motion latent space. Extensive experiments demonstrate the effectiveness of the proposed approach.",
            "strength_and_weaknesses": "## Strengths\n\nThe proposed idea is interesting. The experiments are thorough and solid. The paper is clearly written.\n\n## Weakness\n\nI do not observe major issues. However, there are some question I hope authors can clarify. Please see below.\n\n## Questions\n\n**Network structure**\n\nIt is a little bit unclear to me how the exact structure of the \"T-Tri-plane Synthesis\" module in Fig. 2 looks like. From Appendix A.2, I feel like it is essentially some top-layers of vanilla StyleGAN2's generator since the first K layers of the original StyleGAN2's generator have been replaced by the \"motion layer\". Can authors clarify?\n\n**Camera pose conditioning**\n\n1. From the quantitative perspective, it is unclear to me whether conditioning the generator on the camera pose in necessary since there is no ablations about removing the generator's camera conditioning.\n\n2. From Fig. 2: it seems like during training, for a specific time $t_i$, the motion generator, the rendering from tri-plane, and the discriminator take the same camera pose $c_i$. Is this correct?\n\n3. For Sec.4.3's \"Camera Conditioning\":\n    1) For Tab. 2(c): it is unclear what the final strategy PV3D chooses is.\n    2) For \"All\" strategy, I think the description of \"condition the whole generator on the shared camera pose\" is misleading. At least from Fig. 2, \"the whole generator\" only has \"mapping network\" that can take camera pose as conditioning. My guess is that authors also refer to the rendering process. However, it is not in the generator. \n    3) For \"Map\" strategy: can authors clarify how to choose the \"shared pose\" given two poses?\n\n**Typos**\n- Eq. (2): there are unmatched parenthesis\n\n**Lacked references**\n\n- Zhang et al., Controllable Radiance Fields for Dynamic Face Synthesis. 3DV 2022. \n- Schwarz et al., VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids. NeurIPS 2022.\n- Zhao et al., Generative multiplane images: making a 2D GAN 3D-aware. ECCV 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarify: the paper is well-written.\n- Quality: the results demonstrate the efficacy of the approach.\n- Novelty: the idea is fresh and interesting.\n- Reproducibility: authors state that the code will be made public.",
            "summary_of_the_review": "This paper tackles the important problem of 3D portrait video generator. The proposed approach is interesting and has been thoroughly verified. Therefore, I vote for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2228/Reviewer_zYuC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2228/Reviewer_zYuC"
        ]
    },
    {
        "id": "xi6PpZ25C7n",
        "original": null,
        "number": 3,
        "cdate": 1666753697608,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666753697608,
        "tmdate": 1666753697608,
        "tddate": null,
        "forum": "o3yygm3lnzS",
        "replyto": "o3yygm3lnzS",
        "invitation": "ICLR.cc/2023/Conference/Paper2228/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a 3D portrait video generative model learned from a dataset of monocular videos. A motion latent code is added to a triplane-based 3D GAN, in addition to an appearance code. A single motion latent controls the motion for the entire temporal sequence. The network also receives the timestep and camera poses as conditioning inputs. Temporal and Static discriminators are used for training.  ",
            "strength_and_weaknesses": "Strengths: These are the first 3D portrait video results. The quality of results are impressive, in terms of geometry, appearance and motion. The paper is well-written, and the evaluations are thorough. \n\nWeaknesses: \n- Since the motion component only controls the first few generator layers, fine-grained motion would be difficult to model. In addition, any lighting effects would not be modeled with the first four latents. I suspect the method does not generate any result where the lighting on the face changes between different timesteps. \n- As mentioned in the limitations, the method only deals with very small motion (upto 16 frames). It would be good to show some results with large motion, just to understand how it would fail. I suspect that the design choice of using a single motion latent vector for all timesteps is the limiting factor here. \n- For inversion, in Fig. 6, a different motion code is optimized for each frame. How different are the optimized codes for different frames? Would it be possible to use these optimized motion codes to retarget a different appearance code, or are they out-of-distribution? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear. The technical ideas presented are novel and original. The method is simple, demonstrating that a recurrent network is not necessary for the task. ",
            "summary_of_the_review": "The paper presents an interesting solution to a novel problem with high-quality results. It would be good to better demonstrate the limits of the method in terms of duration of synthesized videos. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2228/Reviewer_7Zbn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2228/Reviewer_7Zbn"
        ]
    },
    {
        "id": "Vh5zML5Evx",
        "original": null,
        "number": 4,
        "cdate": 1667137108736,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667137108736,
        "tmdate": 1667139573471,
        "tddate": null,
        "forum": "o3yygm3lnzS",
        "replyto": "o3yygm3lnzS",
        "invitation": "ICLR.cc/2023/Conference/Paper2228/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to synthesize multi-view consistent portrait videos from 2D observations.\nThe framework is built upon recent state-of-the-art 3D-aware image GAN and aids extra-temporal information to it.\nSeveral designs are introduced for both temporal smoothness and multi-view consistency.\nThe demo videos show high-quality results.",
            "strength_and_weaknesses": "Strengths:\n1. This paper is well-written.\n2. The quality of multi-view videos is good.\n3. The motion synthesis layer is well-designed. The feature modulated by the motion code are fused with apperance feature in a residual manner, which is well-motivated and resonable.\n\nWeakness:\n1. The video discriminator only receives two frames, which harm the ability to synthesis long videos. I am wondering whether this method can enable long-term video synthesis.\n2. I am confused about the details of the generation pose condition. Do you simply concatenate the camera pose with latent code like EG3D? If so, are there any 2D billboard artifacts?\n3. For the discriminator pose condition, how do you inject the discriminator with two poses? If the [vi, vj] are embedded into a single vector, maybe the discriminator cannot differentiate the alignment between pose pairs and image pairs.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the results are good.\nI think several designs proposed in this paper are effective, and the experimental results also give detailed ablation on them.",
            "summary_of_the_review": "Shown in the above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2228/Reviewer_Y7rC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2228/Reviewer_Y7rC"
        ]
    }
]