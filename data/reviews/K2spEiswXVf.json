[
    {
        "id": "ZtjIEt8YRYF",
        "original": null,
        "number": 1,
        "cdate": 1666095446643,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666095446643,
        "tmdate": 1669039903537,
        "tddate": null,
        "forum": "K2spEiswXVf",
        "replyto": "K2spEiswXVf",
        "invitation": "ICLR.cc/2023/Conference/Paper5086/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new approach to perform transfer-learning for Hyperparameter optimization where offline evaluations are used to fasten the tuning on a new task. The method proposes to leverage recent work that uses classification to train surrogates by learning to classify good/bad configuration. In particular in this work, the classifier takes a combination of global (task agnostic) and local (task specific) features and is learned with a Bayesian Logistic Regression layer which is made possible with the use of several approximations. \nIn addition, a gradient boosting approach is used to improve the final fit of the model at each iterations. Experiments are conducted on several tabular benchmarking suites  (hpobench, mlbench) and also artificial examples to study the effect of noise levels. The method proposed is show to be competitive or better than the baselines proposed.",
            "strength_and_weaknesses": "The strengths of the paper are:\n* tackle a very relevant and impactful problem\n* provides a novel technical contributions with several ideas that could be leveraged in future work (refinement with gradient-boosting, use of EI equivalent likelihood-free criterion)\n* good coverage of experiments: a reasonable range of benchmarks/different blackboxes are considered \n\nWeaknesses:\n* lack of runtime analysis: no runtime is given for how long the method takes to return suggestion (the appendix mentions 2048 epochs on top of that, a gradient boosting tree is fitted). However, this can significantly worsen the results if they were reported against wallclock time (as proposed in other works) since then almost all time will be spent in fitting models and the method would likely underperform random-search. Without additional details, one cannot assess if the method will have any practical relevance (a method taking 5 minute to suggest the next candidate will likely not be very applicable).\n* the set of baselines is relatively considered is relatively small (only 2 transfer learning and 2 non transfer baselines). They are several baselines that could be easily added: BORE for non transfer, but also BORE/LFBO with search space pruned to bounding box of best previous evaluations [Peronne 2019]. Ideally, another additional transfer-learning baseline on top of this simple bounding-box approach would also be added in order to better assess the quality of the method regarding state-of-the-art (2 methods for transfer is a small number).\n* details are lacking, some part of the methods were not clear to me:\n  * 4.1: was difficult to get through while it is a key part of the paper, I would suggest adding the dimensions for the different variables and give the final expression of the classifier that sums its two input (which is given in 4.2). In any case, the text alone was not sufficient for me to get exactly how $\\phi \\dot z_t$ is obtained as I could see several ways to achieve this depending on input dimensions.\n  * 4.2: the expression of L^LFBO given in Eq. (3) does not take arguments as input\n  * 4.3: how exactly is the Gradient boosting combined with the classifier was not entirely clear to me. I would recommend to write it down formally rather than with words (given the current text description, there could be many ways on how a GB would be fitted to reduce the residuals)\n* the set of ablations is small and several complexity are not justified (for instance the approximation done instead of the direct optimization of (3), see additional details for a bigger description on this.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is generally OK except for some details missing and some notations missing (should be easy to fix).\nThe quality of the paper is good although it could be improve by adding more baselines and ablations to show better that the method actually improves state-of-the-art. Finally, the paper provides an original contribution and references well previous work.\n",
            "summary_of_the_review": "The paper provides an interesting and novel method to a relevant problem with several ingredients that may be leveraged by future work (for instance using GB to improve surrogate quality). However, as it stands the experimental section has only too little baselines to really its performance against state-of-the-art (only 2 baselines for transfer) and some part of the models are unclear. I believe those could be potentially addressed before camera ready.\n\nAdditional details:\n* many typos, it would be valuable to run a spell-check:\n  * meta-leared\n  * observaitons\n  * This benchmarks\n  * Coppola\n  * ensmeble\n* Eq (3): interestingly, the scale is back in the loss (but the surrogate dont have to predict values proportional to it)\n* A potential ablation would be to use BORE to illustrate the benefit of using LFBO in your case, other possible ablations I was wondering about were: using only local/global part of the model, fitting (3) directly as mentioned in the begining of 4.2. The latter would be important to have given that otherwise all the complexity of the approximations (laplace and logits) would not be justified.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5086/Reviewer_Xog3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5086/Reviewer_Xog3"
        ]
    },
    {
        "id": "CwzkuTpvZX1",
        "original": null,
        "number": 2,
        "cdate": 1666328374528,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666328374528,
        "tmdate": 1666485768478,
        "tddate": null,
        "forum": "K2spEiswXVf",
        "replyto": "K2spEiswXVf",
        "invitation": "ICLR.cc/2023/Conference/Paper5086/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the problem of meta Bayesian optimization (BO), which aims to warm-start the BO process by exploiting knowledge from related tasks. In this paper, the authors propose warm-starting the acquisition function, which takes the form of a classifier in the likelihood-free BO setting. Gradient boosting is further incorporated to combat distributional shifts. ",
            "strength_and_weaknesses": "Strengths:\n\n(+) The problem of (meta) Bayesian optimization is an important problem with several applications.\n\nWeaknesses:\n\n(-) The comparison with existing works is lacking, both in the techniques and in the empirical evaluations.\n\n(-) The lack of a theoretical guarantee (which is common in the BO literature).\n\n(-) The writing can be improved (as detailed below).  \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is generally hard to follow, many concepts require prior knowledge and are not sufficiently explained in the paper. More detail below.\n\nNovelty: The novelty aspect is low as a similar idea has been explored by Volpp et al. (2020). The proposed meta-learning framework is almost a direct application of Berkenkamp et al. (2021).\n\nReproducibility: There is no code included with the submission.\n",
            "summary_of_the_review": "The paper presents an interesting idea to tackle an important problem. However, the difficulty or challenges of the problem are not highlighted enough. The comparison with related works is lacking. And the writing can be improved. \n\nDetailed comments:\n\n- The idea of meta-learning an acquisition function has been explored by Volpp et al. (2020). Could the authors elaborate on the differences and advantages of the proposed algorithm to that of Volpp et al. (2020)? In addition, this should also be included as a baseline in the experiment section.\n\n- The paper is generally hard to follow since the readers assume knowledge from several other key papers. I have to read several other works on the BORE framework and BaNNER to understand several parts of the paper. I encourage the authors to make the paper more self-contained by reintroducing concepts from essential related works like Berkenkamp et al. (2021), Tiao et al. (2021), and Song et al. (2022).\n\n- When the input space is a simple Euclidean space without any structure (think of minimizing the function $f(x) = x^2 \u2013 4x$), how does the feature embedding $h(\\cdot)$ works? I suppose feature embedding is only useful when there is some structure in the inputs (e.g., images).\n\n- Since BO is a black-box optimization algorithm, can the authors indicate more clearly what are the objective functions that we are optimizing in the experiment section? It is difficult for readers to comprehend and assess results without knowing what are we optimizing.\n\n- From my understanding, UCB is one of the most commonly used acquisition functions, with a nice theory in BO. However, the paper did not mention or compare with UCB. Is there a good reason for that? Can the authors compare the proposed algorithm with GP-based BO methods with EI/PI and UCB?\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5086/Reviewer_Xo11"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5086/Reviewer_Xo11"
        ]
    },
    {
        "id": "CgKqhIRAf2o",
        "original": null,
        "number": 3,
        "cdate": 1666640111705,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640111705,
        "tmdate": 1666640111705,
        "tddate": null,
        "forum": "K2spEiswXVf",
        "replyto": "K2spEiswXVf",
        "invitation": "ICLR.cc/2023/Conference/Paper5086/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a meta-learning method for likelihood-free Bayesian optimization. In particular, the proposed approach is a combination of two existing solutions, a meta-learning approach from BaNNER and a likelihood-free LFBO, together with the gradient boosting method. The proposed solution is able to work with high-dimensional inputs and handle heterogeneous scales and noises across different tasks.\n",
            "strength_and_weaknesses": "The main strength of the paper is in the empirical performance which is shown to outperform two of the existing works, ABLR, and GC3P. However, it has several weaknesses as follows.\n\n1. The technical solution is not novel since it is based mostly on the two existing works BaNNER and LFBO.\n\n2. Although the paper claims that the meta-learned classifier can balance between exploration and exploitation, the proposed approach requires a gradient boost method to correct the errors. It means that the meta-learned classifier does not allow exploration properly. This is unlike multi-task GP, where the correlation between tasks can be learned from the data without any additional ad-hoc method (such as gradient boosting) for correction.\n\n3. While Thompson sampling often relies on a good approximation of the posterior distribution, the proposed approach only uses the Laplace method to approximate the posterior distribution which is a very simple approximation method. Better approximation methods such as variants of MCMC and/or variational inference techniques should be applied.\n\n4. The numerical representation of the task (z) is simply optimized to be close to a standard (multivariate) normal distribution. I am wondering if the proposed method can work with a multi-modal task distribution, i.e., prior tasks form 2 clusters where tasks in a cluster are similar and tasks between clusters are different. In a principled Bayesian approach such as multi-task GP, it can correctly correlate a new task with an existing task.\n\n5. It is unclear to me about the choice of \\tau (for prior tasks and the current task) in the proposed algorithm.\n\n6. While the paper reviews a lot of related works on meta-learning for BO, the experiments only consist of 2 existing works while ignoring the others such as those using GP surrogates (e.g., running experiments on low-dimensional problems).\n\n7. Section 3.1 lacks many well-known BO methods such as GP-UCB, predictive entropy search, max-value entropy search, and knowledge gradient-based methods.\n\n8. Since BO is about sample efficiency, can gradient boosting work reasonably well with a small training dataset?",
            "clarity,_quality,_novelty_and_reproducibility": "As discussed above, the novelty of this paper is limited since it is based mostly on the two existing works BaNNER and LFBO. I also have several concerns about the proposed solution as elaborated in the above weaknesses. Regarding clarity, as the Laplace approximation is a well-known technique, the paper can reduce the explanation of the Laplace approximation to include more explanation on the regularization (to learn the task representation), the gradient boosting, and the choice of \\tau.",
            "summary_of_the_review": "Due to the limited novelty, concerns about the proposed solution, and the lack of baselines in experiments, the paper may require further improvements to fit the conference.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5086/Reviewer_YYiE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5086/Reviewer_YYiE"
        ]
    },
    {
        "id": "cmqqIeTeZAj",
        "original": null,
        "number": 4,
        "cdate": 1666900411782,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666900411782,
        "tmdate": 1666900411782,
        "tddate": null,
        "forum": "K2spEiswXVf",
        "replyto": "K2spEiswXVf",
        "invitation": "ICLR.cc/2023/Conference/Paper5086/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The work proposes a likelihood-free Bayesian optimization strategy with a meta-learning scheme. Given multiple tasks, it trains task-agnostic and task-specific components in order to predict a probability that measures how likely a solution is, inspired by BORE and LFBO. By utilizing Bayesian logistic regression, it determines a query point. In addition, the authors use a gradient boosting model to predict a residual of the model. Eventually, the experiments demonstrate the effectiveness of the proposed method, compared to other existing methods.",
            "strength_and_weaknesses": "Here I describe the strengths and weaknesses of this paper.\n\n### Strengths\n\nIt solves an interesting topic where multiple historical tasks are given, by applying a likelihood-free framework.\n\nThe proposed method is quite novel. In particular, a combination of some components such as mean prediction layer, residual prediction layer, and gradient boosting is interesting.\n\nThe purpose of the respective components and the respective loss functions is well-described.\n\n### Weaknesses\n\nPresentation and writing can be improved. The current version is okay, but I think it can be polished more.\n\nIteration budgets for the experiments are too low. I think that you should give a larger iteration budget in order to show the convergence of the algorithm tested.",
            "clarity,_quality,_novelty_and_reproducibility": "### Questions\n\n* For the experimental results, the variance (or standard deviation) of the experiments (i.e., shaded regions) is somewhat odd. Since $y$-axis is a log scale, the variance at the last of iterations should be larger than the variance at the beginning of iterations.\n\n* A feature extractor can be called as ResNet? According to Appendix C, your feature extractor is different with ResNet. I think that the name should be changed.\n\n* I would like to ask about gradient boosting. Which gradient boosting is used in this paper?\n\n* Following the above question, if you use gradient descent, how did you optimize a function $C$ (Line 10 or Line 12 of Algorithm 1)?\n\n* In the experiments, three proposed method, i.e., MALIBO wo GB, GB, and GB-TS, are tested. Does MALIBO wo GB include TS? Why did not you test MALIBO wo GB w TS or MALIBO wo GB wo TS?\n\n### Minor issues\n\n* In Page 4, $\\max(y - \\tau)$ seems like a typo; please fix it.\n\n* In the caption of Figure 2, $x_t$ should be $\\boldsymbol x_t$.",
            "summary_of_the_review": "Please see the above text boxes.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5086/Reviewer_YSQy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5086/Reviewer_YSQy"
        ]
    }
]