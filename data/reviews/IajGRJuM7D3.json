[
    {
        "id": "g9y9dooYm12",
        "original": null,
        "number": 1,
        "cdate": 1666550858594,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666550858594,
        "tmdate": 1666550858594,
        "tddate": null,
        "forum": "IajGRJuM7D3",
        "replyto": "IajGRJuM7D3",
        "invitation": "ICLR.cc/2023/Conference/Paper483/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes to use monotone and orthogonal operator to build implicit graph neural network. The adopted approach is well build upon monotone operator theory. The new operators allows more stable (via orthogonal operator) or expressive (via monotone operator) implicit GNN models with provable convergence. The discussion is super extensive. The experiments on node-level and graph-level tasks show the superiority of the method.  ",
            "strength_and_weaknesses": "Strengths\n1. This work is strong in its math-grounded modeling technique. In particular, it connects GNN modeling with monotone operator theory, which is exciting. Although such connection has been leveraged to study deep equilibrium model [1], I think it is still novel to apply such connection to study GNN models. \n\n2. The derivation and the discussion are super extensive, covering different types of operators to match different properties, different operator splitting iterative methods, acceleration ideas, back-propagation ideas.   \n\n3. The experiments make sense and show the benefits of the model.\n\nWeaknesses\n1. The main concern of this paper is about its exposition. As it tries to discuss too many things, many details in the main text are missed and I have to check the appendix even if I am fairly familiar with monotone operator theory. Some discussions may be not necessary, kind of detour, and may distract the readers. \n\nFor example, in Sec. 2, the authors advertise a lot about the new well-posed condition, where they claim that Eq. (2) is well posed even if $W$ has eigenvalue less than -1. However, this result is not very practically useful, because in sec. 3, their finally adopted $W$ still have to have eigenvalues with absolute values less than 1 to guarantee convergence. Moreover, in Remark 2, the way to make W symmetric will naturally remove the asymmetric part of W in the monotone operator ($F- F^T$). \n\n2. Some ablation studies are missing. For example, the authors also adopt diffusion convolution in Eq.(12), where the diffusion convolution automatically has the benefit in capturing long range dependence. So, it is unclear that in the node classification tasks, whether the benefit comes from diffusion convolution or implicit operators. Here,  I also do not know what \"N3D5\", \"N5D5\" used in the model name in the experiment means, which are not defined. I assume they are related to the diffusion convolution. Please clarify. Also, provide the study of using different convolutions to show the benefits indeed come from implicit modeling. \n\n3. Node-level experiments are too few. Actually, Long range dependence is not that crucial for graph-level tasks, because one can adopt transformer models and run on dense graphs by adding jump-hop edges. This is also why the proposed models are hard to outperform many baselines in graph-level tasks (the used baselines and datasets in this work are actually not SOTA and large). LRD is more important for node-level tasks. However, this work only provides one dataset for such evaluation. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good though the content is dense. To understand many things, one needs to be very familiar with monotone operators and check appendix. \n\nQuality: Good. The technical contributions are great. The experiments make sense while are not as strong as their technical parts. The used datasets are small. The ablation studies are not sufficient. \n\nNovelty: Good. The first work to build the connection between monotone operators and GNN modeling. \n\nReproducibility: Good. ",
            "summary_of_the_review": "The paper is strong in its technical contributions. The empirical contributions are about the acceptance bar. More extensive studies are suggested. The exposition of main text can be improved by showing the results more directly instead of detouring the discussion. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper483/Reviewer_Ti7k"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper483/Reviewer_Ti7k"
        ]
    },
    {
        "id": "FooUNaGqqO",
        "original": null,
        "number": 2,
        "cdate": 1666558859521,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558859521,
        "tmdate": 1669147536813,
        "tddate": null,
        "forum": "IajGRJuM7D3",
        "replyto": "IajGRJuM7D3",
        "invitation": "ICLR.cc/2023/Conference/Paper483/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method to stabilize the training of implicit graph neural networks (gnns) and in less required iterations. \nThe proposition is based on learning a monotone operator and employing splitting techniques for a better solution of the iterative problem.\n\nSeveral experiments are conducted and show the improvement of the proposed method compared to previous implicit gnns.",
            "strength_and_weaknesses": "Pros:\n1. The authors provide a sufficient background of implicit gnns\n2. The paper is well motivated and mostly easy to follow\n3. There are several interesting experiments \n\nCons:\n1. The paper lacks discussion of relevant prior work. For example, the use of Cayley parameterization was previously proposed in \"CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters\". In the sense of learning long-range dependencies several works proposed various approaches, for instance learning multiple hop filters \"Path Integral Based Convolution and Pooling for Graph Neural Networks\" or dedicated oscillatory layers in \"PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations\" and in \"Graph-Coupled Oscillator Networks\".\n\n2.While the experiments do show the significance of the proposed method compared to existing implicit gnns, it is hard to evaluate the performance of the proposed method compared to recent work in the field of gnns due to two reasons.\nA. The authors do not compare where possible to recent methods. For example table 1 lacks many works that perform significantly better. See for example \"Improving graph neural network expressivity via subgraph isomorphism counting.\", \"Weisfeiler and lehman go topological: Message passing simplicial networks.\" , \"Weisfeiler and lehman go cellular: Cw networks.\" , \nB. Besides this experiment, all other datasets are not directly comparable with other works. This is not bad, but to really assess the performance of this method I think that it needs to include experiments that are more directly comparable like node classification on Cora,Citeseer,Pubmed,OGBN-ARXIV and others. \n\n3.Regarding equation 1. This formulation of adding the initial features looks similar to the operation in \"Simple and Deep Convolutional Graph Neural Networks\" which is not discussed in this paper. Can the authors explain?\n\n4. Regarding the claim at the bottom of the first page. I am not convinced that it is true that IGNN does not suffer from the very same problem. What is the difference here? Why should IGNNs that smooth the node features not oversmooth if the propagation is the same?\n\n5.I understand that the proposed method requires less iterations, but I do not know how much time each iteration of the proposed method takes. I believe that if the authors add run times their results will be more convincing.\n\n6.Proposition 2 is not clear - where does the final eqation W = ... comes from?\n\n7.One of the hyperparameters of this method is the order of diffuion. However in different experiments the authors use different orders. It is hard to deduce what is the actual influence of the order this way. I think that the paper can benefit from an ablation study of the various hyperparameters.\n\n8.What are the chosen hyperparameters in your experiments? How did you choose them?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - The paper is mostly clearly written.\n\nQuality - The paper lacks comparison with many existing methods and also a background discussion on other methods that capture long range dependencies.\n\nNovelty - The method itself seems new.\n\nReproducibility - Many details to reproduce the method from the text are missing.",
            "summary_of_the_review": "The paper suggest an interesting method to improve implicit graph neural networks and the results compared to such methods seem promising. However there is a lack of discussion with other relevant works, and the experimental part is limited and hard to compare with other methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper483/Reviewer_FLVV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper483/Reviewer_FLVV"
        ]
    },
    {
        "id": "rZm8_-QzK2I",
        "original": null,
        "number": 3,
        "cdate": 1666835944704,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666835944704,
        "tmdate": 1669094368471,
        "tddate": null,
        "forum": "IajGRJuM7D3",
        "replyto": "IajGRJuM7D3",
        "invitation": "ICLR.cc/2023/Conference/Paper483/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to improve the stability, accuracy, and efficiency of implicit graph neural networks (IGNN) by new parameterizations (i.e., the Cayley transform-based orthogonal parameterization and monotone parameterization) and advanced solvers (i.e., operator splitting, Anderson acceleration). Theoretical justification for the well-posedness for the proposed MIGNN is presented, and empirical experiments demonstrate the performance of MIGNN.",
            "strength_and_weaknesses": "# Strength\n\n1. The paper makes a nice observation that the accuracy of IGNN seems to correlate with the eigenvalue of $W$ in the implicit model. This is an interesting observation and motivates the work in the paper.\n\n2. The paper generalizes the well-posedness condition for IGNN and proposes more flexible constructions based on monotone operator theory. New parameterizations are proposed to define a new model MIGNN.\n\n3. The paper comprehensively discusses the technical details of related works, which is helpful to understand the idea in the paper. \n\n4. Extensive experiments are presented with discussions.\n\n# Weakness\n\n1. The contribution and significance of the proposed ideas are unclear. For instance, many techniques in MIGNN have been extensively studied in the literature and even in the study of GNNs, such as Anderson acceleration, Neumann series approximation, high-order graph diffusion, eigendecomposition, etc. Moreover, monotone operator theory and operator splitting are not new (even in the context of deep learning). Therefore, the proposed idea seems an incremental combination of many techniques but the main contribution is unclear.\n\n2. Among all the techniques in the proposed MIGNN, the flexible parameterization in Section 3 seems to be new and interesting. However, the paper fails to justify the effectiveness of this key innovation due to the lack of a comprehensive ablation study. The improvements over IGNN may be due to other existing techniques that can be trivially applied in IGNN as well such as Anderson acceleration, Neumann series approximation, high-order graph diffusion, eigendecomposition, etc. In fact, Figure 3 partially confirms this since the high-order diffusion in IGNN-D5 significantly improves the performance of IGNN. \n\n3. The comparison with IGNN is unfair. In fact, each forward iteration of MIGNN has more computation and graph aggregations than each forward iteration of IGNN due to the Neumann series approximation and diffusion convolutions. For instance, it is mentioned in the paper that \"Each node can access information from its K-hop neighbors using the K-th order Neumann series approximated PR iteration\". Therefore, the concept of iteration needs to be clearly and fairly defined when comparing their stability, accuracy, and efficiency. \n\n4. There is a lack of computation complexity analysis. In fact, the computation cost of MIGNN is pretty high, and it is unclear how it outperforms IGNN or EIGNN theoretically and empirically.\n\n5. The motivation for monotone parameterization in Section 3.2 is unclear. Why do you define G as L/2? What are the intuition and advantages\uff1f\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The novelty and contribution are limited. The experiments are not convincing.",
            "summary_of_the_review": "The paper introduces interesting observations and ideas to improve IGNN. However, the contribution and significance are unclear, and the evaluation is not convincing enough to justify the effectiveness of the proposed algorithm.\n\n## After rebuttal\nThe revision significantly improves the paper. I am willing to increase my score.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper483/Reviewer_h7JA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper483/Reviewer_h7JA"
        ]
    },
    {
        "id": "6nbQDuYfhF",
        "original": null,
        "number": 4,
        "cdate": 1668003777082,
        "mdate": 1668003777082,
        "ddate": null,
        "tcdate": 1668003777082,
        "tmdate": 1668003777082,
        "tddate": null,
        "forum": "IajGRJuM7D3",
        "replyto": "IajGRJuM7D3",
        "invitation": "ICLR.cc/2023/Conference/Paper483/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a variant of Implicit Graph Neural Network (IGNN) based on the monotone operator theory. First, this paper formalized the convergence of IGNN as a monotone inclusion problem and derived the sufficient condition for well-posedness. Then, two types of parameterization for the weight matrix were proposed based on this condition: the orthogonal parametrization and the monotone parametrization. In addition, to speed up the convergence of IGNN, this paper proposed two acceleration methods based on the forward-backward splitting scheme and the Pearceman-Rachford splitting scheme, respectively. Finally, this paper applied the proposed model to the chain classification task on synthesis data and to the graph and node prediction tasks on real data to assess the prediction accuracy and computational efficiency.",
            "strength_and_weaknesses": "\u3010Strengths\u3011\n- Provides basics of monotone operator theory, which enables the paper to be more accessible to those unfamiliar with monotone operator theory.\n\n\n\u3010Weaknesses\u3011\n- Novelty of the proposed method is limited because the methodology primarily relied on the combination of existing methods.\n- Interpretation of $\\lambda_1(W)$, which may be related to IGNN convergence and prediction accuracy, is somewhat questionable.",
            "clarity,_quality,_novelty_and_reproducibility": "\u3010Clarity\u3011\nThe mathematical part is clearly written. Although I was unfamiliar with the monotone operator theory, I could understand the paper thanks to the brief review in the appendix.\n\nP3 (3): At first reading, it was difficult to interpret the monotone inclusion problem, $0\\in (\\mathcal{F}+\\mathcal{G})(x)$. It is better to clarify that $\\mathcal{F}$ and $\\mathcal{G}$ are set-valued functions and that $\\partial f$ is subgradient.\n\nAppendix E.2: Proposition 2 is in Section 3. However, its proof is in Appendix E.2, whose title is Proofs for Section 2.\n\n\n\u3010Quality\u3011\nThis paper claimed at the beginning of Section 3 that the monotone parameterization increases the expressive power of IGNNs. However, I wonder how this claim is supported. If I understand correctly, the superiority of the monotone parameterization came from two points (1) $G$ is positive definite, and (2) $W$ can represent any matrix with eigenvalues less than 1. However, these two points have been achieved in the original IGNN. Therefore, I want to clarify this point.\n\nRegarding the empirical evaluation, I would like to clarify how the authors chose the experiment setups. For example, I have the following questions:\n- In Figure 3, IGNN-D5 is used as the baseline in addition to IGNN, while Figure 4 uses only IGNN.\n- How were the N and D parameters determined?\n- The synthesis dataset uses the orthogonal parameterization (Section 5.1), whereas the real dataset the monotone parameterization (Section 5.2). How were these parameterizations chosen?\n\nOne research question was that as $\\lambda_1(W)$ approached 1, the convergence of the fixed-point calculation became slower while the prediction accuracy increased. The result in Figure 6 and Figure 7 effectively answered this problem to some extent. Specifically, IGNN and MIGNN-N1D1 achieve similar accuracy, but the elapsed time and number of iterations required for the computation are reduced. It implies that MIGNN has improvements on this problem.\nOn the other hand, in Section 5.2, the large value of $\\lambda_1(|W|)$ and the better accuracy simultaneously happened to MIGNN. Therefore, it is not known from these results alone whether the MIGNN solved the above problems in this setting.\n\nRelated to the above, the interpretation of the quantity of $\\lambda_1(|W|)$ looks somewhat inconsistent. Specifically, in some places (e.g., Section 3.1, Section 5.2), large $\\lambda_1(|W|)$ is interpreted favorably, while smaller $\\lambda_1(|W|)$ was considered good in other places (e.g., Section 4.1.1, Section 5.3). Therefore, I want to clarify what the authors think is the appropriate scale for $\\lambda_1(|W|)$.\n\n\n\u3010Novelty\u3011\nIf I understand correctly, the improvements proposed in this paper are as follows:\n1. Orthogonal parametrization of the weight matrix $W$\n2. monotone parameterization for $W$\n3. Anderson accelerated operator splitting scheme to compute fixed points of IGNN\n4. Use of higher order powers of adjacency matrix $A$ as $G$\nAs pointed out in this paper, modifications similar to the first and third points were observed in other NN models (RNN and FNN). On the other hand, the second point is a newly proposed parameterization based on the theoretical results of this paper (Proposition 1). The fourth point is an improvement method used in many GNNs (e.g., N-GCN [Abu-El-Haija et al., 20]).\n\n[Abu-El-Haija et al., 20]: http://proceedings.mlr.press/v115/abu-el-haija20a.html\n\n\n\u3010Reproducibility\u3011\nThe experimental code is provided in almost complete form. Therefore, I think we can check the details of the experiments, although I have not run this experimental code by myself.",
            "summary_of_the_review": "Theoretical part is clearly written and sound. Also, it is accessible to those who are not familar with the monotone operator theory.\nIf I understand correctly, most improvements are the application of existing techniques for computing fixed points by iterative algorithms, except for the monotone parameterization. Also, I have questions about the design of experiment settings and the interpretation of the experiment results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper483/Reviewer_Y8Ag"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper483/Reviewer_Y8Ag"
        ]
    }
]