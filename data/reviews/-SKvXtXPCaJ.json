[
    {
        "id": "0zeIg21Ujx",
        "original": null,
        "number": 1,
        "cdate": 1666635084831,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635084831,
        "tmdate": 1666635084831,
        "tddate": null,
        "forum": "-SKvXtXPCaJ",
        "replyto": "-SKvXtXPCaJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3985/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper solves the problem of  learning for control by learning a inverse mapping between action sequence and states sequence.  To handle the distribution shift, the authors proposed  a general algorithm called interactive inversion which interactively  learn the inverse mapping under the current policy, then apply it to the desired output to obtain a new policy for it. Under certain assumptions, the proposed algorithm is proved to converge to the desired mapping theoretically.  By applying the iterative inversion, the authors further present a IT-IN algorithm for learning actions given any desired state sequence. Through experiments, the authors show the proposed algorithm has an improved performance on imitating diverse behaviors compared to reward based RL methods.",
            "strength_and_weaknesses": "This paper presents a new viewpoint of the problem of learning control which directly mapping state sequence to action sequence. \nCompared to the RL methods, the proposed algorithm does not need a pre-defined reward function which could be hard for many real applications. The analysis looks valid to me. And enough experiments are presented to support the statement in this paper. \n\nThe assumptions raised in this paper are pretty strict to satisfy in real applications. Like stated in section 3, the conditions can be interpreted as F can be well approximated by a linear function. This means the mapping from state sequence to the action sequence should be close enough to a linear function in order to have convergence guarantee.  Do these assumption hold in the examples under the experiment section? \n\nI would also question the size of the steering dataset for more complex tasks with more intents. In order to learn the mapping function, it is necessary  to have the steering dataset sufficient enough to capture all the sequences needed for accomplishing tasks. Could you give more explanation about how to construct such dataset?",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method in this paper is novel. The idea makes sense to me. This paper is well written and proofs are easy to follow.",
            "summary_of_the_review": "The proposed method is shown to perform significantly better than other works. Sufficient proofs are provided to support the statement in this paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3985/Reviewer_VhZe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3985/Reviewer_VhZe"
        ]
    },
    {
        "id": "LsPGBIpRR8",
        "original": null,
        "number": 2,
        "cdate": 1666807271345,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666807271345,
        "tmdate": 1669921096989,
        "tddate": null,
        "forum": "-SKvXtXPCaJ",
        "replyto": "-SKvXtXPCaJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3985/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents iterative inversion, an approach for learning control from video demonstrations (without actions) and online interaction. The approach works as follows:\n- Start with a random exploration policy\n- Train an inverse dynamics model on the online data mapping a sequence of states to a sequence of actions\n- Apply the inverse dynamics model to the demo trajectory to get the reference action sequence\n- Update the policy to match the reference action sequence\n- Repeat while the policy improves+explores until it matches the reference video demo.\n\nThere are a few additional tricks to stabilize thing but that is the essence of the method. The inverse dynamics modeling is done in a seq-to-seq fashion using a VQ-VAE image encoder and a transformer sequence model. \n\nIn experiments, they first confirm that the method indeed steers behavior toward the demos effectively in some toy tasks, then compares to some RL methods on image-based Hopper/Reacher.\n",
            "strength_and_weaknesses": "*Strengths*\n\n- This is an interesting approach/problem. Learning to match a video demonstration through online RL is indeed an important and well studied problem, and the paper presents an interesting take on it.\n\n*Weaknesses*\n\n- The first major limitation of the work is the novelty of the proposed method. There is a vast literature of works that learn inverse dynamics models for learning to generate actions from observations, from Schaal et al. to more recent works like Ghosh et al. and Zheng et al. So fundamentally the idea of supervised learning of an inverse model to match demonstrations is not new, nor is the idea of using such an inverse model iteratively online during agent interaction (while GCSL trains a goal conditioned policy, extending the policy to follow the sequence of goals in a demonstration is a trivial extension). The main thing that appears to be new about the proposed approach is the method of training the inverse model, specifically through a VQ-VAE and a transformer that maps the full sequence of states to a sequence of actions directly (rather than operating on single s_t, s_t+1 pairs). I actually think this part of the contribution is quite interesting and novel, and could make sense as an improvement over prior works. I would advise the authors to emphasize this as the main contribution of the work. \n\n- The second issue is with the experiments. While the toy experiment showing how the method works is nice, there really isn't a comprehensive comparison to the relevant baselines. Even the RL baselines used are only tangentially relevant to the proposed method. Fundamentally the proposed method is an inverse RL method (demonstrations --> online interaction to produce a policy that matches the demos). Therefore the work should compare to the state-of-the-art inverse RL methods like GAIL from observation or the various extensions like Berseth et al. and Reddy et al, as well as methods like GCSL (Ghosh et al.) on more challenging benchmarks like vision-based robotic manipulation. \n\nBerseth et al. Towards Learning to Imitate from a Single Video Demonstration. 2019. \nReddy et al. SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards. ICLR 2020.\nSchaal et al. Real-Time Robot Learning With Locally Weighted Statistical Learning. ICRA 2000. \nGhosh et al. Learning to Reach Goals via Iterated Supervised Learning. ICLR 2021.\nZheng et al. Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories. 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is written clearly. Novelty is limited.",
            "summary_of_the_review": "Overall this is an interesting paper tackling an interesting problem. However, the work needs to better position its contribution w.r.t the many similar works in the field, and expand its experiments to more challenging domains and consider more relevant baselines. \n\n--- Post rebuttal\nThe rebuttal has addressed most of my concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3985/Reviewer_2R3c"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3985/Reviewer_2R3c"
        ]
    },
    {
        "id": "3AIdpZWMLn",
        "original": null,
        "number": 3,
        "cdate": 1667881395289,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667881395289,
        "tmdate": 1667881395289,
        "tddate": null,
        "forum": "-SKvXtXPCaJ",
        "replyto": "-SKvXtXPCaJ",
        "invitation": "ICLR.cc/2023/Conference/Paper3985/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors try to solve the problem of inverting a deterministic dynamics model when query access to the said model (being able to query the next state given the current state and the current action). The performance of the trained inverse model is evaluated on a distribution of reference state trajectories.\n\nThe core idea proposed is to assume that the dynamics is invertible, and we try to find the inverse of the dynamics which we can apply to the observed state trajectory to find its actions. Specifically the mapping from a sequence of actions to a sequence of states $F : A^n \\rightarrow S^{n-1}$ is invertible so $F^{-1}$ is well defined. Then we learn $F^{-1}$ with a parametric model $G_\\theta$ such that $G_\\theta \\circ F \\approx F^{-1} \\circ F = \\text{id}$. Crucially we assume access to a \"reasonable\" loss over the action sequences which we use to train $G_\\theta$. The trajectories on which we optimize $G_\\theta$ is a mixture of samples from the distribution of reference trajectories and trajectories from randomly interacting with $F$.",
            "strength_and_weaknesses": "Strengths:\n1. The basic approach (algorithm) seems reasonable and novel. Instead of access to a loss measuring the similarity of state trajectories, it assumes access to a loss measuring the similarity of action sequences during training. However, it fails to explicitly discuss the connection and assumption between the two distances (over states and over actions). (see Weakness1)\n\nWeaknesses:\n1. The formal problem formulation and the exposition has gaps. If the ultimate goal is to reproduce the reference state trajectory (as suggested by the evaluation protocol), then we are assuming that low action-action loss implies low state-state loss, correct? I imagine that differences in early actions in a sequence would have a larger impact than the differences in later ones. How do MSE over states and over actions relate to each other in your experiments?\n1. The proposed problem setting does not seem adequately motivated. Why do we want to learn an inverse model that is accurate over a small (in comparison to all possible trajectories) set of state trajectories? In learning dynamics via interactions, a key challenge is the difficulty to access many states (unlike in IID setting) with a simple policy, e.g., uniformly random policy. Are the reference trajectories helping us access such region of the state space?",
            "clarity,_quality,_novelty_and_reproducibility": "The exposition of relevant works may be enhanced by more direct comparison to imitation learning which seems to better match the problem setting of this work (for the absence of rewards). It should also be compared to other works on learning dynamics. \n\nI also think that it might help motivate the proposed problem setting by providing some concrete motivating examples/application. If I am not mistaken, the experiments have reference trajectories whose actions we know but choose to forget making the experiments feel a little artificial. As you suggested with the intent/goal-conditioned experiments, how should one use the trained inverse model to control when we need to provide the next state?\n\nDo you think that the assumption of bijectivity on $F$ is too restrictive for applying your approach? In particular, consider $T=1$ in (2), any different pairs of $(s_0, s_1)$ need to have different actions $a_0$.",
            "summary_of_the_review": "I like aspects of the proposed approach and the problem of learning inverse dynamics model. But the presentation could be made clearer. In particular, the problem should be formulated more clearly. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3985/Reviewer_9eeh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3985/Reviewer_9eeh"
        ]
    }
]