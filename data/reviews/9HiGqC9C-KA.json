[
    {
        "id": "NE4GiW1EHbz",
        "original": null,
        "number": 1,
        "cdate": 1666555167717,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666555167717,
        "tmdate": 1666555167717,
        "tddate": null,
        "forum": "9HiGqC9C-KA",
        "replyto": "9HiGqC9C-KA",
        "invitation": "ICLR.cc/2023/Conference/Paper3300/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes SIMPLEKT, a simple but tough-to-beat KT baseline that is simple to implement, computationally friendly and robust to a wide range of KT datasets across different domains. ",
            "strength_and_weaknesses": "Strength\n\n1. The paper is well written and well presented.\n\n2. The source code is publicly avaiblale.\n\n3. The experimental results are good, comparing to existing baselines.\n\n\nWeakness\n1. The technical contributions of this work are very limited, simply adopting the tranformer model for their tasks. Overall contributions are below the bar of ICLR in my mind.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the motivations and ideas are clear in this work.\n\nQuality: The techniques of this work is limited, directly using Transformer to solve their tasks.\n\nNovelty: The novelty of this work is not high,\n\nReproducibility: The reproducibility of this work is good, since it is simple and source code is available at this time.",
            "summary_of_the_review": "In summary, the topic in this work is interesting, however the overall contributions of this work are not very enough for ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3300/Reviewer_vuA2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3300/Reviewer_vuA2"
        ]
    },
    {
        "id": "wZb61jaDAwa",
        "original": null,
        "number": 2,
        "cdate": 1666671134342,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671134342,
        "tmdate": 1666671134342,
        "tddate": null,
        "forum": "9HiGqC9C-KA",
        "replyto": "9HiGqC9C-KA",
        "invitation": "ICLR.cc/2023/Conference/Paper3300/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work aims to address inconclusive findings in the literature about which models perform best for Knowledge Tracing (KT). In this paper, the authors present a baseline which convincingly outperforms most published work despite being simpler in nature. ",
            "strength_and_weaknesses": "STRENGTHS\n- This work gives a very comprehensive review of dozens of work in the KT space and how they work.\n- By comparing against 12 previous models, this weighs in on disagreements in the literature about model comparisons.\n- Demonstrating that such a simple model can outperform many deep learning-based approaches provides meaningful insight into the nature of the task, which can inspire future work and model designs.\n\nWEAKNESSES\n- I could not tell whether the 12 comparison models had publicly available code or were reimplemented by the authors (or something else). Clarification would be helpful.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The clarity for this work is fair. It could improve with some rewrites with an eye towards the reader's experience:\n    - Define \"KC\" in the abstract before referencing it. It is currently not defined until the Introduction.\n    - The first paragraph of page 2 has a heavy mix of citations and prose. Whereas there is not much prose in the citation-heavy of paragraph of page 1 (hence being able to get away with it the first time), it is much harder to read the page 2 paragraph. It could be helpful to restate/recap the important claims to ensure they do not get get missed by the reader.\n    - Although Figure 1 is very helpful for generally understanding the KT task, it is hard to follow the distinction between D1 (datasets containing info about both q's and KCs) and D2 (datasets containing info of *either* q's or KCs). One example of an instance from each type would be helpful to see.\n\nQuality: This work is high-quality. By comprehensively surveying the literature, the authors identify a gap in approaches, which they fill using a simple model. This simple model employs tactics which are well-suited for the task (eg attention, question-difficulty info, etc) to demonstrate that many previous approaches were not as effective as once understood, despite their bells and whistles. This is a very useful finding, and it is done by measuring across 7 datasets.\n\nNovelty: The novelty of this approach is, on the one hand, low, because it uses a simple approach. However, it demonstrates why a \"simple\" model is nonetheless able to perform better than nearly a dozen \"more novel\" models.\n\nReproducibility: This work is highly-reproducible. It provides a link to an anonymized repo with code and one of the datasets.",
            "summary_of_the_review": "In this paper, the authors present a baseline which convincingly outperforms most published work despite being simpler in nature. The baseline compares against 12 other models -- such a comprehensive comparison is itself valuable to the field. Additionally, the works conducts an ablation study, which provides important insight into understanding what makes the simple model nonetheless effective (e.g. how much is from the question-difficulty modeling?).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3300/Reviewer_nkiF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3300/Reviewer_nkiF"
        ]
    },
    {
        "id": "f_rcKTJNNLs",
        "original": null,
        "number": 3,
        "cdate": 1666672195587,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672195587,
        "tmdate": 1666672195587,
        "tddate": null,
        "forum": "9HiGqC9C-KA",
        "replyto": "9HiGqC9C-KA",
        "invitation": "ICLR.cc/2023/Conference/Paper3300/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper builds a simple yet strong baseline, named SimpleKT, for knowledge tracing (KT). The newly introduced baseline is motivated by the Rasch model in psychometrics to capture the individual differences in questions of the same KC. It only contains a few simple operations, i.e., element-wise product, addition, and the ordinary dot-product attention, which is simple to implement. Surprisingly, the proposed simple method can beat most of the 12 representative baselines on 7 public datasets.",
            "strength_and_weaknesses": "Strengths:\nA simple-to-implement and strong baseline is constructed for the knowledge tracing (TK) task, which can facilitate future KT research.\n\nWeaknesses:\nI am not familiar with the knowledge tracing task. Thus, I cannot provide useful comments and suggestions to improve this paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is easy to understand even for a reader without much background on the knowledge tracing task.\n",
            "summary_of_the_review": "A simple-to-implement and well-motivated baseline model is presented for the Knowledge tracing task, which wins most of 12 existing methods on 7 public datasets. The newly constructed baseline can facilitate future research on KT.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3300/Reviewer_EgtS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3300/Reviewer_EgtS"
        ]
    }
]