[
    {
        "id": "jf9EMUVAZIZ",
        "original": null,
        "number": 1,
        "cdate": 1665612308227,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665612308227,
        "tmdate": 1665612308227,
        "tddate": null,
        "forum": "3TfSOxiRiFH",
        "replyto": "3TfSOxiRiFH",
        "invitation": "ICLR.cc/2023/Conference/Paper549/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper hypothesizes that systematic generalization is fundamentally at odds with the tendency of deep learning models to share sub-modules. It provides both a theoretical description and derivation of the issue as well as supporting experiments with several deep learning architectures.",
            "strength_and_weaknesses": "Strengths: the hypothesis advanced by this paper is new and interesting; it deserves to be developed further.\n\nWeaknesses:\n- The paper is not very well written; see below for some examples where the writing should be improved.\n- The propositions and theorem are trivial.\n- The vision experiments seem quite unnatural, since images from different datasets are averaged (see Section 3.2). Also the text experiments are not particularly natural (they concatenate two unrelated sequences and ask to predict both labels).\n\nA general question that might be worth addressing: the shape of the regions is not taken into account? For instance, to prove Proposition 1 (using Assumption 2) you can simply merge regions, but it might be unnatural to merge them, given the geometry.\n\nExamples of unclear parts in Section 2.1:\n- \"$Y$ contains $K$ factors $Y_1$, . . . , $Y_K$\" is not clear. Does it mean that $Y = Y_1 \\times \\dots \\times Y_K$? But this would be at odds with the (unclear) statement \"which can be entangled\".\n- Is $X_\\text{train}$ the (ordered) sequence of inputs or is it a set that contains all the input data? (same for $Y_\\text{train}$ and test)\n- \"The values for each factor $i$ are included in the training output\" is also unclear\n- \"A model $f$ maps input $X$ to the prediction of output $f (X)$\": so is $X$ the set of all inputs or a single input?\n\nExamples of unclear parts in Section 2.2:\n- \"deep learning more or equally prefers $f$ over $g$\" -> \"deep learning prefers $f$ over $g$ more or equally\" (a bit more clear in my opinion)\n\nExamples of unclear parts in Section 3.1:\n- \"$Y_1$ is chosen from all possible labels\": what does this mean? That $Y_1$ is the set of all labels?\n- The two datasets share the same input space $X$?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the general idea of the paper is easy to understand but the details are not explained in a very clear way.\nNovelty: as far as I know, the hypothesis introduced in this paper is new.",
            "summary_of_the_review": "This paper introduces an interesting and probably novel idea but it does not a good job at developing it. I believe it does not meet the ICLR bar.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper549/Reviewer_4tfA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper549/Reviewer_4tfA"
        ]
    },
    {
        "id": "o7YUn2fisf",
        "original": null,
        "number": 2,
        "cdate": 1666277251664,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666277251664,
        "tmdate": 1669027898206,
        "tddate": null,
        "forum": "3TfSOxiRiFH",
        "replyto": "3TfSOxiRiFH",
        "invitation": "ICLR.cc/2023/Conference/Paper549/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper investigates systematic generalization of multi-label classifications in settings where the input space is shared between the different labels. The authors suggest that deep learning models are biased towards feature reuse, which conflicts with systematic generalization to new combinations of known classes. They explore this hypothesis with theoretical analyses under toy assumptions, and empirical experiments across a variety of architectures. Generally, they find that more sharing of features reduces generalization accuracy in their settings.",
            "strength_and_weaknesses": "Strengths:\n* The questions posed are interesting.\n* I appreciate the breadth of architectures considered, especially sharing different numbers of layers.\n\n\nWeaknesses:\n\nThe way the paper is presented fundamentally ignores the No Free Lunch theorem [e.g. Adam, 2019]. There is no system that can generalize perfectly on every task and training dataset\u2014there cannot be a conflict between an architecture class and systematic generalization writ large. We have to ask the question of how the inductive biases of the model class fit the class of tasks we are interested in solving.\n* DL researchers are well aware of the feature sharing bias\u2014it forms the basis of auxiliary task training and/or pretraining methods, as the authors note. The reason such methods tend to improve generalization is because sharing features is useful on real-world datasets. For example, the input features learned solve masked language modeling tasks empirically improve systematic generalization performance substantially even on tasks like SCAN and CFQ [Furrer et al., 2020]. There are even theoretical accounts of why feature sharing can improve generalization in the presence of noise [e.g. Lampinen et al, 2019]. \n* The datasets used in the paper are therefore cleverly created to make feature sharing an actively harmful strategy. But to do so, the authors rely on essentially adversarial dataset design, where they combine input stimuli in very unnatural ways (averaging images, or concatenating completely unrelated pieces of text), and then enforce extremely strong correlations between these inputs at train time, which are completely reversed at test time. There is no reason given to think that this process has anything to do with any real-world data generating process.\n* Therefore, I would challenge the authors to demonstrate *real-world tasks and datasets*, not artificially, adversarially created ones, in which their observations apply.\n* Otherwise, it seems to me that feature sharing is a *feature, not a bug* of deep learning. Nobody has ever claimed that deep learning is capable of generalizing systematically in every task anyone can come up with\u2014that would violate the NFL theorem. But I\u2019d argue that the DL family is empirically the most successful system for generalizing on real world datasets, in part because of feature sharing.\n\nArchitectures and training paradigms:\n* \u201cWe choose a layer and duplicate the following layers, keeping the number of all hidden nodes in each layer if feasible\u201d \u2014 it is not clear to me whether this means that each \u201cbranch\u201d of the architecture has the same number of nodes as before, or half the number of nodes. If the former, the number of parameters will be larger in networks that split earlier, thus confounding the comparison (since overparameterized models tend to generalize better). \n* More generally, it would be interesting to see the impact of parameterization on these effects\u2014one might expect somewhat less feature sharing in wider networks, for instance, though it\u2019s unclear how strong the effect would be.\n* And it would be interesting to see the effect of methods like dropout [Srivastava et al., 2014] or mixup [Zhang et al., 2017] which are known to improve generalization.\n\n\nReferences\n------------\n\nAdam, Stavros P., et al. \"No free lunch theorem: A review.\" Approximation and optimization (2019): 57-82.\n\n\nFurrer, D., van Zee, M., Scales, N., & Sch\u00e4rli, N. (2020). Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. arXiv preprint arXiv:2007.08970.\n\nLampinen, A. K., & Ganguli, S. (2019). An analytic theory of generalization dynamics and transfer learning in deep linear networks. In International Conference on Learning Representations.\n\nSrivastava, Nitish, et al. \"Dropout: a simple way to prevent neural networks from overfitting.\" The journal of machine learning research 15.1 (2014): 1929-1958.\n\nZhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017). mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper could be improved. For example:\n* Showing examples of the task stimuli in the paper\u2014particularly the visual ones\u2014would I think help to emphasize how unnatural the tasks are.\n* The data preparation could be rewritten to be clearer, by first specifying that the data generating process goes from sampling a pair of labels to sampling the corresponding input.\n* Split architecture details were unclear to me (noted above).\n\nThere is some originality and quality if the above weaknesses are addressed.",
            "summary_of_the_review": "See comment above for my post-response update. I am updating my score in accordance with the improvement in the paper, but I am concerned that key issues still remain unresolved, and not discussed with enough nuance.\n\nOriginal review\n-------------------\n\nIf this paper were completely rewritten\u2014to describe the experiments as identifying a particular class of problems in which DL architectures seem not to generalize systematically due to feature-sharing rather than a built-in conflict\u2014I believe it could be an acceptable paper in some venue. If, in addition to that, the authors were to identify real-world, non-adversarial datasets where their observations bear out, and perform some of the architecture/training experiments suggested above, I would consider it a strong paper for NeurIPS. As it is, I think it is misleading.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper549/Reviewer_hir1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper549/Reviewer_hir1"
        ]
    },
    {
        "id": "kQxQWHww5Q",
        "original": null,
        "number": 3,
        "cdate": 1666815857296,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666815857296,
        "tmdate": 1669921289515,
        "tddate": null,
        "forum": "3TfSOxiRiFH",
        "replyto": "3TfSOxiRiFH",
        "invitation": "ICLR.cc/2023/Conference/Paper549/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates systematic generalization in deep neural networks. Systematic generalization here refers to the ability of an algorithm to produce outputs that were not observed during training time. A potential reason for this is postulated: the lack of systematic generalization in deep neural networks is due to function sharing, i.e. that each layer in the network uses a common representation from the previous layer. Experiments show that networks with fewer shared intermediate layers exhibit a greater degree systematic generalization than those with more shared layers.",
            "strength_and_weaknesses": "Strengths\n- The broad aim of investigating stronger forms of generalization that move beyond the i.i.d. case is interesting.\n- The experimental observation that having fewer shared layers leads to better systematic generalization holds across multiple diverse architectures and datasets. \n- As far as I know, investigating function sharing as a reason for lack of systematic generalization is a novel approach.\n\nWeaknesses\n- The paper is not written very clearly. Sections that are difficult to understand include: the mathematical notation (e.g. writing that $f$ is a \"model\" but not explaining that this is simply a mapping from the input space to output space), the experiment section (particularly how the labels were generated and what the different evaluation metrics mean), and the discussion section.\n- Some intuitions are claimed but not supported by adequate evidence: that deep neural networks prefer to learn a simple function and combine with previously learned functions, and that neural networks greedily learn functions in order of simpler to more complex.\n- The experiments are not complete. Some relevant but missing pieces of information include training accuracy and computation time for the various levels of sharing. On a related note, the motivation for including the test set and random set accuracy metrics is unclear.\n- It is not clear how the knowledge introduced by this paper can be effectively used to improve systematic generalization. Training a multitude of independent networks, one for each underlying factor, does not seem like a practical course of action due to storage and computation constraints.",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned above, there are some significant issues with clarity, but I believe it is possible for them to be resolved in an updated version of the paper. The paper also seems novel enough in that it looks at function sharing as a potential underlying cause for lack of systematic generalization. The quality is below average, with issues including incomplete experimental evaluation and unsupported claims (e.g. greedy learning of functions and the mechanisms underlying function sharing). I also found the design choice to average inputs from two separate datasets to be unconventional.\n\nOne point that seems quite relevant but not addressed by the paper is the extent to which the phenomenon in Figure 2 is caused by a softmax activation, which assumes that classes are mutually exclusive. This seems to be an alternate reason that the case in Figure 2(b) does not arise, since this region occupied by the orange dot would be a region of low confidence and thus the network would be incentivized to sharpen the decision boundaries. Could this possibly be resolved by assuming a multi-output loss function, e.g. multiple sigmoids? Regarding reproducibility, code is included and thus reproducing the results does not seem to be a major barrier.",
            "summary_of_the_review": "After rebuttal: The paper has been improved by the inclusion of real-world experiments. I would echo the other reviewers in suggesting that these move to the main paper. As pointed out by Reviewer hir1, a more complete discussion about when function sharing is likely to be helpful seems necessary. Expanding on why function sharing occurs as started in Appendix C would also be useful. I have updated my score accordingly. \n\n---\n\nOverall, there are some potentially interesting ideas in this paper, but the clarity, unsupported claims, and incompleteness of the experiments are somewhat significant issues. The contribution of this paper is on the more incremental side and consists primarily of showing that sharing fewer layers can improve systematic generalization. However, it is not clear how to take these insights and apply them to solve out-of-distribution detection on real-world problems.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper549/Reviewer_e3dk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper549/Reviewer_e3dk"
        ]
    },
    {
        "id": "OPHp_0G0jGx",
        "original": null,
        "number": 4,
        "cdate": 1667149988736,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667149988736,
        "tmdate": 1667149988736,
        "tddate": null,
        "forum": "3TfSOxiRiFH",
        "replyto": "3TfSOxiRiFH",
        "invitation": "ICLR.cc/2023/Conference/Paper549/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper hypothesise that function sharing is one of the reasons why deep learning models can\u2019t perform systematic generalization. The paper demonstrate that as the degree of parameter sharing increases, the systematic generalization drops. From the practical stand point, the papers argues for sparsity in models that somewhat learn symbolic functions (in term of disentangling feature attributes). Although it\u2019s not touched upon but I believe the paper can be seen from modularity perspective where each module encampasses a particular underlying function, describing certain factor of the input. ",
            "strength_and_weaknesses": "### Strengths\n- The paper provides empirical evidence of why parameter sharing (function sharing) leads to performance drop in systematic generalization for deep learning models.\n- I like the problem space and believe that the authors are on to something tangible here but the lack of rigour in analysis didn't convince me.\n\n### Weaknesses\n- I have issues with problem formulation and writing. Some details of the work are not framed correctly and it\u2019s hard to understand it since no context from previous literature is provided while introducing and explaining new concepts.\n- Eg. what is implied as functions in deep neural networks? Is it an individual parameter or a set of parameters?\n- What does the three equations at the end of sec 3.1 refer to?\n- There is no clear description of the dataset. I can see that it contains factor but what are those factors? How are those factors combined?\n- On the same note, could you please provide description of the datasets and models used, separately?\n- The results section just provide information on the models tried. No information on training, and test, train splits of dataset provided.\n- The author try to ablate different model architectures, however they use different datasets across those architectures thus it\u2019s hard judge if the results are consistent across those architectures or the differences arise from the difference in datasets.\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the writing needs to be improved. It was not easy to follow all the details:\n- From the start, it was difficult to understand what a function means in deep learning context? To understand it better w.r.t machine learning literature, can \u201cfunction sharing\u201d be reframed in terms of modularity?\n- Figure 2 is hard to understand. Eg. what does the term \u201cfunction\u201d refers to in the diagram?",
            "summary_of_the_review": "This work need major re-writing as most of the concepts were not framed correctly. Moreover, they are some shortcomings in the evaluation section as I explained in the weaknesses section. I believe there is some value in this work, but it needs written with clarity.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper549/Reviewer_w8Ad"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper549/Reviewer_w8Ad"
        ]
    }
]