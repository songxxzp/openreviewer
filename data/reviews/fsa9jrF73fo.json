[
    {
        "id": "CLoth-9vrK",
        "original": null,
        "number": 1,
        "cdate": 1666612407687,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612407687,
        "tmdate": 1666635687976,
        "tddate": null,
        "forum": "fsa9jrF73fo",
        "replyto": "fsa9jrF73fo",
        "invitation": "ICLR.cc/2023/Conference/Paper3850/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to refine orthogonal bases, as commonly used for \"proper orthogonal decomposition\" (POD) algorithms with a stochastic, data driven optimization. The paper focuses on a class of reversible, circulation-based flow descriptions. The improvements in terms of reconstruction errors of the optimized basis are evaluated for Taylor vortices and different rising plume variations.\n",
            "strength_and_weaknesses": "As strengths of the paper I see the clear reductions in terms of reconstruction errors compared to regular POD approaches. This is clearly demonstrated, and it's neat to see how much difference this fine tuning step can make. Also, I'm not aware of existing works using this  optimization procedure.\n\nAs weaknesses, I primarily see the narrow scope of the work, and its somewhat detached nature from learning approaches. The work relies on a Eulerian DEC discretization for reversible flows, instead of targeting general flow-like, divergence free motions. These are not too widely used, as far as I can tell. In addition, the focus on improving POD methods further limits the scope of applications. POD methods are popular as baselines, and for special applications, but not really state of the art. In addition, the clear lack of comparisons with learned baselines highlights that the focus on basis constructions is not a big topic at ICLR. From an ICLR paper I would have expected links and evaluations of how this method fares in the context of other learning-based approaches. This is mentioned in the outlook, but unfortunately missing in the current version.\n\nI would also recommend to better justify the focus on reversible flows. As mentioned above they're not widely used, and not very relevant for game applications. After reading about games as targeted areas of applications in the introduction I was expecting the paper to take a different direction.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the writing of this paper could be significantly improved. First of all, I found the title of the paper misleading: there is very little \"learning\" going on. Rather, as explained later on in the text, the paper focus on a data-driven fine tuning of a basis. \n\nAlso, the related work is a strange read considering how the proposed algorithm works. Lagrangian approaches are discussed in detail, while the large body of Eulerian approaches for learning fluids is mostly left out. E.g., the divergence-free learning from Tompson et al. 2017, or the learned representations from the DeepFluids Kim et al. 2019 seem much more relevant than graph networks.\n",
            "summary_of_the_review": "That being said, I think the direction of the paper is interesting. Improved orthogonal bases could definitely find applications, despite potentially being not up-to-par with learned representations. In its current state, I think the work is not really suitable for ICLR, but potentially other more CFD focused venues. For ICLR, I would expect a stronger focus on learning, and actual demonstrations that the proposed method is up to par with the current state of the art for learned fluid simulations.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3850/Reviewer_Zt39"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3850/Reviewer_Zt39"
        ]
    },
    {
        "id": "jdrvzijBlc",
        "original": null,
        "number": 2,
        "cdate": 1666772343047,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666772343047,
        "tmdate": 1666772343047,
        "tddate": null,
        "forum": "fsa9jrF73fo",
        "replyto": "fsa9jrF73fo",
        "invitation": "ICLR.cc/2023/Conference/Paper3850/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a machine learning approach to identify model-reduced dynamical systems. The main idea is to use stochastic (Riemann) optimization which minimizes the expected trajectory-wise reduction error over a distribution of initial conditions. This is achieved by formulating the fluid dynamics problem as an invertible state transfer function. \nThe paper identifies fluid velocities as a point on a manifold where the reduced trajectory depends on the choice of the subspace of fluid velocities. This map from subspaces to reduced trajectories is shown to be globally differentiable.\nThe paper compares against Proper Orthogonal Decomposition as an example of data-driven sub-manifold model.\n\n",
            "strength_and_weaknesses": "Strengths:\n-\tLearning reduced fluid dynamics is a very relevant research direction\n-\tThe idea is fundamentally interesting, i.e. to use automatic differentiation to optimize numeric algorithms which have pre-calculated parameters that are used as initialization.\n-\tI appreciate the time reversible fluid dynamics section, and the transition to reduced model optimization. \n-\tThe results over the baseline look quite convincing, where it is however hard to place the baseline.\n\nWeaknesses:\nMy general comment is that it is hard to place this work both wrt to existing numerical methods, but also with respect to neural solvers, neural surrogates. That makes it really tough to properly weigh the pros and cons wrt to e.g. Proper Orthogonal Decomposition, Dynamic Model Decomposition, but also operator learning methods or PINNs. Furthermore, I would like to see a test against a model which does not preserve time reversibility. \n-\tThe only comparison is against Proper Orthogonal Decomposition (POD) which as stated in the introduction \u201cis flawed in that it ignores the temporal dependence of state variables\u201d. Naturally the question is for example how Dynamic Model Decomposition (DMD) is doing for the problems.\n-\tAlso from a Deep Learning perspective, e.g. Figure 4 could be learned by an operator learning that takes the initial state as input and outputs the state of the system after e.g. every 100th step. On the other hand, if done with e.g. PINNs this could be performed completely without training data. Just using the boundary conditions for the loss and the equation for the residual loss. It would be really interesting to have a comparison wrt to speed and accuracy. \n-\tPyTorch and more specifically the automatic differentiation capability of Pytorch is used as an optimization tool. It is however hard to judge from the paper what exactly the parameters are that are optimized. How many parameters are optimized for the different problems? Algorithms 1 and 2 in the appendix help a great deal but it took me a great deal to scroll back and forth. Is it possible to place the algorithms in the main paper and write refer to the important parts in Section 3,4?\n-\tThe loss should be a bit more central to the writing of the paper. In Fig 1 the most important components are the reduced-order fluid model and the trajectory-wise discrepancy loss. However, Sections 4.3 takes a huge part of the main paper and is actually hard to follow. In my opinion the readability of Section 4 can be improved by focusing more on the relevant parts and not let the reader figure out what they are. \n-\tWhy is e.g. JAX not used for optimization? Did the authors consider that? \n-\tHow do optimization time vs inference time relate between the different models?\n-\tHow do the individual components of the algorithm relate to the performance, e.g. how does the performance change for other loss terms? \n-\tIs it correct that for the first benchmark a single trajectory is used, whereas in the second benchmark more trajectories are used for optimization? On which trajectories is the testing done afterwards? Shouldn\u2019t there be more than just one trajectories to get a better performance estimate?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper introduces quite complex topics, it is somehow hard to understand what is novel and what does already exist. Algorithms help a lot to understand the approach",
            "summary_of_the_review": "The idea and the topic in general are quite relevant.  For the current version of the paper, it is however hard to place this work both wrt to existing numerical methods, but also with respect to neural solvers, or neural surrogates. That makes it really tough to properly weigh the pros and cons wrt to e.g. Proper Orthogonal Decomposition, Dynamic Model Decomposition, but also operator learning methods or PINNs.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3850/Reviewer_J4j9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3850/Reviewer_J4j9"
        ]
    },
    {
        "id": "sxMcpymXGv",
        "original": null,
        "number": 3,
        "cdate": 1667177656017,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667177656017,
        "tmdate": 1667177656017,
        "tddate": null,
        "forum": "fsa9jrF73fo",
        "replyto": "fsa9jrF73fo",
        "invitation": "ICLR.cc/2023/Conference/Paper3850/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a method to learn the time-reversible, nonlinear dynamics of an incompressible fluid. For computational tractability, the authors learn a reduced model over a limited set of initial conditions with tensor precomputation. The authors show that their approach predicts solutions significantly closer to the full order method than conventional methods.",
            "strength_and_weaknesses": "The authors address the challenging problem of predicting the state evolution of high-dimensional, time-reversible, nonlinear fluid dynamic systems; however, I am uncertain how to understand the magnitude of the contribution. While they demonstrate strong performance against POD; as I understand, they also initialize with POD. I am also unsure why the authors choose to evaluate against a nondescript POD baseline rather than DEIM or another intrusive reduced order method noted in the related work. Moreover, as the authors mention, the method takes considerably longer to run than the baseline approach.",
            "clarity,_quality,_novelty_and_reproducibility": "*Quality:* The submission appears to be technically sound, and the authors seem forthcoming about the limitations of the work.\n\n*Clarity:* The submission is clearly written and well organized.\n\n*Originality:* The work adapts an existing approach for learning a reduced order model to the nonlinear setting, which appears nontrivial, though I am not certain of this. The work is well situated in the existing literature.",
            "summary_of_the_review": "The paper is clearly written and well organized, and appears to extend an existing method in a nontrivial way.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3850/Reviewer_eGGS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3850/Reviewer_eGGS"
        ]
    }
]