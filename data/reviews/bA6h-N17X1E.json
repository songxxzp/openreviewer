[
    {
        "id": "J9IZ1rvUtA",
        "original": null,
        "number": 1,
        "cdate": 1666224314042,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666224314042,
        "tmdate": 1666224314042,
        "tddate": null,
        "forum": "bA6h-N17X1E",
        "replyto": "bA6h-N17X1E",
        "invitation": "ICLR.cc/2023/Conference/Paper1470/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an unsupervised network (graph) embedding technique. Rather than using unstructured vectors to represent nodes, the embedding framework uses sums of Kronecker products of learnable vectors to represent the nodes. This is inspired by entanglement in quantum physics where similar representations are used to represent quantum states.",
            "strength_and_weaknesses": "Strengths:\n\nS1. The idea itself is interesting. Using compressed tensor formats in ML has been used for a variety of things like analyzing deep NN expressiveness and compression of NN weight layers. I haven't seen it in the context of node embedding before (although my knowledge of the network embedding literature is limited).\n\nS2. Intuitively, it seems like the proposed technique could lead to a substantial reduction in the number of parameters required for a given level of performance in a down-stream task.\n\nWeaknesses:\n\nW1. The main weakness is the experiments. The network reconstruction experiments are not convincing. It is not clear why the ability to reconstruct the original network is important for any interesting down-stream tasks. For example, simply storing the whole network itself only requires 2*(#edges) numbers, and will give perfect \"network reconstruction\" without any computational cost (so this is a \"trivial method\" that any other proposal ought to beat). By contrast, the proposed methods node2ket and node2ket+ require many more parameters than are required to store the original network, but give much worse reconstruction accuracy. Based on the provided parameters in the paper and appendix, I calculated the following parameter requirements for the datasets in Table 1.\n\nGR-QC:\n- Exact representation: 2*(#edges) = 28,992\n- node2ket: 8 * 5242 * 16 = 670,976 (23x more than exact representation)\n- node2ket+: (2229 + 2495 + 2656 + 2861 + 2978 + 3113 + 5242 + 5242) * 16 = 429,056 (15x more than exact representation)\n\nBC: \n- Exact representation: 667,966\n- node2ket: 8 * 10312 * 16 = 1,319,936 (2x more than exact representation)\n- node2ket+: (7122 + 7606 + 7989 + 8296 + 8559 + 8754 + 10312 + 10312) * 16 = 1,103,200 (2x more than exact representation)\n\nDBLP: \n- Exact representation: 99,254\n- node2ket: 8 * 12591 * 16 = 1,611,648 (16x more than exact representation)\n- node2ket+: (4874 + 5995 + 6944 + 7775 + 8372 + 12591 + 12591 + 12591) * 16 = 1,147,728 (12x more than exact representation)\n\nPPI: \n- Exact representation: 153,168\n- node2ket: 8 * 3890 * 16 = 497,920 (3x more than exact representation)\n- node2ket+: (3372 + 3429 + 3458 + 3505 + 3890 + 3890 + 3890 + 3890) * 16 = 469,184 (3x more than exact representation)\n\nSimilarly, the circular ladder only requires 54 numbers for exact representation, while node2ket uses 64 parameters.\n\nThe only case I saw where there was some compression was in Fig 2 (b) when node2vec uses C=2 and d=4. In this case, node2vec uses 0.12 the number of parameters required to store the original network.\n\nThe Link prediction experiment in Sec 5.3 is poorly explained. How is the prediction done? What do you mean by \"mixture of masked edges and noise edges\"? I couldn't find a clarification in the appendix.",
            "clarity,_quality,_novelty_and_reproducibility": "The text is sometimes a bit hard to follow due to lacking language, but this is not a huge issue. There are some typos, for example:\n\n- Page 6: Fisrt -> First\n- Page 7: abilitty -> ability\n- Page 7: in in -> in\n- Page 9: reconstruction reconstruction -> reconstruction\n\nThere are also multiple parts of the paper that are poorly explained:\n- First paragraph in Sec 4.1 is hard to follow.\n- In Sec 4.2: \"We compute the gradient and update TU embeddings directly without a complete procedure of forward propagation\". Do you mean that you are able to avoid using automatic differentiation since you can easily compute the gradients yourself? If so, you should clarify this.\n- In Eqs. (5) and (6): You should clarify that this is the gradient of _one_ term in the sum in Eq. (2).\n- Theorems 1 and 2: Writing \"min # params\" is not very clear. It would be better if you wrote in words what the bound is (e.g., \"It is sufficient to use x parameters to achieve a generated embedding which is at least p\".)\n- Fig. 2 caption: \"n2k\" is an abbreviation which is never defined.\n- Toward the end of Sec 4, the part \"ii) TEBs are designed to have a flexible ... iii) achieves to fully ... full embedding.\" These two points are vague. For (ii), it would be much better if you could precisely say which parameters are fixed to what in word2ket/word2ketXS. For (iii), it's not clear what you mean.\n- How is Prec@N defined?\n- The last 4 sentences in the 1st paragraph of Sec 5.2 (\"First we sample ... for different methods.\") don't make sense.\n- Is the y-axis in Fig. 3 (b) percent or percentage points?\n",
            "summary_of_the_review": "The method proposed in this paper is an interesting idea, but the experiments are not convincing at all. In particular, it's not clear why network reconstruction matters for down-stream tasks. The paper is also unclear in multiple places; the writing needs to be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1470/Reviewer_s1h6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1470/Reviewer_s1h6"
        ]
    },
    {
        "id": "i9-BYOvRjRq",
        "original": null,
        "number": 2,
        "cdate": 1666248479486,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666248479486,
        "tmdate": 1666248479486,
        "tddate": null,
        "forum": "bA6h-N17X1E",
        "replyto": "bA6h-N17X1E",
        "invitation": "ICLR.cc/2023/Conference/Paper1470/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studied a novel network embedding method inspired by quantum fields and modeled with tensors and contractions (i.e., outer product). More specially, the embedding vector for each node is represented by a linear combination of rank-one tensors, equivalent to the CP model studied in numerical algebra. In the paper, the authors also discussed the compressive power of the proposed method in a theoretical way. In the experiment part, this paper showed superior performance on network analysis tasks and evaluated the parallelizability and interpretability, which is crucial in practice.",
            "strength_and_weaknesses": "++Strength\n\n1. Very interesting model and the combination with quantum is also very smooth and clearly discussed.\n2. The improvement in the empirical performance is remarkable.\n\n\u2014 Weakness\n\n1. The theoretical results are not well explained.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality:**\n\nThe full paper is quite well-written and clearly introduces the main contributions of the work. The clarity and quality of the paper can be improved if authors pay some attention to the following several points:\n\n1. \n2. The clarity of the \u201cobjective function\u201d part can be further improved. As one of the readers (or reviewers) of this paper, I\u2019m not familiar with network embedding well. So when I went through the \u201cobjective function\u201d part, I got confused with some vocabulary, such as what \u201cnegative sample\u201d and the distribution $\\mathbb{P}_n$ stand for in this scenario? and why in the second term of Eq. (2) there is no index $k$ in the summation over $k\\in{}[K]$? Maybe this knowledge is well-known by network analyzers, but it would be more friendly for general ML researchers if this part could be introduced more clearly.\n3. In Sec 4.3 the paper discussed the theoretically optimal to compressive power (although I fully understand what the meaning of \u201ccompressive power\u201d is). It would be better if more explanations (such as insights) could be given following each theoretical result. For example, why does the logarithm appear in the equations? If the assumptions are plausible in practice? and so on.\n\n**Novelty:**\n\nAlthough this might not be the first paper to apply tensors to network embedding, the proposed methods and the used tricks are very interesting and inspireful for tensor researchers such as the graph partition used in NODE2KET+. \n\n**Reproducibility**\n\nThe experimental settings are clearly introduced.",
            "summary_of_the_review": "This is excellent work at least in the tensor community. The problem focused in this paper would inspire lots for tensor researchers to explore more valuable applications in machine learning. It also gives a bridge to connect machine learning to quantum computing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1470/Reviewer_vWD3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1470/Reviewer_vWD3"
        ]
    },
    {
        "id": "yIJ6Jnkdbua",
        "original": null,
        "number": 3,
        "cdate": 1666829237478,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666829237478,
        "tmdate": 1666829237478,
        "tddate": null,
        "forum": "bA6h-N17X1E",
        "replyto": "bA6h-N17X1E",
        "invitation": "ICLR.cc/2023/Conference/Paper1470/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper employs the tensorized embedding proposed in word2ket to the network embedding. It is similar with the second order part in network embedding method LINE, which is motivated from word2vec. Its superiority is verified on the tasks of topology reconstruction and link prediction.",
            "strength_and_weaknesses": "Strength \n- It is interesting to employ tensorized embedding to network embedding field.\n\nWeakness\n- The novelty is very limited. Most of the parts are similar to word2ket. Thus, the contribution to network embedding field is very limited, since only the objective function is different. \n- There is no contribution on the graph topology exploitation. The topology is only used in the the objective function, which is the same as pervious network embedding ones.\n- The experimental evaluations are not sufficient. The performance on node representation is only verified on topology-related tasks. However, that on node classification task is not given. Furthermore, the performance on topology-based tasks are only compared with the classic methods, instead of the SOTA such as hyperbolic space embedding methods.\n- The writing is redundant. Most content is similar to word2ket.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Both novelty and clarity need to be improved.\n",
            "summary_of_the_review": "My main concern are the limited novelty and contribution and insufficient evaluations show in weakness.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1470/Reviewer_koXd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1470/Reviewer_koXd"
        ]
    }
]