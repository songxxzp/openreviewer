[
    {
        "id": "2XcD38GHUHC",
        "original": null,
        "number": 1,
        "cdate": 1666618234867,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666618234867,
        "tmdate": 1666618234867,
        "tddate": null,
        "forum": "TMnxVoWdX_M",
        "replyto": "TMnxVoWdX_M",
        "invitation": "ICLR.cc/2023/Conference/Paper1593/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work reviewed the existing solution of Dual BN for adversarial robustness by focusing on the affine parameters in BN. It is argued that the stronger adversarial robustness in Dual BN is mainly attributed to additional affine parameters. Extensive ablation studies are presented to validate the argument. Moreover, it is argued that domain gap between adversarial and clean samples are not significantly larger than between noisy and clean samples, suggesting new explanations are required. \n",
            "strength_and_weaknesses": "strength:\n\n1. There are many empirical evaluations to support the hypotheses\n\n2. The logic and experiments are easy to follow.\n\nWeakness:\n\n1. This is more like a commentary paper pertaining to an existing paradigm, i.e. Dual BN. Despite some insights into the effectiveness of affine parameters, there is no novel algorithm proposed.\n\n2. From practical point of view, is there any recommended practice that one could adopt based on the analysis? Answering this quesiton could substantially improve the quality of this paper. \n\n3. I am wondering whether this analysis could generalize to modern architectures, e.g. ViT, where BN no longer exists.\n\nMinor:\n\n4. It is recommended to evaluate mixing up NS_adv and NS_clean in Table 1 such that the impact of using more data to estimate BN is eliminated.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper presents the idea clearly. The hypothesis on affine parameters' impact is novel. The proposed ablation studies should be easy to reproduce.",
            "summary_of_the_review": "Overall, this work presents interesting empirical insight into the effectiveness of Dual BN. However, the major weakness lies in the lack of novel solutions. It is hard to grasp useful practical guidelines for improving adversarial robustness from this work.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1593/Reviewer_8GPb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1593/Reviewer_8GPb"
        ]
    },
    {
        "id": "8A8xa2XWUPf",
        "original": null,
        "number": 2,
        "cdate": 1666677568526,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677568526,
        "tmdate": 1667618475691,
        "tddate": null,
        "forum": "TMnxVoWdX_M",
        "replyto": "TMnxVoWdX_M",
        "invitation": "ICLR.cc/2023/Conference/Paper1593/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the motivation of applying dual BN in a hybrid adversarial training scene, where the model is trained on both adversarial samples and clean samples. In contrast to the popular belief that normalization statistics should be estimated separately for clean samples and adversarial samples to achieve stronger robustness, this paper reveals that what makes dual BN effective lies in two sets of affine parameters. Moreover, the paper demonstrates that the adversarial-clean domain gap is not as large as many might expect and it is similar to its noisy-counterpart under the same perturbation/noise magnitude. Overall, this paper raises an interesting topic but the algorithm part is a bit weak.",
            "strength_and_weaknesses": "Strength:\nThis paper reconsiders the motivation of applying dual BN in adversarial training with hybrid samples and explores the effectiveness of normalization statistics and affine parameters separately.\n\nThis paper provides a closer looker at dual BN: disentangling the mixture distribution for normalization statistics and introducing two sets of affine parameters (AP). Then they find that the performance improvement mainly comes from more affine parameters.\n\nThe authors point out a visualization flaw in previous work: when AP is fixed, the robustness gap between the NS calculated on clean or adversarial samples is quite subtle.\n\n\nWeakness:\n\nThis paper lacks a description of the existing development and problems in the field of applying batch normalization in adversarial training. It is not clear why they want to explore the reasonableness of the motivation for applying dual batch normalization proposed in the previous work. For example, is there some bad phenomenon with the application of dual batch normalization? or some limitations not considered in previous work? In a word, the authors do not give a reason why they carry out this work. \n\nThe cross BN model looks like a mixture of ordinary BN and Dual BN. There is not much to take in this model. Although the authors provided two variants: setup1 and setup2, the contribution still looks incremental.\n\nIn Figure 2, Cross-AT method has a similar performance to Hybrid AT. However, from the statement that \u201cthe cross-AT still keeps its clean branch in the Hybrid-AT.., but the model weights are updated only by the adversarial branch\u2019\u2019, Cross-AT does not use clean samples to update the model weights, how does its performance on clean samples achieve the similar with Hybrid AT?\n\nIn the last paragraph of section 5, the paper claims that \u201cfuture works utilizing dual BN in Hybrid AT are suggested to discard the two-domain hypothesis and embrace the two-task hypothesis\u201d, the authors emphasize they have two intriguing findings. However, the authors have not clarified what contributions this work will bring to the development of the related field. \n\nIn Page 2 Line 13, the authors claim that \u201cThis conjecture is motivated by an interesting observation This conjecture is motivated by an interesting observation that swapping NS in the Hybrid-AT has little influence on the model performance\u201d. Although the experiment results shown in Figure 2 may prove the little influence of Normalization statistics, the experiment setting itself is hard to interpret. That is, the authors may not illustrate what motivates them to this experiment or if is there any interpretable meaning about the operation of swapping NS.  \n\nIn the last paragraph of section 3, the authors claim that \u201cSuch a success inspires us to suspect that the merit of dual BN over single BN in Hybrid-AT \u2026\u201d. However, as the results shown in Figure 2, although there is no significant drop in performance after \u201cCross-AT\u201d, there is also no significant gain to prove the effectiveness of \u201cCross-AT\u201d. Therefore, the claim of \u201csuccess\u201d may not be accurate.\n\nIn table 2, although the test accuracy of Setup1(AP_{adv}) has improved compared to Single BN and Dual BN in PGD-10 and AA, the test accuracy has obviously dropped in the clean dataset. So the claim on Page 2 Line 2 \u201ctwo sets of APs alone significantly improve the robustness.\u201d may be weakly supported by the experiment.\n\nThe paper conducts a lot of experiments but lacks some necessary theoretical analysis in dual BN and Hybrid-AT. Moreover, a novel effective method is expected according to the theoretical finding.\n\n\nMinor issues:\n\nIn the second contribution listed in section 1: \u201cWe point out that it also introduces two sets of AP\u201d, what\u201cit\u201drefers to is not clear.\n\nIn section 2.1, the part named \u201cExperimental setups\u201d is expected to be close to the experiment result in section 4 or section 5 of the article.\n\nIn Table 2, Table 3, there is no description of symbolic subscripts of AP_adv  AP_clean,.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see my comments above.",
            "summary_of_the_review": "The contribution of the paper is limited and the reason why exploring the reasonableness of the motivation for applying dual batch normalization proposed in the previous work is not clear.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1593/Reviewer_uM2g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1593/Reviewer_uM2g"
        ]
    },
    {
        "id": "qIXOvqG1du",
        "original": null,
        "number": 3,
        "cdate": 1666678949336,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678949336,
        "tmdate": 1666678949336,
        "tddate": null,
        "forum": "TMnxVoWdX_M",
        "replyto": "TMnxVoWdX_M",
        "invitation": "ICLR.cc/2023/Conference/Paper1593/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper firstly attempts at training a model with cross-domain BN statistics. Through untwining NS and AP in dual BN, it demonstrates that what makes it effective lies in two sets of AP instead of disentangled NS (as claimed in prior work).\nIt points out a hidden flaw in prior work for visualizing the NS to highlight a (large) domain gap between adversarial and clean samples. After fixing it, it reveals that the adversarial-clean domain gap is not as large as prior work suggests.",
            "strength_and_weaknesses": "\\+ Through untwining NS and AP in dual BN, it demonstrates that what makes it effective lies in two sets of AP instead of disentangled NS (as claimed in prior work).\n\n\\+ It points out a hidden flaw in prior work for visualizing the NS to highlight a (large) domain gap between adversarial and clean samples. After fixing it, it reveals that the adversarial-clean domain gap is not as large as prior work suggests.\n\n\\- The technical contribution may be limited. This paper perform a lot of experiments with the dual BN for Hybrid-AT. It tests many different AT and BN settings. But it mainly uses Hybrid-AT techniques and variants of BN. The technical contribution may be limited. \n\n\\- The application of the proposed method may be limited. The model must have BN layers so that it can be applied. Besides, it basically have two paths in the model for adversarial and clean data. During training, it knows how to activate each path since the model knows whether the data is clean or adversarial. But during inference without knowing the data is clean or adversarial, how can the model know which path to activate?  ",
            "clarity,_quality,_novelty_and_reproducibility": "see the strength and weakness. ",
            "summary_of_the_review": "It mainly compares with the work (Xie & Yuille, 2020) and points out some key insights different from Xie & Yuille, 2020. It performs comprehensive experiments to check certain assumptions. But it mainly use existing techniques. The technical contribution may be limited. And the application may be limited. It may still have problems to apply this method in practice. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1593/Reviewer_UFTM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1593/Reviewer_UFTM"
        ]
    },
    {
        "id": "DhP-SHuuGJb",
        "original": null,
        "number": 4,
        "cdate": 1667602274494,
        "mdate": 1667602274494,
        "ddate": null,
        "tcdate": 1667602274494,
        "tmdate": 1667602274494,
        "tddate": null,
        "forum": "TMnxVoWdX_M",
        "replyto": "TMnxVoWdX_M",
        "invitation": "ICLR.cc/2023/Conference/Paper1593/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper makes the interesting observation that the disentangled affine transformation is what really matters in the Mixture BN, instead of the disentangled BN statistics as previously believed by the community.",
            "strength_and_weaknesses": "Overall, I think the message conveyed by this paper is simple yet important: The disentangled affine transformation is what really matters in the Mixture BN, instead of the disentangled BN statistics as previously believed by the community. The experimental results effectively support the conclusions. The paper is also well-written and is easy to follow for readers without expert knowledge in Mixture BN. \n\nI have one curious question for the authors:\n\nIn order to provide more evidence that clean and adversarial samples have similar BN statistics when we don't use disentangled affine transformation, could you please show similar visualization results as those in Figure 4, but remove all affine transformations in all BN layers? This can be achieved by disabling affine transformation when defining PyTorch BN layers. Usually, in normal training, disabling the affine transformations in BN layers won't harm performance much. \n",
            "clarity,_quality,_novelty_and_reproducibility": "please see above",
            "summary_of_the_review": "please see above",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1593/Reviewer_dNrE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1593/Reviewer_dNrE"
        ]
    }
]