[
    {
        "id": "l5wtAl3T4C",
        "original": null,
        "number": 1,
        "cdate": 1666548523374,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666548523374,
        "tmdate": 1666548523374,
        "tddate": null,
        "forum": "CBfYffLqWqb",
        "replyto": "CBfYffLqWqb",
        "invitation": "ICLR.cc/2023/Conference/Paper6235/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a quality-diversity algorithm for reinforcement learning that enables both the RL optimization step and population repertoire update step to make changes to agent parameters. The proposed algorithm allows updates to agent hyperparameters to reduce the sensitivity of the approach to hyperparameter choice of the underlying RL algorithm. Scaling is achieved by allowing parallel evolution of agent populations and a JAX implementation for high performance. Results show strong performance (matching competitive alternatives) on robotic control tasks, particularly in terms of deceptive environments and when evaluated not only on final performance but also coverage of strategies.",
            "strength_and_weaknesses": "## Strengths\n\n1. Comparable performance to existing algorithms while being able to solve hard exploration tasks (HumanoidTrap, AntTrap) fast (in terms of environment steps).\n2. Generality. The approach presented can readily be coupled with many RL algorithms and directly addresses hyperparameter sensitivity issues in those algorithms. It would be interesting to see results showing how this helps compared to baselines on other RL tasks.\n\n\n## Weaknesses\n\n1. Few major performance improvements over alternatives. The results would be stronger if there were clear environments that previous algorithms simply failed to solve that PBT-MAP-Elites solves. The closest result is the HumanoidTrap result. This is not a major weakness.\n2. No data on scaling. One of the main benefits of the approach is that it \"3) can scale to large population sizes\", but this is never tested. Adding experiments showing this scaling and how it compares to alternatives would greatly benefit the paper.\n3. No data on (wallclock) speedup. Can the speedup of running experiments in JAX be quantified? The JAX implementation is claimed as being efficient, but no empirical evidence backs this claim compared to other simulators.\n\n\n## Feedback & Questions\n- Figure 2: The colors for ME-ES and PBT are hard to distinguish, particularly when shaded areas overlap the medians. Consider slightly different colors for readability. Also it would help to order the columns to have all \"*Uni\" columns and \"*Trap\" columns grouped together.\n- Figure 3: The axis text is too small to read. Please make it larger.\n- Note: At least two references are duplicated: Pierrot 2022a & 2022b, Lim 2022a & 2022b.\n- What is the speedup of using the JAX implementation compared to alternatives?\n- The introduction and related work are both quite lengthy and cover overlapping material. Consider condensing the introduction to highlight the main contributions and novelty, with the related work discussing the particular limitations of past efforts. This can help keep the narrative clear for readers and focus attention on the specific novelty of the work reported.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: Overall clear. The text is readable, but awkwardly structured: long discussions of literature occupy the first 3 pages of the text before introducing the core problem and contributions.\n- Quality: Good. The results show reasonable performance and success on hard exploration tasks. The experiments demonstrate rigor for fair comparison.\n- Originality: Modest. The novelty in the work lies in the design decisions made for representing policies (by including hyperparameters, replay buffers, &c. in agents) and allowing two processes to modify policies.\n- Reproducibility: Modest (possibly high?). The appendix provides algorithm details and the text is clear about the major experiment and algorithm design features. No code is provided for the implementation, but the text describes open sourcing the implementation so this may be forthcoming.",
            "summary_of_the_review": "The technical novelty of the paper is modifying MAP-Elites to use a PBT-like structure for the agent population. This provides the benefit of generality to RL implementation and automatic tuning of hyperparameters (inherited from PBT). This is certainly new, though incremental in the sense that both PBT and MAP-Elites have been thoroughly explored in the past. There is substantial benefit to be had from further investigation of this kind of combination of RL meta-algorithms (like PBT and self-play) with evolutionary approaches, particularly from quality diversity.\n\nEmpirical results show some strong results in the set of tasks the algorithms are tailored to: deceptive domains. Most tasks show competitive performance with alternatives. All results are in relatively simple robotic control tasks (compared to controlling robotic manipulators or real robot locomotion), leaving plenty of room for stronger tests of the algorithm capabilities. No empirical results examine the speedup claims of the JAX implementation or the scalability claims for the algorithm, though both seem very plausible.\n\nTaken together the work makes a step forward for quality-diversity approaches to RL in deceptive continuous control tasks. Whether this approach has wider value remains an open question.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6235/Reviewer_Y8Hf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6235/Reviewer_Y8Hf"
        ]
    },
    {
        "id": "3dn1FoaCNbP",
        "original": null,
        "number": 2,
        "cdate": 1666677139863,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677139863,
        "tmdate": 1666677139863,
        "tddate": null,
        "forum": "CBfYffLqWqb",
        "replyto": "CBfYffLqWqb",
        "invitation": "ICLR.cc/2023/Conference/Paper6235/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the limitations of reinforcement learning (RL) based MAP-ELITES approaches, an approach to quality diversity (QD) optimization. The authors highlight the following four problems with existing approaches: (i) high sensibility to hyperparameters; (ii) training instability; (iii) high variability in performance; and (iv) limited parallelizability. \n\nTo overcome these limitations, the authors propose a population-based RL-based MAP-EILTES approach. Each agent in the population encodes the policy parameters, the hyper-parameters for the training, and some other internal parameters used during the training. The proposed framework is a generic framework in that one can easily replace the RL baseline to train each agent. \n\nThe proposed approach is compared with other MAP-Elites variants with SAC and TD3 as their RL baselines on five robotics environments that are often used in QD-RL literature. Superior performance, in particular on environments with deceptive reward signals, are observed.",
            "strength_and_weaknesses": "# Strength\n\nApproach: The approach is relatively simple. The RL baseline can be easily replaced. And it is parallel implementation friendly. \n\nEvaluation: The advantages of the proposed approach over existing MAP-ELITES approaches have been demonstrated on robotics environments in terms of the best fitness, coverage, and QD score. In particular, the advantage is pronounced on environment with deceptive reward signals is reported.\n\n# Weaknesses\n\nClarity: The algorithm is described thoroughly by natural language in the main text. It is not easy to precisely understand, in particular, for those who are not familiar with MAP-ELITES. The algorithm is provided in the appendix, but the details are not given there. Moreover, because of a different symbol used in the algorithm and in the main text (M vs N), it was hard to fully understand. \n\nAlgorithm validity: The authors say that the meta-learning mechanism for the hyper-parameter is included in the algorithm. However, as far as I understand, the hyper-parameter is only randomly sampled uniformly over the domain. whenever an agent in the population is replaced. It does not look like optimizing the hyper-parameter values.\n\nMotivation and Algorithm Design and Experimental Evaluation: The authors highlighted four difficulties (written in the summary part) and proposed the approach to address these difficulties. However, it was not clear from the paper how they are addressed. Item (i) seems to be addressed by the meta-learning of the hyper-parameter. However, as I wrote above, it was not clear why this makes sense. I couldn't find discussion related to Item (ii) and (iii). The experiments are not designed to evaluate these points. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow in general. However, because the algorithm is written only by the natural language, it is hard to understand precisely. I see the novelty in the proposed framework including hyper-parameter learning mechanism. However, the approach is relatively a straight-forward extension of existing MAP-ELITES approaches. The experimental details are provided in the appendix. ",
            "summary_of_the_review": "I see the advantages of the proposed approach. However, because of the lack of the discussion of the algorithm validity and (empirical) analysis of the algorithmic behavior, I think that the claims are not sufficiently supported in the paper. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6235/Reviewer_XjfY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6235/Reviewer_XjfY"
        ]
    },
    {
        "id": "VZWX1220PR",
        "original": null,
        "number": 3,
        "cdate": 1667358740585,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667358740585,
        "tmdate": 1667358740585,
        "tddate": null,
        "forum": "CBfYffLqWqb",
        "replyto": "CBfYffLqWqb",
        "invitation": "ICLR.cc/2023/Conference/Paper6235/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a framework that allows using any reinforcement learning (RL) algorithm within a population of agents. The contribution is to use quality diversity methods to evolve populations and maintain their diversity. The most commonly used method is MAP-ELITES, but MAP-ELITES does not work well on high-dimensional search spaces. The contribution of the paper is the development of a version of MAP-ELITES, called PBT-MAP-ELITES, that does not depend on a specific RL agent, is robust to hyperparameter choices, and scales to large population sizes.  Some experimental results are included for an example problem.",
            "strength_and_weaknesses": "Strengths: \nThe changes made to MAP-ELITES are simple but seem to be effective.\nWeaknesses: \n- the PBT-MAP-ELITES algorithm is a variation of PGA-MAP-ELITES, which seems quite simple even though it appears to be effective. \n- There are many variations of MAP-ELITES that are used in the experiments but for which there is no explanation in the paper. \n- The experimental results included are relatively limited and are not described with enough details to be reproducible. \n- The paper assumes familiarity with all the specific robotics problems used to test the algorithm.  \n- The figure with the experimental results (Fig.2) is hard to read.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear but not always precise and sufficiently detailed. I do not think is possible to replicate the results. Too many details are missing.",
            "summary_of_the_review": "The paper presents an algorithm that is a new variation of MAP-ELITES. It evolves a population of agents, scales well to large population sizes, and is robust to hyperparameter choices. The experimental results that are included in the paper show good performance compared to other algorithms, but they are described in a succinct way, making it very hard to replicate the results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6235/Reviewer_bFfK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6235/Reviewer_bFfK"
        ]
    }
]