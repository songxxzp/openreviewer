[
    {
        "id": "OX3dyJWw2RX",
        "original": null,
        "number": 1,
        "cdate": 1666662444028,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662444028,
        "tmdate": 1668675139391,
        "tddate": null,
        "forum": "J6F3lLg4Kdp",
        "replyto": "J6F3lLg4Kdp",
        "invitation": "ICLR.cc/2023/Conference/Paper2905/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces \u201cSparsity May Cry\u201d Benchmark (SMC-Bench), a collection of carefully curated 4 diverse tasks with 12 datasets, for a more general evaluation and unveiling the true potential of sparse algorithms. Evaluation reveals several important and unusual findings. ",
            "strength_and_weaknesses": "Overall, I enjoy this work a lot and think it will make a big/good splash in the sparsity community. But I also believe its scientific rigor needs to be enhanced in several aspects before being treated as an indeed solid benchmark. See details below.\n \nFirst, it might be too early to declare sparsity will cry! For example, only first-order, unstructured pruning has been so far tested, which represent a strong yet still restricted subclass of modern pruning methods. I would definitely suggest the authors to pick representatives from other pruning families, such as second-order pruning.\n\nSecond, how the authors could justify their chosen datasets and tasks are indeed more challenging than ImageNet? IMHO, the sole fact that current pruning methods work better on ImageNet than those cannot directly support the claim, since it could also be some task types that are just more \u201cpruning (un)friendly\u201d. \n\nSimilarly, how the authors can justify their chosen models (admittedly very large already) are sufficiently \u201coverparameterized\u201d for those new difficult tasks? For example, would it be possible that, if continuing to increase the model size on those tasks, the generalization performance will keep improving, while pruning algorithms also start to gain back their effectiveness?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Writing is very clear. High technical quality and novelty as a \u201cnew perspective\u201d benchmark in sparsity and pruning.",
            "summary_of_the_review": "Overall, I would tend to accept this paper. If the author can address my concerns. I would be more convinced.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2905/Reviewer_yFvQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2905/Reviewer_yFvQ"
        ]
    },
    {
        "id": "9zYno96vWH",
        "original": null,
        "number": 2,
        "cdate": 1666681408275,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681408275,
        "tmdate": 1666681408275,
        "tddate": null,
        "forum": "J6F3lLg4Kdp",
        "replyto": "J6F3lLg4Kdp",
        "invitation": "ICLR.cc/2023/Conference/Paper2905/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors assemble a large-scale, challenging, and more diverse benchmark, SMC-Bench, for pruning and sparse training algorithms. A careful evaluation indicates that SMC-Bench significantly challenges current SOTA sparse algorithms, exposes their limitations and points to new research opportunities. ",
            "strength_and_weaknesses": "Strength:\n- This paper raises a very interesting new angle on the uprising field of sparse neural networks. Given the enormous that sparse neural networks have achieved, the authors ask a timely question: are current benchmarks sufficiently challenging for them, and (if indeed so) are we too optimistic in declaring the prevailing win of sparsity? The authors choose a few latest datasets and tasks that require substantial reasoning and structure information. This new benchmark will make an exciting addition to several existing sparse NN benchmarks such as Frankle et al. (2021).\n\n- Comparing a large range of before-training, during-training and after-training pruning methods (including both static and dynamic), the authors observe several surprising phenomena that are absent in the current evaluation. Most alarmingly, all of the SOTA sparse algorithms bluntly fail to perform on SMC-Bench, sometimes at significantly trivial sparsity e.g., 5%. The failure does not appear specific to one approach but unanimously across all sparse NN approaches that were evaluated. That points us to some rather fundamental rethinking of both sparse NN training and evaluation currently.\n\n- The authors also identified that model \u201cprunability\u201d is intimately related to task difficulty: those models trained on difficult tasks suffer more from pruning compared to easier tasks. Also, some best pruning algorithms start to perform indistinguishably from the random pruning baseline on the new challenging benchmark, and iterative pruning does not necessarily generalize better than one-shot pruning either. All those findings offer brand-new knowledge of broad interest to the sparse NN community.\n\nWeaknesses:\n- The paper did not identify a root cause for the failure of SNNs on SMC-Bench, although exploring then excluding a few factors. I feel one main reason is probably intimately related to the \u201clazy training\u201d dynamics emerging in sufficiently overparameterized models, e.g., after training weights will only change very little from initialization. \nMoreover, in one of their sanity checks on whether pruning embedding layers or not, the authors found SNIP to significantly underperform others because it prunes too much of the pre-trained embedding while others not. I am wondering whether we could ad-hoc skip that embedding, for all pruning methods and then compare them?  \n\n- I strongly suggest the authors to add to their comparison one or more second-order pruning methods. such as Optimal Brain Damage, Fisher Pruning, or the recent Optimal BeRT Surgeon. \n\n- Also missed in the current benchmark is the structured pruning family: will they be more robust or fragile when task difficulty and data volume scale up? How about sparse MoE methods?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: easy to read and understand\n\nQuality: solid and comprehensive work\n\nNovelty: high\n\nReproducibility: good (the authors promised to open-source)\n",
            "summary_of_the_review": "This paper proposes an interesting method and the major concerns lie on more comparisons in the experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2905/Reviewer_wzLQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2905/Reviewer_wzLQ"
        ]
    },
    {
        "id": "P0yGHJ2KWC",
        "original": null,
        "number": 3,
        "cdate": 1666826932187,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666826932187,
        "tmdate": 1666826998508,
        "tddate": null,
        "forum": "J6F3lLg4Kdp",
        "replyto": "J6F3lLg4Kdp",
        "invitation": "ICLR.cc/2023/Conference/Paper2905/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a new set of benchmarks dubbed \"SMC-Bench\" (Sparsity May Cry). The authors correctly remark that the ever-growing research in sparsifying neural networks is often (if not always) evaluated on limited set of benchmarks which are often one of the very well understood, and \"relatively easy\" datasets.\n\nThe authors then walk us over the different ways of sparsifying a neural network - providing a good summary of the current state of the art.\n\nThe paper continues with the introduction of the dataset, and providing experiments that author use to support their claims. ",
            "strength_and_weaknesses": "The paper attaches an obvious question that seems not to have been addressed yet.  It's obvious that the authors spent an enormous amount of time and effort to collect all the results.\n\nThe authors promise to open-source their dataset, however no more details are provided. Additionally one would expect a well organized list of the \"10 open source repositories\" used for creating the benchmark. The open-sourcing and hopefully stream-lining future benchmarks is the most valuable part of the authors' work.\n\nI would also expect the authors to evaluate the networks on the \"weak\"/\"easy\" datasets - to confirm that their implementations/runs are consistent with what was claimed in the intro",
            "clarity,_quality,_novelty_and_reproducibility": "The manuscript is mostly very clear - tho, as mentioned above, I'd expect a bit more details (or better organized/prezented) list of used benchmarks, and more info about open-sourcing.\n\nThe reproducibility is limited with the current state - but hopefully will be trivial after the authors open-source.",
            "summary_of_the_review": "Please see above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2905/Reviewer_JUeu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2905/Reviewer_JUeu"
        ]
    },
    {
        "id": "WLJDqRFb_9",
        "original": null,
        "number": 4,
        "cdate": 1667226298608,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667226298608,
        "tmdate": 1669940997105,
        "tddate": null,
        "forum": "J6F3lLg4Kdp",
        "replyto": "J6F3lLg4Kdp",
        "invitation": "ICLR.cc/2023/Conference/Paper2905/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper provides a new benchmark and results for evaluating sparsity methods on diverse tasks called SMC-Bench.\nThroughout difficult tasks the paper finds that existing works on sparsity may fail at very early in the level of sparsity in comparison to what is known previously in the literature for rather simpler tasks such as imagenet classification or language modeling tasks.\nThis result can imply a few different things including ineffectiveness of existing methods and the need for new ones or even the need of re-thinking the benefit of sparsity itself.\n",
            "strength_and_weaknesses": "Strength\n- This paper provides a range of new and difficult evaluation tasks for pruning and serves as a new benchmark.\n- Overall the paper is well written.\n\nWeaknesses\n- The results achieved do not seem to provide significantly new or unknown information in the community. They basically show that iterative magnitude based pruning after training works well. The discovery is where pruning methods start to fail, but for more complex and difficult tasks it is quite obvious and expected that they will fail earlier. In this sense, the paper quite exaggerates the benefit of this new benchmark.\n- This paper serves to provide empirical results but also draw comparisons and conclusions based only upon it rather than any analysis of methods themselves; it is like A is better than B because of the performance, but without providing why. Perhaps it suits the purpose of creating a benchmark though, but to point out a limitation.\n- All evaluated methods are saliency-based pruning methods, among which all methods are magnitude based approaches except for random and SNIP. This is definitely a degrading factor, considering that throughout the paper the authors address in words a lot of different pruning approaches even including variational approaches and sparsity-inducing regularization based approaches.\n- The benchmark is missing one critical aspect of evaluating sparsity methods: lack of consideration of the total computations needed to achieve the trained performant sparse networks. Precisely, given a network model, how many forward/backward passess (or gradient computations) are needed for the entire process of finding and training the sparse network result, for each and every method being compared? Why is this important? It is because the benchmark claims to serve as a new standard evaluation protocol providing comparisons that one is better than the other. With different budgets or without even mentioning it properly is simply not fair for that matter. Potential evaluation protocols may include (1) for a fixed budget of computations, how good one method compares to the others in terms of final accuracy or sparsity level, or (2) for a fixed target accuracy/sparsity, how efficiently/faster one method achieves the goal compared to the others. It would also be better to include, at least in words level, how simple or difficult it is to tune the algorithms or hyperparameters (which includes the winning process in the LTH). Without these considerations, this benchmark may only provide a partial result.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, original, and reproducible.",
            "summary_of_the_review": "This paper develops a new benchmark for evaluating pruning. There still remain concerns to be resolved for the paper to serve as a new pointer for sparsity research.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2905/Reviewer_11zQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2905/Reviewer_11zQ"
        ]
    }
]