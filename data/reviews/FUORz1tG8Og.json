[
    {
        "id": "rHJmhX3S-f",
        "original": null,
        "number": 1,
        "cdate": 1666578333251,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578333251,
        "tmdate": 1666578333251,
        "tddate": null,
        "forum": "FUORz1tG8Og",
        "replyto": "FUORz1tG8Og",
        "invitation": "ICLR.cc/2023/Conference/Paper761/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to use implicit neural representations to model and efficiently solve PDE dynamics. This allows to solve dynamics in the latent space instead of the whole parameter space (lower dimension), without discretizing it. The latent space and network are trained on a range of sample PDEs for a given task, and employed to solve PDEs with different parameters.\nSolving the temporal evolution is still done with an explicit PDE constraint at each time step, providing updates of latent space parameters, instead of the explicit parameters which have much higher dimensions.",
            "strength_and_weaknesses": "Using neural networks to reduce the dimensionality of physics phenomenon in a learned and non linear but smooth manner is a very good motivation, and as noted by the authors it had already been explored. Here it is also combined (for the first time) with an IMPLICIT representation, which allows for quantization-free modelling.\nThe latter point has two benefits: the pipeline requires less memory, and can easily adapt to any resolution by simply querying the network more or less densely.\n\nThe experimental section is very rich, displaying the applicability of the method for many applications (image blurring, heat equation, advection, elastodynamic deformation, \u2026). Comparing network outputs to a full fledged PDE solver (slow) and other network based solvers quantitatively and qualitatively favors the proposed pipeline: it is almost on par with the full PDE solver, and largely more accurate than other network based approaches.\nA runtime and memory analysis also demontrates that the proposed approach is much more efficient once trained.\n\nMy main doubt is that such a pipeline could only (locally) interpolate between seen behaviors from the training set. How does this prevent applicability? How much care must be taken in crafting a training set that spans a sufficiently large spectrum of possible dynamics?\n\nAnother concern is about runtime comparison with full order models: for a fair comparison, maybe the training set generation and network training time would have to be accounted for in some way. Indeed, the trained approach can be seen as a way to smartly \u201cmemorize\u201d simulation results to do much faster predictions. But this has an upfront computational cost, which only gets amortized when the resulting network is used for many different simulations.",
            "clarity,_quality,_novelty_and_reproducibility": "Very clear motivations, technical exposition and experimental presentation. Fully reproducible, partially thanks to an extensive (but not too verbose) supplementary material.",
            "summary_of_the_review": "Clear motivation, elegant method and strong experimental section.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper761/Reviewer_7uQY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper761/Reviewer_7uQY"
        ]
    },
    {
        "id": "R6IKdEUfWZ",
        "original": null,
        "number": 2,
        "cdate": 1666629518679,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629518679,
        "tmdate": 1666629518679,
        "tddate": null,
        "forum": "FUORz1tG8Og",
        "replyto": "FUORz1tG8Og",
        "invitation": "ICLR.cc/2023/Conference/Paper761/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes to use a reduced order model (ROM) to accelerate PDE resolution.\nThey differ from prior ROM approaches by directly modeling the continuous function of interest, and not an already discretized function.\nTo approximate the continuous PDE solution, CROM relies on a latent space and use implicit neural representations as decoder, which enable them to address any discretization. The PDE is solved in a lower dimensional space, enabling them lower memory consumption. Experiments are conducted on several PDE depicting various discretizations, and comparisons with several baselines are performed.",
            "strength_and_weaknesses": "On overall, the paper is clear, well-written and easy to follow. The method has many advantages compared to traditional ROM approaches, among which applicability to any discretization (which appears as a great deal of current research in dynamical systems) and reduced memory footprint and consumption.\nHowever, some steps in the method are not mentioned in the main paper, only in the appendix, which hinders the overall comprehension of the implementation, as well as its reproducibility. Also, some statements are misleading (developed hereafter).",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\nEven if the paper is clearly drafted, I have some questions about some of the steps in the method.\n\n### Step 1: Network Inference\nIt is not clear to me how you compute $\\dot{q_n}$ ?\n\n### Step 2: PDE Time-stepping\nAgain about time derivative, how do you compute $\\dot{f_{n+1}}$ ?\n\n### Spatial Sample reduction\n\nYou say that the well-posedness depends strongly on the selection of the points where you solve the PDE.\nI have several concerns about this selection.\n\nYou arguee that eq(7) is well-posed if $r \\leq d|M|$, I am not sure why?\n\nAbout the integration sample, I have two main concerns.\nFirst, in introduction of part 4, you say that integration samples are chosen at the user\u2019s discretion, which is in fact not true as you give an algorithm (introduced in Appendix B). I think you should give more insights in the main paper about this greedy algorithm and how you select the mesh you are solving the PDE on. Notably, how do you fix $M$?\n\nSecond, you say in Appendix B that Algo 1 gives a pre-computation, what do you mean by that?\n\nFinally, adaptative resolution seems like a great benefit in your paper. However, you remain quite elusive on how you implement it, could give some insights ?\n\n### Experiments\n\nTo what models are you comparing to ?\nYou refer to two papers, but show only one image of POD in Fig 5.\n\n## Vocabulary\n\nYou\u2019re talking about a low-dimensional manifold $g_{\\theta_g}$, which sounds weird to me. I would rather say that the low-dimensional manifold is your latent space $q$.\n\nYou are talking about \u201clatent space dynamics\u201d in both Fig 1 and Part 5 (where you say that you \"validate the latent space dynamics\").\nHowever, you don't really learn this dynamics, as you consider known the PDE and you use it to predict the next $q_{t+1}$. This is somewhat misleading.\n",
            "summary_of_the_review": "The paper combines various recent methods (ROM, PINN, INR) to tackle an interesting problem, namely the acceleration in the resolution of PDEs. Some minor shortcuts hinder the comprehension of the method. To add clarifications on the first and last step of the implementation would enhance the overall quality.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper761/Reviewer_XNfR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper761/Reviewer_XNfR"
        ]
    },
    {
        "id": "29FtNH14rI",
        "original": null,
        "number": 3,
        "cdate": 1666714290690,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666714290690,
        "tmdate": 1666714290690,
        "tddate": null,
        "forum": "FUORz1tG8Og",
        "replyto": "FUORz1tG8Og",
        "invitation": "ICLR.cc/2023/Conference/Paper761/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new way to solve PDEs using implicit neural representations (INRs) combined with the discrete time discretization/integration and neural network inversion. By leveraging lower dimensional manifolds or learned basis functions via INRs, the proposed method achieved efficient numerical schemes to temporally evolve solutions on a lower dimensional manifold. Also, the high-quality lower dimensional manifold that can be modulated by problem parameters allows more accurate solutions. The experiments demonstrate that the proposed method is efficient and more accurate than a baseline method ROM.\n",
            "strength_and_weaknesses": "Strengths:\n* The paper is well-written and explains complex concepts and the pipeline very clearly. Considering the numerous mathematical concepts, this paper is impressively concise. Especially the breakdown of the proposed method through Section 4.1 to 4.3 with Figure 3 makes this paper accessible to a general audience, I think.\n* The combination of Neural PDE Solver / ROM and Implicit Neural Networks is an interesting and promising direction, although implicit neural networks are usually nothing but MLPs. \n* The proposed method achieved significant performance gain as well as speed-up.  \n* The proposed method has the flexibility of INR that can be applied to any scale/resolution. \n\t\t \nWeaknesses:\n* The authors did not compare the training time. The key to high-quality solutions here is the quality of manifolds/basis functions. Also, the learned basis functions could be obtained by modulation using mu or features from the encoder e_{\\theta_e}. It usually requires a non-trivial amount of data and training time. The efficiency of training needs to be compared. Also, the model sizes need to be compared for a fair comparison with the baseline method. \n\t\t\nQuestions:\n1. How was g modulated by mu? In the literature, INR is often modulated by Hypernetworks that output the model parameters for another neural network.\n2. Training of the encoder is key to accurate solutions. How do you generate the training data? Is the method sensitive to the distribution of training data?\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: This paper is well-written.\n* Novelty & Quality: The paper proposes an interesting construction bridging two emerging topics: a hybrid/neural PDE Solver and INR. It achieves significant gains in accuracy and efficiency.\n* Reproducibility: The pipeline is complicated and requires many details. Without more implementation details or code, the main paper itself is not sufficient to reproduce this work. I checked the supplement, but it does not include code.",
            "summary_of_the_review": "Overall, this paper is well-written and studies a new combination to solve PDE more efficiently and accurately compared to other alternatives. By learning basis functions/manifolds with INR, the proposed method achieved impressive speed-ups and memory efficiency on top of higher approximate quality. This is a promising result and will inspire many other researches.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper761/Reviewer_yRQN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper761/Reviewer_yRQN"
        ]
    },
    {
        "id": "wf0HTCiCvM",
        "original": null,
        "number": 4,
        "cdate": 1667478484089,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667478484089,
        "tmdate": 1667916161796,
        "tddate": null,
        "forum": "FUORz1tG8Og",
        "replyto": "FUORz1tG8Og",
        "invitation": "ICLR.cc/2023/Conference/Paper761/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The CROM approach proposes to solve the driving high dimensional PDE in a low-dimensional manifold, without having to discretize the continuous high-dimensional vector field. The discretized high dimensional spatio-temporal solution is projected in the low dimensional manifold through an encoder. The encoder produces a latent representation of the continuous vector field. A positional-dependent decoder is then employed to reconstruct the continuous-vector field. In order to evolve the PDE in time few saples of the continuous vector field are required to minimize the residual of the driving PDE with a PINNs-like loss. The limited number of spatial samples y and the low dimension of the latent representation allow to boost the solution of the PDE with a considerable reduction of computational resources (e.g. time and memory) without degrading the accuracy of the solution.\n\nSeveral technical solutions are also proposed to effectively solve the dynamics of latent space (e.g. greedy sapling of the spatial points, numerical approximation of the gradients, solution of the least squares problem for the network-inversion).\n\n ",
            "strength_and_weaknesses": "The approach can be seen as ROM-PINNs method, as also asserted by the authors. Strength And Weaknesses of the approach are then related to the PINNs approach plus the benefits introduced by the reduced representation of the manifold. The proposed approach can effectively accelerate the numerical calculation of PDE without being agnostic to the physics. The same tool can also be used to dinamically remesh the high dimensionla solution. Nevertheless, despite \"The ROM community has demonstrated that these three steps can yield strong long-time stability even on stiff and chaotic dynamical systems\" (cit.) the PINNs are well known to fail with chaotic systems. Therefore, it would have been interesting to understand whether or not model reduction is of benefit to the solution of chaotic dynamical systems also when a PINNs-like criteria is used to evolve the solution in time.\n\nDespite the evolution of the solution in time is mesh-free, the whole method is not. In the Network Inference Step the latent vector at time zero is assumed to be given. Since the latent vector is the result of an encoder network, a discretized vector field (or at least a point-cloud) is always necessary. For PINNs a discretized vector field is also necessary in the training stage to enforce the initial condition, but this is not needed in inference.\n\nThe main limitation of the approach concerns the projection of the high dimensional solution into a low dimensional space. Authors claim that the low dimensional manifold is smooth. The smoothness is proven empirically for a rather simple test case but is not enforced in the training stage. Any kind of regularization is introduced for the latent state. This might be a major issue when dealing with physical systems that can undergo transitions of bifurcations. I suspect that the experiments in the paper are rather sipmle to experience such issue.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well written. The method is new and the reproducibility of the work is ensured by an exaustive Appendix section.",
            "summary_of_the_review": "The proposed approach is novel and interesting. The paper might have a coniderable impact in machine learning and numerical simulation community. For this reason I recommand acceptance of the paper. I suggest to the authors to be clearer on the limitations of the approach (i.e. discretized field needed to initialize the latent state, no guaranties on the smoothness of the latent space at least in the proposed version). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper761/Reviewer_YNJM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper761/Reviewer_YNJM"
        ]
    },
    {
        "id": "iTkzKm3Z4n",
        "original": null,
        "number": 5,
        "cdate": 1667482588905,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667482588905,
        "tmdate": 1667482588905,
        "tddate": null,
        "forum": "FUORz1tG8Og",
        "replyto": "FUORz1tG8Og",
        "invitation": "ICLR.cc/2023/Conference/Paper761/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThis work proposes a novel reduced order modeling approach based on deep neural networks. Here, the solution of the problem is approximated by a neural network, rather than a basis of mesh elements as is done in most reduction approaches. Dimensionality reduction is conducted through a *context* vector which is non-linearly encoded from an initial full order solution. Despite using a non linear reduction scheme, the approach manages to use the original model equations to advance the reduced state in time, bypassing the need for the development of a surrogate forecasting system.",
            "strength_and_weaknesses": "\n**Strengths :**\n\n* Combination of intrusive simulation based on a *PINNs-like* scheme and non-linear dimensionality reduction which is a key challenge in reduced order modeling.\n* \"Mesh free\" approach : Although a discretised solution is still necessary for the initialisation of the CROM, the model is able to predict the solution at arbitrary inference points during the simulation.\n* The method outperforms classical reduction methods on the proposed test cases\n\n**Weaknesses :**\n\n* The main weakness of the paper are the test cases which might be a bit too simple. 1D diffusion problems are used to showcase the method and the case of the burger's equation clearly presents the interest of using a non-linear dimensionality reduction method. However, the case of the Navier-Stokes equations could be extended to more complex conditions such as the cylinder flow, or the fluidic pinball to demonstrate the performance of the approach on a more realistic test case.\n* Tying back to the previous point, issues such as non-markovian effects in the dynamics might appear when dealing with the reduction of complex chaotic problems, which are not treated in this work.\n* The name of the method should be changed, as a paper on cluster based reduced order modeling ([1]) already uses the acronym.\n\n*[1] E. Kaiser, B. R. Noack, L. Cordier, A. Spohn, M. Segond, M. Abel, G.Daviller, J. \u00d6sth, S. Krajnovic\u0301 and R. K. Niven (2014). Cluster-based reduced-order modelling of a mixing layer. [*Journal of Fluid Mechanics* , **754**](http://journals.cambridge.org/action/displayIssue?jid=FLM&volumeId=754&seriesId=0&issueId=-1), pp 365-414.*",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and clearly outlines the ideas behind the approach. The test cases are well presented and the various algorithms provided help clarify the method.",
            "summary_of_the_review": "The paper proposes a method able to combine non-linear dimensionality reduction and intrusive simulation. Despite the need for more complex test cases, the paper clearly demonstrates the ability of the approach to efficiently model dynamical problems.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper761/Reviewer_NfXC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper761/Reviewer_NfXC"
        ]
    }
]