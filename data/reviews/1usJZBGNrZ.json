[
    {
        "id": "atevwlcCxcC",
        "original": null,
        "number": 1,
        "cdate": 1666610621666,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666610621666,
        "tmdate": 1666610621666,
        "tddate": null,
        "forum": "1usJZBGNrZ",
        "replyto": "1usJZBGNrZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1188/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a closed form update for offline RL. A few major assumptions and steps of derivations apply: (1) policy is Gaussian or mixture of Gaussians; (2) apply Taylor expansion to the optimization objective. The paper shows some theoretical properties of the proposed method, and empirical improvements over prior work baselines.",
            "strength_and_weaknesses": "=== Strength ===\n\nThe paper provides an application of Taylor expansion based objective + behavior regularization in the offline RL setting, and has shown empirical improvements over baseline algorithms. This might motivate carrying out analytic updates (based on Taylor expansion derived objective) in future investigation on offline RL.\n\n=== Weakness ===\n\nThe theoretical and conceptual contributions of the paper are a bit weak. Though the contribution of deriving analytic updates based on Taylor expansion + Gaussian policy assumption is technically solid, it is not clear why carrying out such updates should be useful in the offline RL setting in the first place. Is this due to statistical efficiency? Does it help stabilize the update? The theory front of the paper is weak in that much of the results come from technical derivation, or Thm 3.1 is a fairly straightforward application of PDL (admittedly, certain adaptations are needed since in this case the action is being expanded in the first order, but not the policy as in the original PDL case). ",
            "clarity,_quality,_novelty_and_reproducibility": "=== Clarity ===\n\nThe paper is written pretty clearly.\n\n=== Quality ===\n\nThe contribution of the paper is not strong enough, both in terms of novelty and theoretical statements. Otherwise, the paper provides some empirical evidence of the merits of the new method.\n\n=== Novelty ===\n\nThe application of Taylor expansion based approach to offline RL is novel, but the method itself is not very novel in the general RL setting.\n\n=== Reproduce ===\n\nThe results should arguably be reproducible because the source code is provided.",
            "summary_of_the_review": "A few main questions.\n\n=== **Taylor expansion based approach to RL** ===\n\nUsing Taylor expansion is not novel in the RL community in general. As the authors may have noted, PDL [1] itself arises from the application of Taylor expansion to the value function objective. Renowned algorithms such as TRPO and PPO have applied the results to trust region setting, where there is a regularization from the behavior policy (or any regularized policy chosen by the algo designers) [2]. Lately, Tang et al. [3] has developed second order Taylor expansion approach to a similar setup in online RL.\n\nAdmittedly, there is some difference between the setup in the current paper and aforementioned work, most notably, here the Taylor expansion is applied to the action directly (mainly because the policy is deterministic) whereas prior work mostly handles stochastic policy. In prior work, the expansion takes place in the policy distribution space; here, the expansion takes place in the action space. Such technical differences aside, I think it is worth discussing current work's relation with such prior work.\n\n[1] Approximately optimal approximate RL, Kakade et al, 2007\n[2] Proximal policy optimization, Schulman et al, 2015\n[3] Taylor expansion policy optimization, Tang et al, 2020\n\n=== **Why closed form update is useful in offline RL case** ===\n\nThough experiments have showed performance gains, I think it helps the paper can be greatly improved to clarify more on the motivational question: why is using closed form update useful in the offline RL case. Intuitively, I think this might make sense because compared to usual SGD based updates, closed form update may reduce the stochastic jittering a bit and hence make the update more stable. Can we characterize the effect in theory? Or can we have some clean simple setup to illustrate this point? This question I think is quite fundamentally interesting overall but is a bit lacking in the current paper.\n\n=== **Theory is a bit weak** ===\n\nThe flagship theoretical result in the paper is Thm 1, where they show an one-step improvement bound on the objective. On the technical side, I think showing the theory does require adaptations from the original PDL, due to the difference between expanding on the action space or policy space, but overall the adaptation is pretty straightforward. \n\nThe main issue I have is that the connection between theory and practice is very weak. The efficiency of any offline RL algorithms over baseline methods, should be due to its statistical efficiency, i.e., given finite amount of offline RL data, can the algorithm achieve good performance. This setup underlies the challenge of offline RL where no new data can be collected. However, Thm 3.1 deals with the \"expectation\" behavior of the algorithm, i.e., how the algorithm behaves when there is infinite amount of data such that finite sample bounds turn into expected improvements. This is not very insightful because by studying the expected behavior of the algorithm, we bypass much of the challenge that underlies offline RL itself.\n\nThere should arguably, be comparison to sample complexity bound mentioned in [4]. If we obtain the finite sample bound version of Thm 3.1, does it improve over the bound derived in CQL? How do they compare? They have different dependencies on diff hyper-parameters on the env, what do they mean in practice?\n\n[4] Conservative Q-learning for offline RL, Kumar et al, 2020\n\n=== **Experiment** ===\n\nThe experiments show that the new algo obtains benefits in certain envs over the SOTA baselines, but understandably the improvements are not uniform and not always. This is expected because different algorithms behave differently under the same env, and we may not expect the same algo to outperform all other SOTA at the same time. I think what's valuable is to elucidate why the new algo performs better than the previous SOTA, e.g., IQL in M and ME dataset, but not as much in the MR dataset (in Table 1). Such a comparison would be valuable to understand relative strengths of diff methods.\n\nAs a side note, I think adding a \"total\" row in the table is not very informative, because aggregating statistics across all games does not help understand the performance much in this specific case.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1188/Reviewer_m3D5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1188/Reviewer_m3D5"
        ]
    },
    {
        "id": "nvwLDGMwKTF",
        "original": null,
        "number": 2,
        "cdate": 1666757295321,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666757295321,
        "tmdate": 1666757357568,
        "tddate": null,
        "forum": "1usJZBGNrZ",
        "replyto": "1usJZBGNrZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1188/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on behavior-constrained policy optimization in offline RL problems. The authors use first-order Taylor approximation to get a closed-form solution for the policy update objective with behavior constraint. The behavior policy can be modeled as a single Gaussian or Gaussian mixture model and this paper provides a closed-form policy improvement operator in both cases. \n\nThe authors described a detailed theoretical derivation of this closed-form operator to map the behavior policy to a higher-valued policy. The policy-improved operator can be incorporated into general actor-critic offline RL algorithms to replace the policy update step. The experimental results empirically demonstrate that the proposed method is better than the existing SOTA methods on D4RL.",
            "strength_and_weaknesses": "Strengths:\nThe authors provide both theoretical foundation and empirical results to demonstrate the efficacy of the proposed method.\nThe evaluation shows convincing metrics in Figure 2.\nThe ablative study is extensive.\nThe proposed method is novel because all prior works conduct the policy update step in offline RL via SGD, but this paper introduces a closed-form solution.\n\nWeaknesses:\n\nAs shown in Algorithm 1, the closed-form policy improvement operator should be compatible with the general actor-critic offline RL algorithm. Could you please show the performance when the operator is combined with other offline RL algorithms, such as CQL and TD3-BC? Does it always bring improvement no matter what's the base offline RL algorithm?\n\nOne strong motivation for this work is that \"training stability poses a major challenge in offline RL\". Could you provide a specific example where existing offline RL algorithms to learn value function and policy via SGD really suffers from the instability issue, and the proposed method directly solved this? Although there are experimental results showing the proposed method is better than existing methods, it is unclear whether the advantage really comes from better training stability. So the claim in the introduction about \"mitigate the issue of learning instability by leveraging optimization techniques\" is not fully supported.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThis paper is generally well-written at a high level. I did not check the derivation in Sec. 3 line-by-line, so no comments about the clarity in details.\n\nQuality:\nOverall the quality is great, especially the evaluation and analysis part. \n\nOne concern about the limitation. As mentioned in Sec. 6, the proposed method requires learning a good Q function. Does it mean that the policy evaluation step in Algorithm 1 should be very accurate? Does it affect the choice of \\Epsilon in Algorithm 1? What will happen if the value estimation suffers from the over-estimation issue on out-of-distribution data points?\n\nNovelty:\nThe proposed closed-form policy improvement operator is novel. In offline RL, it is the first work to replace SGD optimization for policy updates with a closed-form solution.\n\nReproducibility:\nThe code is provided in supplementary materials.\n\n\n\n",
            "summary_of_the_review": "A novel optimization approach for policy update with behavior constraints in offline RL, well supported by theoretical derivation and extensive evaluation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1188/Reviewer_ypTb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1188/Reviewer_ypTb"
        ]
    },
    {
        "id": "pxisCqpO_7",
        "original": null,
        "number": 3,
        "cdate": 1667355354469,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667355354469,
        "tmdate": 1667355354469,
        "tddate": null,
        "forum": "1usJZBGNrZ",
        "replyto": "1usJZBGNrZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1188/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a method for constraining a policy for offline reinforcement learning. In particular, based on the first order Taylor approximation of the objective, the authors introduce closer form policy improvement operators.",
            "strength_and_weaknesses": "# Strength\n* The idea of having a closed-form solution for the policy improvement step is exciting and novel.\n* The idea is easy to implement and can be used as a standalone method and in combination with offline reinforcement learning methods such as OnestepRL and IQL, which do not require fitting a policy for value learning.\n* The method improves over the baselines on a standard benchmark for offline reinforcement learning. \n\n# Weaknesses\n* The method outperforms the policy extraction scheme used in the original implementation of IQL on the antmaze tasks. However, the experimental evaluation needs to be extended to demonstrate how the method combined with the iterative policy evaluation performs on antmaze tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "In overall, the paper is easy to follow. The method is novel.",
            "summary_of_the_review": "The paper is interesting and introduces a simple method that outperforms the baselines. Therefore, I recommend this paper for acceptance. I will increase my score if the authors also add the results on antmaze for the method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1188/Reviewer_CVbg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1188/Reviewer_CVbg"
        ]
    }
]