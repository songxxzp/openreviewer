[
    {
        "id": "7qNV-dVy05T",
        "original": null,
        "number": 1,
        "cdate": 1666422202256,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666422202256,
        "tmdate": 1666422202256,
        "tddate": null,
        "forum": "0qmwFNJyxCL",
        "replyto": "0qmwFNJyxCL",
        "invitation": "ICLR.cc/2023/Conference/Paper756/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the effects of data augmentation in adversarial contrastive learning (ACL). The authors show that stronger data augmentation can result in a larger distribution gap between training data and test data, and a smaller classwise distance. Therefore, the authors propose a piecewise decay augmentation schedule for ACL, namely DynACL. The empirical results validate that DynACL can achieve comparable and even better adversarial robustness than supervised adversarial training (Madry et al., Towards deep learning models resistant to adversarial attacks, 2017).",
            "strength_and_weaknesses": "Strength\n+ This paper provides an interesting and insightful investigation of the effects of data augmentation in ACL. The empirical investigation validates that stronger data augmentation can be harmful to ACL. \n+ The proposed method is compatible with variants of ACL method and has no extra computational consumption. The experiments validate DynACL can significantly improve adversarial robustness compared to previous self-supervised adversarial training methods.\n\nWeaknesses\n- The investigation totally depends on the empirical phenomenon. Say the classwise distance. The paper only shows the phenomenon. But it seems that the paper does not provide more (theoretically) analyses of the reason for this phenomenon.  \n- The proposed dynamic scheduling is somewhat na\u00efve. And, it is strange that the training procedure still uses the most aggressive data augmentation at the final stage, which has been pointed out as harmful to ACL.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-organized and written. I can easily follow most of the parts. The novelty of the proposed method seems to be fair. This paper has good reproducibility since the authors have provided the algorithm in the appendix and illustrated the hyper-parameters in detail.",
            "summary_of_the_review": "Overall, the empirical results solidly support the effectiveness of the proposed method. However, the analysis of some investigation (e.g., why a more aggressive data augmentation can incur a smaller classwise distance?) is not very clear. Therefore, I tend to borderline accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper756/Reviewer_Kni5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper756/Reviewer_Kni5"
        ]
    },
    {
        "id": "CKv4H5kSWI",
        "original": null,
        "number": 2,
        "cdate": 1666600876172,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600876172,
        "tmdate": 1666600876172,
        "tddate": null,
        "forum": "0qmwFNJyxCL",
        "replyto": "0qmwFNJyxCL",
        "invitation": "ICLR.cc/2023/Conference/Paper756/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper first reveals a dilemma about the augmentation strength that either strong or weak data augmentations are harmful to self-supervised adversarial training (self-AT). To resolve the dilemma,  the paper proposes a simple remedy named DynACL (Dynamic Adversarial Contrastive Learning). DynACL adopts a dynamic augmentation schedule along the training process that gradually anneals the strength from strong to weak, then adopts a fast post-processing stage for adapting it to downstream tasks. ",
            "strength_and_weaknesses": "Strength:\n\na)\tThe paper reveals the reason behind the robustness gap between self-AT and sup-AT and investigates the effect of data augmentation strategy on self-AT.\n\nb)\tThe proposed DynACL shows a significant improvement in clean accuracy and robustness over existing self-AT methods.\n\nc)\tThe DynACL is also more computationally efficient.\n\nd)\tThis paper is well written.\n\nWeaknesses:\n\na)\tIn Table 1, the standard accuracy (SA%) of DynACL on STL-10 is significantly lower than other self-AT methods, which is not explained in the paper.\n\nb)\tThe metric RA(%) in Table 2 is not explained.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized, but the presentation has minor details that could be improved. \nThe technicality of the paper appears to be sound, and the theoretical knowledge is explicit.\nThe idea to propose a dynamic augmentation strategy along the training to improve the robustness of self-AT is novel.\nThe code is not available, but the experimental details are sufficient. It doesn't seem difficult to reproduce.\n",
            "summary_of_the_review": "The paper is well written, the contribution is clearly presented and the experimental results are thoroughly executed. I would tend to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper756/Reviewer_UQhW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper756/Reviewer_UQhW"
        ]
    },
    {
        "id": "C80-6o1yeor",
        "original": null,
        "number": 3,
        "cdate": 1666701074621,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666701074621,
        "tmdate": 1668500238792,
        "tddate": null,
        "forum": "0qmwFNJyxCL",
        "replyto": "0qmwFNJyxCL",
        "invitation": "ICLR.cc/2023/Conference/Paper756/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper suggests dynamic adversarial contrastive learning which gradually anneals from a strong augmentation to a weak augmentation. Further, the authors propose fast post-processing stage for adapting it to classification tasks which boost the robustness. From this simple and effective strategy, DynACL reduces the gap between supervised AT and selfsup AT. ",
            "strength_and_weaknesses": "**Strength**\n\n- This paper proposes a simple and effective scheduling method for selfsup-AT which could surprisingly improve the robustness.\n- The paper is well-written and shows convincing motivations.\n- This paper suggests novel aspects of the relationship between augmentation and adversarial training in selfsup-AT.\n- This paper demonstrates extensive experimental results including diverse ablation experiments and semi-supervised settings.\n\n**Concerns and Questions**\n- I hope the authors describe the difference between the DynACL++ and AdvCL post-processing which pseudo labels the unlabeled examples with k mean clustering. I think it is overemphasized claims that fast post-processing is a novel idea.\n- Does proposed technique also can be adapted to RoCL? And also boost the performance of this method too?\n- Does dynamic scheduling also benefit the performance of vanilla simCLR?\n- The proposed dynamic scheduling which gradually decreases the strength of the augmentation seems a somewhat empirical approach seems bit simple approach without any convincing motivation or evidence. It is understandable that we should control the strength of augmentation but I am not sure proposed method is the most effective design of data augmentation. Because if there is some standard to find the adequate strength of the augmentation at a certain training stage of self-AT (i.e., classwise distance, or MMD), an adaptive strength controller could show better robustness than the gradual scheduling.\n- I am not quite sure this approach could be generalized to different architectures, or different frameworks. Since the approach is a naive approach, hyperparameter search may be always needed for optimal performance. And the current gradual scheduling could not be the best option for some circumstances (where may need a longer step size). I think adaptive data augmentation scheduling [1,2] could also be applied to self-AT which could improve the current approach more intuitively and make the approach to be more universal to diverse circumstances.\n\n[1] ADAAUG: LEARNING CLASS- AND INSTANCE- ADAPTIVE DATA AUGMENTATION POLICIES, ICLR 2022\n\n[2] MetaAugment: Sample-Aware Data Augmentation Policy Learning, AAAI 2021",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** The paper is easy to understand and well-written.\n\n**Quality:** The presentation of the paper is good and well-organized.\n\n**Novelty:** The dynamic gradual design of the data augmentation seems not very novel to me but it is simple and effective.\n\n**Reproducibility:** The paper has well reproducibility which elaborates well on the details.",
            "summary_of_the_review": "Overall, I recommend acceptance to this paper. This is well written and has intuitive motivation with empirical results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper756/Reviewer_umnn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper756/Reviewer_umnn"
        ]
    }
]