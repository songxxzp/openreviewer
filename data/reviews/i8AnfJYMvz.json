[
    {
        "id": "mwvuOUJtor",
        "original": null,
        "number": 1,
        "cdate": 1665764630643,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665764630643,
        "tmdate": 1665764630643,
        "tddate": null,
        "forum": "i8AnfJYMvz",
        "replyto": "i8AnfJYMvz",
        "invitation": "ICLR.cc/2023/Conference/Paper6329/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors introduce a novel approach for Preference-based Reinforcement Learning in an offline setting. In contrast to other methods, they do not learn an explicit reward function, but use information matching against a latent representation of an (approximate) optimal trajectory. The latent representation is learn via a transformer approach. The algorithm is evaluated on three benchmark domains, including analyzation of three specific aspects of the approach. Namely, an qualitative evaluation of the embedding space in terms of distance to an optimal trajectory, under a simplified learning scheme and effects of using different points in the latent space as conditioner.",
            "strength_and_weaknesses": "The paper is very well written and clear to understand, with one, small exception (see clarity). The approach itself mainly builds on existing ideas, but goes beyond a simple, straight-forward combination. Furthermore, it is nicely aligned with the current State-of-the-Art and represents a meaningful step forward. The relation to related work is clearly state. The evaluation is ok, but could be improved. Mostly, because only one of the 4 competitors (Tbl.2) is a true PbRL algorithm. Furthermore, 3 domains (with 3 variants each) is ok, but leaves room for substantial improvement. Especially, non-locomotion tasks should be considered. Additionally, the required number of preference queries should be evaluated, as this is usually a limiting factor in PbRL (due to costly, human involvement). The qualitative ablation studies are a good addition to the evaluation. ",
            "clarity,_quality,_novelty_and_reproducibility": "The approach is novel and relevant to the research area. The paper is of high quality and very good to follow, including the mathematical sections. However, to the reviewer it is not completely clear why $I_{\\theta}$ is updated by (6) and (8) independently and not a joint objective. This way, it seems like, the magnitude of the updates may implicitly define a tradeoff between potentially conflicting targets?\nBC in Tabl.2 is never introduced. Reproducibility is likely, as code is available.\n\n",
            "summary_of_the_review": "The paper is very good and the clarity issue are not relevant for acceptance, but could improve the paper further. Only the evaluation is a relevant concern, but is still extensive enough to warrant a recommendation.\n\nIn fact, the reviewer believes, that the method is even more capable than mentioned. In theory, the method should be applicable to preference signals, based on a non-Markovian expert evaluations. This is an important issue for real world PbRL, because real humans cannot be assumed to conform to the Markov property. Therefore a discussion of this assumption (or even evaluation), would be quite interesting.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6329/Reviewer_LFVP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6329/Reviewer_LFVP"
        ]
    },
    {
        "id": "P9rWWLm3l1g",
        "original": null,
        "number": 2,
        "cdate": 1666656833440,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656833440,
        "tmdate": 1666656833440,
        "tddate": null,
        "forum": "i8AnfJYMvz",
        "replyto": "i8AnfJYMvz",
        "invitation": "ICLR.cc/2023/Conference/Paper6329/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents an approach for learning to solve task from offline data using preference feedback and no reward function. Instead of learning a reward function, a latent embedding is learned that encode information about what it means to complete the target task. The latent embedding, z, is learned from preferences over the offline data used to train the policy. The latent embedding z and the policy are learned separately, where there during policy learning and inference, next actions are conditioned on both z and the current observation. Policy training is conducted following Hindsight Information Matching. Experiments are conducted in the Hopper, Walker, and Half cheetah locomotion tasks from D4RL with offline data coming from the replay buffer of a medium quality policy, rollouts from a medium quality policy, and rollouts from a medium-expert quality policy. To evaluate the quality of the learned latent embedding z*, a t-SNE plot is used to visualize how similar z* is a an ideal trajectory. Across conditions, z* is close to the ideal trajectory. The performance of policy rollouts using the \"optimal\" z embedding, a z embedding from a trajectory with large returns, and a z embedding with low returns are compared. Across conditions rollouts from the \"optimal\" z embedding out perform the z embedding from the trajectories with high and low returns. Policy rollout performance is also compared against other offline RL approaches (decision transformer with the ground-truth reward, decision transformer with a learned reward, CQL with the ground-truth reward, and behavior cloning). In 1/3 of the conditions, the proposed methods out perform the baselines. Finally, the impact of iterating between the Hindsight Information Matching objective and policy learning is evaluated via t-SNE projections and visualization. In all but the medium condition, the learned z* and the ideal trajectory are further apart than in the first experiment. ",
            "strength_and_weaknesses": "Strengths:\n- The idea is really interesting. This is probably one of the first works I have seen that attempts to learn a policy from preferences in the absence of a reward function. Moving away from reward functions seems like it could have benefits and improve policy learning. \n- The work is well grounded in the literature and authors do a good job of contextualizing where the method fits and how it is different from what has been done.\n- A good amount of detail has been given to specify how OPPO works meaning the method and the results should be reasonable to reproduce. \n\nWeakness:\n- The proposed method does not consistently outperform the baselines. Clear benefits of the method are not well motivated, especially given the results. For example, the authors motivate their approach by stating that learning reward functions from preferences may result in a reward function that is not correctly calibrated to the preferences. The authors do not follow up with specific scenarios where or under which the reward functions are not well calibrated. It would have been nice to see clear cases where mis-calibration occurs when learning reward functions and then results demonstrating that OPPO is well calibrated in the identified scenarios. \n- The authors only present results using a synthetic preference labeller that gives perfect preference feedback with respect to the ground truth reward. However, humans are not likely to provide such high quality labels. As the quality of the offline dataset impacts rollout performance, it is likely that preference label quality will also impact rollout performance. It has been shown in the BPref (Lee et al. 2021) paper that labeller quality does impact policy performance.\n- I did not see a discussion about the impact of the amount of preference feedback on rollout performance. In online PbRL methods where the reward function is learned, the amount of preference feedback greatly impacts policy performance. It would have been great to see results on this and for there to have been a discussion.\n- The authors claim that their method is guaranteed to learn the optimal latent embedding z, however no proof that optimality is guaranteed is provided. The experiments (i.e. t-SNE plots) are not enough evidence to conclude optimality is guaranteed. \n- Prior online PbRL papers have included results from human experiments where actual humans provide the preference feedback. It would have been nice to see results with humans providing the preference feedback.\n- The authors do not appear to have compared against Shin & Brown 2021, who they call out as a very similar method that learns a reward function from preferences over an offline dataset and then use offline RL methods to learn a policy. It may have been present in the baselines, but it wasn't clear to me.",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is a creative combination of Hindsight Information Matching methods and preference-based reinforcement learning. It seems fairly novel, however it would have been nice to a discussion on the differences between the OPPO and Skill Preferences (https://proceedings.mlr.press/v164/wang22g/wang22g.pdf), both of which use preference feedback to learn latent embeddings used to condition a policy's behavior. In the case of Skill Preferences, the latent embeddings correspond to skills (sub-tasks) instead of complete tasks as in OPPO. Additionally in Skill Preferences, after the skill embeddings are learned, online PbRL is used to learn a policy to select a skill and used a preference-learned reward function. However, it would have been nice to see a discussion of the key differences in how the latent embeddings are learned. \n\nFrom a reproducibility perspective, enough detail seems to have been provided that it should be possible to reproduce the results. However, I did not attempt to reproduce the results and therefore cannot say for certain. It would be great for the authors to release the code and make OPPO more reproducible. The hyper-parameter values for the number of preferences given does not appear to be given, but I may be missing it. \n\nIn terms of clarity and quality of writing, there are a number of places where the authors would benefit from another read through of the paper. There were many sentences with an awkward construction, e.g. \"Sequence modeling enables to model behavior without....\". While the construction did not make the paper incomprehensible, reducing the number of sub-clauses and parentheticals in each sentence would make the paper much easier to read and follow. ",
            "summary_of_the_review": "In summary, the paper presents an interesting idea that seems of benefit to pursue. However, given that OPPO did not consistently outperform the baselines, better motivation for why the method should be used instead needs to be provided. The benefits of OPPO over the baseline need to made more clear. Additionally, there are further experiments that would be helpful to see to fully understand the benefits and robustness of the method. My recommendation for the paper is therefore based upon the need for more experiments and the lack of consistent improvements in performance relative to baselines. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6329/Reviewer_uVj7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6329/Reviewer_uVj7"
        ]
    },
    {
        "id": "2TlTkFBU9m",
        "original": null,
        "number": 3,
        "cdate": 1666669317157,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669317157,
        "tmdate": 1666669317157,
        "tddate": null,
        "forum": "i8AnfJYMvz",
        "replyto": "i8AnfJYMvz",
        "invitation": "ICLR.cc/2023/Conference/Paper6329/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies preference-based offline reinforcement learning where we only have access to the preferences over offline trajectories. Compared to the standard offline RL setting, PbRL doesn't assume the reward function is available, which is more realistic. To solve the problem, the authors propose an iterative algorithm, which optimizes two objectives iteratively and can directly output a policy, instead of first learning a reward function from the preferences and then training a policy on top of it. Empirically, the authors compare their algorithm with several baselines on the D4RL benchmark and demonstrate improved performance. ",
            "strength_and_weaknesses": "Strength:\n- The problem setup in this paper is more realistic than most of the offline RL papers, where the reward is always available for each state-action pair. \n\n- The proposed algorithm is new and can greatly improve the performance over baseline methods on the D4RL benchmark.\n\nWeakness:\n- The motivation of OPPO is unclear to me. Especially, why optimizing two (heuristic) objectives iteratively is a better choice than solving two separated optimizations, i.e., first, learn a reward function based on the preference, and then fit a policy. Can you provide a more rigorous justification for it? \n\n- In addition, I also noticed that DT+$r_{\\psi}$ performs quite close to (only slightly worse than) OPPO, but it performs slightly better than DT+r (ground-truth reward). This is quite surprising, can you explain why this is the case? Also, it seems the OPPO has little advantage over the two-stage algorithm, DT+$r_{\\psi}$. \n\n- The paper is not easy to follow, it has too many vague, informal and unsupported arguments. E.g., \n\n> \"OPPO learns the optimal (offline) policy (\u03c0(a|s, z\u2217)) directly and thus avoids the potential information bottleneck caused by separately learning a reward function\"  what do you mean by information bottleneck?\n\n> \"A better estimate of the optimal embedding provides hints for the encoder to extract more valuable features, while a better hindsight information encoder, on the other hand, accelerates the search process for the optimal trajectory in the high-level embedding space\":  what do you mean by \"hints\"? Can you provide evidence for supporting this argument?\n\n> \"It is worth mentioning that the posterior of the optimal embedding $z^\\star$: why call it a posterior? Shouldn't $z^\\star$ just be a point?\n\n- What is $\\ell$? I did not see the definition of it.\n\n- Is $\\pi(\\cdot|s,z)$ a parameteric model?\n\n- In general, offline RL relies on pessimism to overcome the distribution shift. It seems that OPPO doesn't need pessimism to perform well. If so, can you explain why it doesn't need?\n\n- Can you provide a comparison on the *Random* datasets, e.g., Hopper-Random, Walker-Random, and Halfcheetah-Random? Also, why is the Ant dataset missing?",
            "clarity,_quality,_novelty_and_reproducibility": "The algorithm is new. Code was submitted, but I did not run it. The writing needs to be improved. There are too many informal arguments.",
            "summary_of_the_review": "Overall, I think the problem studied in this paper is very interesting and close to reality. However, I am not really convinced with the motivation of the proposed algorithm, and why we need to do so as in this paper. In addition, the writing of this paper needs to be improved. There are too many vague/ambiguous and informal arguments. The authors should make explainations more formally, and provide either theoretical justifications or empirical results to backup their arguments. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6329/Reviewer_JqyW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6329/Reviewer_JqyW"
        ]
    },
    {
        "id": "EmT3z3Gp3D",
        "original": null,
        "number": 4,
        "cdate": 1666682123508,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682123508,
        "tmdate": 1666682123508,
        "tddate": null,
        "forum": "i8AnfJYMvz",
        "replyto": "i8AnfJYMvz",
        "invitation": "ICLR.cc/2023/Conference/Paper6329/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper looks at the problem of offline RL from human preferences. Instead of first learning a reward model from human preferences and then training with offline RL, this paper learns a context embedding from trajectories, and then learns a policy that is conditioned on the context. Then, to optimize for the human preferences, it learns the optimal context, which can be fed into the policy. \n",
            "strength_and_weaknesses": "\nGenerally I like this research direction, and I think that learning context from offline data can be useful for other tasks as well, such as learning options\n Unfortunately, it doesn\u2019t look like the method convincingly outperforms existing baselines. However, it does perform competitively, and I think that the fact that it doesn\u2019t learn a reward model is a nice aspect.\nI like the experiments showing that the learned optimal context aligns with preferences. \nA question: what is the main difference between learning a reward model and learning a context + the optimal context? It seems almost the same thing. Could they be thought of as the same thing in a broader unified perspective?  Why would we expect OPPO to perform better than learning a reward model? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The method seems like a clear extension of HIM, but seems novel enough. The algorithm 1 is confusing. It seems reproducible enough. ",
            "summary_of_the_review": "I think this is a nice idea, and it should be useful for the community. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6329/Reviewer_kZ4s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6329/Reviewer_kZ4s"
        ]
    }
]