[
    {
        "id": "otUYVs0gSB0",
        "original": null,
        "number": 1,
        "cdate": 1666385185884,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666385185884,
        "tmdate": 1668644532986,
        "tddate": null,
        "forum": "iF0B-U0J5fG",
        "replyto": "iF0B-U0J5fG",
        "invitation": "ICLR.cc/2023/Conference/Paper1327/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes MultiMix which is a variant of the Mixup data augmentation technique. While Mixup creates augmented data by only considering interpolation between two data points, MultiMix considers the interpolation between $m$ data samples. In greater detail, MultiMix creates $n$ new augmented samples from $m$ data samples by linear mixing them with random weight vectors that are drawn from a Dirichlet distribution.  Moreover, the authors extend MultiMix to Dense-Multimix by removing spatial pooling and applying the loss function densely over all tokens/patches in some special deep-learning architectures. In the case that has label information, the author uses online self-distillation for creating soft labels for augmented data. On the application side, the authors conduct experiments in image classification, robustness to adversarial attacks, object detection, and out-of-distribution detection. The proposed techniques improve the classification accuracies on CIFAR10, CiFAR100, and TI better than baselines in both clean and adversarial attacked data. The proposed techniques also yield better accuracy in objective detection on VOC07+12 (SSD model) and MS-COCO (FASTER R-CNN model). ",
            "strength_and_weaknesses": "## Strength\n\n* The paper is well-organized and easy to follow.\n* The experiments are conducted on various datasets and applications. The proposed techniques improve the performance slightly.\n* Multimix is more robust to the adversarial attack than the previous baselines. \n* The dense Multimix idea is interesting to perform mixup techniques in deep learning.\n\n## Weaknesses\n\n* The idea of using more samples to do mixing is not new. The authors should include those variants in the comparison. The authors have mentioned $m$-Mixup. The other variant that the paper should include is $k$-MIXUP [1] which also considers $m$ samples as MultiMix, however, $k$-mixup finds the optimal mapping between two mini-batches of size $m$ via optimal transport.\n\n[1] k-Mixup Regularization for Deep Learning via Optimal Transport, Kristjan Greenewald, et al\n\n* MultMix needs to sacrifice the computation time for the performance. However, I see that the improvement in performance is not very significant while the computational time is only reported in the image classification application. The authors should report the computational time for all other applications for completeness.",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: The paper is well-written.\n* Quality:  The experiments are extensive, namely, they are conducted on various applications and datasets.\n* Novelty: The idea of Multi-Mix is not very original. However, the extension to Dense Mult-Mix is new to the best of my knowledge.\n* Reproducibility: The code is not submitted, however, experimental settings are reported carefully.",
            "summary_of_the_review": "The paper misses some important baselines that also consider $m$ samples for creating new augmented data as mentioned in the weakness part. Moreover, the trade-off between performance and computation is not well-studied. A figure is needed with the x-axis as computational time and the y-axis as the performance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1327/Reviewer_Vnzf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1327/Reviewer_Vnzf"
        ]
    },
    {
        "id": "ikv-0y6DiO",
        "original": null,
        "number": 2,
        "cdate": 1666584829401,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584829401,
        "tmdate": 1667715607602,
        "tddate": null,
        "forum": "iF0B-U0J5fG",
        "replyto": "iF0B-U0J5fG",
        "invitation": "ICLR.cc/2023/Conference/Paper1327/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an extension of the mixup data augmentation method where all examples in a batch are mixed with coefficients sampled from Dirichlet distribution, and the number of mixup examples can be as many as one wishes with more Dirichlet samples. On top of this, the method further adds ensemble distillation and dense augmentation and losses, which is shown to further improve the accuracy. \n\nThe paper argues that mixup on the top layer before logit is the best and also most efficient.\n",
            "strength_and_weaknesses": "strengths\n* proposed an extension of mixup with better accuracy and higher efficient \n\nWeakness\n* the contribution seems a bit incremental \n* it\u2019s unclear how the dense mixup and self-distillation are integral part of the proposed method, can we apply the same techniques to the baseline methods?\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: the paper is overall well written \n* quality: fair, contribution is somewhat incremental\n* novelty: fair, it feels like combining multiple existing techniques together, in an orthogonal way\n* reproducibility: seems good \n",
            "summary_of_the_review": "This paper proposed an extension of the mixup data augmentation method combined with some other techniques such as dense augmentation/loss and self-distillation with ensemble prediction. Though it demonstrates slightly better accuracy, it\u2019s unclear how the different parts (mixup, dense augmentation, self-distillation) interact with each other. \n\nquestions\n* in experiments, half of the time it\u2019s using put augmentation, the other half MultiMix, unclear how this hyperparameter affect performance. Do we must have input mixup? Why?\n* What prevents us from applying dense augmentation and self-distillation to other mixup baselines?\n* for manifold mixup, did you also apply mixup in the last layer?\n* previous papers show mixup more examples doesn't help, which is contrary to what this paper argues. But there is no ablation study to still mixup two examples, but mixup more batches. How do we know the gain is from mixing up more examples or simply because we have more tuples? \n* high-level question: penultimate layer is already linear representation (i.e., input of logistic regression), and mixup is intend to regularize by enforcing linearity of the representation, in this case, why would it still help to apply mixup? In another work, would it help to apply mixup for logistic regression? (or softmax for multi-lass)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1327/Reviewer_rQBR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1327/Reviewer_rQBR"
        ]
    },
    {
        "id": "7Y76CpaTbS",
        "original": null,
        "number": 3,
        "cdate": 1666769356163,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666769356163,
        "tmdate": 1666769356163,
        "tddate": null,
        "forum": "iF0B-U0J5fG",
        "replyto": "iF0B-U0J5fG",
        "invitation": "ICLR.cc/2023/Conference/Paper1327/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a suite of Mix-based augmentation techniques that improve the performance of neural networks.",
            "strength_and_weaknesses": "Strength:\n1. The notations in the paper are rigorous.\n2. The writing of the paper is easy to follow.\n3. The proposed techniques are empirically solid.\n\nWeakness:\n1. None of the proposed techniques is theoretically justified. In other words, it is hard to judge in advance whether an approach helps before actually trying it out. Therefore, describing some scenarios where these techniques FAIL may be helpful.\n2. It looks to me that the paper is an ensemble of tricks for training neural networks --- It does not solve any problem.\n3. The scale of the experiments is limited --- the largest dataset is Tiny ImageNet. While this scale may be sufficient for a theoretical paper, it is less convincing for an empirical paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is rigorously written and easily accessible.",
            "summary_of_the_review": "Overall, I think the proposed augmentation techniques are empirically sound, which expands the toolbox of augmentations for training neural networks. However, none of these techniques has been theoretically justified; therefore, it is hard to judge whether these techniques are applicable in a given (new) scenario.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1327/Reviewer_BWYW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1327/Reviewer_BWYW"
        ]
    }
]