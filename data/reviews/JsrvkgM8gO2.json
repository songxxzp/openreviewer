[
    {
        "id": "mKzmy5FEJoU",
        "original": null,
        "number": 1,
        "cdate": 1666739612702,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666739612702,
        "tmdate": 1666739612702,
        "tddate": null,
        "forum": "JsrvkgM8gO2",
        "replyto": "JsrvkgM8gO2",
        "invitation": "ICLR.cc/2023/Conference/Paper3287/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work considers the effect of large stepsize for GD in non-convex optimization. It shows theoretically that, under certain conditions, with a large stepsize GD can escape/avoid undesirable minimum, and in comparison, GD/SGD with small stepsize might converge into the undesirable minimum. \n\nThen the paper turns to present some empirical results. By re-using a mini-batch for multiple times, the work can, approximately, control the stepsize and the gradient noise in SGD in separate. By comparing this SGD variant with small/large stepsize SGD, some insights are drawn that the effect of large stepsize for SGD can go beyond simply a larger gradient noise. The second part of the experimental results are for verifying the provided theory. ",
            "strength_and_weaknesses": "# Strength \n1. The work considers a very important topic: understanding the effect of large stepsize in SGD. Despite the caveats in the theory (please see more in below), I (and I believe many others in the community) share somewhat similar understanding as the authors have proved in their theorems. \n2. I find most of the presentation is good. I especially like the cartoon figures, which speak for themself in a very nice way. \n3. The design in Section 5.1 is very interesting to me. I think this design could inspire more follow-up study on SGD (stepsize and noise).\n\n# Weakness\nMost of my criticism about this work will be about their theorems.\n\n1. Thm1 could have cited [Wu, et al, 2018]. Note that the \"linear stability\" in [Wu, et al, 2018] can exactly explain the results in Thm 1. Because for the choice of stepsize, $x\\_\\\\dagger$ is not linear stable but $x\\_\\*$ is linear stable. Granted, Thm1 discusses the behavior of the GD in an integrated manner, but the techniques are standard, using the assumptions on the one-point-convexity. I strongly suggest to add some discussion on [Wu, et al, 2018]. \n\n2.  Definition2 could be made more clear by specifying in the condition that $M$ is a fixed set (connected? compact? containing $x\\_\\*$? ...). \n\n3. Theorem 1, could define what is $\\\\mathcal{L}$. I assume it refers to some measure over the randomness of the initialization?\n\n4. Lemma 2 and Theorem 2. The failure probability has a crude, exponential dependence on dimension $d$ and number of iterates $T$. Such an exponential dependence makes the result rather weak and less interesting. I would like to see some discussion on how tight these are. That is, can we improve the dependence on $d$ and $T$ or not? Is the current exponential dependence purely artificial?\n\n5. Both Theorem 1 and 2 put strong assumptions on the landscape of the loss (e.g., exactly two minimum). Moreover, proposition 3 is dedicated for a special example. I am not against to look at special cases for drawing intuition, but the theorems in their current form fail to qualify this work as a solid theory work, in my humble opinion.  \n\n6. I feel the theory and the experiments have only tangential connections. Indeed, the theorems are all for optimization and have little to do with generalization/learning. But crucial in the experiments are the test accuracy for each algorithm. Is it possible to connect the specific loss landscape required for the theorem to some nature learning settings? If not I can hardly see why these losses are important examples for understanding the learning ability of SGD. \n\n# Missing Literature\nIn fact, multiple existing works have investigated the effect of large stepsize and discussed their importance for SGD (and are not from the perspective of enlarging the gradient noise). For an incomplete list, please see the following and the references therein:\n\n[1] Wu, Jingfeng, et al. \"Direction matters: On the implicit bias of stochastic gradient descent with moderate learning rate.\" ICLR 2021.\n\n[2] Beugnot, Gaspard, Julien Mairal, and Alessandro Rudi. \"On the Benefits of Large Learning Rates for Kernel Methods.\" COLT 2022.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarify is fine.\n\nQuality is OK.\n\nNovelty is OK.\n\nReproducibility is good. ",
            "summary_of_the_review": "Please see above. I will consider re-evaluate based on the authors reply but at this point I do not think this work qualifies being published. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3287/Reviewer_hE2g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3287/Reviewer_hE2g"
        ]
    },
    {
        "id": "RcbiWofr_w",
        "original": null,
        "number": 2,
        "cdate": 1666876666899,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666876666899,
        "tmdate": 1666876666899,
        "tddate": null,
        "forum": "JsrvkgM8gO2",
        "replyto": "JsrvkgM8gO2",
        "invitation": "ICLR.cc/2023/Conference/Paper3287/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tries to  explain why large learning rate is important in non-convex optimiztion.\nThis paper shows that  GD with large step size\u2014on\ncertain non-convex function classes\u2014follows a different trajectory than GD with\na small step size, which can lead to convergence to a global minimum instead of a\nlocal one.\nThe intuition behind this paper is direct and easy to follow.\nThe proofs in this paper are solid.",
            "strength_and_weaknesses": "Pros: The inituition of this paper is easy to follow and it can explain why a large learning rate is preferred in a class of non-convex functions. The proofs in this paper are solid and easy to understand.\n\nCons: The writing seems a little messy. Lemma 1 is not easy to read. I also doubt  whether the claim in this paper holds for training neural networks. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has some good insights for explaining why one should large learning in optimizing non-convex function.\nThe writing seems a little messy.",
            "summary_of_the_review": "This paper tries to  explain why large learning rate is important in non-convex optimiztion.\nThis paper shows that  GD with large step size\u2014on\ncertain non-convex function classes\u2014follows a different trajectory than GD with\na small step size, which can lead to convergence to a global minimum instead of a\nlocal one.\nThe intuition behind this paper is direct and easy to follow.\nThe proofs in this paper are solid.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3287/Reviewer_opfE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3287/Reviewer_opfE"
        ]
    },
    {
        "id": "1azDmOliBq",
        "original": null,
        "number": 3,
        "cdate": 1666883873473,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666883873473,
        "tmdate": 1666885616044,
        "tddate": null,
        "forum": "JsrvkgM8gO2",
        "replyto": "JsrvkgM8gO2",
        "invitation": "ICLR.cc/2023/Conference/Paper3287/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper shows that a large learning rate can provably escape local minima and reach global minima for smooth and one-point-strongly-convex functions, while a small learning rate would not work. The paper further shows that a higher stochastic noise is not enough to close the gap with the model trained with a large learning rate.  Some experiments are conducted to disentangle the effects of stochastic noise and learning rate, to compare small and learning rates when training neural networks.",
            "strength_and_weaknesses": "Strength:\n\n1. The paper is clearly-written and the main message is clear.\n2. The literature review is sufficient.\n\nWeaknesses:\n1. The large learning rate is indeed important in neural network training, but the authors did not analyze the neural network training trajectory of gradient descent theoretically: instead, the authors simply assumed the function is smooth and one-point-strongly-convex, and there are exactly two minima. I am not sure the insight obtained in this paper can be generalized in neural networks since some arguments are made over some contrived functions (e.g., the function plotted in Figure 5).\n\n2. I am wondering what is the real technical contribution of this paper. It seems to be that the proof technique is standard: it uses standard proof roadmaps for smooth functions and combines with the one-point-strong-convexity to obtain the relationship between two consecutive iterates. \n\n3. The experiments are not well-designed. In Section 5.1, to disentangle the effects of learning rate and the stochastic noise, the authors should consider noiseless case instead of noisy case with different scale: the reason is that the noise does not only affect the current solution but also affects the whole trajectory of gradient descent. Why the authors turns off batch normalization? \n\n4. The insight of Section 5.2 is very similar to Figure 1 in [Li et al. NeurIPS 2019]. Although [Li et al. NeurIPS 19] did not plot the principal component but it is clear that small and large learning rate gradient descent have different trajectories (Please refer to Section 4 in [Li et al. NeurIPS 2019]).\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good. The paper is very clear about their findings and contribution.\n\nQuality and Novelty: Low to Medium. The authors only considered an oversimplified case of nonconvex optimization with smooth and one-point-strongly-convex loss functions. The authors did not compare their results with some formal studies on large learning rates in neural networks, such as [Li et al. NeurIPS 2019].\n\nReproducibility: Medium. The code is provided, but there are not multiple runs based on different random seed. In addition, the experiment design has some issues which I mentioned in the Weaknesses section.",
            "summary_of_the_review": "The paper considered the effects of a large learning rate and showed that under smooth and one-point-strong-convex assumptions large learning rate is better than a small learning rate. However, it is unclear that how relevant these results are because the authors did not analyze the trajectory of a certain neural network but some contrived functions (e.g., Proposition 3). The experimental design has some issues. Overall the insight is not significant compared with previous work, such as [Li et al. NeurIPS 2019], which formally analyzed the effect of the learning rate in a certain neural network.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3287/Reviewer_juk2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3287/Reviewer_juk2"
        ]
    }
]