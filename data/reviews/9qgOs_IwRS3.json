[
    {
        "id": "y0N6pfW0CIW",
        "original": null,
        "number": 1,
        "cdate": 1666476314578,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666476314578,
        "tmdate": 1669310788319,
        "tddate": null,
        "forum": "9qgOs_IwRS3",
        "replyto": "9qgOs_IwRS3",
        "invitation": "ICLR.cc/2023/Conference/Paper1992/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a new decentralized learning algorithm that bases on the proposed cross gradient aggregation (CGA) algorithm. Specifically, the authors leverage the existing self-gradient and cross-gradient concepts to develop the neighborhood gradient clustering (NGC) algorithm. This proposed method replaces the local gradients of the model with the weighted mean of the self-gradients, model-variant cross-gradients, and data-variant cross-gradients, which is better able to handle the parameter variations in non-IID scenarios. To reduce the communication bottleneck, the authors develop the compressed version of NGC, CompNGC. To validate both algorithms, the authors utilize a benchmark dataset with various model architectures and the results show the superiority of the proposed algorithms.",
            "strength_and_weaknesses": "Strength: this paper presents a novel algorithm on top of an existing one and shows the better model performance compared to the existing baseline. The investigated topic is quite interesting as non-IID remains a challenging problem in the decentralized learning area. The paper is easy to follow and the presentation is clear.\n\nWeaknesses: the novelty in this work is quite marginal. It looks the only novelty is to replace the gradient calculation step in CGA with the weighted mean of self-gradients, model-variant cross-gradients, and data-variant cross-gradients. Such a change is just a minor incremental work, which fails to provide sufficient contributions. Also, concepts are just based on the existing works. Given this, if the authors could provide detailed theoretical analysis on NGC and CompNGC, that may make this paper technically solid and sound. However, the authors only provide empirical results, which to me is not promising as well. If this paper is posited as an applied paper, then comprehensive results are necessarily required, including diverse model architectures, datasets, tasks, topologies and numbers of agents. Through these, if NGC and CompNGC remain competitive and outperform baselines, then the work will look very technically solid and sound. However, the existing experimental results to me are not very comprehensive and promising. \n\n*******************************Post-rebuttal************************************\nI appreciate the detailed response and additional results in the revised paper from the authors. After carefully reviewing the responses, I think the additional results have made the work better. However, the overall novelties in this work is still low to me and it still has much room for improvement in terms of theory. Though I would raise my score given the much better empirical evidences for the proposed algorithm in the revised version, the paper with the current form is still marginally below the acceptance standard. ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the presentation in this paper look good, but the big weakness as mentioned above is the novelty, which is quite marginal. The current form of the paper cannot provide sufficient contributions. ",
            "summary_of_the_review": "This paper presents a new decentralized learning algorithm called NGC to cope with the issues in decentralized learning settings. The authors provide algorithm design and empirical evidence to show case the superiority. This work seems incremental, failing to providing good novelties. The authors should analyze the proposed algorithms theoretically and present more experimental result to validate.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1992/Reviewer_b2jc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1992/Reviewer_b2jc"
        ]
    },
    {
        "id": "KNlfeQ20fRx",
        "original": null,
        "number": 2,
        "cdate": 1666684796547,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684796547,
        "tmdate": 1666684796547,
        "tddate": null,
        "forum": "9qgOs_IwRS3",
        "replyto": "9qgOs_IwRS3",
        "invitation": "ICLR.cc/2023/Conference/Paper1992/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the decentralized distributed training under the non-iid data distribution setting. Different from the baseline method D-PSGD, it leverages self- and cross-gradient information to modify the gradient update for each agent. ",
            "strength_and_weaknesses": "Strength\n1. The motivation of this paper is clear and interesting.\n2. The proposed method looks reasonable.\n3. Experiments have some promising results.\n\nWeaknesses\n1. As pointed out in Introduction section, the idea of leveraging cross-gradient information has already been used in Esfandiari et al. (2021). Therefore, the novelty of this paper is limited in this sense.\n2. It would be great to include some convergence analysis in this paper. Many decentralized learning methods have convergence analysis, such as D-PSGD. Actually, Esfandiari et al. (2021) which also uses cross-gradient information, has convergence analysis as well.\n3. The experiments are only conducted on one dataset, i.e., CIFAR10. It\u2019s better to use more datasets. Decentralized learning algorithms are particularly useful for training large models on large distributed datasets. The authors are encouraged to use large datasets (such as ImageNet).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Look good to me.",
            "summary_of_the_review": "The paper proposed an interesting method for decentralized learning but it has several weaknesses. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1992/Reviewer_LFjx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1992/Reviewer_LFjx"
        ]
    },
    {
        "id": "_tr9Zlred4",
        "original": null,
        "number": 3,
        "cdate": 1666727890945,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727890945,
        "tmdate": 1666727890945,
        "tddate": null,
        "forum": "9qgOs_IwRS3",
        "replyto": "9qgOs_IwRS3",
        "invitation": "ICLR.cc/2023/Conference/Paper1992/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a new algorithm that utilizes groups of stochastic gradients collected from neighborhood nodes in a decentralized learning environment with heterogeneous data distributed in different locations. The proposed algorithm is communication efficient and achieves better empirical performance on non-IID data sampled from the CIFAR-10 Dataset.",
            "strength_and_weaknesses": "Strength: the paper is quite well-written and easy to follow.\nWeakness: the working mechanism of the proposed method is not fully demonstrated. Other than the empirical performance on a benchmark data set, it is not clear to me why this method work or under what circumstances the proposed method will work best.\n\n1. The paper has emphasized the concept of \"non-IID\" data. But what types of data can this method handle? It would be very ambitious to claim that it will work for any type of setting. So it is necessary to clarify or at least give some concrete examples of the non-IID data types under consideration.\n\n2. What types of models are most suited for the proposed method? Regression problems? Classification problems? or other tasks? Different models have very different structures on the gradients and one needs to be more specific.\n\n3. The term \"clustering\" in the title implies some sort of unsupervised learning algorithms which I do not see in the paper.  To the best of my understanding, the paper seems to assume that the data distributions among the neighboring nodes are somehow \"homogeneous\". Otherwise, I do not understand why taking a weighted average of a bunch of inhomogeneous gradients will help improve the model's performance.  Please clarify.\n\n4. Since there are no theoretical justifications for the proposed method, I think carefully designed synthetic experiments are necessary to study the advantages and limitations of the proposed method. Otherwise, I have little confidence in the generalizability of the method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is very clear and easy to follow. The proposed methodology is novel but needs a more comprehensive justification.",
            "summary_of_the_review": "The paper is very well written but the supporting evidence for the model performance is not sufficient.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1992/Reviewer_y4qJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1992/Reviewer_y4qJ"
        ]
    }
]