[
    {
        "id": "B1HUnmtiQB",
        "original": null,
        "number": 1,
        "cdate": 1666007705005,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666007705005,
        "tmdate": 1666007705005,
        "tddate": null,
        "forum": "GPPmQdU3k7",
        "replyto": "GPPmQdU3k7",
        "invitation": "ICLR.cc/2023/Conference/Paper3191/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose introducing Bayesian inference to self-supervised learning for improved model generalization capability. Specifically, they view the traditional learning of the BYOL as a MAP estimation; accordingly, Bayesian posterior sampling methods like the cSGHMC can be leveraged to replace the vanilla SGD optimizer for posterior inference of the model parameters. Experiments on semi-supervised classification tasks are conducted.",
            "strength_and_weaknesses": "Strength.\n(1) The research direction is interesting.\n\nWeaknesses.\n(1) The underlying logic and notations are in a mess. \n(2) The novelty is quite limited.\n(3) The presented techniques are not convincing overall.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the current manuscript are poor. In fact, I think the manuscript is not completed yet. The novelty is quite limited.\nAs the proposed techniques are kind of easy, the reproducibility is believed satisfactory.",
            "summary_of_the_review": "(1) As mentioned in the Abstract, a Bayesian self-supervised learning method is proposed. So why use semi-supervised classification tasks to evaluate its effectiveness? Similarly, how would those experiments justify your goal of ``improving performance in downstream tasks,'' as mentioned in the introduction?\n\n(2) In Eq. (7), what are the definitions of the likelihood and the prior, when we talk about a self-supervised learning method? Besides, since X is a mini-batch, Eq. (7) is actually quite confusing; you are not performing cSGHMC on the mini-batch X, right? Similarly, there are other notations that are confusing/wrong.\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3191/Reviewer_YvPk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3191/Reviewer_YvPk"
        ]
    },
    {
        "id": "Br1HQj-frdO",
        "original": null,
        "number": 2,
        "cdate": 1666628130564,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628130564,
        "tmdate": 1666633550972,
        "tddate": null,
        "forum": "GPPmQdU3k7",
        "replyto": "GPPmQdU3k7",
        "invitation": "ICLR.cc/2023/Conference/Paper3191/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "propose a Bayesian technique by enforcing a prior over parameters for self-supervised learning. The main idea is based on the BYOL to learn the representation and combine that with Cyclical SGHMC. Paper applies the method to two datasets for semi-supervised classification and one dataset for out-of-distribution detection. ",
            "strength_and_weaknesses": "Strength: \n- Rather an important problem. \n- It is simple to follow and straightforward. \n- Enough background is provided.\n-------------------------------------------------------\nWeakness: \n- This paper is highly incremental meaning it is a direct combination of two things. Using BYOL for representation learning and then using a variation of HMC to sample from the distribution both of which were introduced in other papers. \n- All the techniques for predictive distribution approximation and etc are very straightforward and the novelty is very minor. \n- Simulation results are extremely insufficient. It would have been useful if a comparison with other state-of-the-art frameworks was provided. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The method is easy to follow as it is a combination of techniques. But overall, it feels like this paper is unfinished and quality is poor. \n\n - Novelty is very minor.\n\n- Not enough information to reproduce their results but I believe since it is an incremental technique, one should be able to reproduce. ",
            "summary_of_the_review": "Even though the problem is rather important but this paper is highly incremental as it comes down to two basic ideas that were introduced and used by others. Experimental results are insufficient and it does not sufficiently compare this framework with other well-understood methods. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3191/Reviewer_tf9n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3191/Reviewer_tf9n"
        ]
    },
    {
        "id": "qxI90Qymny_",
        "original": null,
        "number": 3,
        "cdate": 1666773512682,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666773512682,
        "tmdate": 1666773512682,
        "tddate": null,
        "forum": "GPPmQdU3k7",
        "replyto": "GPPmQdU3k7",
        "invitation": "ICLR.cc/2023/Conference/Paper3191/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a Bayesian version of the self-supervised learning algorithm (BYOL), where the cyclical stochastic gradient MCMC (cSGMCMC) is employed for approximate posterior inference. Instead of point estimates, the proposed approach constructs posterior distributions of encoder parameters and uses them for downstream tasks. The proposed approach is demonstrated to outperform non-Bayesian baselines on benchmark datasets.",
            "strength_and_weaknesses": "Strengths\n- Bayesian self-supervised learning is relatively underdeveloped; the paper is tackling an interesting problem.\n\nWeaknesses\n- Limited novelty. I see no contribution other than a mere application of cSGMCMC to BYOL.\n- Bayesian inference requires Monte-Carlo approximation which requires multiple forward passes through encoders. This is a critical downside, considering that we typically employ deep neural networks with millions of parameters for encoders. I'm not sure whether the benefit from the performance improvement outweighs the increased inference cost.\n- Experiments were done only for relatively small-scale datasets (CIFAR-10 and CIFAR-100).\n- When training Bayesian neural networks, one should be careful about using data augmentations, since the resulting model might not be interpreted as a valid Bayesian model. To bypass this technical difficulty, existing works studying BNNs often discard data augmentations; I think training BYOL is even harder than vanilla BNNs to be cast as a Bayesian inference problem, especially due to the use of slow-update parameter $\\xi$ (which also is a function of $\\theta)$. For instance, given the data augmentation policy and moving average parameter $\\xi$, what would be a valid likelihood? This is not an easy question to answer.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall I think the paper is easy to follow. However, I don't see a significant contribution to be considered novel. The authors did not provide detailed information to reproduce experiments or a code.",
            "summary_of_the_review": "I think in the current form the submission is not ready for publication, mainly due to the fact that it is a straightforward application of cSGMCMC to BYOL without much care.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3191/Reviewer_6qBF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3191/Reviewer_6qBF"
        ]
    }
]