[
    {
        "id": "vkR2BgZ4iKe",
        "original": null,
        "number": 1,
        "cdate": 1666471752403,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666471752403,
        "tmdate": 1669639723292,
        "tddate": null,
        "forum": "JHklpEZqduQ",
        "replyto": "JHklpEZqduQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2782/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel methodology to perform non-parametric outlier synthesis. It suggests using KNN to identify boundary points, and then generating outlier samples by sampling from a gaussian distribution centered in such samples and discarding the synthetic samples that very likely are closer to the in-distribution data. They suggest regularising the in-distribution training performance by adding a loss term for outliers. They perform some experiments and ablations.",
            "strength_and_weaknesses": "Strengths:\n- Although not extremely novel (which is not a big problem for me), the method is simple and easy to implement. The authors provide a clear algorithm box in appendix. \n- The method is heuristic but principled, and illustrated with a simple and immediate diagram\n- The writing is mostly clear, although the structure of the paper could be improved\n- The ablations are interesting and poke interesting aspects\n\nWeaknesses:\n- Q1: Many times the performance improvements over the baselines seem marginal when measured with some metrics. Could the authors report means and standard deviations for 5-seed experiments for all the proposed methods?\n- Q2: (Very important) The story of the paper is about out-of-distribution detection. However, the experiments in which the network is trained from scratch on the in-distribution data are marginal and not extensive enough (see Q3, Q4, Q5). Using a pre-trained model that has been trained on web-scale data is not particularly meaningful for the plain out-of-distribution (or open set) detection (although recent literature does it, neglecting the definition itself of the problem) since the network already knows how to extract representations about the out-of-distribution data, that make the task of distinguishing in-distribution from out-of-distribution much easier and also ill-posed. It's important for the literature to stop pretending pre-training has no impact just because this allows to report inflated numbers. For the way you frame your paper, I would not suggest putting so much emphasis on large pre-trained models and not on trained-from-scratch ones. If you want to focus on pre-trained models fine-tuning then you should probably change the framing of the paper.\n- Q3: The training setup for experiments training from scratch is also pretty bizarre (why training a ResNet34 for 600 epochs? it seems like it's overtraining) Could you please justify why?\n- Q4: The used baselines are not among the best performing. Could you please compare also with KFAC-LLLA [1], SNGP [2], Deep ensembles [3],, Generalized-ODIN [4] and BatchEnsemble [5]?\n- Q5: To assess the goodness of your method, could you please at least show results of training from scratch on CIFAR-10 and ImageNet on a ResNet and some other architecture of your choice? \n- Q6: The proposed method is applied only for part of the training. I can imagine this is to prevent the second term of the loss from compressing the in-distribution features at the early stages of training when the representations are still not well-formed. Could the authors further discuss this point and provide experiments about it? Especially because the way your method affect representations yields to Q7.\n- Q7: Given the behaviour of the model, I would expect the proposed procedure might potentially have adverse effects on the calibration of a model and on its data-shift robustness . Could you perform experiments measuring these quantities on distribution-shift (for ImageNet, there's plenty of datasets: ImageNet-A/R/v2/Sketches/C etc. etc., for CIFAR-100 there's only CIFAR-100-C, but for CIFAR-10 there's also CIFAR-10.1 and CIFAR-10.2) and in-distribution data? For calibration, you may use the Mean Calibration Error. No need of re-training, just test on these datasets. I don't care if the results are bad, but if something bad happens it should be reported as a limitation. \n\n\nMargin Improvements: \n- Adding up arrows/down arrows to metrics of the tables\n- I hope in Table 4 you reported the accuracies and not the error. \n\n[1] https://arxiv.org/abs/2002.10118\n[2] https://arxiv.org/abs/2006.10108\n[3] https://arxiv.org/abs/1612.01474\n[4] https://arxiv.org/abs/2002.11297\n[5] https://arxiv.org/abs/2002.06715",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, the framing of the story has some problems (as pointed out in the weaknesses). Although the paper is not extremely novel, the proposed approach is principled and reasonable, and clearly described, the ablations are interesting and consider relevant aspects. The information provided seems enough to reproduce the results.",
            "summary_of_the_review": "The paper is overall interesting, but needs to significantly improve the experimental evaluations. If the authors can significantly improve the experiments I'm happy to improve the score. \n\n------------\nPost rebuttal update: The authors have satisfactorily addressed most of my concerns. I therefore update my score to 6, inviting the authors to integrate the new experiments and comments in the new draft.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2782/Reviewer_M7jy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2782/Reviewer_M7jy"
        ]
    },
    {
        "id": "x8c4tP_V52",
        "original": null,
        "number": 2,
        "cdate": 1666654198838,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654198838,
        "tmdate": 1666654198838,
        "tddate": null,
        "forum": "JHklpEZqduQ",
        "replyto": "JHklpEZqduQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2782/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "When deploying ML models in the field, it is important to be able to detect when incoming data is an outlier. This can be quite difficult. This paper proposes an approach which learns to generate out-of-distribution data, and uses it to train a classifier. This classifier can then be used in deployment with the original ML model, to reject data that is too different from the training distribution of the trained model.\n\nPrevious work proposes synthesizing virtual outliers from the low-likelihood regions of the feature space, which is more tractable than synthesizing outliers in the input space, but it modeled the distribution of the feature space as class-conditional Gaussians. The authors instead propose to perform non-parametric outlier synthesis.",
            "strength_and_weaknesses": "- The paper addresses a problem that is important and impactful.\n\n- I am not convinced that the move from Gaussian priors on the feature space to non-parametric forms is an improvement. Existing techniques for generative models train the encoder such that the embeddings have a Gaussian distribution. A valid objection is that this Gaussian distribution can only be expected to hold over the training data. However, nonparametric models require selection of kernel functions, which generally require domain-specific knowledge of the phenomenon in question. If domain-specific knowledge can inform the kernel of the OOD distribution, why can it not inform the selection of the ID in the first place?\n\n- A natural drawback of detecting outliers at the feature level is that a given input-to-feature mapping may do an inadequate job of describing an input that is OOD. In other words, if the OOD sample is also OOD for the the feature space, I am not sure that this approach could help much.",
            "clarity,_quality,_novelty_and_reproducibility": "It is a natural extension to existing work on sampling outliers in feature-space. The experiments are well-done and show improvement with respect to existing approaches.",
            "summary_of_the_review": "The work is a small increment to existing work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2782/Reviewer_8Tpk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2782/Reviewer_8Tpk"
        ]
    },
    {
        "id": "D4zt70_yW14",
        "original": null,
        "number": 3,
        "cdate": 1666680508333,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680508333,
        "tmdate": 1666680767056,
        "tddate": null,
        "forum": "JHklpEZqduQ",
        "replyto": "JHklpEZqduQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2782/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a non-parametric method for OOD generalization. They consider a particular form of OOD, where the objective is to abstain from classification on points that are considered OOD. They define the ground truth of such points according to a level set, $\\mathcal{L}$, which comprises of all points with probability density (in the training distribution) at most $\\beta$. \n\nThey propose a loss function that is a linear combination of the classification risk over in-distribution samples with the error rate of the OOD detector that is implicit within the classifier. At a high level, their training algorithm works as follows:\n\n1. Use nearest neighbors to estimate the probability density of points in the training sample.\n2. Use the parameter $\\beta$ to create estimates on which points are outliers.\n3.  Use Gaussian noise centered at randomly chosen outliers to create synthetic outliers.\n4. apply rejection sampling to make the synthetic outliers conform to the desired distribution.\n5. Finally, apply nearest neighbors for both classification and outlier detection.\n\nHere, nearest neighbors is always applied in the feature space which is learned. They then validate their algorithm over extensive experiments on multiple datasets. ",
            "strength_and_weaknesses": "This paper has an interesting and intuitive algorithm, and carefully validates this algorithm over a wide variety of datasets. Although their algorithm lacks theoretical guarantees, I believe it is still a very interesting and important contribution -- subsequent work could very well lead to more theoretical insights about this problem. \n\nIn addition to addressing a relevant problem, I think this paper proposes an interesting high level approach to applying non-parametric algorithms for OOD generalization, namely outlier synthesis. A key property of classical non-parametric theory is that it generalizes well within the support of a probability distribution (in the large sample limit). To have any hope of giving results on OOD samples, non-parametric methods need training data that is also ``OOD\", and this paper gives a natural way of generating such data. \n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is novel (to the best of my knowledge) and relatively easy to follow.",
            "summary_of_the_review": " I think that this work should be accepted. I am quite interested in theoretical problems that this paper naturally introduces, and I find their algorithm quite aesthetically pleasing (beyond being effective). ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2782/Reviewer_fjV3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2782/Reviewer_fjV3"
        ]
    }
]