[
    {
        "id": "N8EgfEmEMH",
        "original": null,
        "number": 1,
        "cdate": 1666619342289,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619342289,
        "tmdate": 1669118678352,
        "tddate": null,
        "forum": "NRxydtWup1S",
        "replyto": "NRxydtWup1S",
        "invitation": "ICLR.cc/2023/Conference/Paper113/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a masked modeling method for convolutional vision backbones. Instead of simply zeroing out the masked regions in the input images, the method exploits the submanifold sparse convolution to handle irregular masked input. The method is evaluated on multiple popular vision tasks and backbone models.",
            "strength_and_weaknesses": "Strengths:\n\n- Transferring the successful experiences of MIM from Transformers to convolutional models is an important and potentially impactful topic. This paper exhibits some promising results of applying MIM to ConvNeXt and ResNet.\n\n- The idea of using the submanifold sparse convolution to handle irregular masked input is well-motivated and new. The hierarchical encoding and decoding design also look interesting.\n\nWeaknesses:\n\n- The motivation shown in Figure 1 may be straightforward but a bit misleading. I think the main advantage of the proposed method based on the submanifold sparse convolution is that the mask patterns are kept in all intermediate features. If we use the \"vanilla sparseconv\" mentioned in Sec 2.3, the model will also be difficult to generalize to raw input even if the input data distributions are close. \n\n- There are several missing references that should be discussed. [r1] is a widely used sparseconv framework in 3D understanding. [r2] is an existing method that uses sparseconv to accelerate 2D image recognition. \n\n[r1] 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR 2019\n\n[r2] Dynamic convolutions: Exploiting spatial sparsity for faster inference, CVPR 2020.\n\n- Many important implementation details are missing. Sec. 4.1 introduces the details of the proposed pre-training method. However, there is no detail about the fine-tuning process for ImageNet and downstream tasks, which is very important information to compare different pre-training methods and reproduce the results. For example, MAE and SimMIM report the results with 50/100-epoch finetuning. Can SparK models achieve competitive results on ImageNet with fewer finetuning epochs than standard training (300 epochs)? Are extra tricks used during fine-tuning?  \n\n- The discussions in Sec 4.2 emphasize the encoding efficiency of the proposed method. From my experience, sparseconv is usually much slower than standard convolutions with similar theoretical complexity since the standard convolution is well-supported and highly optimized on GPUs. I guess 60% masked patches may not lead to significantly higher training speed than models without masked input in practice. The encoding cost shown in Table 1 is very ambiguous. I think it would be better to report the wall-clock time of the whole system and training process like the MAE paper to clearly compare different methods.\n\n- The comparisons of small backbones are unfair.  ConvNeXt-S have doubled computational costs/parameters compared to ViT-S/Swin-T.\n\n- Why are the detection frameworks used for small and base backbones in Table 2 different? Besides, it would be better to report the performance of the baseline supervised ConvNeXt models in both settings.\n\n- According to Table 4, it seems ConvNeXt-L cannot outperform MAE (ViT-L). It would be better to add more discussions and analyses of this. \n\nMinor issues:\n\n- How about the results if we zero out features of the masked regions (before all 7x7 convs) or replace them with the mask embeddings instead of zeroing out input? Considering sparseconv requires extra libraries/extensions and might not be easy to use, this implementation can be a competitive baseline.\n\n- The formats of citations are different in some sentences. For example, in Section 2.2, there are both \"Bao et al. (2021)\" and \"(Devlin et al., 2018)\".",
            "clarity,_quality,_novelty_and_reproducibility": "- **Clarity:** The paper overall is easy to read and follow.\n\n- **Quality:** Good.\n\n- **Novelty:** Although the idea of using sparseconv to handle irregular input is not new for both 2D and 3D tasks, it seems new to use it for improving MIM of convnets.\n\n- **Reproducibility:** Code is not attached in the submission. Many important details are missing. It would be difficult to reproduce the results based on the information provided in the current submission.\n",
            "summary_of_the_review": "The method presented in this paper is interesting and may be useful in future research and applications. However, there are still many important issues that require further clarification and discussion including missing comparisons, baselines and experimental details. I would upgrade my rating if the above-mentioned issues are addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper113/Reviewer_71p6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper113/Reviewer_71p6"
        ]
    },
    {
        "id": "5M79QcIfCc",
        "original": null,
        "number": 2,
        "cdate": 1666644092938,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644092938,
        "tmdate": 1666644130069,
        "tddate": null,
        "forum": "NRxydtWup1S",
        "replyto": "NRxydtWup1S",
        "invitation": "ICLR.cc/2023/Conference/Paper113/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a method of pre-training convolutional neural networks via masking. They mask parts of the input image, take a convolutional network as encoder, and add a U-net style decoder, with which they predict the parts which were masked out. After pre-training, the decoder is removed, and the encoder is fine-tuned for downstream tasks, similar to recent Transformer-based approaches.\n\nThe approach is experimentally validated on ImageNet classification and COCO object detection tasks and shows improvements over other contrastive and masked pre-training methods with models of similar size in terms of the number of parameters and computational complexity.",
            "strength_and_weaknesses": "Strengths:\n- well written and easy to follow\n- strong results on using networks pretrained with the proposed method for object detection, which could be attributed to the tested object detection architecture, Mask R-CNN, designed for convolutional networks.\n- detailed ablation study. Would be interesting to see if object detection sees improvements when using zero-out masking mode.\n- the provided PyTorch code is useful to have better understanding of the approach\n\nWeaknesses\n- the authors do not show results on large backbones with 86+ classification accuracy. It is not clear if the approach does not scale to larger backbones, or the authors were limited by computational resources. Including such results would be very useful for future work.\n- no statistical significance evaluation, which makes validating the claims difficult, especially in the ablation study. What is the variance in the experiment?",
            "clarity,_quality,_novelty_and_reproducibility": "The approach is simple, the authors describe it in detail and provide code of the decoder. It should not be difficult to reproduce the results.\nQuestions to the authors:\n- How well is SparK able to reconstruct images compared to Transformer? Do they reach similar MSE test error?\n- How well does the method work with linear probing? An evaluation in linear probing mode is missing, even though it is common for contrastive methods.",
            "summary_of_the_review": "I suggest accept, seems like a good paper. It would be beneficial to have linear probing results in addition to fine-tuning.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper113/Reviewer_tYbJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper113/Reviewer_tYbJ"
        ]
    },
    {
        "id": "8JZcPDGDL_",
        "original": null,
        "number": 3,
        "cdate": 1666692484944,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692484944,
        "tmdate": 1666693159537,
        "tddate": null,
        "forum": "NRxydtWup1S",
        "replyto": "NRxydtWup1S",
        "invitation": "ICLR.cc/2023/Conference/Paper113/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper works on the pretext task learning a subfield of self-supervised learning. It follows the track of predicting the original image given random non-overlapped masks. To make it work on non-transformer architectures, the authors propose to use a hierarchical and sparse convolution network to pre-train the network with the reconstruction loss on the masked region. To show the advantages of the paper, the authors provide extensive comparisons with other baseline methods on several downstream tasks. The authors also conduct the ablation study to show the effectiveness empirically. ",
            "strength_and_weaknesses": "Pro:\n1. This paper solves the patch mask on the ConvNet by using a \"sparse\" convolution operation. At the same time, the authors also incorporate the U-Net-like design, which passes the encoding mask to the decoding part to preserve the mask information.\n\n2. This paper provides strong performance across all the tasks and architecture in a self-supervised learning setting.\n\n\nCons:\n\n1. The authors seem to overclaim several two important contributions. \n\na. Removing information of masked parts is difficult for ConvNet. This has been identified by submanifold sparse convolution, which aims to preserve the structure of the unmasked regions. Therefore, it should not be a difficulty anymore.\n\nb. The author claims in the experiment that their work proves that the convnet is not inferior to the transformers. This seems wrong to me. I do think ConvNext has already shown that.\n\n2. In Fig.1, since the second row is the histogram, I can understand that using zeros as masks could increase the number of zeros, but I do not understand how the sparsely dropping does not decrease the frequency of other pixels (Fig. 1c).\n\n3. Table 5 shows that even without the hierarchical encoding, ConvNext outperforms the transform architectures. Indeed, the hierarchical encoding works, but not impressive. The performance gained mostly due to ConvNext itself.\n \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, the paper is well-organized and easy to read.\n\n\nNovelty: The originality and novelty of this paper are very limited. This paper follows the way of MAE, SimMIM, and BEiT.  Compared to BEiT, MAE, and SimMIM prove that simply reconstructing the mask region (l_1 and l_2 norm) will be good enough for the pre-training. This paper transfers the same idea to the ConvNets by incorporating sparse convolution. Therefore, the contribution to the community is much less than the published ones. \n\nReproducibility: It could be able to reproduce the paper, however, some details are not clear.",
            "summary_of_the_review": "Overall, I do appreciate the messages that this paper sends to us. It reinforces the claim of ConvNext that convolution networks can be better than transformers. However, the proposed framework seems very engineering oritented.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper113/Reviewer_Zhkz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper113/Reviewer_Zhkz"
        ]
    }
]