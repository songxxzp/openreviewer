[
    {
        "id": "rIArsN9S5V",
        "original": null,
        "number": 1,
        "cdate": 1666544390278,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666544390278,
        "tmdate": 1670297011280,
        "tddate": null,
        "forum": "D1Iqfm7WTkk",
        "replyto": "D1Iqfm7WTkk",
        "invitation": "ICLR.cc/2023/Conference/Paper318/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a spatially adaptive equivariant partial differential operator (ePDO) based networks. Motivated by the prior work on ePDO by Jenner & Weiler (2021), where a linear PDO is restricted to have constant coefficient matrix shared spatially for translation equivariance, the authors propose a nonlinear PDO scheme that is both spatially adaptive and translation equivariant. The spatially-adaptive coefficients in this work are instead learned based on the feature vector using a generator. When the symmetry group is an affine group acting on $\\mathbb{R^2}$, constraints on the coefficients have been derived to ensure equivariance. The coefficient generation (subject to the derived equivariant constraint) is implemented using an equivariant multilayer perceptron. Numerical experiments are conducted to demonstrate the improved performance on MNIST-rot and Imagenet classification.",
            "strength_and_weaknesses": "**Strength**\n1. The paper is well-written and organized. It is pleasant to read.\n2. The motivation of the proposed work is very clear: linear PDOs with translation equivariance is restricted to have shared coefficients. To achieve spatial adaptivity and maintain equivariance, nonlinear models are necessary.\n3. Even though I have not read the details of the proofs, the theoretical results look natural and convincing.\n4. In terms of implimentation, the authors reduce the model size by learning a diagonal coefficient matrix using an equivariant multilayer perceptron, which is novel.\n\n**Weakness**\nMy concern of this paper mainly stems from the experiments\n1. Compared to the prior work steerable PDOs, the proposed neural ePDOs, because of the additional eMLP for coefficient learning, will surely have a larger model size even when learning only a diagonal coefficient matrix. Can the authors clarify why there are less params for neural ePDOs compared to steerable PDOs in the tables?\n2. Since the dataset use MNIST-rot (without flipping), what is the benefit of $D_{16}$ as the symmetry group?\n3. This is related to the second question: can the authors report the result of E2CNN($C_16$) instead in table 1?\n4. Can authors report the comparison of the actual training/testing *time* instead of the computational flops in the experiments?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well-written, and the message is clear. The idea builds on the prior works on ePDOs, but making it spatially adaptive is novel.",
            "summary_of_the_review": "I think the paper is well-motivated and it has high quality in terms of both novelty and clarity. However, the experimental sections of the paper can be improved. I am willing to improve my rating based on the authors' response.\n\n**Post author feedback**\nThe authors answered my concerns in detail. I am willing to increase the rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper318/Reviewer_rHY6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper318/Reviewer_rHY6"
        ]
    },
    {
        "id": "86CP8S4U4_B",
        "original": null,
        "number": 2,
        "cdate": 1666614782042,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614782042,
        "tmdate": 1666614782042,
        "tddate": null,
        "forum": "D1Iqfm7WTkk",
        "replyto": "D1Iqfm7WTkk",
        "invitation": "ICLR.cc/2023/Conference/Paper318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper designs a novel neural ePDO. PDO has wide applications but needs linearity to ensure the translation equivariance. This paper proposes to use an equivariant MLP as the neural ePDO to realize the translation equivariance. The authors give comprehensive theory and experiments.",
            "strength_and_weaknesses": "I am impressed by the designed method. The authors design a novel equivariant MLP as the neural ePDO to approximate the PDO with translation equivariance. This idea is novel and makes sense.\n\nThe authors theoretically verify the translation equivariance of the proposed neural ePDOs. \n\nThe authors also conducted comprehensive experiments on MNIST and ImageNet. The results show the proposed method significantly outperforms existing methods.\n\nMy major concern is on the literature review and comparison. The authors are encouraged to well position the present work in the existing works.",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "Overall, I recommend accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper318/Reviewer_vWQG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper318/Reviewer_vWQG"
        ]
    },
    {
        "id": "7jpb835A_A",
        "original": null,
        "number": 3,
        "cdate": 1666759875083,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666759875083,
        "tmdate": 1669933297915,
        "tddate": null,
        "forum": "D1Iqfm7WTkk",
        "replyto": "D1Iqfm7WTkk",
        "invitation": "ICLR.cc/2023/Conference/Paper318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces an equivariant neural network architecture based on partial differential operators (PDOs) in the layers.  These operators are constrained to be translation and rotation equivariant.  Relative to previous work, PDO-eConv (Shen et al, 2020) and Steerable PDO (Jenner & Weiler , 2021), this work adds non-constant coefficients to the PDOs.  This changes the equivariance constraint relative to prior work and the authors work out the new equivariance constraint.  They also provide a version of the constraint under the assumption the operators are diagonal.  Experiments show the architecture is more efficient than Steerable PDO as it attain better accuracy on rotated MNIST and ImageNet using fewer parameters.  ",
            "strength_and_weaknesses": "### Strengths \n- PDO based networks have been considered by several authors recently including equivariant versions and its reasonable and novel to consider an extension with non-constant coefficients.  In order to maintain shift equivariance, the coefficients must be functions of input function.  So $ f (\\partial_x f)$ is okay but $x (\\partial_x f)$ is not.   \n- The authors do a good job deriving the equivariance constraints for the coefficient networks in both the full and diagonal case.\n- The experimental results are pretty convincing. The most important baseline is Steerable PDO and that is compared to.  Variations such as discretization, different intermediate representation types, and data augmentation are considered.  In both experiments the current method outperforms using fewer parameters than baselines. \n\n### Weaknesses / Questions \n- Some things are missing from the experiments.  Variance should be reported over multiple runs.  How are the hyperparameters tuned?  It is not clear they were.  How does performance of the model and baseline vary as the number of parameters vary?    I'd also like to see a non-equivariant ablation using non-linear PDOs but without the equivariance constraint.\n- The diagonal operators are claimed to be more efficient.  I'd like to see how much more efficient.  That is, I'd like to see a comparison to the largest practical model which can be built using full matrices. \n- The experiments are both image classification.  Given that the motivation for PDOs is in differential equations, it would nice to see an application for modeling differential equations or a non-invariant application.  One is left wondering what specifically about PDOs is important for solving vision tasks?  If I replaced the FD $\\partial^{\\mathbf{i}}$ operator with other fixed operators with similar statistics would I get similar performance?  If so, our understanding for why this network is effective changes.  Perhaps, it is merely that the operators $\\partial^{\\mathbf{i}}$ provide a more efficient basis for the space of spatial kernels than elementary matrices and nothing to do with their meaning as PDOs. \n- In 5.1, it says $\\rho = p \\rho_0$.  If true, this seems very constrained.  It would be good to do a comparison to other choices.  That said, I don't really understand where $p \\rho_0$ is actually used in the architecture since the experiments section refers to using regular and quotient representations. \n- Is it clear the diagonal assumption is not overly restrictive?  The experiments offer good evidence it is not, but it would be nice to see theoretical evidence as well. \n\n### Minor Points\n6.1 Para 2, Incorrect Reference to Weiler & Cesa for PDOs.  \nEqns. 9, 10.  The notations for how the tensor product representations act on the coefficients are a bit unclear.    \nPage 4 \"discription\" -> description\nPage 3 \"semi-product\" -> \"semi-direct product\"",
            "clarity,_quality,_novelty_and_reproducibility": "The paper could use some editing and proof-reading.  There are some typos and grammatical mistakes.  The introduction is also fairly long and vague.  More seriously, there are some fairly broad and undefended claims in the exposition which should be removed, softened or cited.  \n- \"our model is much less redundant than steerable PDOs\"\n- \"As in practice, regular representation and quotient representation (see supplementary material) are\nmostly adopted for equivariant networks for their superior performance.\"\n- \"which can be regarded as a set of functional equations that are not easy to solve.\"\n- \"the mappings between smooth functions are usually defined as partial differential\noperators (PDOs) such as Laplacian or divergence in the physics area\"\n- \"From the perspective of gradients, the parameters of the PDOs are updated by the globally pooled loss gradients, which results in sub-optimal feature learning at each position.\"\n\nThere does not appear to be code, but the complete proofs, method details, and experimental details are provided and so the results could reasonably be reproduced. \n\nThough relatively similar to PDO-eConv and Steerable PDO, this method adds non-constant coefficients to the PDOs which requires generalizing the equivariance constraint.  It is novel. ",
            "summary_of_the_review": "This paper makes a novel and reasonable generalization of previous equivariant PDO work.  The derivation and method seem sound. The experimental results seem quite strong although not as thorough as they could be.  The choice of experimental domains is limited to invariant classification task.  The paper presentation and polish could use work.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper318/Reviewer_j4DB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper318/Reviewer_j4DB"
        ]
    },
    {
        "id": "GXnkr_xOMVm",
        "original": null,
        "number": 4,
        "cdate": 1666794837918,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666794837918,
        "tmdate": 1666794837918,
        "tddate": null,
        "forum": "D1Iqfm7WTkk",
        "replyto": "D1Iqfm7WTkk",
        "invitation": "ICLR.cc/2023/Conference/Paper318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper describes an equivariant implementation of non-linear partial differential operator (PDO) based networks. A key contribution of the paper is in the non-linear adaption of equivariant PDOs (ePDOs), which makes them more parameter efficient and expressive. The idea is intuitive and sensible, based on the idea that the operators should be locally adapted to local patterns. This is achieved by letting a local feature vector determine the PDO coefficients locally. This coefficient predictor then has to be equivariant as well, and this is achieved by equivariant MLPs. The paper then provides a clear recipe/theory for the conditions under which the PDO is indeed equivariant, which guides the construction of an efficient implementation. The proposed method is extensively validated on rotated MNIST as well as on ImageNet, and confirm the added benefit of working with non-linear ePDOs as opposed to linear ePDOs.",
            "strength_and_weaknesses": "**Strengths**\n\n1. The paper implements an intuitive and appealing idea: making the main layers of NN non-linear and adaptive to the present features locally. In some sense this also happens in transformers via attention, or via deformable convolutions [Dai] and possibly other works, however, what makes this approach interesting is that is efficient in that the operations (PDOs) are strictly local. Their non-linear adaption boilos down to a point-wise (equivariant) MLP.\n2. The proposed work is timely, and builds up recent advances in PDO-based deep learning.\n3. The experiments are well done and clearly underpin the benefit of non-linear ePDOs vs linear ePDOs and normal resnets.\n4. The paper is theoreticall sound.\n5. The paper is overall well written and precise, but could occasionally benefit form added details and intuition.\n\nDai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., & Wei, Y. (2017). Deformable convolutional networks. In Proceedings of the IEEE international conference on computer vision (pp. 764-773).\n\n**Weaknesses**\n\n1. Upon first reading, it is hard to distill the differences between the ePDO of Jenner and Weiler, from the contributions in this paper. It is clear though that it is the modification of the PDO coefficient being dependent on the input (which is an important contribution in itself!), however, the kernel constraint is very hard to digest. It would have been nice to put this in perspective relative to the original constraint posed in the paper by Jenner and Weiler.\n2. The paper is a bit untransparent when it comes to the \"steerabble\" aspect of things. Parts are clear, vector that collects derivatives can in principle be steered by some representation of G, like in Eq.5, but it is unclear what the types of the input and output feature fields are, and how to set them. It may help a lot of some intuition is provided on how to pick those reps, also in relation to the kernel constraint. Overall, I think an intuitive breakdown of Eq 8 would be helpful.\n3. Some minor details are missing: \n    1. to which sub-group is the quotient representation defined?\n    2. The method is overall more efficient (in memory and flops), could it be made more precise where this effiency comes from? The operations themselves are more expensive than the original ePDOs, right? Is it mainly due to working with less independent feature channels (e.g. choosing a lower p in the non-linear case compared to the linear ePDO setting)?\n    3. In proposition 3, why is it important to consider regular or quotient representations (which are also regular), and not irreducible representations? I.e., why not formulate it for any representation of G?\n    4. Might this have to do with sec 5.1 where the equivariant MLPs use ReLUs. If working with irreps, not all activation functions may be allowed (though this seems to have nothing to do with proposition 3 otherwise).\n    5. What sigma did you pick for the Gaussian derivatives?",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned above the paper could benefit from additional details and intuitive explanations, though overall the paper is sound.\n\nThe work is of high quality and relies on state-of-the-art techniques, the experiments are appropriate.\n\nThe work is novel.\n\nIn regards to reproducibility, all theoretical details as well as an appendix with details are provided, but releasing code with this submission would greatly improve reproducibility (translating the work to code may be a challenge!)",
            "summary_of_the_review": "The paper is a great submission to ICLR; it is novel and sound, but could still benefit from additional details and intuitive explanations. I judge these improvement to be possible within the rebuttal period/before the cam-ready as they can mostly be fixed with textual improvements. I therefore recommend accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper318/Reviewer_V42w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper318/Reviewer_V42w"
        ]
    }
]