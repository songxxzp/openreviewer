[
    {
        "id": "glXY4si4hYV",
        "original": null,
        "number": 1,
        "cdate": 1666238376777,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666238376777,
        "tmdate": 1669355463210,
        "tddate": null,
        "forum": "yZCpZrUqzK0",
        "replyto": "yZCpZrUqzK0",
        "invitation": "ICLR.cc/2023/Conference/Paper1710/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a method for identifying when a distirbutional shift from the training data has been received in a DNN. The method considers a window of recent instances as opposed to identifying single OOD instances.",
            "strength_and_weaknesses": "## Strengths\n\n- The method is well motivated and sound.\n- The method outperforms previous methods with impressive gains in computational complexity.\n\n## Weaknesses\n\n- Do the metrics outlined in section 2 such as MMD and KS-BBSD have the implcit assumption that the data is coming in from an unbiased sample of the Q distribution? For instance, would everything break down if the test time environment made predictions by streaming many instances of a single class at once? \n\n- I ask the question above, because the testing data being an unbiased sample from the data seems to be an implicit assumption of the method, and one which might not be realisitic for an entire range of models in practice, which may hinder its usefulness. I realize this is a consequence of the chosen problem setting, but I think at the very least, this should be stated as a limitation which (I think) applies to the selected baselines as well.\n\n- Page 4: \"more precisely one minus the entropy...\" Entropy of a softmax $h$ is in the range $[0, \\infty]$. So why would one want to look at $1 - h$? Does this mean to only look at the class prediction index as a single Bernoulli variable?\n\n- The ImageNet experiments in Table 3 aggregate over all corruptions of the different imagenet datasets. It would be nice to see breakdowns of the performances on individual datasets as well (maybe in the appendix) to see if any patterns emerge between the different types of datasets. \n\n- The intuition around equation 2 and 3 could be expanded, maybe in the appendix. The understanding of the whole paper rests on these two equations, and they were both hard to digest at first without great effort for me. I think it would be easier for most readers to grasp if equations 2-3 could be justified with a non-rigorous, but intuitive explanation. \n\n## Minor\n\n- Page 2: \"As shown here, the KS-BBSD...\" as shown where? This sentence is unclear.\n- is the sorting in algorithm 1 ascending or descending? Or does it not matter?\n- Section 4.2: $(b^*_j, \\theta_j)$ is said to be a threshold and a bound, respectively, but the algorithm has those terms reversed. Shouldn't it say \"bound and a threshold, resepctively?\"",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n\nOther than some minor issues above, everything is clear. I would specifically like to see an intuitive explanation of equations 2-3 added for clarity.\n\n## Quality\n\nThe quality is high, in my opinion\n\n## Novelty\n\nTo my knowledge, the paper is sufficiently novel",
            "summary_of_the_review": "Overall, I think this is a solid work, which gives an adequate contribution. I would still like to see the concerns raised above be addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1710/Reviewer_TC5F"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1710/Reviewer_TC5F"
        ]
    },
    {
        "id": "e120zmGsgx",
        "original": null,
        "number": 2,
        "cdate": 1666658441340,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658441340,
        "tmdate": 1666658441340,
        "tddate": null,
        "forum": "yZCpZrUqzK0",
        "replyto": "yZCpZrUqzK0",
        "invitation": "ICLR.cc/2023/Conference/Paper1710/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper focuses on improving distribution shift detection given a pre-trained deep model. It proposes a coverage-based statistic and uses the statistic to detect distribution shifts. The statistics are proven to have a high-probability coverage of the in-distribution data. The detection step is through a two-sample t-test. Experiments on CIFAR and ImageNet show the proposed method outperforms existing baselines. ",
            "strength_and_weaknesses": "Strengths:\n- The paper does a good work of explaining its scope. \n- The experiments are of high quality. Experiments are comprehensive and evaluate a lot of datasets. \n\nWeaknesses:\n- My main concern is the novelty of the paper. The paper proposes a new method for distribution shift detection. However, based on the description of the authors, the proposed method seems like a quick combination of two works (Langford & Schapire, 2005 and Geifman & El-Yaniv, 2017) without much modification. Maybe state clearly which efforts you made to adapt the two referenced works to the current task.\n- I feel the main idea and the motivation of the proposed method can be better explained intuitively with plain language. So the method can get better clarity. For example, \u201c... find better summary statistics of the ID data inspired by generalization coverage bound\u2026\u201d (just an example, please ignore my terminology if I\u2019m wrong).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper describes the considered problem and clarifies its scope very well. The experiments are of high quality. But the novelty and clarity of the methods still need to be addressed.\n\nI also have the following questions while reading the paper:\n- Why do you choose $log_2 m$ as the number of target coverages?\n- How do you solve the *equation* in Lemma 4.1 and what is the complexity? What do you mean by *equation*? Curiously, what is an argmin of an inequality? \n- When the majority of the window contains normal data, how do you determine the window as a distribution shift or normal samples?\n- Because some results in the tables are quite close, it would be good to report the error bar as well to explain the significance.\n- When I was reading the paper, I didn\u2019t feel the *selective classifier* fit into the paper. I could still get a rough picture without it. Or if I only knew the confidence function, I could still get the idea. \n",
            "summary_of_the_review": "The paper proposes a novel distribution shift detection method for deep models. The authors seem to quickly combine two works (Langford & Schapire, 2005 and Geifman & El-Yaniv, 2017) for their proposed solution. The novelty of the work is not enough without further addressing this. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1710/Reviewer_Uqk1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1710/Reviewer_Uqk1"
        ]
    },
    {
        "id": "3w2WfAs9wu4",
        "original": null,
        "number": 3,
        "cdate": 1666673588771,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673588771,
        "tmdate": 1666673588771,
        "tddate": null,
        "forum": "yZCpZrUqzK0",
        "replyto": "yZCpZrUqzK0",
        "invitation": "ICLR.cc/2023/Conference/Paper1710/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new out-of-distribution detection method when sampled are provided within a given window.  The core algorithm aims to find optimal coverage lower bound and the detection threshold given confidence parameter, and target coverage. The authors provide theoretical analysis and empirical studies on small-scale and large-scale datasets.",
            "strength_and_weaknesses": "\nStrengths\n- The method considers the window-based settings, which is relatively less explored in the literature.\n- The method is theoretically motivated and performs quite well in practice. \n\nWeaknesses\n- Lack empirical comparison with single-instance OOD detection methods. The paper mentions that \"Single-instance methods are trivially applicable to a window\". However, no methods are compared. What are the practical implications of \"not considering population statistics over the window\"? I wonder if authors provide empirical comparisons with representative methods in the literature, such as MaxLogit score [1] derived from the logit space and KNN score [2] derived from the feature embeddings.\n- Unlike the majority of instance-based methods in the literature, the proposed method is limited to window-based inputs. e.g., it might be better to change the title to indicate the specific set up.\n\n\n[1] Hendrycks et al., Scaling Out-of-Distribution Detection for Real-World Settings, ICML 2022\n[2] Sun et al., Out-of-Distribution Detection with Deep Nearest Neighbors, ICML 2022",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear and well-organized.  ",
            "summary_of_the_review": "The paper is theoretically motivated, sample-efficient compared to the baselines, and empirically performs well. However, the empirical evaluation can be further expanded. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1710/Reviewer_Mwu4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1710/Reviewer_Mwu4"
        ]
    },
    {
        "id": "NjrKGrvU-O",
        "original": null,
        "number": 4,
        "cdate": 1668208470244,
        "mdate": 1668208470244,
        "ddate": null,
        "tcdate": 1668208470244,
        "tmdate": 1668208470244,
        "tddate": null,
        "forum": "yZCpZrUqzK0",
        "replyto": "yZCpZrUqzK0",
        "invitation": "ICLR.cc/2023/Conference/Paper1710/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a distribution shift detection method, with the information of k-window data instances. The paper proposes a coverage-based method with a guaranteed bound, based on this, a further detection algorithm is proposed for efficient processing with the advantages of algorithmic complexity in terms of both space and time. The evaluation results also demonstrate the potential of the proposed technique.",
            "strength_and_weaknesses": "Strength:\n\n- Important problem\n- Group-based distribution shift detection is less investigated, the proposed techniques are Sound and feasible solution\n- Some formal guarantees of the bound are analyzed\n- Extensive evaluation to demonstrate the potential usefulness\n- Promising results\n\nWeakness:\n\n- Evaluation is only performed on image data for classification tasks.\n- The scenarios of the distribution shifts are mostly simulated, not real scenarios from the real world.\n- Unclear about the insights of \u201ck\u201d selection and its relation to the population size of training data or the original data distribution P.\n- Unclear how to extend to streaming data cases.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is mostly well-organized and written. The paper is somehow novel, which investigates the distribution shift detection from a multi-instance angle, which also gives a nice discussion of previous work.",
            "summary_of_the_review": "Overall, I enjoyed reading this paper, the proposed techniques should be sound and feasible. The evaluation is relatively comprehensive on the case of CIFAR and ImageNet. In addition, the authors also provide a formal analysis of the bound analysis,  as well as the detection complexity analysis of the proposed algorithms.\n\nEven though, the paper still posts a few concerns that the authors could consider for further enhancement.\n\n- The proposed methods rely on quite a few hyper-parameters, I would recommend add more discussion on their insights, the heuristic to choose them, as well as evaluating the impact of such selection.\n\n- During the evaluation, the authors mostly compared with KS and MMD methods, and simply believed that the single-instance-based methods e.g., for OoD detection would not work, which is not fully convincing to me. I would highly recommend adding comparisons with some SOTA instance-based OoD/Uncertainty detection liked methods, under the window k, for comparative analysis. This could be critical from my perspective, especially if the instance-based-OoD detection already works quite well, which is also efficient in some cases, e.g., under what conditions.\n\n- Regarding the evaluation scenario, I understand authors might try hard to try simple scenarios to simulate distribution shift, but the evaluated scenarios are quite artificial, which might be quite far away from real-world cases. I would recommend authors (1) considering some real-world cases, (2) as well as evaluating on more diverse tasks.\n\nFor example, the following paper could be a good reference.\n\n**WILDS: A Benchmark of in-the-Wild Distribution Shifts**\n\n***Pang Wei Koh,\u00a0Shiori Sagawa,\u00a0Henrik Marklund,\u00a0Sang Michael Xie,\u00a0Marvin Zhang,\u00a0Akshay Balsubramani,\u00a0Weihua Hu,\u00a0Michihiro Yasunaga,\u00a0Richard Lanas Phillips,\u00a0Irena Gao,\u00a0Tony Lee,\u00a0Etienne David,\u00a0Ian Stavness,\u00a0Wei Guo,\u00a0Berton Earnshaw,\u00a0Imran Haque,\u00a0Sara M Beery,\u00a0Jure Leskovec,\u00a0Anshul Kundaje,\u00a0Emma Pierson,\u00a0Sergey Levine,\u00a0Chelsea Finn,\u00a0Percy Liang***\n\u00a0*Proceedings of the 38th International Conference on Machine Learning*\n,\u00a0PMLR 139:5637-5664,\u00a02021.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1710/Reviewer_RPBQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1710/Reviewer_RPBQ"
        ]
    }
]