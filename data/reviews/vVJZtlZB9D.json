[
    {
        "id": "R0Gg9vz3CH1",
        "original": null,
        "number": 1,
        "cdate": 1666661883337,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661883337,
        "tmdate": 1666661883337,
        "tddate": null,
        "forum": "vVJZtlZB9D",
        "replyto": "vVJZtlZB9D",
        "invitation": "ICLR.cc/2023/Conference/Paper1261/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This work introduces a unifying framework to prove the strong lottery ticket hypothesis for general equivariant networks. Firstly, the authors prove that any fixed width and depth G-equivariant network that uses a point-wise ReLU activation function can be approximated with high probability to a pre-specified tolerance by pruning a randomly initialized overparameterized G-equivariant network to a G-equivariant subnetwork. Additionally, this work proves such a prescribed overparametrization scheme is optimal and provides a corresponding lower bound. Experiments on E(2)-steerable CNNs, k-order GNNs, and MPGNNs validate the proposed theory.",
            "strength_and_weaknesses": "### Strength:\n\n1. This work proposes a theoretical justification of the strong lottery ticket hypothesis for general equivariant networks.\n2. This paper is well-organized, starts from the theoretical results of general equivariant networks, and follows by specific case studies as well as empirical evidence.\n3. Experiments are conducted across several equivariant networks. \n\n### Weakness:\n\n1. Whether the proposed framework can further be extended to equivariant networks with other activation functions besides point-wise ReLU?\n2. Minor: The y-axis is covered by other subfigures in Figure 3.",
            "clarity,_quality,_novelty_and_reproducibility": "None",
            "summary_of_the_review": "None",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1261/Reviewer_Tcgp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1261/Reviewer_Tcgp"
        ]
    },
    {
        "id": "gW_6h2cxtl",
        "original": null,
        "number": 2,
        "cdate": 1666675981206,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675981206,
        "tmdate": 1668573815220,
        "tddate": null,
        "forum": "vVJZtlZB9D",
        "replyto": "vVJZtlZB9D",
        "invitation": "ICLR.cc/2023/Conference/Paper1261/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides a framework for proving the Strong Lottery Tickey Hypothesis (SLTH) for G-equivariant networks based on the proof by Pensia et al. They show that within this framework, several of the known results (Pensia et al., da Cunha et al., Burkholz et al.) can be considered instantiations of the general theorem.",
            "strength_and_weaknesses": "**Strengths:**\n1. The paper seems to be technically sound and a generalization of several known Strong LTH results. I am not yet convinced that Equivariance is an important property of DNNs but it seems that there is some interest in the area.\n2. The paper is fairly well written with few typos which I will point out.\n\n**Weaknesses:** (I will also include my questions for clarification here)\n1. In the end of section 2 it is stated \"With no assumption on the inputs, ...\" - I don't follow this statement. Can you elaborate?\n    - Also, merely a few paragraphs later, the inputs are assumed to be bounded in the unit $\\ell_2$ sphere and the same decomposition is invoked. So I don't follow the criticism of Pensia et al. here.\n\n2. Minor typos:\n    - In Section 3.1, \"To approximates $f$...\" is a typo.\n    - In Section 3.2, \"is far from optimal as their result give guarantees..\" is a typo.\n    - In Section 4.2 and a few other places, the $\\times$ operator seems to be typeset incorrectly. Please check.\n    - Please use \\citep{} when citing multiple works in the end of a sentence.\n\n3. End of Section 3.1 states that this result improves on the overparameterization of Pensia et al., since G-equivariant networks have fewer effective parameters.\n    - However, Pensia et al., prove a result about pruning an overparameterized dense network to recover a dense target network while the result here is to approximate  G-equiariant net. Does this result give a tighter bound even when approximate dense networks? Otherwise, the comparison is incorrect and unfair.\n\n4. Section 3.2 states that the lwoer bound is (tight)\n    - This is untrue. The bound is loose by a $\\log(n)$ factor which they claim can be culled since it arose from a poor choice norm. However, in the appendix they say that a rescaling \"might\" eliminate the term.\n    - It is also stated that by choosing the weights carefully, it can be eliminated. But the randomness is essential for the definition of a Strong LT. Also, if one is allowed to choose the weights exactly, you could choose a bespoke overparamterization for each target network which makes the entire result useless.\n    - I'm still not convinced that the $\\log(n)$ term is easy to get rid of.\n\n5. In Section 1, it is stated that \"Moreover, Theorem 1 can be extended to the settings where overparamatrized networks have depth $L+1$ as in Burkholz (2022b)\"\n    - However, all the results in the paper seemed to need $2L$ depth. Can you point me where this is shown?\n\n6. Table 3 results:\n    - The overparameterization required in the results seems to be extremely high. The experiments are in-line with Pensia et al., but the overparameterization there seemed to be more like $10$. Also, in Ramanujan et al., the required overparmaterization is $2$ or $4$. Is there a reason the overparamterization is so large here?\n\n7. Lemma 3 in Appendix:\n    - It is stated that these results are extensions of those specified in Pensia et al. I believe this result is stated as a remark in Pensia et al. Please check.\n\n8. Appendix B.3: Lower bound:\n    - I don't understand several of the statements following Theorem 2. It is stated that the assumptions are \"only used to ensure that the lower bound holds\" - I'm not sure how this is a valid justification of the assumption. It seems fairly essential to the proof.\n    - I agree that the assumptions are mild and have no problem with their usage, but these statements are somewhat misleading.\n\n9. Table 4 and Table 5 are very hard to read. Please clean them up.\n\n10. Similarity to Pensia et al.:\n    - The proof techniques, statements and even the lower bound seem almost identical to Pensia et al. with a switch in terminology to account for equivariant networks.\n    - While I don't question that this is a valid contribution, I can't help but feel there is a simpler way to transfer these proofs to this domain.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity, Quality, Novelty:**\n- The paper is fairly well written with minor typos and formatting errors as mentioned above.\n- I think the notation is somewhat dense, and while I feel it can be made simpler, it is not obvious to me how.\n- As I mentioned above, the proofs and statements seem extremely similar to Pensia et al., so I would hope that there is an easier way to transfer those results to this domain. The authors cite Pensia et al., but I would have liked them to acknowledge the extent to which they borrowed techniques from there.",
            "summary_of_the_review": "I think the paper is a reasonable contribution. I have some questions and clarifications but if those are answered, I would recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1261/Reviewer_THqA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1261/Reviewer_THqA"
        ]
    },
    {
        "id": "6uYBf4Sr3_",
        "original": null,
        "number": 3,
        "cdate": 1666809773658,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666809773658,
        "tmdate": 1666809773658,
        "tddate": null,
        "forum": "vVJZtlZB9D",
        "replyto": "vVJZtlZB9D",
        "invitation": "ICLR.cc/2023/Conference/Paper1261/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proves the Strong Lottery Ticket Hypothesis (SLTH) in the context of equivariant neural networks. The SLTH states that for every network f, and every sufficiently overparametrized, randomly-initialized network h, there is a pruning of h that recovers f.\n\nPrior work: In the context of fully-connected networks, the SLTH was proved by Malach et al.\u201920 and then strengthened by Pensia et al.\u201920, who gave upper and lower bounds.\nThe SLTH was extended to CNNs by da Cunha et al.\u201922 and Burkholz \u201922, where the key new technical contribution was to give a pruning scheme that preserved translation equivariance of the network.\n\nThis work: This work generalizes the arguments of Pensia et al\u201920 and Burkholz\u201922 to apply to any equivariant networks. The pruning scheme bears similarity to Pensia et al.\u201920, but with the main difference that it works by pruning weights in an equivariant basis.\nThe main theorem proving SLTH is discussed in the contexts of E(2)-steerable networks and permutation equivariant networks, and experiments are given to verify its validity.\n",
            "strength_and_weaknesses": "Strengths:\n* The SLTH is of great interest to the community, and this papers\u2019 results seem to nicely generalize all previous papers, in particular recovering the result for CNN.\n* The construction is neat and easy to follow.\n\nWeaknesses:\n* The setting of SLTH is not novel, and has been previously studied. The proof techniques in this paper are also quite similar to the proof techniques of previous papers, but generalized to equivariant networks in general (beyond MLPs and CNNs). Nevertheless, I understand that this is not an actionable criticism for the authors.",
            "clarity,_quality,_novelty_and_reproducibility": "* The clarity and organization of the writing is very good.\n* The proofs seem correct.\n* Novelty: see above.",
            "summary_of_the_review": "This paper is well-written and unifies and generalizes previous results on Strong Lottery Ticket Hypothesis for MLPs and CNNs. I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1261/Reviewer_3WVk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1261/Reviewer_3WVk"
        ]
    },
    {
        "id": "PwP9VXJniO",
        "original": null,
        "number": 4,
        "cdate": 1666923709564,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666923709564,
        "tmdate": 1666923959421,
        "tddate": null,
        "forum": "vVJZtlZB9D",
        "replyto": "vVJZtlZB9D",
        "invitation": "ICLR.cc/2023/Conference/Paper1261/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Recent work has shown that the strong lottery ticket hypothesis (SLTH) can be extended to classical convolutional neural networks, with a similar level of overparameterization as for dense networks. CNNs are translation equivariant (ignoring pooling and edge effects), so a natural question to ask if the SLTH can be extended to more general G-equivariant networks. The main contribution of this paper is to show SLTH can indeed be generalized to G-equivariant networks. \n\nThe question whether there should exist equivariant strong lottery tickets for sufficiently overparatermized G-equivariant NNs is not trivial at first, but in hindisight fairly obvious (as explained below). The lack of triviality partly stems from the fact that the sub-network extraction can easily result in loss of equivariance, defeating the purpose. Nonetheless, the authors show that this is possible i.e. a target GCNN with fixed width and depth (and which uses a pointwise non-linearity like the ReLU) can be approximated with high probability by a subnetwork in a random GCNN which has double the depth, and has the width increased by a log factor. This subsumes results of Pensia et al. and Orseau et al. for MLPs and that of Bukrholz et al. and da Cunha et al. for CNNs. The authors also show that a log factor increase in width is actually optimal, and this work irrespective of the overparameterization strategy employed. \n\nThe authors rely on the use of a G-equivariant basis -- which is a basis of equivariant linear maps between two vector spaces. The error metric the authors consider (for the approximation quality of the subnetwork) is uniform approximation over a unit ball. The approaches of Pensia et al. da Cunha et al., Burkholz et al. are adapted to work for the equivariant case. More specifically, the subset sum approach fails if applied as is. The idea for pruning then becomes pruning out the weights that combine the equivariant basis (rather than pruning in a canonical basis as parameters of the weight matrices can't be directly pruned). Thus what might seem non-trivial at first is fairly obvious -- we simply need to prune the parameters that combine elements of this basis. However, this introduces an additional difficulty of dealing with pointwise non-linearities like ReLU effectively -- which is an additional technical challenge that the authors address.\n\nFinally, the authors present case studies for specific choices for G. For the translation group their results generalize earlier results. The main theorem is also applied for the E(2) steerable case, permutation equivariant networks, and message passing networks. Experimental results show that the proposed approach is able to approximate the target network adequately. ",
            "strength_and_weaknesses": "- The problem is well-motivated and important. G-equivariant networks are now becoming increasingly important in applications, especially in the physical sciences. It is thus important to understand if an equivalent of the strong lottery ticket hypothesis also holds for them. \n- The results are solid to the best of my knowledge (although I have only worked through the proofs while skipping some details). The approach is fairly obvious -- they adapt earlier work, but not to prune the weights themselves, but the coefficients that are used to combine the equivariant bases. \n- The case-studies for the 4 cases considered are useful to position the results in context. \n- The results generalize earlier results for MLPs and CNNs. \n- Experimental resuls show a proof of concept that their approach is able to find such sub-networks. \n- Paper is well-written and is easy to follow. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clear and easy to read, and has a very clear motivation. \n- The quality of the results, in my opinion, is solid. These results are extensions of existing results, with a change in the pruning strategy, but they do provide a unified perspective on them, which is useful. To the best of my knowledge no such unifying perspective on SLTH for G-equivariant networks exists.\n- Have the authors considered just working with the Clebsch-Gordan products? (Such as in Clebsch-Gordan Nets?) They also provide a way to simply work with a quadratic non-linearity in Fourier space, avoiding pointwise non-linearities altogether. ",
            "summary_of_the_review": "See above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1261/Reviewer_g7iD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1261/Reviewer_g7iD"
        ]
    }
]