[
    {
        "id": "kKY2cnTCSVV",
        "original": null,
        "number": 1,
        "cdate": 1666321469509,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666321469509,
        "tmdate": 1669426960117,
        "tddate": null,
        "forum": "1n1c7cHl3Zc",
        "replyto": "1n1c7cHl3Zc",
        "invitation": "ICLR.cc/2023/Conference/Paper5567/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method that allows replacing the text encoder in a text-to-image model without retraining the entire model. It uses a Model Translation Network (MTN) that is trained with text corpus data (or parallel text data for multilingual text encoders) to align the representation spaces of two text encoders. The paper demonstrates two application use cases: to update the text encoder of an existing T2I model to a more powerful one, and to update the text encoder of a monolingual T2I model to a multilingual one.",
            "strength_and_weaknesses": "My intuition divides the paper\u2019s contribution into three major components.\n\n1. An algorithm to translate between any two text encoder systems. The method is relatively simple and affordable, and uses three types of losses (adversarial loss, alignment loss and reconstruction loss) which are all very intuitive. The method also seems generic enough and I can imagine future applications to other domains. The writing is also very clear.\n\n2. Applying the MTN to upgrade the BERT encoder of the LDM to the T5-3B encoder. This is an example of upgrading the monolingual text encoder of a T2I model. The result is a bit disappointing in this use case because\n  - It was not until this part of the paper that I realized that end-to-end finetuning of the T2I is still needed in order to recover or improve its performance. Without the finetuning step, performance actually drops. Therefore it is not strictly \u201cplug and play\u201d of the new text encoder and the MTN. This is not mentioned in the paper until the experiments section, and therefore comes as a surprise and disappointment.\n  - Because of the need for finetuning, it becomes a bit more complicated to argue the benefit of using the MTN. Firstly, in terms of performance, the paper compares baseline LDM, T5+FT, T5+MTN, and T5+MTN+FT, and argues that T5+MTN+FT performs better than both the baseline and T5+FT. It would be informative to also compare with LDM+T5 trained from scratch, and would be very convincing if T5+MTN+FT gets very close to that. Secondly, in terms of computational cost, it would be useful to point out the cost of training LDM from scratch, and how much cost is saved by using T5+MTN+FT vs training from scratch. (If I understand correctly, one of LDM\u2019s advantages is training affordability, which means MTN may not be saving much. It would be nice to show this is not the case.)\n\n3. Applying the MTN to upgrade the CLIP text encoder of SDM to the multilingual XLM-Roberta-L. This is a successful application. The hybrid text inputs are interesting as well, but not that surprising. I think it is more contributed by the capability of XLM-Roberta-L than the MTN, since XLM-Roberta-L aligns representations across languages very well, and therefore is able to handle hybrid text encoding as well. One disadvantage is that it requires parallel corpus to train the MTN, whereas retraining SDM with XLM-Roberta-L does not require extra data and should also automatically work with multilingual prompts. \n\n\n**Questions:**\n- In the user study using Amazon Turk (Fig 4), were images from the three models displayed side by side, and the Turk picks the best out of three? If that is the case, I am not sure the results (31.7%, 32.7%, 35.7%) significantly deviate from chance statistically.\n- What is the dataset used for finetuning T5+FT and T5+MTN+FT?\n- Is the text encoder frozen during finetuning T5+FT and T5+MTN+FT?\n- What\u2019s the performance for LDM retrained with T5? How does it compare with T5+MTN+FT?\n- What's the compute comparison between LDM retrained with T5 and T5+MTN+FT?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly. The proposal of switching the text encoder component of T2I without retraining is interesting and as far as I am aware, novel. The algorithm to translate between any two text encoder systems is also novel as far as I am aware.",
            "summary_of_the_review": "Overall I like the idea proposed in the paper. I think it is important to think about how to reuse trained models and components, as in many cases it is unaffordable or wasteful to retrain end-to-end models. The proposed algorithm to align two text encoders\u2019 representations is simple and clear. The main drawback for me is that I am not convinced of its benefit in upgrading the text encoder in a T2I model, as shown by the results in Section 5.2. We may need better results, or better-organized results, to justify the claim that it \u201cboosts the performance of existing T2I models\u201d. Therefore I think it is marginally below the acceptance threshold. I am willing to change my score if either the claims or the results are adjusted to align better with each other.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no major concerns. The method, since used on T2I models, is subject to the same risks as any other T2I models.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5567/Reviewer_1TQj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5567/Reviewer_1TQj"
        ]
    },
    {
        "id": "l-FMgGOe3U",
        "original": null,
        "number": 2,
        "cdate": 1666611325184,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611325184,
        "tmdate": 1666611325184,
        "tddate": null,
        "forum": "1n1c7cHl3Zc",
        "replyto": "1n1c7cHl3Zc",
        "invitation": "ICLR.cc/2023/Conference/Paper5567/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to efficiently replace the text encoder of the text-to-image generation model to improve the generation quality (or to make a multilingual text-to-image generative model). To this end, the authors proposed Model Translation Network (MTN) in which training loss is motivated by domain adaptation and cross-domain alignment. The proposed method shows that the performance gain is consistent over the recent latent diffusion model (LDM) and can make LDM a multilingual text-to-image generative model. ",
            "strength_and_weaknesses": "**Strength**\n\n(1) The research direction itself is quite interesting, i.e., replacing the text encoder for a better (or multilingual) generative model.\n\n(2) The presentation and organization are clear.\n\n(3) The author provides comprehensive generated results (in the Supplementary).\n\n----\n**Weakness**\n\n(1) Ablation study is missing. Only the quantitative results are given in Appendix (A.3). To claim the effectiveness of each component, the qualitative results should be given.\n\n(2) The choice of using domain adaptation is somewhat unclear. Since authors have to run an additional discriminator, it may increase the computation, which is slightly against the motivation. Does this bring a significant improvement?\n\n(3) The performance gain is somewhat marginal (in Table 1). Since the LDM is not properly reproduced, the gain seems to be about -0.5 FID or -1.5 FID, which is slightly marginal. Can the author try a bigger language model to achieve more gain? E.g., providing encoder size and FID results like in Figure 4 of [1].\n\n(4) Comparison with a naive approach for a multilingual generation. One can simply use a state-of-the-art machine translation model (e.g., CONT [2]) on top of LDM (or SDM) and fine-tune it. Does the proposed approach outperform such a naive approach?\n\n(5) (minor) Some analysis of the hyper-parameter will be great (e.g., hyper-parameter sensitivity)\n\n[1] Saharia et al., Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, NeurIPS 2022\\\n[2] An et al., CONT: Contrastive Neural Text Generation, NeurIPS 2022",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe writing and the presentation are clear. If the author can provide more clear mathematical notation, in Section 4.1, it will be much better, e.g., some problem setups in a mathematical formula.\n\n**Quality**\n\nThe overall quality is fine.\n\n**Novelty**\n\nThe method itself is a combination of existing works, but the research direction is novel.\n\n**Reproducibility**\n\nSince the authors did not provide the code and the hyper-parameter, I believe it is hard to be reproduced.",
            "summary_of_the_review": "I recommend weak acceptance. I believe the target problem is quite interesting and has the strength to be accepted. However, some concerns (mentioned in the weakness part) make me choose the weak acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5567/Reviewer_2Z7M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5567/Reviewer_2Z7M"
        ]
    },
    {
        "id": "_xUgaNVY5do",
        "original": null,
        "number": 3,
        "cdate": 1666643921037,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643921037,
        "tmdate": 1669345465896,
        "tddate": null,
        "forum": "1n1c7cHl3Zc",
        "replyto": "1n1c7cHl3Zc",
        "invitation": "ICLR.cc/2023/Conference/Paper5567/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors proposed to address the problem of replacing a text encoder in a text-to-image model without re-training the whole pipeline from scratch. In doing so, it is proposed that MTN (Model Translation Network) projects the output of the new text encoder to the old one's output space, enabling the replacement. Experiments show both qualitatively and quantitatively the proposed baseline is capable of doing a replacement.\n",
            "strength_and_weaknesses": "# Strength\n\nThe problem this paper addresses is a novel and very interesting one per se, and is well-motivated. If fully fulfilled, this would lead to a large number of applications.\n\nThe authors make a reasonable effort to show the performance of this method. The effort in qualitative experiments is great in that lots of examples are shown for comparison. The method would be welcomed by the community.\n\n# Weakness\n\nWith the strength said, I'm not fully convinced of the proposed method, as detailed below:\n\nProposed algorithm for alignment: The proposed method projects the new text-encoder's output space to the old encoder's space, thus can serving as a wrapper over new text encoder. However, several issues are presenting:\n\n1. The training loss, which consists of three losses (eq 3,4,5), are complex and the dynamic is unclear. No ablation study is presented to provide the justification of such a design.\n2. It is unclear if the architecture of MTN could handle fixed length representation or the variant length ones.\n3. The technical contribution of the proposed MTN is unclear. Related works in bridging the latent space of different language models are not discussed. Just to name a few, in machine translation:\n    - Kulshreshtha et al. Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study\n    - Cao et al. Multilingual Alignment of Contextual Word Representations\n4. The performance contribution, or how good the MTN works, is unclear. MSE loss seems to be okay as shown in the experiment, however there is not a grounding of exactly how well MTN works.  We have no idea if the qualitative results come from the MTN being just okay but the generative model for images is powerful enough to compensate for MTN. Ideally, a good ablation is to have two runs of the same language models with different initial parameters and using the proposed technique to replace one with another.\n5. One argument for this work is that a new language model's ability could be leveraged. However if it's demonstrated through an language model in another language, a justification should be provided through comparing it with simply translating the prompt.\n\n# Questions for Authors\n\n1. How much computation is required for the proposed MTN?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is a clear one. However it suffers issues in quality and novelty as mentioned above.\n",
            "summary_of_the_review": "While this paper addresses an interesting problem and is well-motivated, the issues in the proposed method and the experiment's design makes it hard to be presented in the conference in its current form. I would thus suggest a rejection of this manuscript based on the current understanding, but I'm happy to change my rating if my concerns and questions are addressed in the discussion.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5567/Reviewer_5yb4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5567/Reviewer_5yb4"
        ]
    },
    {
        "id": "2tco441JNu",
        "original": null,
        "number": 4,
        "cdate": 1666685559156,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685559156,
        "tmdate": 1666685559156,
        "tddate": null,
        "forum": "1n1c7cHl3Zc",
        "replyto": "1n1c7cHl3Zc",
        "invitation": "ICLR.cc/2023/Conference/Paper5567/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed an efficient model translation network (MTN) for text-image generation (T2I) by leveraging off-the-shelf pretrained language models with a pre-trained T2I diffusion model, such as T5 and XLM-R. The experiments demonstrate the superiority of this method.",
            "strength_and_weaknesses": "Strength:\n\n1.Good performance.\n\n2.This paper is well written and organized. Motivation is simple and straightforward.\n\n3.As mentioned in the paper, this paper is able to train a model of T2I in an economical way without retraining whole model, and it can even be a multilingual as input as prompt by using XLM-R model.\n\nWeaknesses:\nLack of ablation of text encoder. is it possible to try different text encoder, such as bert, roberta, clip's text encoder, etc.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Lack of novelty, and please revise other mistakes carefully.",
            "summary_of_the_review": "I prefer to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5567/Reviewer_jTu3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5567/Reviewer_jTu3"
        ]
    }
]