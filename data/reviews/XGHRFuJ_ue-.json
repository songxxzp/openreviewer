[
    {
        "id": "jJ1Fv2EnY7",
        "original": null,
        "number": 1,
        "cdate": 1666799473545,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799473545,
        "tmdate": 1666799473545,
        "tddate": null,
        "forum": "XGHRFuJ_ue-",
        "replyto": "XGHRFuJ_ue-",
        "invitation": "ICLR.cc/2023/Conference/Paper2246/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This\u00a0paper\u00a0proposes\u00a0a\u00a0new\u00a0technique\u00a0to\u00a0enhance\u00a0open-domain\u00a0chatbot\u00a0with\u00a0human\u00a0feedback\u00a0information.\u00a0The\u00a0authors\u00a0attribute\u00a0the\nless-engaging\u00a0response\u00a0to\u00a0the\u00a0distribution\u00a0gap\u00a0between\u00a0human-human\u00a0conversation\u00a0and\u00a0proxy\u00a0human-machine\u00a0conversation.\u00a0Therefore,\u00a0they\u00a0design\u00a0a\u00a0data\u00a0collection\u00a0procedure\u00a0to\u00a0collect\u00a0human\u00a0revisions\u00a0to\u00a0machine\u00a0response\u00a0and\u00a0construct\u00a0a\u00a0dataset\u00a0based\u00a0on\u00a0that.\u00a0Utilizing\u00a0the\u00a0established\u00a0dataset,\u00a0the\u00a0authors\u00a0adopt\u00a0the\u00a0classical\u00a0generate-then-rerank\u00a0paradigm\u00a0to\u00a0first\u00a0generate\u00a0multiple\u00a0candidates\u00a0and\u00a0then\u00a0select\u00a0one.\n",
            "strength_and_weaknesses": "Strengths:\n1.\u00a0The\u00a0paper\u00a0is\u00a0well-written\u00a0and\u00a0easy\u00a0to\u00a0follow.\n2.\u00a0The\u00a0authors\u00a0ablate\u00a0several\u00a0aspects\u00a0of\u00a0their\u00a0method.\n3.\u00a0The\u00a0method\u00a0obtains\u00a0good\u00a0results\u00a0in\u00a0human\u00a0evaluation.\n\nWeakness:\n\n\n\n1.\u00a0A\u00a0major\u00a0proposal\u00a0of\u00a0the\u00a0paper\u00a0is\u00a0to\u00a0use\u00a0human\u00a0feedback\u00a0to\u00a0improve\u00a0the\u00a0performance\u00a0of\u00a0dialogue\u00a0system.\u00a0However,\u00a0it\u00a0is\u00a0not\u00a0a\u00a0new\u00a0idea\u00a0in\u00a0my\u00a0opinion.\u00a0There\u00a0is\u00a0rich\u00a0literature[1-5]\u00a0discussing\u00a0how\u00a0to\u00a0continue\u00a0learning\u00a0from\u00a0human\u00a0feedback\u00a0after\u00a0deployment,\u00a0which\u00a0is\u00a0closely\u00a0related\u00a0to\u00a0your\u00a0method.\u00a0\u00a0Out\u00a0of\u00a0the\u00a0domain\u00a0of\u00a0dialogue,\u00a0there\u00a0is\u00a0also\u00a0a\u00a0large\u00a0body\u00a0of\u00a0work\u00a0studying\u00a0the\u00a0improvement\u00a0of\u00a0models\u00a0from\u00a0human\u00a0feedback,\u00a0such\u00a0as\u00a0summarization[7][6]\u00a0or\u00a0question\u00a0answering[8].\u00a0They\u00a0should\u00a0at\u00a0least\u00a0be\u00a0included\u00a0in\u00a0the\u00a0related\u00a0work,\u00a0or\u00a0the\u00a0readers\u00a0will\u00a0have\u00a0difficulty\u00a0putting\u00a0the\u00a0work\u00a0into\u00a0its\u00a0literature.\n\n2.\u00a0The\u00a0evaluation\u00a0protocol\u00a0is\u00a0not\u00a0very\u00a0convincing\u00a0to\u00a0me.\u00a0\n(1)\u00a0Metrics:\u00a0Only\u00a0human\u00a0evaluation\u00a0is\u00a0conducted\u00a0and\u00a0no\u00a0automatic\u00a0metrics\u00a0are\u00a0involved.\u00a0I\u00a0acknowledge\u00a0that\u00a0some\u00a0automatic\u00a0metrics\u00a0like\u00a0BLEU\u00a0or\u00a0ROUGE\u00a0may\u00a0be\u00a0poorly\u00a0correlated\u00a0with\u00a0human,\u00a0but\u00a0some\u00a0neural\u00a0metrics\u00a0such\u00a0as\u00a0BERTscore\u00a0should\u00a0be\u00a0considered.\u00a0Furthermore,\u00a0pure\u00a0human\u00a0evaluation\u00a0renders\u00a0the\u00a0model\u00a0performance\u00a0hard\u00a0to\u00a0reproduce.\n(2)\u00a0Dataset:\u00a0In\u00a0the\u00a0statistics\u00a0evaluation,\u00a0only\u00a0100\u00a0cases\u00a0are\u00a0sampled\u00a0from\u00a0the\u00a0test\u00a0set\u00a0and\u00a0evaluated.\u00a0Why\u00a0not\u00a0use\u00a0all\u00a0the\u00a0test\u00a0set?\u00a0Do\u00a0you\u00a0sample\u00a0multiple\u00a0times\u00a0and\u00a0conduct\u00a0a\u00a0repetitive\u00a0experiment\u00a0or\u00a0just\u00a0report\u00a0the\u00a0result\u00a0from\u00a0a\u00a0single\u00a0sampling?\u00a0Why\u00a0not\u00a0perform\u00a0and\u00a0compare\u00a0on\u00a0some\u00a0widely-used\u00a0open-domain\u00a0dialogue\u00a0dataset\u00a0such\u00a0as\u00a0Douban[9]\u00a0or\u00a0STC[10]?\n(3)\u00a0Baselines:\u00a0PLATO-XL\u00a0is\u00a0up\u00a0to\u00a011B\u00a0while\u00a0EVA2.0\u00a0is\u00a02.8B\u00a0and\u00a0CDial-GPT\u00a0is\u00a0104M.\u00a0So\u00a0I\u00a0doubt\u00a0whether\u00a0your\u00a0comparison\u00a0is\u00a0fair.\u00a0Why\u00a0not\u00a0implement\u00a0your\u00a0method\u00a0on\u00a0some\u00a0smaller\u00a0version\u00a0of\u00a0PLATO\u00a0model?\n\n\n\n[1]Learning\u00a0from\u00a0Dialogue\u00a0after\u00a0Deployment:\u00a0Feed\u00a0Yourself,\u00a0Chatbot!\n[2]Learning\u00a0New\u00a0Skills\u00a0after\u00a0Deployment:\u00a0Improving\u00a0open-domain\u00a0internet-driven\u00a0dialogue\u00a0with\u00a0human\u00a0feedback\n[3]Improving\u00a0alignment\u00a0of\u00a0dialogue\u00a0agents\u00a0via\u00a0targeted\u00a0human\u00a0judgements\n[4]Training\u00a0a\u00a0Helpful\u00a0and\u00a0Harmless\u00a0Assistant\u00a0with\u00a0Reinforcement\u00a0Learning\u00a0from\u00a0Human\u00a0Feedback\n[5]BlenderBot\u00a03:\u00a0a\u00a0deployed\u00a0conversational\u00a0agent\u00a0that\u00a0continually\u00a0learns\u00a0to\u00a0responsibly\u00a0engage\n[6]Learning\u00a0to\u00a0summarize\u00a0from\u00a0human\u00a0feedback\n[7]Self-critiquing\u00a0models\u00a0for\u00a0assisting\u00a0human\u00a0evaluators\n[8]WebGPT:\u00a0Browser-assisted\u00a0question-answering\u00a0with\u00a0human\u00a0feedback\n[9]Sequential\u00a0matching\u00a0network:\u00a0A\u00a0new\u00a0architecture\u00a0for\u00a0multi-turn\u00a0response\u00a0selection\u00a0in\u00a0retrieval-based\u00a0chatbots\n[10]Neural\u00a0responding\u00a0machine\u00a0for\u00a0short-text\u00a0conversation.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Some\u00a0important\u00a0details\u00a0are\u00a0missing:\n1.\u00a0The\u00a0architecture\u00a0of\u00a0your\u00a0model.\u00a0Even\u00a0if\u00a0you\u00a0directly\u00a0use\u00a0the\u00a0plato-XL\u00a0as\u00a0your\u00a0backbone,\u00a0a\u00a0brief\u00a0introduction\u00a0to\u00a0its\u00a0architecture\u00a0is\u00a0necessary\u00a0for\u00a0completeness.\n2.\u00a0How\u00a0do\u00a0you\u00a0implement\u00a0the\u00a0s()\u00a0function\u00a0in\u00a0Eq.2?\u00a0\n3.\u00a0You\u00a0adopt\u00a0top-K\u00a0sampling\u00a0to\u00a0produce\u00a0multiple\u00a0candidates.\u00a0So\u00a0what\u00a0is\u00a0the\u00a0value\u00a0of\u00a0K\u00a0in\u00a0your\u00a0experiment?\u00a0How\u00a0many\u00a0candidates\u00a0are\u00a0generated\u00a0for\u00a0each\u00a0case?\n\nThe\u00a0novelty\u00a0of\u00a0the\u00a0proposed\u00a0method\u00a0is\u00a0under\u00a0question.\u00a0The\u00a0paper\u00a0learns\u00a0the\u00a0human\u00a0feedback\u00a0information\u00a0by\u00a0jointly\u00a0training\u00a0a\u00a0discriminator\u00a0and\u00a0a\u00a0generator.\u00a0Actually,\u00a0generate-then-rerank\u00a0is\u00a0a\u00a0popular\u00a0paradigm\u00a0and\u00a0has\u00a0been\u00a0used\u00a0in\u00a0many\u00a0prior\u00a0works\u00a0such\u00a0as\u00a0[1][2][3].\n\n[1]CALIBRATING\u00a0SEQUENCE\u00a0LIKELIHOOD\u00a0IMPROVES\u00a0CONDITIONAL\u00a0LANGUAGE\u00a0GENERATION\n[2]JOINT\u00a0GENERATOR-RANKER\u00a0LEARNING\u00a0FOR\u00a0NATURAL\u00a0LANGUAGE\u00a0GENERATION\n[3]BRIO:\u00a0Bringing\u00a0Order\u00a0to\u00a0Abstractive\u00a0Summarization\n",
            "summary_of_the_review": "The\u00a0authors\u00a0propose\u00a0a\u00a0data\u00a0collection\u00a0strategy\u00a0to\u00a0collect\u00a0human\u00a0feedback\u00a0for\u00a0an\u00a0open-domain\u00a0chatbot.\u00a0\u00a0post-training\u00a0an\u00a0existing\u00a0pre-trained\u00a0dialogue\u00a0model\u00a0(PLATO-XL)\u00a0with\u00a0the\u00a0collected\u00a0data\u00a0achieves\u00a0some\u00a0preliminary\u00a0results.\u00a0However,\u00a0the\u00a0proposed\u00a0method\u00a0is\u00a0not\u00a0original\u00a0and\u00a0the\u00a0evaluation\u00a0setup\u00a0is\u00a0defective.\u00a0I\u00a0would\u00a0suggest\u00a0that\u00a0a\u00a0major\u00a0revision\u00a0of\u00a0the\u00a0manuscript\u00a0is\u00a0needed\u00a0to\u00a0reach\u00a0the\u00a0publishing\u00a0requirement\u00a0of\u00a0ICLR\u00a0or\u00a0other\u00a0top-tier\u00a0conferences.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2246/Reviewer_rtcq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2246/Reviewer_rtcq"
        ]
    },
    {
        "id": "Huw4P7ETA2",
        "original": null,
        "number": 2,
        "cdate": 1666845674764,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666845674764,
        "tmdate": 1666845674764,
        "tddate": null,
        "forum": "XGHRFuJ_ue-",
        "replyto": "XGHRFuJ_ue-",
        "invitation": "ICLR.cc/2023/Conference/Paper2246/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a a data collection strategy combined with human preference capture and a joint objecting learning method to boost the performance of pre-trained dialogue models. The paper is very well-written, contains descriptions of the collected dataset alongwith  data analysis, as well as a learning strategy that leverages joint training via cross-entropy and contrastive learning (contrasting human preference dialog with model generated and random responses). The paper also conducts extensive human evaluation of the model responses alongwith ablation studies. ",
            "strength_and_weaknesses": "- the paper uses standard ML methods for the training strategies without any significant technical innovation of the work. \n- However, the model training setup is pretty interesting, combining mask learning with contrastive learning using implicit human preferences.\n- the model training does not explicitly seem to leverage the revisions which would have been interesting to model. \n- the step 1 (crafting the dialog opening) isn't very innovative. Authors mention they suggest topics in the guidelines but there isn't any AI assisted or explicit strategy to encourage users to discuss diverse topics. \n- addition AI assistance in the interface could help with faster data collection (sentence completion in the text box for example and if users select or reject the sentence level word completions) \n- all the results suggest that this model is better on all the studies metrics in all the comparisons. This is a very strong result; however similar analysis on another dataset (maybe another language) would convince the readers much better on the strength of the model fine-tuning. \n  \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and reproducible (code will be released). There's very interesting NLP contribution here but less novelty in terms of learning representation methods used. The work takes inspiration from the LaMDA model training strategies but claims to overcome the identified gaps in the LaMDA model. The paper uses standard generation strategies (top-K sampling instead of others such as diversity based sampling methods in literature to encourage more diverse model responses). \n\n",
            "summary_of_the_review": "Overall the paper presents a very solid approach, dataset contribution and evaluation results for open-ended chatbot development in Chinese. The model fine-tuning method significantly improves the performance of the pre-trained models. The paper has more NLP results innovation than a learning representation innovation. The joint modeling strategy is used in many works today (NLL loss with a contrastive loss) so the technical novelty isn't the highlight of this very solid contribution.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2246/Reviewer_ftAn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2246/Reviewer_ftAn"
        ]
    },
    {
        "id": "oortDnuan2Y",
        "original": null,
        "number": 3,
        "cdate": 1666994815917,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666994815917,
        "tmdate": 1666994815917,
        "tddate": null,
        "forum": "XGHRFuJ_ue-",
        "replyto": "XGHRFuJ_ue-",
        "invitation": "ICLR.cc/2023/Conference/Paper2246/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors propose a new framework called Diamante that is aimed at improving the performance of open domain chatbots by incorporating human feedback (both explicit and implicit preferences).  The proposed framework has a generation-evaluation training paradigm that is aimed to optimized response generation and preference estimation together.\n\nContributions:\n1. The authors collect a new dataset for open domain chitchat conversation in Chinese and is called the Diamante dataset\n2. A new joint generation and evaluation framework for optimizing generation and human preference estimation together.\n3. Results from automated and human evaluation show that the existing approaches trained with the new training strategy achieves significant gains on metrics.",
            "strength_and_weaknesses": "Strengths and Weakness:\n1. Collection of a new crowd sourced dataset for open domain conversational agent in Chinese Language.  How's is the quality of the dataset measure? What are steps taken to avoid bias and safety issues from being part of the dataset? \n2. The joint training-generation framework is interesting in terms of modeling response generation. What is hypothesis behind having r_h > r_m > r_r?\n3. Results show promising gains for the proposed approach on human evaluation both static and self chat evaluation.\n4. Looking at the results from Table 6, it seems like most of the gains come from the dataset. The gains from joint training seems pretty minimal compared to Plato-XL.\n ",
            "clarity,_quality,_novelty_and_reproducibility": "The originality of the work is from the dataset collected and the new training paradigm proposed to model response generation and model human preference.",
            "summary_of_the_review": "In this work, authors propose a new dataset for open domain conversations in Chinese and a new training paradigm that achieves gains in performance through human evaluation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2246/Reviewer_tDkV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2246/Reviewer_tDkV"
        ]
    },
    {
        "id": "uM4oFbPHgNO",
        "original": null,
        "number": 4,
        "cdate": 1667430027179,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667430027179,
        "tmdate": 1667430027179,
        "tddate": null,
        "forum": "XGHRFuJ_ue-",
        "replyto": "XGHRFuJ_ue-",
        "invitation": "ICLR.cc/2023/Conference/Paper2246/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, authors propose a new framework named Diamante to improve the Chinese open-domain pre-trained dialogue models with human annotations, since these models have difficulty in generating engaging utterances during the conversation. Diamante first builds a Chinese dataset by requiring human annotators to choose or amend the model-generated responses. Based on the dataset, it designs a joint-training process that considers both the generation loss and the human preference estimation loss. Experimental results demonstrate the effectiveness of Diamante compared with existing Chinese open-domain pre-trained dialogue methods.",
            "strength_and_weaknesses": "Strength:\n\n- Well written and easy to follow\n\n- It is the first in Chinese to consider human preference in pretrained dialogue models based on human evaluations\n\n- Experimental results show a significant improvement over existing methods in all metrics\n\nWeaknesses:\n\n- Limited novelty: It seems that the major difference between Diamante and LaMDA which also focuses on the alignment of human preference is the language. Although the authors claim that their joint-learning objective can be more effective than sequential learning in LaMDA, there are no experiments to support it. Authors should further clarify their contributions which currently are incremental as the combination of LaMDA and Chinese dialogue models such as PLATO-XL\n\n- Insufficient details: Since the alignment of human preference relies on the quality of human evaluations, it is fundamental to demonstrate essential details such as the demographics of the crowd workers or the instructions for them to annotate, which is included in LaMDA. Authors are suggested to provide these details to address possible concerns about the quality of the human annotations. Moreover, if the annotations are implicit, it should be difficult for methods to distinguish the difference between safe and unsafe utterances. Thus authors should also provide a detailed description of their dataset, including the average turns of the conversation, and how and why these utterances are selected, revised, or rewritten from machine-generated utterances.\n\n- Missing Examples: Authors only present the examples of Diamante, which can not demonstrate the improvement without the comparison of the examples of baselines including PLATO-XL and common commercial chatbots. Besides, the topics of the examples only cover insensitive topics such as food and people. Authors should show more examples of their methods and baselines for generating responses to sensitive topics such as Society or Culture. It is unclear if Diamente can yield more safety responses like LaMDA, especially on these sensitive topics.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. The quality of the script is good, although there are still several typos such as \"18% of the utterances\" rather than \"18% utterances\". However, more examples and details of the human evaluations should be provided to support the claims of the authors. Moreover, the novelty is incremental since the proposed method is the composition of existing models, especially when compared with the similar previous effort LaMDA. It seems that the authors have released the dataset and published their code, which should be less challenging to reproduce their results.",
            "summary_of_the_review": "The paper is well written and focuses on an important issue in Chinese dialogue models that there are no previous efforts to consider human preferences. However, the authors should better clarify their difference to LaMDA and the benefits of their proposed joint training based on implicit annotations. To support their claims, more details of the human evaluations and dialogue examples should also be presented. Based on the current version, I would not recommend accepting this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2246/Reviewer_fd1Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2246/Reviewer_fd1Y"
        ]
    },
    {
        "id": "WUyMfUsnjPn",
        "original": null,
        "number": 5,
        "cdate": 1667490994881,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667490994881,
        "tmdate": 1667490994881,
        "tddate": null,
        "forum": "XGHRFuJ_ue-",
        "replyto": "XGHRFuJ_ue-",
        "invitation": "ICLR.cc/2023/Conference/Paper2246/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThis work aims to train chatbots that generate responses aligned with human preferences. The training dataset is generated with human annotation. Specifically, the annotators start a conversation with a chatbot on a certain topic. At each turn, the annotators can provide feedback on the generated response. The feedback can come in the form of revisions to the generated response or a complete rewrite. After correction, the dialogue is continued, and the \u201cresponse \u2192 feedback\u201d steps are repeated.\n\nDuring fine-tuning, the corrected responses are used in the standard perplexity loss. Additionally, a \u201cpreference estimation\u201d loss is included to encourage the model to rank the corrected responses over the original model response.\n\nHumans rate the responses generated by the proposed model to be more coherent, informative, and engaging. \n",
            "strength_and_weaknesses": "### Strengths\n\n\nIncluding human preferences and feedback in generation models is an important research direction. The paper takes meaningful steps to achieve this in a non-English setting. The methodology for data collection is sensible, and the scale of the dataset is sufficiently large to be of interest to the broader community.\n\n\n### Weaknesses\n\n1. In the preference estimation loss (Equation 2), the authors always assume that human-generated responses are better. However, it is mentioned in Table 1 that 18% of the responses were selected as being already good. Isn\u2019t there a contradiction here? Essentially, Equation 2 will sometimes force the model to learn *not* to select an otherwise perfectly valid response?\n\n2. **Evaluation**\n\n2.1: This is probably a clarification: was there any overlap in the set of human annotators the same for data creation and evaluation (the paper mentions \u201cin-house data specialists\u201d)? If yes, there is a risk that the method overfits not to human preferences but to biases of a select set of reviewers. For example, consider 5 annotators: A, B, C, D. A and B like cheerful responses, and C and D like brief, serious responses.\n\nSuppose the responses from the baseline are either cheerful or serious with a 50% probability.\nIf we collect data using annotators A, B and also make them evaluate, the baseline performance will be ~ 50%, and your method will achieve a score of nearly 100%. The other extreme follows from using C, D as evaluators. Thus, the right way to go about it is to randomly select the groups for evaluation and data generation.\n\n\n2.2. \u201cWe select 20 high-frequency topics from a deployed chatbot and ask in-house data specialists to interact with these chatbots for 7-14 rounds\u201d. How much data was used for evaluation?\n\n2.3 The annotator agreement is moderate (closer to the boundary of low and moderate), indicating that the method may generate better responses simply because of more fine-tuning.\n\n3. **Engagement vs. utility:** The key assumption in this work is that engagement is proportional to human preferences. Is that necessarily true? For example, a chatbot that produces brief/terse responses may not be preferred for conversation in general but might be okay for a chatbot as long as it\u2019s useful. \n\nOther notes/good to have: \n\na) engaginess should probably be changed to \u201cengagement.\u201d \n\nb) For human evaluation results in Table 2, significance tests are missing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to understand. Some methods used in this work are derived from existing work. ",
            "summary_of_the_review": "Making text generation methods sensitive to feedback is an important research direction, and this paper makes useful contributions. My current recommendation is a weak accept, but I\u2019ll change my scores after the authors\u2019 feedback.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2246/Reviewer_Q4VE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2246/Reviewer_Q4VE"
        ]
    }
]