[
    {
        "id": "Hbam7Wmo_j",
        "original": null,
        "number": 1,
        "cdate": 1666666270326,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666270326,
        "tmdate": 1668827179329,
        "tddate": null,
        "forum": "oX3tGygjW1q",
        "replyto": "oX3tGygjW1q",
        "invitation": "ICLR.cc/2023/Conference/Paper3218/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a new perspective for multi-task RL that to distill each control policies in each task using techniques inspired from the minimum description length principle. Specifically, the proposed algorithm learns a policy parameters distribution that both contains the control policy parameters and constraint to be close to the sparsity induced prior. The paper then shows some motivation of the minimum description length control with some constructed scenarios and show the performance analysis in the tabular case. Finally, the paper has some experiments on both synthetic and popular RL benchmarks. ",
            "strength_and_weaknesses": "# Strength\n* The usage of the sparsity induced prior is nicely motivated throughout the paper, the intuition seems quite natural under the paper's constructed scenarios. \n* The paper contains both theory and practical results. \n* The overall flow of the paper is good and well organized.\n\n# Weakness\n* The paper's theoretical study seems a little underwhelming given that it's only in the tabular case, and thus the results do not seem super exciting (more on this below). For example, some recent theory paper already studies multi-task RL in the linear case [1,2,3].\n* The literature review does not seem complete, some previous works in multi-task RL are missing, for example, [4]\n* Is there any reason that the paper does not include the deep multi-task RL baselines in the experiments? The current baselines do not seem super strong, especially if we want to compare in the dm control suite benchmark.\n\n# Questions:\n* Despite the analysis is conducted in the tabular case, the $(1-\\gamma)^{-6}$ rate looks quite suboptimal. Intuitively this rate should have nothing to do with the multi-task setting itself. It would be great to have some clarification.\n* I am rather confused about the rate w.r.t. the number of task $K$. I may overlook something but is the current result saying that $\\kappa^{\\alpha^k}$ stays rather constant despite $K$? Thus what's the benefit here in terms of an increasing number of tasks?\n\n[1] Pacchiano, A., Nachum, O., Tripuraneni, N. and Bartlett, P., 2022. Joint Representation Training in Sequential Tasks with Shared Structure.\n\n[2] A. Agarwal, Y. Song, W. Sun, K. Wang, M. Wang, and X. Zhang. Provable benefits of representational transfer in reinforcement learning.\n\n[3] Y. Cheng, S. Feng, J. Yang, H. Zhang, and Y. Liang. Provable benefit of multitask representation learning in reinforcement learning.\n\n[4] Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning.",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity, Quality\nThe paper is overall clear and the writing quality seems good.\n\n## Novelty \nThe paper provides an interesting perspective. \n\n## Reproducibility\nThe submission includes codes and the paper has detailed sections for the experiments. ",
            "summary_of_the_review": "Overall the idea of enforcing simpler representations into multi-task RL is quite interesting. However, the theory result is a little underwhelming, and the experiments may need stronger baselines. Also there is still some confusion around the current version. Thus I would recommend a weak reject at this point. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3218/Reviewer_rmSg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3218/Reviewer_rmSg"
        ]
    },
    {
        "id": "A_q2L6ABuT",
        "original": null,
        "number": 2,
        "cdate": 1666917960729,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666917960729,
        "tmdate": 1666918086894,
        "tddate": null,
        "forum": "oX3tGygjW1q",
        "replyto": "oX3tGygjW1q",
        "invitation": "ICLR.cc/2023/Conference/Paper3218/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a method of combining the minimum description length principle to the finding of optimal policies for multitask RL problems. This allows the model to trade off between adapting to new information, and maintaining simplicity, thus adapting to epistemic uncertainty naturally.\n\nAfter introducing the method, the paper provides discussions about interpretations of some key terms, and carries out some theoretical analyses regarding performance and sample complexity. \n\nExperiments are carried out in the FOURROOMS environment and the DeepMind Control suite against several baselines. ",
            "strength_and_weaknesses": "### Strengths\n- Good structuring of the exposition. \n- Clear intuitive explanation before jumping into the maths\n- Nice theoretical analysis\n- Experiments seem good, with both simple and complex tasks. \n\n### Weaknesses\n- Some paragraphs could be split up.\n- ",
            "clarity,_quality,_novelty_and_reproducibility": "\n### Clarity\nThis paper is written well, in clear language. In the experiments section, the paragraphs for each setting could be split up for clarity.\n\n### Quality\nI believe that the research here is of good quality.\n\n### Novelty\nI haven't seen work like this elsewhere, to the best of my knowledge.\n\n### Reproducibility\nCode and experimental details provided.",
            "summary_of_the_review": "My score is based on the fact the seeming novelty of the work, how well it is written, as well as the suite of experiments carried out. I wasn't able to check the correctness of the theory. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3218/Reviewer_uHBy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3218/Reviewer_uHBy"
        ]
    },
    {
        "id": "Fj_PKNjzIpu",
        "original": null,
        "number": 3,
        "cdate": 1666978726008,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666978726008,
        "tmdate": 1666978780425,
        "tddate": null,
        "forum": "oX3tGygjW1q",
        "replyto": "oX3tGygjW1q",
        "invitation": "ICLR.cc/2023/Conference/Paper3218/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper deals with the relevant multi-task RL problem. The authors propose a novel method that follows the \"minimum description length\" principle, to learn a common structure between the tasks. The aim of the proposed approach is to improve the generalization. The authors provide sample complexity results in the finite state-action spaces and provide some experiments to show the effectiveness of the novel algorithm.",
            "strength_and_weaknesses": "Strengths:\n\n- The problem is relevant to the community.\n\n- As far as I know, the idea of using the \"minimum description length\" principle is novel and well-motivated. The idea is very interesting.\n\n\nWeaknesses:\n\n- The main weakness of the paper is the clarity. The paper, in general, is well-written, but some parts are hard to be understood and the connection with the SotA is not very clear. For example, what is the main difference between the proposed method and Moskovitz 2022? I am sure that there are differences between the two approaches, but I would like to see this difference written more clearly in the paper. \n\n- The performance analysis section could be improved. For example, this sentence is not very clear:\n\"is to obtain an upper bound on the average KL between default policies sampled from the default policy distribution and an optimal policy for a task sampled from the default policy distribution\"\nMaybe is a task sampled from the task distribution? \n\n- Default policy performance section: The connection with FTRL is quite interesting, although here we are not in an adversarial setting, so we are more interested in minimizing the internal regret instead of the external one, or am I missing something? \n\n- Control policy performance section: In proposition 4.2, the $\\ge$ is a typo and has to be instead a $\\le$? Why do we have this dependency of $(1-\\gamma)^6$?\n\n- Experiments: why the authors do not compare their method with the principled RPO algorithm proposed in Moskovitz 2022 (TVPO)? \n\n- The plots are not readable for color-blind people.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe clarity can be improved (see weaknesses).\n\nQuality:\n\nThe quality of the paper is good since the authors provide a novel algorithm, some theoretical results, and experiments.\n\n\nNovelty:\n\nThe algorithm is novel, although is not clear what are the main \"philosophical\" differences with respect to \n\nReproducibility:\n\nThe authors provide code to reproduce the results. However, I encourage the authors to provide more details in the README to reproduce the results in an easier way (and provide a file with dependencies).\n\n\n",
            "summary_of_the_review": "See Weaknesses and Strengths.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3218/Reviewer_TuJe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3218/Reviewer_TuJe"
        ]
    },
    {
        "id": "u38MpFrcnK_",
        "original": null,
        "number": 4,
        "cdate": 1667236418057,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667236418057,
        "tmdate": 1667236418057,
        "tddate": null,
        "forum": "oX3tGygjW1q",
        "replyto": "oX3tGygjW1q",
        "invitation": "ICLR.cc/2023/Conference/Paper3218/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies multitask RL and proposes a new framework based on minimum description length (MDL) principle, called MDL-control (MDL-C). The framework contains the learning of the control policy and the default policy. The control policy is trained according to Regularized Polic Optimization regarding the default policy, which is expected to learn the common structure; while the default policy (distribution) is formalized as the hypothesis that meets the MDL principle. Sparsity-inducing priors and variational dropout are motivated and adopted for the effective implementation of learning the default policy distribution. This paper also presents the theories on the regret upper bound of the default policy and the complexity of learning near-optimal control policy. In the experiments, the proposed method is evaluated in FoorRoom and DMC suite with multiple settings, demonstrating the effectiveness in learning desired default policy and the superiority over the considered baseline methods.",
            "strength_and_weaknesses": "$\\textbf{Strengths:}$\n+ The paper is well written and organized.\n+ The theoretical results of the proposed algorithm is discussed (for both the control policy and the default policy).\n+ The related work and background are well enclosed and connected in the text.\n+ Multiple settings and evaluation/analysis aspects are considered in the experiments.\n\n&nbsp;\n\n\n$\\textbf{Weaknesses and Questions: }$\n\n\n\nAccording to Algorithm 3 in the appendix, the default policy replay is only added when the control policy is near-optimal, based on which the default policy distribution is trained. Meanwhile, the control policy has a KL constraint on the default policy distribution. I am wondering how the initial phase is dealt with, in another word, is it possible that the initial default policy distribution prevents the control policy from learning a near-optimal policy and in turn the default policy replay remains empty?\n\nI think this may be related to a proper scheduling of the hyperparameter $\\alpha$. Thus, how is $\\alpha$ selected or scheduled? And how different choices of the value or schedule influence the performance?\n\n\n&nbsp;\n\n\n\nFor the experiments, my first question is on the baseline method RPO. According to the formulation in Equation H.2, assuming $\\pi_{\\phi}$ to be the default policy, what does \u2018regularized policy optimization with no constraint on the default policy (RPO)\u2019 mean? Does it mean that the $\\pi_{\\phi}$ is learned by MLE (or Behavior cloning), i.e., without adopting the KL with a prior distribution?\n\n\n&nbsp;\n\n\nI recommend the authors to additionally consider other domains and tasks in DMC suite (e.g., Finger and Quadruped). Two domains are insufficient to me for a convincing experimental evaluation.\n\nI appreciate that the authors provide the feature weight curves which help a lot in understanding the effectiveness of MDL-C. I am curious about the feature weight curves of the baseline methods (e.g., PO and RPO), can the authors provide the curves especially for DMC? If not, what is the reason?\n\nAnother question is, why is RPO not considered in the parallel setting? And similarly, ManualIA is not shown in the sequential setting.\n\n\n&nbsp;\n\n\nFinally, as the authors present a motivation from the perspective of generative model of optimal policy parameters, with on a group-based assumption. I think it will be interesting and insightful if the authors analyzes the learned optimal parameters (since a shared policy may be used, it may consider the learned representation of features) a posteriori.\n",
            "clarity,_quality,_novelty_and_reproducibility": "$\\textbf{Clarity: }$\n\nThe writing and presentation of the proposed method is almost clear. The organization of this paper is good.\n\n&nbsp;\n\n$\\textbf{Novelty: }$\n\nAlthough a few prior works present similar high-level ideas, to my knowledge, the theoretical results and the proposed method are novel.\n\n&nbsp;\n\n$\\textbf{Quality: }$\n\nThe theoretical results are clear and the proposed method is almost clear and sound yet I also have questions for the authors. The empirical evaluation and analysis is kind of insufficient in my personal opinion.\n\n&nbsp;\n\n\n$\\textbf{Reproductibility:}$\n\nThe proposed method is clear and most experimental details are provided in the appendix. The source codes are also provided.\n",
            "summary_of_the_review": "According to my detailed review above, I think this paper is marginally above the acceptance threshold mainly due to the novelty of the proposed framework along with the theoretical results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3218/Reviewer_DtWg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3218/Reviewer_DtWg"
        ]
    }
]