[
    {
        "id": "7iAOA683O6n",
        "original": null,
        "number": 1,
        "cdate": 1666329707731,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666329707731,
        "tmdate": 1668832791869,
        "tddate": null,
        "forum": "WWYHBZ1wWzp",
        "replyto": "WWYHBZ1wWzp",
        "invitation": "ICLR.cc/2023/Conference/Paper2943/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper suggests using a function that lower-bounds the optimal Q function (such as Monte-Carlo returns) as target values in Q-learning methods for faster convergence. The authors provide a theoretical justification for this technique and propose several variants for episodic/goal-reaching/non-episodic settings. They demonstrate that the proposed technique applied to Q-learning methods (SAC, DDPG, TD3) helps improve performance in diverse environments.",
            "strength_and_weaknesses": "## Strengths\n- The proposed method is simple and sensible.\n- Their theorems provide insights into their method.\n- The limitations of the proposed method and theorems are well addressed throughout the paper.\n\n## Weaknesses\n- The main weakness of this work is on its experimental setup. They use somewhat unusual combinations of base RL algorithms and environments. For example, they use SAC for Atari games, which is an uncommon choice especially given that Atari games have a discrete action space. I believe it would be desirable to test and compare their method with more widely used setups. How does the method perform with DQN or Rainbow on Atari games? Also, the authors only evaluate their method on an unusual subset of 17 games. Is there any previous work that uses the same setting? Could the authors provide additional results on the other games?\n- The novelty of the method is limited (please see the section below for details).\n- The proposed value target lower bounding technique can only be applied to deterministic environments. That being said, I believe this technique can still benefit some (deterministic or stochastic) environments in the real world, especially when initial target values are not informative enough.\n- The code is not released.\n\n## Questions\n- How do the authors modify SAC to deal with discrete actions?",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n- The paper reads well and is easy to follow.\n\n## Quality\n- The paper is written in a rather informal manner and I believe some sentences/paragraphs need to be polished. A few examples are:\n    - Section 1 ends abruptly. It would be better to move the last paragraph to another section (or merge it with the previous one) and introduce a brief overview of the results.\n    - (Sub)section titles are not capitalized in a consistent way (e.g., Value target lower bounding vs An Illustrative Example).\n    - Section 6, \"It is quite easy to introduce biases and inefficiencies into the process and end up with a suboptimal or inefficient algorithm.\": This requires supporting evidence.\n    - Section 6, \"In the experiments, the Lagrangian multiplier is fixed rather than being learnt, which would likely lead to suboptimal solutions.\", \"We lower bound the value target directly, which is simpler, more efficient, and likely more optimal.\": These require supporting evidence too.\n    - The paper contains a dummy acknowledgment section.\n\n## Novelty\n- The proposed value target lower bounding technique (lb-DR) is previously used by Fujita et al., as acknowledged by the authors. Nevertheless, the paper still provides a generalization of this technique and justifies it theoretically.\n- While the theoretical results in the paper seem novel (and correct), I am not familiar with the literature in this area and am not completely certain about its theoretical novelty.\n\n## Reproducibility\n- The code is not released.\n\n## Minor issues\n- Appendix A.3: Netowrk -> Network",
            "summary_of_the_review": "I believe this is a borderline paper. Since most of my concerns lie in the experimental results, I would be happy to increase my score if the authors provide a more extensive evaluation of their method.\n\n*Post-rebuttal update*: Thank you for the response. I checked the comparisons between lb-DR and DDQN on four Atari games in Appendix A.6, but I am still not fully convinced regarding lb-DR's performance as the number of games is highly limited (4 out of 56 games for DDQN and (an unusual set of) 17 games out of 56 for SAC), especially given the simplicity of the method. Hence, I would vote for a weak reject for now.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2943/Reviewer_JN96"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2943/Reviewer_JN96"
        ]
    },
    {
        "id": "ubEKJwefYE",
        "original": null,
        "number": 2,
        "cdate": 1666646248255,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646248255,
        "tmdate": 1668234745535,
        "tddate": null,
        "forum": "WWYHBZ1wWzp",
        "replyto": "WWYHBZ1wWzp",
        "invitation": "ICLR.cc/2023/Conference/Paper2943/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper explores the use of lower bounds for for value targets in order to improve the sample efficient in Reinforcement Learning algorithms. The authors outline the design of easily computable value lower bounds across different environment settings (episodic or continuous) and algorithms (n-step TD, SAC, HER). They theoretically justify the use of these lower bounds for faster convergence of value iteration. The intuition behind this is closely related to seeding the initial value to a lower bound during value iteration in tabular settings for faster convergence. Moreover, the empirical results demonstrate that the use of value lower bounds under different settings does improve the sample efficiency of these algorithms.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is thorough in that the authors do highlights subtle differences between convergence and contractions, and I do agree with authors that we can generally ignore that fact for empirical studies as we apply this for multiple amounts of time.\n    \n- This also builds into and connects to a lot of episodic learning papers where it can be viewed as simply adding a lower bound term into the value learning and get better rewards[1,2,3], It would be wonderful if the authors could throw some light towards this as these advances have mostly been without theoretical insights.\n    \n- The authors provide reasoning for discrepancies in the results such as \u201cbreakout\u201d where the clipping of rewards has a significant effect in the lower bounds.\n    \n\nWeakness:\n\n- It is not clear on this method will improve sample efficiency for algorithms that are designed to be sample efficient to begin with, for example rainbow. Will simply increasing the number of backups during training iterations have a similar effect?\n    \n\nMinor clarity\n\n- When we say \u201c\u201cOne such condition is when reward sequences are reproducible, under which, the empirical return from a single experience provides an unbiased estimate of the lower bounded value target\u201d ([pdf](zotero://open-pdf/library/items/4UI97JPI?page=31)) \u201d Does it mean that it is stochastic in nature ? [pg 31]\n    \n- Could we reprhase \u201cupper bounds are not as readily available as lower bounds\u201d to \u201ctight upper bounds are not as readily available as tight lower bounds\u201d. As v_max can be a trivial upper bound. [pg 30]\n    \n\n[1] Ma, X., Yang, Y., Hu, H., Liu, Q., Yang, J., Zhang, C., Zhao, Q., & Liang, B. (2022). Offline Reinforcement Learning with Value-based Episodic Memory.\u00a0*ArXiv, abs/2110.09796*.\n\n[2] Sarrico, M., Arulkumaran, K., Agostinelli, A., Richemond, P.H., & Bharath, A.A. (2019). Sample-Efficient Reinforcement Learning with Maximum Entropy Mellowmax Episodic Control.\u00a0*ArXiv, abs/1911.09615*.\n\n[3] Lin, Z., Zhao, T., Yang, G., & Zhang, L. (2018). Episodic Memory Deep Q-Networks.\u00a0*ArXiv, abs/1805.07603*.",
            "clarity,_quality,_novelty_and_reproducibility": "-",
            "summary_of_the_review": "\n\nOverall, the paper builds on the rich body of work done previously with different algorithms and the experiments shown are comprehensive and show promising results. However, it further experiments with more sample efficient algorithms such as rainbow and its variants is most welcome.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2943/Reviewer_8emL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2943/Reviewer_8emL"
        ]
    },
    {
        "id": "wAPqsHP9Pjc",
        "original": null,
        "number": 3,
        "cdate": 1667186482280,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667186482280,
        "tmdate": 1667186482280,
        "tddate": null,
        "forum": "WWYHBZ1wWzp",
        "replyto": "WWYHBZ1wWzp",
        "invitation": "ICLR.cc/2023/Conference/Paper2943/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper replaces the value target in TD-based methods with a lower bound of the optimal value function. Convergence was shown in the tabular case and the authors introduced several lower bounds that can be applied to practical algorithms. The authors demonstrated the effectiveness of their method on a series of challenging high-dimensional tasks.",
            "strength_and_weaknesses": "Strength:\n- The proposed method is simple and straightforward and can potentially be augmented to many RL algorithms\n- The theoretical analysis does appear to be correct and the convergence result is quite intuitive\n- The related work section is well-written. Since many similar but not entirely identical methods have been proposed, I think the authors did a good job of placing their work within the context of other literature and differentiating their algorithm\n\nWeakness:\n- The authors claimed to introduce a faster technique, however the only analysis on how the proposed method could be \u201cfaster\u201d is the third to last paragraph where the authors claimed that the new algorithm is at least as fast as the original. If faster RL is one of the claims of the algorithm, some new analysis in terms of convergence rate would be much appreciated.\n- I could be missing something here, in Section 3 you mentioned using the observed discounted return as a lower bound. When the environment is deterministic, this is certainly true, however in a stochastic environment, this would no longer be the case since it is always possible to get a very large return on an arbitrary rollout. Similarly, the lower bounds proposed in Section 3.2 has a similar issue.\n- The illustrative example in Section 4.3 is not entirely clear, could you give some additional clarification on why the proposed method speeds up learning in this case?\n- Though variations of both SAC and DDPG work for discrete action spaces, both were originally designed for continuous action spaces and are not standard for Atari. I understand the choice of SAC and DDPG since the purpose is to show improvements on a TD-based algorithm but was there a particular reason for using the Atari environments here instead of e.g. MuJoCo?\n- To add on to the previous point, I think while the experiments do show that the proposed methodology can work on complex difficult tasks, it doesn\u2019t help us understand where the improvements actually come from. In this regard, a toy example to demonstrate for example under what scenarios we see faster convergence would be extremely helpful.\n- 3-5 seeds seem very small to give any conclusive results though I understand in some labs computational resources could be limited.\n",
            "clarity,_quality,_novelty_and_reproducibility": "See above, paper is clearly written for the most part with some minor issues (e.g. Section 4.3), I do not see any major reproducibility issues.",
            "summary_of_the_review": "While I think the paper does have some merits and introduces a simple yet effective idea, I do not believe the paper is ready for publication in its current form.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2943/Reviewer_7q5U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2943/Reviewer_7q5U"
        ]
    }
]