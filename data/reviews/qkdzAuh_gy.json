[
    {
        "id": "JsVvdy5Ybg",
        "original": null,
        "number": 1,
        "cdate": 1666248121332,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666248121332,
        "tmdate": 1666248121332,
        "tddate": null,
        "forum": "qkdzAuh_gy",
        "replyto": "qkdzAuh_gy",
        "invitation": "ICLR.cc/2023/Conference/Paper1402/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work discovers a concrete example on the WILDS-Camelyon17 dataset that ID performance is negatively correlated to OOD performance. A theoretical example based on a simple linear model is also presented to show the trade-off between ID and OOD performance. The authors explain why past studies missed such a negative correlation and bring recommendations to the OOD generalization studies. ",
            "strength_and_weaknesses": "Strength:\n1. This work verifies the trade-off between ID and OOD performance happening in a real-world dataset.\n2. The authors present detailed discussions about the experimental details and the comparison to the past studies.\n3. The authors provide a theoretical analysis of a linear model.\n\nWeakness:\n(I list my questions in this section.)\n\n1. The authors do not clearly introduce the OOD generalization task and evaluation. \n    - **Q1.** Is it a generalization problem from the training dataset to a specific test dataset? or from the training dataset to a set of multiple test datasets? \n    - **Q2.** Do you consider the average accuracy or the worst-case (e.g. worst-domain) accuracy?\n\n2. The authors do not discuss the assumptions or conditions of the OOD generalization problem. \n    - **Q3.** For example, if there is no additional information about the unseen test datasets and only the training dataset is accessible, what are the reasons to reject a model with good ID performance? Is this a subjective guess on test data, rather than a scientific model selection scheme? \n    - **Q4.** Do the negative correlations between ID and OOD performance imply that the OOD generalization task is unlearnable?\n\n3. This work focuses on a specific case of OOD generalization from the perspective of models and algorithms, but ignores the weakness of the common-used datasets, e.g. the domain generalization benchmarks. \n    - **Q5.** We consider a multidomain dataset, e.g. PACS. According to the negative correlation between ID and OOD accuracy, is it possible that there exists a fifth domain such that the current SOTA domain generalization method performs worse than ERM? \n    - **Q6.** Can we conclude that current domain generalization benchmarks lack enough domains to evaluate OOD performance?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the experimental part are good.  I think the experiments are reproducible. Novelty is limited. The quality of the recommendations and conclusions needs to be improved.",
            "summary_of_the_review": "See Strength and Weakness.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1402/Reviewer_HGmQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1402/Reviewer_HGmQ"
        ]
    },
    {
        "id": "07-L7s_WCJZ",
        "original": null,
        "number": 2,
        "cdate": 1666576379233,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576379233,
        "tmdate": 1666576379233,
        "tddate": null,
        "forum": "qkdzAuh_gy",
        "replyto": "qkdzAuh_gy",
        "invitation": "ICLR.cc/2023/Conference/Paper1402/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies patterns in the performance of ML models on in-distribution (ID) and out-of-distribution (OOD) data. The central claim of this work is that ID performance can be inversely correlated with OOD performance in practice, which has not been explicitly observed before in deep ML models. This work then provides theoretical justification in an ERM model for why this might occur, and discusses the implications these results have on the broader research field. \n",
            "strength_and_weaknesses": "I appreciate the thorough exposition on prior works and past observations in the paper\u2019s introduction. Experimentally, I think the use of diversity-inducing methods to enlarge the sampling space of models is clever and well motivated. The recommendation and discussion sections are also particularly informative, highlighting possible over-simplifications in conclusions drawn by prior works. \n\nHowever, I am leaning towards recommending rejection of this work in its current state. There are three important weaknesses in this paper that limits its contribution towards our collective knowledge about OOD generalization.\n\nThe first weakness is that the empirical observations made in this work are entirely based on experiments conducted on the Camelyon17-Wilds dataset. This begs one to question whether the claims made in this paper have any transferability to wider data domains, such as natural images, chest x-rays, satellite images, and NLP datasets. The Camelyon17-Wilds dataset defines \u201cin\u201d and \u201cout\u201d of distribution based on the hospital of origin of the slide image. This is comparatively a narrow scope of \u2018in\u2019 distribution, where the distributional shift is almost entirely accounted for by a change in the staining protocol. Other types of distributional shifts, such as those that include both subpopulation shifts and feature changes, might induce different generalization behaviors. \n\nThe second weakness lies with the model training protocol. In particular, this work studies the ID and OOD performance of models that arise from training the last layer of a network (a linear model) while varying the initial seed and number of epochs. A danger with this design choice is that observations and theories established for linear models are rarely true for deep neural networks (e.g. the \u201cdouble descent\u201d generalization curve of over-parameterized models, and the fragility of influence estimation in the deep learning context), and unconverged models behave differently from fully converged models. As such, the observed negative correlation between ID performance and OOD performance could be the direct result of using gradient descent with linear models. For instance, if one considers a case where some spurious features are weakly and positively correlated with an invariant feature in the training domain, then the predictive power of these spurious features to the label is necessarily smaller than the invariant features. One can then apply standard arguments (e.g. https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/slides/lec01.pdf) to show that the implicit regularization effect of the gradient descent algorithm causes the invariant features to be learnt early on, and the spurious (and weakly predictive) features are only learnt later on in the optimization process. If this were true, one would predict that by regularizing the number of epochs in the optimizer, one can achieve better generalization on test domains where the predictive direction of such spurious features are flipped. This seems to be indeed the case as the authors observe that \u201cthis variation across training epochs is responsible for much of the newly observed patterns\u201d. Modern deep networks, with both explicit and implicit regularizations, may not require such early-stopping to \u201cunlearn\u201d spurious features, and as such do not behave in the same fashion as the models studied in the paper. The authors argue that diverse architectures are not sufficient when studying these trends, but this doesn\u2019t absolve the authors of the responsibility of investigating architectures beyond linear models if they wish to make any general claim about neural network training. \n\nThe third weakness lies in the lack of quantitative evaluations for hyperparameters and choice of metrics. From the plots in Appendix A, one observes that the OOD accuracies vary significantly between different pretraining seeds. Furthermore, not every plot exhibits the negative correlation behavior. A careful investigation of the characteristics of the pretrained feature extractor and visualizing other metrics (such as those used in Wenzel 2022) could hope to elucidate factors that can explain these variations.\n\nRegarding questions, I think most of my concerns can be addressed experimentally through additional datasets and models. I will raise my score accordingly. \n\nLastly, while not directly affecting my score, I think the presentation of Theorem 1 could be made more concise by replacing several repeated expressions with appropriate symbols. The author might also consider modifying the analysis in replacing the \u201cfeature selection\u201d model with a direction of steepest descent argument as I linked above to reduce the amount of simplifications required in the analysis. \n",
            "clarity,_quality,_novelty_and_reproducibility": "See above",
            "summary_of_the_review": "See above",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1402/Reviewer_yZ1A"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1402/Reviewer_yZ1A"
        ]
    },
    {
        "id": "lxpNY5YyNRq",
        "original": null,
        "number": 3,
        "cdate": 1666658818839,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658818839,
        "tmdate": 1666658818839,
        "tddate": null,
        "forum": "qkdzAuh_gy",
        "replyto": "qkdzAuh_gy",
        "invitation": "ICLR.cc/2023/Conference/Paper1402/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper highlights that an inverse correlation can exist between the in-distribution (ID) and out-of-distribution (OOD) performance of a model. Empirically, the paper supports the claim by training different linear heads with ERM and regularized ERM in [1]. The paper also provides a theoretical study for linear models on the negative impact of spurious features. \n\n\n[1] Teney et al., Evading the simplicity bias: Training a diverse set of models discovers solutions with superior OOD generalization. CVPR 2022",
            "strength_and_weaknesses": "Strengths:\n- Investigation on the impact of spurious features and the correlation between ID and OOD performance is an important task. \n- The paper provides extensive background and review of related works.\n\nWeaknesses: \n- Organization can be largely improved. For example, Section 1, 2, and 7 are mostly devoted to review of literature and comparison with previous works. A more concise organization would be better. The background section in Section 3 can be moved to Appendix which diverts the attention of readers.  \n- Insufficient novelty due to lack of in-depth analysis. It is well expected that the presence of spurious features and reliance on it can negatively impact the OOD performance. The analysis only applies to linear models, which is insufficient to support the empirical results.\n- Lack of empirical evidence. The paper only uses ERM and regularized ERM as training objectives. No domain generalization methods are investigated, which leads to an open question whether the trend still hold with methods that are specifically designed for improving domain generalization.",
            "clarity,_quality,_novelty_and_reproducibility": "Related works and background are well written and easy to follow. Other sections can be largely improved with a cleaner and concise organization. The overall novelty is concerning given the current presentation. ",
            "summary_of_the_review": "The paper provides an interesting empirical evaluation on a specific phenomenon on the impact of spurious correlation and OOD generalization. However, the paper provides limited theoretical insights and lack sufficient analysis. The empirical study is limited to ERM on a single dataset. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1402/Reviewer_94SR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1402/Reviewer_94SR"
        ]
    },
    {
        "id": "Vhym-f7_Pj",
        "original": null,
        "number": 4,
        "cdate": 1667251155257,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667251155257,
        "tmdate": 1667251155257,
        "tddate": null,
        "forum": "qkdzAuh_gy",
        "replyto": "qkdzAuh_gy",
        "invitation": "ICLR.cc/2023/Conference/Paper1402/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "An emerging observation is the strong empirical correlation between IID and OOD robustness. This paper presents a counterexample, showing that IID-OOD performance can be inversely correlated on the WILDS-Camelyon17 dataset. The paper also presents a series of arguments to counter specific claims presented in past work, bringing nuance to the discussion of IID-OOD correlation.",
            "strength_and_weaknesses": "Strengths:\n* I like the claim that when trying to study IID-OOD correlations, the space of models that people have considered in past work is actually quite small. And, by training with diversity-promoting regularizers, one can uncover more of the pareto frontier.\n\nWeaknesses:\n* I found the paper's central claim to be a bit unsurprising. Concretely, most past work in this area, e.g., the \"Accuracy on the line\" paper (Miller et al. 2022), claim there is a strong correlation between IID and OOD performance for many datasets and distributions. However, this is not a \"for all\" claim: the paper is not arguing that every dataset and shift has this same correlation. Instead, they are merely pointing out that this correlation is surprisingly strong and evident in many existing benchmarks.\n* For that reason, I found the paper's main result, that one can find a *single* dataset where the correlation does not hold, to be rather unsurprising. Especially given that the dataset they study, WILDS-Camelyon17, is already a dataset for which Miller et al. 2021 says the IID-OOD correlation is rather weak. \n* Moreover, there are other cases in the literature where the IID-OOD corelation is weak or breaks down. For example, the NLP spurious correlations body of work points out a lot of cases where similar models generalize in widely different ways (e.g., https://arxiv.org/abs/1911.02969). The CLIP paper presents another case where finetuned models can have different IID-OOD tradeoffs (improving IID can hurt OOD), and there are numerous follow-up work's that try to mitigate this trade-off. ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the evaluation focuses on a single dataset. Using the results from this dataset, in my opinion, the paper attacks a bit of strawman argument. In particular, I do not think many people would disagree with the paper's claim that \"Focusing on ID performance alone may not lead to optimal OOD performance\". Instead, past work is merely arguing that IID performance can get you surprisingly far.",
            "summary_of_the_review": "The evaluation is quite limited, and the paper is more of a position piece. Overall, I do not find the argument extremely compelling.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1402/Reviewer_nE6K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1402/Reviewer_nE6K"
        ]
    }
]