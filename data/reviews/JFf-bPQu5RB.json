[
    {
        "id": "5EzZO5_EIx7",
        "original": null,
        "number": 1,
        "cdate": 1666159684695,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666159684695,
        "tmdate": 1666159684695,
        "tddate": null,
        "forum": "JFf-bPQu5RB",
        "replyto": "JFf-bPQu5RB",
        "invitation": "ICLR.cc/2023/Conference/Paper3579/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposed a novel loss function called leveraged asymmetric loss with disambiguation (LASD) for the problem of multi-label learning from single positive labels (SPL) where only one positive annotation is available. A pair of leverage parameters are employed to address the severe negative-positive imbalance. This paper links the SPL loss with losses for ordinary multi-label classification from the perspective of risk consistency and proves the consistency of the proposed LASD loss to Hamming loss. Finally, experiments including comparing it with other state-of-art methods and ablation are conducted to validate the effectiveness of their method.",
            "strength_and_weaknesses": "Strengths:\n1. A pair of leverage parameters are employed to address the severe negative-positive imbalance of the SPL problem and the usage of consistency regularization can further improve the performance.\n2. The proposed method is easy to understand and reproduce.\n\nWeakness:\n1. The most significant contribution of this work is analyzing an SPL loss function from the perspective of risk consistency. However, the assumption of Theorem 5.3 is unreasonable, in the inequation $ \\left|1_{\\left[f_k(x) \\geq \\tau\\right]}-1_{[k \\in \\vec{y}]}\\right| \\leq \\varepsilon $, the left part only has the value of 0 and 1, letting $\\varepsilon \\rightarrow 0$ is equivalent to assuming that the pseudo positive labels selected by the threshold are just real positive labels. In this case, the proof of the consistency between LASD loss and Hamming loss makes no sense.\n2. Theorem 5.4 guarantees the effectiveness of LASD for the SPL problem in solving the severe positive-negative label imbalance by properly selecting the leverage parameters $\\lambda_{+}, \\lambda_{-}$. But another import hyper-parameter $\\tau$ which is used to select pseudo-positive labels in LASD loss is neglected. It would be better if there is theoretical guidance for the selection of the threshold rather than assuming that the pseudo-positive labels selected by the threshold are just real positive labels.\n3. Leveraged asymmetric loss (LAS) has been proposed to alleviate the extreme imbalance of negative-positive labels in the SPL problem. LASD loss just adds a threshold to select the pseudo-positive labels which also has been widely used in previous work. There is not enough innovation in this work.\n4. there is a lack of experiments to verify the assumption of Theorem 5.3 and Theorem 5.4.\n5. In Table 2, the improvement of the performance on datasets VOC12 and CUB is not obvious enough.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper links the SPL loss with losses for ordinary multi-label classification from the perspective of risk consistency and proves the consistency of the proposed LASD loss to Hamming loss. Extensive experiments are conducted to validate the effectiveness of their method. However, the assumption of Theorem 5.3 that the pseudo-positive labels selected by the threshold are just real positive labels is unreasonable, which makes the proof of the risk consistency unconvincing.\n\nLeveraged asymmetric loss (LAS) has been proposed to alleviate the extreme imbalance of negative-positive labels in the SPL problem. LASD loss just adds a threshold to select the pseudo-positive labels which also has been widely used in previous work, the same is true for consistency regularization. There is not enough innovation in this work.\n",
            "summary_of_the_review": "A pair of leverage parameters are employed to address the severe negative-positive imbalance of the SPL problem and the usage of consistency regularization can further improve the performance. However, the assumption that the pseudo-positive labels selected by the threshold are just real positive labels is unreasonable and there is a lack of experiments to verify the assumption.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3579/Reviewer_HXy7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3579/Reviewer_HXy7"
        ]
    },
    {
        "id": "S7vWgBnGfD",
        "original": null,
        "number": 2,
        "cdate": 1666253332024,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666253332024,
        "tmdate": 1666253413939,
        "tddate": null,
        "forum": "JFf-bPQu5RB",
        "replyto": "JFf-bPQu5RB",
        "invitation": "ICLR.cc/2023/Conference/Paper3579/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper seeks to study a weakly supervised problem called Single Positive Labels (SPL) in multi-label learning where only one positive label is observed for each examples. It proposes a loss function named leveraged asymmetric loss with disambiguation (LASD) following multi-label version of focal loss for SPL problem, and tries to prove that learning with the proposed loss is consistent.",
            "strength_and_weaknesses": "Strengths:\n1. The topic is interesting.\n2. The method is easy to understand.\n3. Although unsuccessful, the theoretical analysis of the proposed method is to be encouraged.\n\nWeaknesses:\n1. The theoretical analysis is deeply flawed. This paper proves the risk consistency of LASD by proving the risk of the classifier learned with LASD approaches to the risk of the classifier **learned with a specifically defined and  (most likely, at least, unproven) non-classification-calibrated multi-label loss function**. Such \"consistency\" is meaningless because classification-calibrated is the most basic requirement for a loss function.\n2. The assumption of Theorem 5.3 is unrealistic. It assumes that if the classifier f can already successfully classify (i.e., the optimal classifier), then LASD is spurious risk-consistent. One wants the learning to be consistent so that the learned classifier is guaranteed to be (asymptotically) equivalent to the optimal classifier, rather than assuming that the classifier is the optimal one first and then proving something else. It's a complete reversal of cause and effect.\n3. Assumption 5.1 and Theorem 5.2 are disconnected from the main content. They are neither an original contributions nor relevant to the approach presented in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "As far as I know, this paper is not the first work to solve SPL problem from the perspective of risk consistency [One Positive Label is Sufficient: Single-Positive Multi-Label Learning with Label Enhancement. Xu, et al.]",
            "summary_of_the_review": "The theoretical analysis of the paper needs further revision and improvement.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3579/Reviewer_PKX1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3579/Reviewer_PKX1"
        ]
    },
    {
        "id": "EDzv_MS5oo",
        "original": null,
        "number": 3,
        "cdate": 1666527545073,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666527545073,
        "tmdate": 1666527545073,
        "tddate": null,
        "forum": "JFf-bPQu5RB",
        "replyto": "JFf-bPQu5RB",
        "invitation": "ICLR.cc/2023/Conference/Paper3579/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the single positive labels problem in multi-label learning. A new loss function called leveraged asymmetric loss with disambiguation (LASD) is proposed, which is an improved version of the existing leveraged asymmetric loss (ASL) with an explicit pseudo label disambiguation. The proposed algorithm further adds a consistency regularization to the LASD loss. Theoretic analysis provides a link between the proposed LASD and Hamming loss. Empirical evaluations are reported using four benchmark datasets against the state-of-the-art literature.",
            "strength_and_weaknesses": "Strengths:\n-\tthe presentation is clear\n-\tthe proposed solution appears to be correct\n-\tthe theoretic analysis appears to make sense\n-\tthe evaluations as well as the ablation studies appear to be extensive\nWeaknesses:\n\nI have two major concerns with this work. First, the novelty. The proposed loss function is an extension from the existing LAS loss. While there is arguably something new with the extension, novelty-wise it is incremental at best.\n\nSecond, the proposed consistency regularization, while conceptually it makes sense, lacks justification in terms of the current form. Why do we have to consider two augmented samples, not one, or not three? It appears to be rather arbitrary and ad hoc.\n",
            "clarity,_quality,_novelty_and_reproducibility": "As I stated earlier, the presentation is clear and the work appears to be reproducible. Also as I stated earlier, the novelty is incremental. So overall the quality of this work needs further justification.",
            "summary_of_the_review": "This work addresses the single positive labels problem in multi-label learning. The paper proposes a new loss function explicitly addressing the asymmetric imbalance between positive and negative labels. The theoretical analysis makes sense and the proposed method is supported by evaluations. I have concern on the novelty and the method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3579/Reviewer_k1k2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3579/Reviewer_k1k2"
        ]
    },
    {
        "id": "kHDDVE8knEB",
        "original": null,
        "number": 4,
        "cdate": 1666787118846,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666787118846,
        "tmdate": 1666787118846,
        "tddate": null,
        "forum": "JFf-bPQu5RB",
        "replyto": "JFf-bPQu5RB",
        "invitation": "ICLR.cc/2023/Conference/Paper3579/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a loss function to learn from multi-label data with only one-positive annotation. Specially, the proposed loss function considers three factors: i) the existing asymmetric loss; ii) it uses a threshold to differentiate true-positive and true-negative in the unlabeled candidate labels. The self-labeled data is further used in the asymmetric loss; iii) a regularizer which ensures the data augmentation will not change the model too much. These items are weighted and added together. The paper analyzes some theoretical properties of the problem and the proposed loss function, and finally, shows sufficient experimental results to demonstrate the effectiveness of the proposed method. ",
            "strength_and_weaknesses": "Strength\n1. It proposes an effective loss function for learning from multi-labeled data with only one positive annotation.\n2. It has shown sufficient experimental results and overall, the performance of the proposed method is superior\n3. The paper is clearly written and easy to follow\n\nWeakness\n1. The proposed technique is a combination of existing techniques, may lack technical novelty\n2. The theoretical results may not be convincing (will explain in detail later)  ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. The quality of the paper is high, especially considering the sufficient experimental results provided. One concern could be the theory part. Specially, Theorem 5.2 is correct, however, it does add value to the study, since this is a too trivial result. Theorem 5.3 and Theorem 5.4, should have added value to the paper, especially considering the equivalence of the proposed loss function to the weighted hamming loss. However, the condition only enforces the positive label (f_k(x)) to be correctly predicted but did not imply anything about the negative labels (f_k(x)<threshold). Without conditions on the correct prediction of negative labels, I cannot be convinced that the proposed loss functions depending on negative labels could have the provided theoretical property. I would like to see more evidence on this part. \n\nSome minor comments\n1. f_k(x) may not be defined in the paper before using it.\n2. Due to the randomness in generating the data, it is better to generate multiple datasets instead of one to reduce the impact of randomness\n3. The paper has provided theoretical results showing the equivalence to weighted hamming loss but the experiments test only MAP. It is better to provide results on more metrics, especially those closely related to the theoretical properties of the proposed method. ",
            "summary_of_the_review": "The paper has shown a simple, straightforward, and effective solution to a kind of weakly supervised multi-label learning problem. The studied problem is important and the solution is empirically effective, although the proposed method is not novel. The theoretical results may also need to be strengthened. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3579/Reviewer_k9E5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3579/Reviewer_k9E5"
        ]
    }
]