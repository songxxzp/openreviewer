[
    {
        "id": "mGcLSuaCYTN",
        "original": null,
        "number": 1,
        "cdate": 1666388721773,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666388721773,
        "tmdate": 1666388721773,
        "tddate": null,
        "forum": "-kAWfaLkPT3",
        "replyto": "-kAWfaLkPT3",
        "invitation": "ICLR.cc/2023/Conference/Paper5396/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an approach to train offline reinforcement learning methods with a mix of data belonging to the target environment or task of interest (e.g., a game) and data belonging to different, but related environments/tasks (e.g., different games). In particular, the paper proposes to use a small set of labeled data from the target environment, plus a large set of unlabeled data from the target environment and a large set of labeled data from the source environments. By using an approach similar to Vide PreTraining, an Inverse Dymanics Model is trained on labeled data and used to annotate unlabeled data. All the data is used to train the final policy. This approach results in the proposed ALPT method. Experiments compare ALPT with a series of baselines to show the appropriateness of the different steps of the proposed approach. Other experiments aim to give insights into the role of dynamics modeling, the effect of using source and target environments with disjoint action spaces, and the benefit of ALPT in other domains such as maze navigation.",
            "strength_and_weaknesses": "STRENGHTS:\n - While the proposed technique is conceptually simple and the technical novelty with respect to previous works such as VPT (Baker et al., 2022), the proposed investigation seems to be useful to me. Indeed, it is reasonable to assume that labeled examples from similar tasks will become are available or will become available with time. In this case, it can be useful to be able to leverage such annotations to improve performance on a related task. The experimental analysis is also carried out trying to answer interesting questions concerning the role of each step of the proposed training pipeline, the effect of the number of source datasets used for dynamics modeling, the performance of the ALPT in the presence of disjoint action spaces, and the benefits of the proposed technique in other domains.\n\nWEAKNESSES:\n - While I found the investigation overall interesting, one may object that the choice of tasks make the analysis and conclusions less general than what observed in other works. Indeed, Baker et al (2022) considered the Minecraft game as its wide variety of things to do, build, and collect, can make results more applicable to varied real-world applications, while Lee et al. (2022) considered a set of 46 Atari games to show the abilities of their model to generalize across diverse scenarios. In contrast, the proposed investigation considers a subset of 5 Atari games (asterix, breakout, freeway, seaquest, and spaceinvaders) which, while maintaining a certain degree of variety, are arguably easier to tackle than Minecraft or the full set of 46 Atari games. In my opinion, this choice makes the paper and results somewhat weaker as compared to the two aforementioned works.\n - The paper reports reasonable ablations and comparisons. However, the main assumption is that a large quantity of labeled data is available for the source environments, while only a small set of labeled data in the target environment is available. Not surprisingly, the single-game variants, which are trained on the 10k subset of the data perform worse than methods trained on the set of 100M source + 10K target action labels. In order to assess the role of having more data as compared to being able to leverage data from diverse scenarios, one could consider the case in which single-game instances are trained on the target environments considering large sets of 100M labeled examples. This scenario is reasonable if we assume that we can label 100M examples on the target environment with the same effort required to label 100M examples on different source environments. More importantly, this setting would allow to assess the mere role of just using more data in the training procedure.\n - Another point which makes the experimental analysis a bit weak is the absence of clear comparisons with state-of-the-art approaches. For instance, one would expect comparisons with the VPT approach. If I understood correctly, this may be very similar to the \"single-game\" variants, but, if this is the case, this should be explicitly pointed out in the paper. Also, it could be interesting to see what happens if decision transformers are replaced by the methods considered for comparisons in Lee et al. (2022), such as C51, CQL, etc. These comparisons would help the reader to better understand the benefit of the proposed approach over current techniques.",
            "clarity,_quality,_novelty_and_reproducibility": "I found the paper not always very clear and easy to follow. For instance: \n - Figure 1 reports that the trained IDM is \"non-causal\". The choice of a non-causal transformer IDM is not well discussed in the paper.\n - What's \"Dist(S)\" reported in section 3.1?\n - The second paragraph of Section 4.2 (During pretraining, ...\") is a bit hard to follow because of different confusing statements. E.g., \"we use the combined labelled dataset, i.e., D_d for all source environments M_d...\", however, D_d is not the combined labelled dataset, but one of the datasets, right?. In contrast, the notation reported in Table 1 with the union of different D_d sets is more clear. I would suggest revising the whole section to improve clarity.\n -  The descriptions of the different baselines described in section 5.2 are hard to follow. I would suggest adding a summary table explaining the different properties of the baselines.\n - Figure 2 reports \"evaluation game performance\". However, this evaluation measure is not mentioned or introduced in the main text.\n - The \"Breakout\" graph in Figure 4 does not have shaded lines (standard deviation).\n\nThe technical novelty of the paper is not very high. Indeed, as the authors state, ALPT is a straightforward application of existing approaches. Nevertheless, I found the conceptual novelty of the paper good.",
            "summary_of_the_review": "In summary, while I found the investigation and analysis very interesting, in my opinion, the quality of the paper is limited by the aspects outlined above. I believe a careful revision of those aspects would make the submission much more solid.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5396/Reviewer_PQGj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5396/Reviewer_PQGj"
        ]
    },
    {
        "id": "wTxeAQqQu4",
        "original": null,
        "number": 2,
        "cdate": 1666655746012,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666655746012,
        "tmdate": 1666655746012,
        "tddate": null,
        "forum": "-kAWfaLkPT3",
        "replyto": "-kAWfaLkPT3",
        "invitation": "ICLR.cc/2023/Conference/Paper5396/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is motivated by the limited availability of actions in datasets when training RL agents. To better leverage the large amount of data not paired with actions, this paper proposes to learn a model to annotate actions for the dataset. More concretely, this paper proposes Action Limited PreTraining (ALPT), which learns an inverse dynamics model (IDM) to label missing actions for datasets in the target environment. ALPT is evaluated in Atari environments and maze navigation environments. ALPT has two stages: the pre-training stage, which trains a DT model and IDM simultaneously on datasets collected in source environments, and the finetuning stage, which finetunes both models on datasets in the target environment. ",
            "strength_and_weaknesses": "Strength:\n - The paper is well-motivated and focuses on realistic problems. The action-limited problem focused in this paper stems from the widely studied learning-from-observation problem. \n - The paper is well-organized and easy to follow.\n - The motivation is clearly stated. \n\nWeakness:\n\n - Unclear methodology description.\n     - In section 4.2, the paper mentions that IDM and DT are trained simultaneously. However, it is still unclear what \u201csimultaneously\u201d means. Are IDM and DT trained iteratively? If so, what are the training steps for IDM (or DT) before switching to another model? What are the total training steps?\n     - In finetuning, is DT initialized with the model pre-trained on all source environments? If so, it would be better to mention it clearly in section 4.2. Moreover, one ablation is initializing DT randomly and training it from scratch on the target datasets. \n \n- Concerns regarding the Atari experiments\n     - In Figure 2, what is the meaning of the x-axis? Does it mean the number of labeled frames used in fine-tuning? \n     - In Figure 2, the evaluation curves of ALPT are quite bumpy across environments. What is the number of episodes used to get each evaluation point on the curve? Will a larger number of evaluation episodes help smooth the evaluation curve?\n     -  **Moreover, in Figure 2, ALPT does not seem to improve along finetuning, which indicates that the finetuning is not helpful. In Asterix, the performance even goes down after finetuning What may cause such experiment observations? One ablation to do is to freeze the IDM model and only finetune DT.\n     - In Figure 2, the DT-IDM baseline seems to have no shaded area. In Figure 4, the breakout experiments have no shaded area. Are they run with 3 random seeds?\n     - Although ALPT improves upon baselines, the absolute rewards in Atari environments are still much lower than the reported values in the multi-game decision transformer\u2019s appendix (https://arxiv.org/pdf/2205.15241.pdf page 24). For instance, DT\u2019s performance in Asterix would have a reward of around 14,706 and ALPT has a reward of 2,500. It is reasonable that DT would perform better since it has the full dataset with actions. However, the large performance gap still raises concerns about the effectiveness of the proposed method.\n\n- Lacking experimental setup descriptions.\n     - For figure 3, what are the selected source environments in \u201cALPT-2 Games\u201d and \u201cALPT-10 Games\u201d?  \n     - The paper mentions that all the hyperparameters of DT are mostly the same as the settings in multi-game DT. It would be better to mention the key hyperparameters in the appendix for references, such as the number of heads and the number of layers. \n     - What is the architecture of the IDM model?  \n- Missing key related works in the literature review. \n     - It would be better to posit this paper as an offline RL paper and review literature related to learning from demonstrations, learning from observations, offline RL, and multi-task behavior cloning. ",
            "clarity,_quality,_novelty_and_reproducibility": "The motivation of the paper is clear. The quality of the literature review, methodology, and experiments can still be largely improved. ",
            "summary_of_the_review": "I would recommend a major revision of the paper and would recommend rejecting the paper. Although the paper is well-motivated, there are key descriptions missing in the methodology part, which makes the paper hard to evaluate. Moreover, the experiments hardly show that finetuning indeed helps.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5396/Reviewer_qasR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5396/Reviewer_qasR"
        ]
    },
    {
        "id": "Jjows3IvNjq",
        "original": null,
        "number": 3,
        "cdate": 1666677101440,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677101440,
        "tmdate": 1666677101440,
        "tddate": null,
        "forum": "-kAWfaLkPT3",
        "replyto": "-kAWfaLkPT3",
        "invitation": "ICLR.cc/2023/Conference/Paper5396/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper explores the problem of learning agent policies from action limited datasets and proposes ALPT, which pretrains an inverse dynamics model (IDM) on multiple environments to provide accurate action labels for a decision transformer (DT) agent on an action limited target environment dataset. The experiments and ablations highlight the importance of pretraining the IDM, as opposed to pretraining the DT agent alone. The results support the importance of generalist inverse dynamics models as an efficient way to implement large-scale RL. In addition, as more labelled data becomes available for training offline RL agents, ALPT provides an efficient way of bootstrapping performance on new tasks with action limited data.",
            "strength_and_weaknesses": "##########################################################################\n\nPros:\n\n- The paper proposes ALPT to use the multi-environment source datasets as pretraining for an IDM, which is then fine-tuned on the action-limited data of the target environment in order to provide labels for the unlabelled target data, which is then used for training a DT agent, which is an interesting idea for learning agent policies from action limited datasets.\n- Through various experiments and ablations, this paper demonstrates that leveraging the generalization capabilities of IDMs is critical to the success of ALPT, as opposed to, for example, pretraining the DT model alone on the multi-environment datasets or training the IDM only on the target environment. In addition, on a benchmark game-playing environment, we show that ALPT yields as much as 5x improvement in performance, with as little as 10k labelled samples required (i.e., 0.01% of the original labels), derived from only 12 minutes of labelled game play (Ye et al., 2021).\n- The paper is well-written and the comparison of benchmark methods is also interesting to read.\n##########################################################################\n\nCons:\n\n- Although this paper provides an interesting perspective on large-scale training for RL, it's just a straightforward application of existing approaches, seems the novelty is not enough.\n- This paper only compared ALPT with the single-game variant and standard DT baselines, lacking enough experiments to support the proposed hypothesis.\n- Missing code link to reproduce experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality and clarity are good but the originality is only marginally significant or novel, and missed code links to reproduce experiments.",
            "summary_of_the_review": "Considering the above pros and cons, my recommendation of the paper is marginally below the acceptance threshold.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5396/Reviewer_aonF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5396/Reviewer_aonF"
        ]
    },
    {
        "id": "oDbtQ04p-G",
        "original": null,
        "number": 4,
        "cdate": 1666853145410,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666853145410,
        "tmdate": 1666853145410,
        "tddate": null,
        "forum": "-kAWfaLkPT3",
        "replyto": "-kAWfaLkPT3",
        "invitation": "ICLR.cc/2023/Conference/Paper5396/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studied the problem of learning agent polices with datasets that have limited labeled actions. Action Limited PreTraining (ALPT) is presented, which pretrain an inverse dynamics model (IDM) on multiple environments to provide accurate action labels for decision transformer (DT) agent on an action limited target environment dataset. Experiments are conducted on game-playing benchmark among a couple of different environments. ",
            "strength_and_weaknesses": "Strength:\nThis work is well motivated. The research problem that's explored is valuable. Pretraining towards generic agents / representations on large-scale datasets has emerged as a dominant approach broadly in learning community. While, one big challenge exists in control and/or RL tasks is the lacking of explicitly labeled actions. Especially, when leveraging real-world video datasets, e.g. Ego4D for control tasks, the gap between semantic level 'activity' vs. physical level 'actions' brings ambiguous and large distribution discrepancies. So, to seek effective pretraining regimes that can leverage such massive datasets without actions labels is important. \n\nWeakness:\nMy main concern is that the proposed approach is hard to scale up. However, on contrary the main motivation of this work is for large-scale pretraining. To achieve the goal, ALPT have to first pretrain two transformers, i.e. one bidirectional T for IDM and a DT, and these two Transformer need to be trained one by one (as training of DT will need the action labels generated by the bidirectional T). Then, ALPT needs an extra finetuning stage that have to the same thing, i.e. finetune two transformer one by one. I am wondering, all these process should be time and computation consuming, especially when dealing with massive scale datasets. In experiments, only a couple of environments with small scale datasets are used for evaluation, which is hard to convince me that the proposed approach can indeed address lacking-of-label issue in large scale pretraining. ",
            "clarity,_quality,_novelty_and_reproducibility": "1. The technical contribution and novelty is limited. As what I mentioned before, the motivation of this work is to address issues for large-scale pretraining. However, both the techniques and experiments do not well validate the claim. The key novelty of the proposed approach is to leverage source and part of labeled target environments to train a model that can generate action labels. It closely related with widely used ideas of generating pseudo labels in transfer learning and semi-supervised learning.\n2. Lack of implementation details and the evaluations are limited. No source codes are provided.\nSome questions:\n1) When training IDM, it is saying that an bidirectional transformer is used, and is said that the model architecture as GPT. I am wondering GPT should be a causal transformer not bidirectional transformer, would you please clarify?\n2) it it interesting to see the different effects of training with IDM vs. forward dynamics model and even a combination of both forward and inverse dynamics model. I am wondering how and why that induce the choice of only IDM.\n3) In sec. 5.5, only a pair of tasks are evaluated. When performing a single task with very simple action space, it very likely that the finetuning stage can already well fit the distribution. I am wondering a multitask setting or zero-shot/few-shot setting. At least without the IDM finetuning. ",
            "summary_of_the_review": "This paper studied the problem of learning agent polices with datasets that have limited labeled actions. Action Limited PreTraining (ALPT) is presented, which pretrain an inverse dynamics model (IDM) on multiple environments to provide accurate action labels for decision transformer (DT) agent on an action limited target environment dataset. Experiments are conducted on game-playing benchmark among a couple of different environments. \nThis work is well motivated. The research problem that's explored is valuable. Pretraining towards generic agents / representations on large-scale datasets has emerged as a dominant approach broadly in learning community. While, one big challenge exists in control and/or RL tasks is the lacking of explicitly labeled actions. Especially, when leveraging real-world video datasets, e.g. Ego4D for control tasks, the gap between semantic level 'activity' vs. physical level 'actions' brings ambiguous and large distribution discrepancies. So, to seek effective pretraining regimes that can leverage such massive datasets without actions labels is important. \nHowever, my main concern is that the proposed approach is hard to scale up. However, on contrary the main motivation of this work is for large-scale pretraining. To achieve the goal, ALPT have to first pretrain two transformers, i.e. one bidirectional T for IDM and a DT, and these two Transformer need to be trained one by one (as training of DT will need the action labels generated by the bidirectional T). Then, ALPT needs an extra finetuning stage that have to the same thing, i.e. finetune two transformer one by one. I am wondering, all these process should be time and computation consuming, especially when dealing with massive scale datasets. In experiments, only a couple of environments with small scale datasets are used for evaluation, which is hard to convince me that the proposed approach can indeed address lacking-of-label issue in large scale pretraining. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5396/Reviewer_hAS5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5396/Reviewer_hAS5"
        ]
    },
    {
        "id": "xcwxeHytBdR",
        "original": null,
        "number": 5,
        "cdate": 1667142269479,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667142269479,
        "tmdate": 1667142269479,
        "tddate": null,
        "forum": "-kAWfaLkPT3",
        "replyto": "-kAWfaLkPT3",
        "invitation": "ICLR.cc/2023/Conference/Paper5396/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper tackles the problem of limited availability of labelled datasets for reinforcement learning tasks. The paper proposes to circumvent this challenge by combining large but sparsely-annotated datasets from a target environment of interest (without densely annotated large scale datasets) with fully annotated datasets from various other source environments. The proposed method employs the generalization capabilities of inverse dynamics modelling (IDM) to label missing action data. The method is evaluated on five games of the atArI benchmark suit.",
            "strength_and_weaknesses": "Strengths:\n* The proposed method is interesting and novel. \n*  The proposed method seem to improve performance significantly.\n* The paper includes adequate baselines and ablations.\n* The paper is well written and easy to understand.\n\n\nWeaknesses:\n* Disjoint action spaces: the performance difference between ALPT and DT1-IDM is limited. The paper should clarify whether the performance difference comes mainly from using an IDM? The paper should also include a discussion on how the proposed method is able to leverage datasets with disjoint action spaces for improved performance.\n* The paper should provide a more detailed explanation and a discussion on why including more source games does not improve performance on the target environment (in Section 5.4).\n* The paper uses only five games as they have a similar shared game structure.  This should be quantified in more detail. How similar does the game structure have to be? In general using only 5 games from the Atari suit is quite limited and the paper should ideally include more games.\n* It would be interesting to include results on the performance difference when training purely on the fully label target environments as a percentage of recovered performance. ",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The paper is interesting and novel.\n\nClarity: the paper is clear and well written.\n\nOriginality: To the best of my knowledge, the proposed idea is novel.",
            "summary_of_the_review": " The claims in the paper are generally well founded with adequate baselines and ablations. The paper demonstrates impressive performance improvements. However, the paper should include more detailed dissuasions on some claims (see above).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5396/Reviewer_NLz9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5396/Reviewer_NLz9"
        ]
    }
]