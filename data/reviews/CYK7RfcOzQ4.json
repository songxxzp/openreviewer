[
    {
        "id": "96JiU03aFMK",
        "original": null,
        "number": 1,
        "cdate": 1666380997049,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666380997049,
        "tmdate": 1666380997049,
        "tddate": null,
        "forum": "CYK7RfcOzQ4",
        "replyto": "CYK7RfcOzQ4",
        "invitation": "ICLR.cc/2023/Conference/Paper2944/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "AudioGen is a text to audio model, focusing primarily on non-speech, non-music audio events. It supports rendering multiple overlapping events simultaneously depending on the given text description and can generalize to unseen text/audio pairs by using a pre-trained language model encoder (T5).\n\nThe paper does several ablation studies exploring the effects of classifier-free guidance, text/audio training data mixing, and multi-stream decoding.",
            "strength_and_weaknesses": "Strengths:\n- The outputs of the model are remarkable and clearly follow the text prompts.\n- There is a clear improvement over previous work, with plenty of both quantitative and qualitative evaluations.\n- Ablation studies help determine the relative benefits of classifier-free guidance, mixing, etc.\n\nWeaknesses:\n- Some sections of the paper were not clear or difficult to follow (see next section).\n- Audio quality is still relatively low resolution (16 KHz).\n- Generated speech is unintelligible.\n- Exact timing of overlapping audio events is not controllable.\n\nThe weaknesses other than the first one are acknowledged in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The text-to-audio field is still quite new, and this paper is a clear and novel step forward in that domain. For the most part, the paper is clear enough that it could be reproduced, though source code and pretrained models would help.\n\nHowever, there are a couple sections that I found unclear:\n\n**Section 3.2, Multi-stream audio inputs.**\n\nIt's not clear to me how this setup works. Do the multiple tokens predicted at the same time cover the same timestep? Is there any conditioning of the tokens on each other, or are they predicted independently? Is the RVQ implementation providing increasing levels of quality, as in Soundstream, or is it somehow related to decoding multiple timesteps at once?\n\nI think some concrete examples and maybe a figure would help clarify this section.\n\n**Section 4.2 Audio Continuation**\n\nI found Figure 2 and its associated paragraph difficult to follow due to the combination of source/random text and audio and what they were being evaluated against. Also, I think there is a typo in the figure because there are 2 lines labeled \"Rnd text\". I think just being a little more explicit in the description would make things clear, describing exactly what was used for the audio prompt, the text conditioning, and the target/evaluation audio.\n\n**Other comments**\n\nSection 1, paragraph 2: Did you mean to say \"astronaut riding a horse\"?\n\nSection 1, paragraph 3: When you that some compositions are unlikely to be in the training dataset, do you have any way of quantifying or testing that?\n\nSection 1, paragraph 4: What size of pre-trained T5 did you use?\n\nSection 3, paragraph 1: tokes -> tokens\n\nSection 3.1: Just to clarify, does this mean that RVQ was not used other than in the multi-stream experiments? So a single token represents a single timestep? Could you also be more clear here about what the stride is? How much time does a token cover?\n\nSection 3.1, Training Objective: I think it's worth discussing why the reconstruction loss is frequency magnitude only but the discriminator can also see phase.\n\nSection 3.2: Instead of saying the architecture is like a decoder-only GPT-2, but with cross-attention added, wouldn't it be simpler to say it's an encoder-decoder Transformer with a frozen (T5) encoder?\n\nSection 4.1 Dataset: Several of the quote marks are backwards.\n\nSection 4.1 Dataset: Do you always train with input text of the form \"dog bark park\"? How does this work when the model is tested with phrases like \"a dog barks while somebody plays the trumpet in a busy street\u201d? Do you just rely on the T5 encoder to encode full sentences to the same representation as the sequence of individual words?\n\nSection 4.1 Hyperparameters: Missing \"hidden size\" before \"768\".\n\n",
            "summary_of_the_review": "This paper makes significant contributions to the relatively new field of text-to-audio. Overall the paper is clear and well written, but a few sections could be made more clear. I believe the paper will be influential in the field.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2944/Reviewer_pGs2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2944/Reviewer_pGs2"
        ]
    },
    {
        "id": "MyymJVdu_wK",
        "original": null,
        "number": 2,
        "cdate": 1666648059730,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648059730,
        "tmdate": 1666648059730,
        "tddate": null,
        "forum": "CYK7RfcOzQ4",
        "replyto": "CYK7RfcOzQ4",
        "invitation": "ICLR.cc/2023/Conference/Paper2944/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The combine existing methods (SoundStream audio codec, Transformer LMs) and datasets in a new way to demonstrate significantly higher quality generation of text-conditional environmental sounds than previous state of the art. Architecture and loss functions are largely the same to previous papers, but the extension to text-to-audio makes this paper a significant contribution to the community and a demonstration of the potential for follow-up research.\n\nThe authors also explore a variety of extensions to the task at hand (augmenting data by mixing audio clips/captions, speeding up inference through predicting multiple independent streams of audio tokens. The experiments are clearly laid out provide both quantitative and qualitative metrics to support the improvement over a baseline method (DiffSound) and the increase of quality with increasing model size. Technical limitations of the approach are appropriately explained.",
            "strength_and_weaknesses": "Strengths\n* Strong empirical results. On both quantitative and qualitative metrics, this paper clearly demonstrates a \"0-to-1\" improvement on a new task of text2audio, even given the limitations of labeled data and long sequence lengths.\n* Evaluation. Good choice of metrics for the experiments at hand, both quantitative and qualitative.\n* Ablation studies motivate the choice of CFG scale and trade-offs of the multi-stream generation strategy.\n\nWeaknesses\n* The benefits/need for the mixing data augmentation strategy is not well supported by experiments. The results in Table 1 from including mixing augmentation are, well \"mixed\" at best. More importantly, an important baseline is missing. For a situation with multiple sources, it would be good to compare to the baseline of generating several audio clips from a model trained without augmentation, and then mixing those outputs posthoc. Clearly this approach has the downside of taking longer to generate, but can handle arbitrary combinations of sources, regardless of the training. It also exposes a shortcoming of the current data augmentation strategy, as it ignores the fact that real audio often has many correlations between sound sources, including acoustic environment, background noise, and interactions in the natural scene. \n* The comparisons with model size in Table 1 could also benefit from clarifying what improvements are due to increasing the size of the decoder (ALM) vs. increasing the size of the encoder (T5). Currently they are both scaled together, and an ablation experiment could help to demonstrate where the benefits emerge.\n* It is unclear if DiffSound was retrained on the same datasets as AudioGen for the comparisons in Table 1. If the model is indeed trained on different datasets (using a pretrained checkpoint) then the comparison is less useful as a comparison of model architectures and needs to be highlighted as such. AudioGen is still an impressive empirical feat, but it would then be unclear how much is due to the model architecture vs. the datasets, and the claim couldn't be made that the AudioGen architecture outperforms the DiffSound architecture in this case. \n* The limitations section doesn't sufficiently address limitations of the datasets. Datasets with fine-grained multi-labels would be a better approach at capturing true scene dependencies than the linear augmentation. Further, the model will be significantly biased by the distribution of the training data in ways that cut across demographics and geography. As the quality of these models improve, this can present bias and representation challenges in a similar manner to text2image models currently, and it's important to note that in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "In general the paper is clearly written, with appropriate motivation, references, and equations. As mentioned in the summary, the novelty of the paper is primarily empirical, demonstrating the viability of text2audio. \n\n Below I point out some things and questions that could be addressed to strengthen the clarity and reproducibility of the paper:\n\n* In the section on page 5, \"Multi-stream audio inputs\" I inferred from the text that each stream is sampled independently given the logits for a frame of audio, is that correct? If so, it might clarify to make that explicit.\n* In describing the datasets, it could help to make a table to clarify the datasets used, their size pre and post filtering, and any other characteristics that vary between datasets such as their primary composition (speech, birds, etc.) and examples of the labels.\n* Figure 2 is a bit confusing compared to the rest of the paper. Especially the graph on the right, where \"Rnd text\" is used for labels for two lines, but I believe in one, it refers to the conditioning and in the other it refers to the reference. frankly, there's not a whole lot in those figures that isn't presented clearer in table 3. I believe the authors are trying to highlight the Delta KL between rnd text and source text in the left, and text cond and no text cond in the right. Just plotting that value itself with a single line would make things clearer I think, or just adding it to table 3 and removing figure 2.\n* Table 2 likewise is a bit redundant with the encoder info duplicated for both rows. Perhaps consider splitting into two tables? Or at least noting the redundancy in the caption as they are independent on the size of the decoder.\n* The paragraph right before \"Limitations\" compares AudioGen in Table 2 to DiffSound. It would help to add DiffSound to table 2 to make the comparison easier.\n\n\nSmall typos:\n* page 2: \"generation in two axis\" -> \"generation in two axes\"\n* page 3: \"audio tokes\" -> \"audio tokens\"\n* page 6: \"we remove annotator\" -> \"we remove annotators\"\n* page 8: \"three encoding mehtods\" -> \"three encoding metrics\"\n* page 9: \"Limitation\" -> \"Limitations\"\n* page 15: Caption says the text is \"a crowd applauds followed by a woman and a man speaking\", but the text above says it should be \"speech and a goat bleating\" which looks correct given the spectrogram.",
            "summary_of_the_review": "This paper presents an impressive empirical demonstration of text2audio using existing techniques. Datasets are carefully curated and methods selected to enable this demonstration, and it is of high significance to the ML community as it points towards the potential of pushing these techniques further, and the limitations of current datasets and methods. \n\nIn general, the experiments are carefully performed, with good metrics and ablations. Several important questions remain to ensure that the comparisons between models are fair, the value of techniques such as mixing data augmentation are justified, and the relative importance of scaling encoders / decoders are understood. Some discussion of future ethical implications would also help the limitations section of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2944/Reviewer_Tiv8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2944/Reviewer_Tiv8"
        ]
    },
    {
        "id": "1pSIQLt5t7",
        "original": null,
        "number": 3,
        "cdate": 1666671222235,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671222235,
        "tmdate": 1666671222235,
        "tddate": null,
        "forum": "CYK7RfcOzQ4",
        "replyto": "CYK7RfcOzQ4",
        "invitation": "ICLR.cc/2023/Conference/Paper2944/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a solution to generate audio from text prompts. The proposed solution uses a Vector-Quantized VAE, and in the latent space a language model is employed. ",
            "strength_and_weaknesses": "Strengths: \n-The results sound impressive. \n-Methodology seems clear. \n-The paper is relatively well written. \n\nWeaknesses: \n-The employed model seems to be very large, and the employed computational resources are enormous.\n-I am not sure if the comparison with diffsound is exactly fair. I am sorry if I am missing something, but it doesn't seems to be indicated how did you get the results with Diffsound. Also, a comparison on how much data each model uses. From what I understand you use more data than diffsound to train your model. Ideally it would be nicer to train the architecture of diffsound on the dataset you have curated. This would help to ascertain if your proposed architecture contributes more significantly than the size of the dataset. I am sorry if you have done this experiment, however from the manuscript I do not see it.  \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "-Some aspects regarding the experimental details are not super clear to me. For instance, as I wrote above I am not sure about under which exact conditions did you compare with Diffsound. Also it is not indicated how long does the training takes with the indicated number of GPUs. \n\n-The proposed architecture is not particularly novel from a technical standpoint, but the curation of several different aspects (dataset, design of the architecture, delivery of the results) make this work good quality. \n\n-Reproducibility is extremely difficult as the number of GPUs used in this dataset is very large. Also the code is not available.  ",
            "summary_of_the_review": "There is some unclear aspects I noted above. However I think this is a good quality work, and therefore it is above the acceptance threshold. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "Unfortunately successful generative models come with the danger to be used for malicious intents. This is not something particularly specific to this work though.  ",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2944/Reviewer_iK2a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2944/Reviewer_iK2a"
        ]
    },
    {
        "id": "hPgpVU8OWe",
        "original": null,
        "number": 4,
        "cdate": 1666894587211,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666894587211,
        "tmdate": 1666894587211,
        "tddate": null,
        "forum": "CYK7RfcOzQ4",
        "replyto": "CYK7RfcOzQ4",
        "invitation": "ICLR.cc/2023/Conference/Paper2944/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors present a system named AudioGen that can generate audio given text description of sound. The system is trained using a set of many audio datasets. Unlike (image, text) pairs, the size of (audio, text) pairs is not large enough, at least in the public domain. This limitation is tackled by using audio tag labels. Although this means what AudioGen does is rather a tag-based generation than text description, this is probably the best output using the currently available dataset. The loss is also carefully chosen, which seems to help. ",
            "strength_and_weaknesses": "Strength:\n- Clever use of available datasets\n- Well designed and executed experiment that shows the system works well.\n- Limitation of the proposed system is given",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity is high. Clear writing about what was done.\n- Quality is high. The proposed system is well designed with state-of-the-art architectures and sensible, intuitive, and efficient idea of using the available datasets.\n- Novelty is medium-high (or high enough)\n- Reproducibility is high. In practice though, with the current computational cost, it is not easy to actually reproduce this work. ",
            "summary_of_the_review": "I believe the paper is good enough to be accepted overall. I'll only describe some issues here.\n- After the text processing, is it correct to call them \"sentences\"? I find this a bit misleading. Although a description-based system does not need to fully reflect what is written in the description (it is probably not possible), the proposed system seems rather (open vocabulary) tag-based generation than sentence-based one.\n- Very minor issue, but In 4.3, last paragraph: \"AudioGen improves the KL score over.. higher FAD\" \u2192 it'd be easier to follow if the scores of DiffSound is presented again in this sentence.\n- In 5. Limitation, I don't think such a system is supposed to generate intelligible speech (it's not a TTS system.)\n- It'll be useful if the text preprocessing code is shared somewhere. Not every reader is knowledgeable on text processing.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2944/Reviewer_J2ki"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2944/Reviewer_J2ki"
        ]
    }
]