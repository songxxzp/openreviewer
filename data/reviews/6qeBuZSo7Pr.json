[
    {
        "id": "Cbtmg9MFFt",
        "original": null,
        "number": 1,
        "cdate": 1666562301363,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666562301363,
        "tmdate": 1668757184318,
        "tddate": null,
        "forum": "6qeBuZSo7Pr",
        "replyto": "6qeBuZSo7Pr",
        "invitation": "ICLR.cc/2023/Conference/Paper3345/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes planning exploratory goals (PEG) for helping an agent explore during training time to achieve diverse goals revealed at test time. PEG brings together the Go-Explore framework, where a goal-conditioned policy brings an agent to a goal, and then an undirected exploration policy explores around that goal, with Latent Explorer Achiever (LEXA), which learns a world model that is used to train an explorer and goal achiever policy. ",
            "strength_and_weaknesses": "Strengths:\n- The paper proposes an interesting idea in bringing together Go-Explore and LEXA and proposing a new objective for what goals to choose.\n- The visualizations are helpful for building intuition, and the 3-Block Stack results are quite nice. The ablations are also very useful. \n\nWeaknesses:\n- It is not clearly described in the main text how the goals in the algorithm are sampled. Maybe I missed this somewhere, but this is a very critical part of the method that is not well described. Can the authors clarify this?\n- Relatedly, it seems that the goals chosen by PEG are not in the replay buffer and are in fact sometimes unreachable, e.g. in Figure 5, and this is also mentioned in the paper text. It seems like none of the comparisons make the assumption of sampling from the state space. This is a pretty big difference. Can the authors clarify this? \n- All the experiments are using low-dimensional state spaces, although LEXA has been shown to work in settings with high-dimensional inputs. It would be nice to see experiments with vision inputs.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: It is a bit difficult to separate this paper's contribution from existing literature. Specifically, to someone who is not fully aware of LEXA, it is unclear in Section 3 and Algorithm 1 which specific parts are different from LEXA/Go-Explore. Also, there are issues with the details of the method not being described clearly. \nQuality: See above weaknesses. \nNovelty: The paper mostly combines a lot of existing ideas and methods.",
            "summary_of_the_review": "Due to lack of clarity and issues with the method/evaluation, I think this paper is not ready for acceptance.\n\nEdit: based on additional experiments and clarifications from the authors during the rebuttal period, I have increased my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3345/Reviewer_WR3C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3345/Reviewer_WR3C"
        ]
    },
    {
        "id": "E16p9aQYQIM",
        "original": null,
        "number": 2,
        "cdate": 1666650423755,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650423755,
        "tmdate": 1668820842028,
        "tddate": null,
        "forum": "6qeBuZSo7Pr",
        "replyto": "6qeBuZSo7Pr",
        "invitation": "ICLR.cc/2023/Conference/Paper3345/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a novel Reinforcement Learning based unsupervised method for the task of goal planning for agent based exploration in a non task-centric paradigm.",
            "strength_and_weaknesses": "# Strengths:\n\n- (1) Readability.\n\nOverall, the readability is fair. The main ideas are well conveyed, justified and articulated while mostly providing sufficient context to the reader, albeit sometimes superficially (cf. below for the downsides). The grammar, and overall the writing, is relatively mature in its current state.\n\n- (2) Experimental Evaluation.\n\nThis one is a blend of benchmark per se, engineering to re-implement pertinent baselines and ablative study of the proposed conceptual contribution.\n\n- (3) The Performance of the proposed method.\n\n- (4) The Reproducibility.\n\nThis one is somewhat of a mixed bag. The positive aspects predominently lie in the proactive care the authors have put into explaining the experimental set-up, the re-implementation of the baselines and the description of the considered dataset environments.\nIn absence of an existing, public, standardized and unified benchmark environment, the proposed package is fair on many aspects (a few hicups though, as discussed below).\n\n# Weaknesses:\nDespite several positive aspects to the submitted paper, there are several noticeable shortcomings. In particular,\n\n\n- (1) How much does it cost?\n\nWhile the experimental evaluation which focuses on the task-oriented performance per se, alternative considerations could be discussed to help better understand the relative positioning of the proposed method and how it contrasts (favourably or not) on algorithmic and resource usage and management considerations.\n\n- (2) Related Work.\nThe section is awkwardly placed as a #5 section, right before the conclusion and is relatively short.\nIt is rather superficial by often characterizing methods at a very top-level, rather than focusing on their key differanciations and relative interconnections from a mechanistic-centric point of view. \n\nTo a lesser extent, it also ommits quite a few pertinent work in the field, e.g.,\n\n(Sub-goal oriented, and references therein)\n* CHANE-SANE, Elliot, SCHMID, Cordelia, et LAPTEV, Ivan. Goal-conditioned reinforcement learning with imagined subgoals. In : International Conference on Machine Learning. PMLR, 2021. p. 1430-1440.\n\n\n(Seminal formulation of several related concepts)\n* YAMAUCHI, Brian. Frontier-based exploration using multiple robots. In : Proceedings of the second international conference on Autonomous agents. 1998. p. 47-53.\n\n* YAMAUCHI, Brian. A frontier-based approach for autonomous exploration. In : Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97.'Towards New Computational Principles for Robotics and Automation'. IEEE, 1997. p. 146-151.\n\n- (3) The emphasis should be much stronger to explain the difference between the added contritbution and the vanilla prior work from Mendonca NeurIPS 2021.\n\nThe corresponding section (slightly more than a 1-pager) to describe it, including the original (prior work) work on which this one is based is way too shallow to support the current state of the submissions.\n\n- (4) Reproducibility:\n\nAdditionally, despite having many implementation details provided, the parameter tuning seems relatively simplistic and unchallenged.\n",
            "clarity,_quality,_novelty_and_reproducibility": "As stated above in the review, the clarity, quality and reproducibility aspects are mostly on point. The reproducibility comes with a relatvie caveat, but remains reasonable regarding the standards in the field.\n\nI firmly believe the reproducibility aspect could be reasonably improved through the authors/reviewers discussions and am looking forward to it regarding the parameter tuning, and resource usage differential between the proposed conceptual methods and the benchmarked alternatives from the state-of-the-art. The amount of focus to explain the differential between the proposed contribution, the original framework on which they build upon, would be more challenging, as it implies a substantial rewriting part.\n",
            "summary_of_the_review": "\nAs it currently stands, the paper has several positive aspects rooting for it. Not having access to the code and finer graine details \nregarding the intrinsics and experiments is a slight turn-off, but regarding the standards of the field and the venue, it remains on the acceptable side.\n\nThe downsides however, include a lack of experimental challenge of the parameter tuning and the overal descriptive focus is not emphasized enough on the conceptual differential between the proposed method and the original prior work it builds upon, i.e., Mendonca et al. NeurIPS 2021.\n\nOverall, it is hence slightly bellow borderline but could I would be happy to revise my stance and discuss the (still/currently) missing bits.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None applicable.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3345/Reviewer_p4aC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3345/Reviewer_p4aC"
        ]
    },
    {
        "id": "wa_sLbryPrl",
        "original": null,
        "number": 3,
        "cdate": 1666652846590,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652846590,
        "tmdate": 1669136750147,
        "tddate": null,
        "forum": "6qeBuZSo7Pr",
        "replyto": "6qeBuZSo7Pr",
        "invitation": "ICLR.cc/2023/Conference/Paper3345/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Existing exploration methods tend to explore at the frontier of the currently seen states. The authors of this paper propose to direct the goal-conditioned policy in a way that induces higher exploration value trajectories for the agent. Specifically they first use the goal-conditioned policy $\\pi^G$ to reach a such a state $s_T$ that maximizes the exploration value $V^E(s_T)$.\n\nExtensive experiments on control as well as robotic manipulation showcase the sample-efficiency of the method. In particular, the method achieves really good results on hard task of block-stacking which the previous works fail at.",
            "strength_and_weaknesses": "**Strengths**\n\n1. The idea is novel, and the paper is clearly written, making it easy for the reader to understand the motivation as well as the method.\n\n2. Thorough experiments to validate the claim of optimizing goal commands directly.\n\n\n**Weaknesses**\n\n**Claims (clarification)**\n\n3. The authors distinguish directly optimizing for goal commands with exploration methods that aim to reach novel states, and say:\n\n    > Note that this does not mean merely commanding the agent to novel or rarely observed states. Instead, PEG commands might be to a previously observed state... PEG only cares that the command will induce the chained GCRL and exploration phases together to generate interesting training trajectories, valuable for policy improvement.\n\n    Shouldn't it be the case that the \"interesting trajectory\" would encounter some novel states -- otherwise the total exploration reward for that trajectory wouldn't be maximum?\n\n\n**Methodology**\n\n4. From equations (2) and (5) and the choice of K=1, is it correct to say the following:\n\n    $\\max_g \\mathbb{E}_{p_\\pi G(.|., g)}(s_T)[V^E(s_T)] \u2248 \\frac{1}{K} \\sum_k V_\\theta ^E(s_T^k) = V_\\theta ^E(s_T)$\n\n\n\n    a) What could be the reason that K>1 doesn't have a large effect on the performance? \n\n    b) Additionally, in appendix A.5, where CEM is compared with MPPI, what were the hyperparameters for CEM? How does CEM perform with more number of trajectories?\n\n\n**Citations**\n\n5. In the Conclusion section, please also cite [1] along with [2] and [3]. Works [1] and [2] were independently published around the same time and share the same idea.\n\n------\n\n**References**\n\n[1] Alexander Neitz, Giambattista Parascandolo, Stefan Bauer, Bernhard Sch\u00f6lkopf, Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models, In NeurIPS 2020.\n\n[2] Dinesh Jayaraman, Frederik Ebert, Alexei A Efros, and Sergey Levine. Time-agnostic prediction:Predicting predictable video frames. ICLR, 2019.\n\n[3] Karl Pertsch, Oleh Rybkin, Frederik Ebert, Chelsea Finn, Dinesh Jayaraman, and Sergey Levine. Long-horizon visual planning with goal-conditioned hierarchical predictors. In NeurIPS 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "**Very very minor issues (which have *NOT* been considered for decision making)**\n\n6. In Appendices A.4 and A.5 PAGE is used instead of PEG. I believe in an earlier version of the manuscript, the method was named PAGE.\n\n7. Typo: Page 8 last line \"achievd\" --> \"achieved\"\n\n8. Please slow down the GIFs speed on the webpage. It is incredibly hard to view the visualizations.\n\n\n**Reproducibility**\n`\nI appreciate the authors open-sourcing the code in the supplementary material.\n",
            "summary_of_the_review": "Given the clear motivation, methodology and experimentation of the paper, I lean towards accepting the paper (*6: marginally above the acceptance threshold*). However, I'd want the authors to address my concerns as well as other reviewers' comments during the rebuttal phase, and my final decision would be subject to the rebuttal.\n\n-----\n\nPost-rebuttal update:\n\nI've updated my score to 8 -- good paper, accept after going through other reviewers' comments and rebuttal. The authors' rebuttal addresses my concerns.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3345/Reviewer_i4Jc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3345/Reviewer_i4Jc"
        ]
    },
    {
        "id": "jwTAeoyVlp",
        "original": null,
        "number": 4,
        "cdate": 1666658734808,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658734808,
        "tmdate": 1666658734808,
        "tddate": null,
        "forum": "6qeBuZSo7Pr",
        "replyto": "6qeBuZSo7Pr",
        "invitation": "ICLR.cc/2023/Conference/Paper3345/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed a method to quickly explore a state space and achieve goals that require long-horizon trajectories. They define the exploration value of a trajectory based on learned transition functions and policies for sampling trajectories, which are added to the training buffer. Their experiments on four tasks from 2D point navigation to 3D locomotion and 3-block stacking demonstrate the effectiveness of their method in comparison to two other baselines for long-horizon multi-goal RL. ",
            "strength_and_weaknesses": "The authors contributed a useful method that speeds up exploration thus resulting in less self-exploration time for agents in a new environment to achieve goals in the furthest states. Their experiment environments are diverse and the results are strong.\n\nHowever, the test goals are selected from a specific distribution of states that require the furthest explorations. In Figure 4, the variance for PEG is very high, which makes the results less convincing given only five seeds. What are the authors' thoughts on an experimental design that could reduce the variance?\n\nFurthermore, the reviewer would like the authors to address the following questions to make the conclusions more convincing:\n\n1. About Algorithm 1, since training prioritizes exploration, how much is exploitation performance affected (e.g. the number of steps to achieve a goal sampled uniform at random in the environment) compared to other baselines? \n2. in Figure 6, many goals are sampled close to the start state (red dots on the left of the blue dot) and a few around the first corner, which increases more in quantity than those in remote states throughout training. Why might that be the case?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written with clear motivations. The approach and experimental sessions are easy to follow. The method, while simple, is effective.",
            "summary_of_the_review": "This is a well-written paper on an important topic. The method has the potential to benefit robot navigation and manipulation in new environments. While the reviewer has concerns over the variance of one of their experiments, the results are overall very convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3345/Reviewer_rUvT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3345/Reviewer_rUvT"
        ]
    },
    {
        "id": "E3wh8CL98w",
        "original": null,
        "number": 5,
        "cdate": 1666704796956,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666704796956,
        "tmdate": 1666704796956,
        "tddate": null,
        "forum": "6qeBuZSo7Pr",
        "replyto": "6qeBuZSo7Pr",
        "invitation": "ICLR.cc/2023/Conference/Paper3345/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents the \"Planning Exploratory Goals\" algorithm, which identifies goal states that are likely to result in many novel observations when used to initialize an subsequent explore-phase upon reaching a termination condition en route to the aforementioned goal. The algorithm overcomes prior issues, such as working with poor quality starting policies and unreachable (exploratory) goals, by taking the agent's learned transition (\"world\") model into account. This enables it to effectively explore in many goal-conditioned reinforcement learning scenarios.",
            "strength_and_weaknesses": "The paper's strengths include clear writing, interesting figures, and an effective new general purpose algorithm for goal-conditioned reinforcement learning. It effectively highlights differences between the new algorithm and baselines in (exploratory) goal setting approaches, with good explanations on how these differences ultimately contribute to better performance. The algorithm is well explained, and ablation tests validate the increased complexity of certain design choices. The description of future work and limitations of the current PEG algorithm (specifically regarding \n\nIn my opinion, the main weakness of the paper is the significance and standardization of the experimental results. The error bars on the results in figure 4 are borderline convincing that PEG is a significant step up from the baselines, particularly in the first 3 experiments. The differences highlighted in Figures 5 and 6 help to explain the marginally increased performance of PEG over baselines in those experiments, but there is an omission of good analysis into the deficiency of baseline policies relative to PEG in the 3-block stack experiment. Furthermore, the paper notes that evaluation goals were picked to \"require extensive exploration\" -- more work needs to be done to justify why this makes the test environments more representative of real-world problems, and why is does not simply bias the experimental results towards PEG (which is explicitly designed for this behaviour). I think that the \"3-block stack\" paragraph of subsection 4.3 could be improved through the use of figures and other analysis to demonstrate issues in the baselines -- I also believe that the use of the exclamation and rhetorical question in this paragraph (\"PEG experiences... throughout training!\" and \"Did the... Explore-Phase?\") are inappropriate and more passive language should be used in this section (particularly because the first statement is critical of prior works).\n\nI would also like to see some discussion of value of information/exploration methods in the related works, since this approach of identifying states with \"high-exploratory value\" seems related to other works in the areas of quantifying the similarity of states and other analysis from general exploration in POMDPS.  And, I would like a bit more rigour in the implementation details (e.g., \"optimization only takes a few seconds\" on what kind of hardware? is this significantly faster/slower than X?).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, the PEG algorithm presented is clever and novel, and the code release suggests that everything will be reproducible. The paper is already quite polished and does not require too much editing for a final copy, although some of the citations (e.g. top of page 2) seem misformatted. As mentioned previously, my only concern with the quality of the paper is with the rigour of the experimental results, which I think can be addressed through increased evaluations (smaller error bars) and a bit more analysis (particularly into 3-block stacking).",
            "summary_of_the_review": "This is a strong paper that improves the state of the art in goal conditioned reinforcement learning in a wide range of problems. The algorithm is interesting and empirically validated, and I think it will inspire good follow-up work and discussion in the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3345/Reviewer_5sEN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3345/Reviewer_5sEN"
        ]
    }
]