[
    {
        "id": "S6RbwvfMv0b",
        "original": null,
        "number": 1,
        "cdate": 1666525928920,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666525928920,
        "tmdate": 1666525928920,
        "tddate": null,
        "forum": "Ki_26lfEmey",
        "replyto": "Ki_26lfEmey",
        "invitation": "ICLR.cc/2023/Conference/Paper2575/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a novel method named ADNT for tackling the multi-source unsupervised domain adaptation (MUDA) problem. Their method mainly contains two well-designed strategies: the contrary attention-based domain merge (CADM) module for domain feature fusion, and the adaptive and reverse cross entropy (AR-CE) loss for robust pseudo-label generation. They verified the large superiority of ADNT on four MUDA benchmarks.",
            "strength_and_weaknesses": "Strength:\n1) The elaborately designed CADM module is inspired by the cross-attention mechanism. By adding a metric-based loss for maintaining a clear decision boundary and a memory M for storing features, the domain feature fusion capability of this module has been further improved. This fusion strategy may generalize to other visual MUDA tasks, like object detection and semantic segmentation.\n2) The proposed new loss function AR-CE focuses on alleviating the influence of noisy pseudo-label generated from the target domain during training, which is inspired by the method SHOT++. By adding the reverse operation, the new loss can achieve pretty good results.\n3) The presented method ADNT has obtained SoTA performances on four common MUDA benchmarks.\n\nWeaknesses:\n1) As mentioned above, the two main strategies involved in the method are derived from the improvement of existing modules. This limits the innovation and novelty of the paper to a certain extent.\n2) The explanation of proposed loss AR-CE around Equations (15), (16) and (17) is ambiguous and difficult to understand. I think authors may need to rearrange the language to describe their design ideas.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this article is clear, the structure and experiment design are reasonable, and it has a relatively high originality.",
            "summary_of_the_review": "Although most of the two main strategies are inspired by other closely related researches, the method proposed in this paper is practical and effective, and can be used as a representative method in the field of MUDA.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2575/Reviewer_2VJ6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2575/Reviewer_2VJ6"
        ]
    },
    {
        "id": "JFPMhZbJOee",
        "original": null,
        "number": 2,
        "cdate": 1666621115724,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666621115724,
        "tmdate": 1666621115724,
        "tddate": null,
        "forum": "Ki_26lfEmey",
        "replyto": "Ki_26lfEmey",
        "invitation": "ICLR.cc/2023/Conference/Paper2575/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies multi-source domain adaptation, an interesting and important domain adaptation topic. Instead of learning domain-invariant feature representations, this paper proposes to transfer domain-specific information and also considers pseudo-label generation. Specifically, this paper proposes (1) a Contrary Attention-based Domain Merge (CADM) module to enable the interaction among the features so that domain-specific information can be mixed, and (2) an adaptive and reverse cross-entropy loss to correct the pseudo labels during training. Experiments are conducted on four benchmark datasets for classification.",
            "strength_and_weaknesses": "Strengths:\n1. The idea of transfering domain-specific information is interesting though I am not sure whether this makes sense.\n2. The proposed CADM and adaptive and reverse cross-entropy loss are relatively novel based on some existing techniques.\n3. The results on some adaptation settings are good and the ablation studies and visualizations are convincing.\n\nWeaknesses:\n1. The motivation is not clearly explained. Learning domain-invariant features and enhancing the category information are easy to understand for multi-source domain adaptation. However, this paper tries to transfer domain-specific information. Since they are domain-specific, how can they be transferred and why do they work for adaptation?\n2. As some methods claimed, different source domains and different examples in each source domain play different roles during the adaptation process. How is such information considered?\n3. It seems that one feature extractor F and one classifer C are firstly pre-trained. Since there are multiple domains, do you think simply combining all source samples and train one common F and C makes sense?\n4. The results on DomainNet are much better than the compared baselines, but the result on other three datasets are only marginally better or even inferior. Why? There is no analysis on the conditions that the proposed method work. \n5. The presentation needs to be revised and improved. For example, what does $D$ indicate (feature dimension in my understanding)? In Introduction, \"Secondly, To enable\"->\"Secondly, to enable\". The format of references, especially the names of conferences and journals, is inconsistent. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The motivation is not clearly explained. Why domain-specific information is important in domain adaptation is not well analyzed. This results in poor clarity. \nThe quality is generally good, especially the CADM module and the proposed new cross-entropy loss function.\nSome important implementation details are missing, and the code is not provided. I doubt the reproducibility of this paper.",
            "summary_of_the_review": "Interesting idea but unclear motivation, generally new components for multi-source domain adaptation, insufficient analysis on the results, and could be improved presentation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2575/Reviewer_buhi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2575/Reviewer_buhi"
        ]
    },
    {
        "id": "Qkq-cKWbxs",
        "original": null,
        "number": 3,
        "cdate": 1667520425132,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667520425132,
        "tmdate": 1667520481847,
        "tddate": null,
        "forum": "Ki_26lfEmey",
        "replyto": "Ki_26lfEmey",
        "invitation": "ICLR.cc/2023/Conference/Paper2575/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper targets transferring multi-source knowledge to the target domain. While the previous studies tried to utilize the generalized knowledge of the multiple source domains, the proposed algorithm focus on domain-specific information. To accomplish the extraction of the domain-specific information, the paper presents new modules of a contrary attention-based domain merge module and an adaptive and reverse cross-entropy loss. The contrary attention-based domain merge module can handle the interaction among the features for the domain-specific information. In addition, the adaptive and reverse cross-entropy loss works as a constraint for stable pseudo-label generation. The overall framework shows the state-of-the-art performance in the various experimental scenarios.",
            "strength_and_weaknesses": "The visualization figures of the paper are helpful to understand the overall framework, and the experimental settings are reasonable to validate the algorithm. The state-of-the-art performance of the proposed algorithm confirms its validity, and several ablation studies verify the effectiveness of each component.\n\nHowever, I have several concerns and questions yet.\n\n1. Weak motivation for sharing the same domain-specific information\n\n   The authors insisted that filtering the domain-specific information for every domain is difficult and often results in losing discrimination ability. Of course, the filtering process can be challenging, but the approach looks more reasonable than the shared domain-specific information. Since the multiple domains would have different characteristics that are helpful to extend the distinguishability of the target domain, it seems more reasonable to extract the different domain-specific information from each domain. In addition, when we want to extract the shared domain-specific information from the multi-source, only the limited information would be extracted when the multiple sources are composed of dispersed domains. I hope to know the additional explanation for this problem.\n\n2. The organization of sampled batch\n\n In the first paragraph of section 3, the sampled batch contains \"b\" samples for each domain. Then, when we have a variety of source domains, the number of domain-wise samples becomes very limited. I hope to know the relationship between the number of domains and the performance of the proposed algorithm.\n\n3. Meaning of Eq.9\n\n  The training loss of Eq.9 seems contradictive with Eq.5. While Eq.5 makes the similar domain information connected less effectively, Eq.9 lets the scattered information be gathered again. Why do we need to consider the two contradictive losses in the integrated form? And what would be the effectiveness of the integration?\n\n4. Missing explanation of Table1, 2, 3\n\n  There is no explanation for Tables 1, 2, and 3. Since the results from Tables 1, 2, and 3 are from the main experiments, the related analysis is essential but missing. \n\n5. MIssing analysis for the performance drop\n\nIn Tables 3 and 4, there appear several performance drops compared to the state-of-the-art algorithms for some scenarios, but no related analysis is explained in the paper. We can catch the effectiveness of the proposed algorithm through the analysis of limitations.\n\n6. Several minor comments\n\n In Figure 4, the font size is too small\nAt the last of Eq. 6 and 7, commas should be followed to continue the sentences.",
            "clarity,_quality,_novelty_and_reproducibility": "Even though the proposed approach seems novel in the research area, the novelty is not supported by well-defined motivation. The lack of motivation makes the designed framework unreasonable, which makes it hard to understand the reason why the performance could increase through the proposed algorithm.",
            "summary_of_the_review": "Due to the weakly explained motivation of the proposed algorithm and the missing analysis of the performance drop, it becomes hard to understand the reasoning of the framework design and its state-of-the-art performance. In addition, the overall organization of the paper should be improved and proofread. The details can be referred to the weakness section.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2575/Reviewer_eMFp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2575/Reviewer_eMFp"
        ]
    },
    {
        "id": "8Jb3hdiuEH",
        "original": null,
        "number": 4,
        "cdate": 1667619917019,
        "mdate": 1667619917019,
        "ddate": null,
        "tcdate": 1667619917019,
        "tmdate": 1667619917019,
        "tddate": null,
        "forum": "Ki_26lfEmey",
        "replyto": "Ki_26lfEmey",
        "invitation": "ICLR.cc/2023/Conference/Paper2575/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work concentrates on two problems in the MUDA task: a) sharing the same domain-specific information across domains to fuse multi-source data; b) generating reliable pseudo labels to alleviate the effect of noisy pseudo labels. Thus, they proposed the Contrary Attention-based Domain Merge modules to pass domain-specific information through EMA-updated domain style centroids. Moreover, a growing class center is applied to optimize intra-class distance. In order to generate reliable pseudo labels, gradients are assigned to the generation process of pseudo labels. Additionally, adaptive and reverse cross-entropy loss is proposed to ensure reliable pseudo-label generation. Extensive experiments were conducted to validate their contributions.",
            "strength_and_weaknesses": "1. Figure 1 doesn't convey the pipeline and process of your model well, especially in the right part concerning the classifier and pseudo-label generation. Adding some sub-figures may be better.\n2. As you proposed to assign gradients to the pseudo label generation process, give a comparison of the traditional gradient-free pseudo generation process and your process in the form of the figure may be better. Or, you may point out the difference by giving annotations of the\narrows and lines in Figure 1.\n3. The adaptive and reverse cross-entropy loss depends on the correctness of generated hard labels. However, there is no guarantee that the hard labels are reliable. The adaptive factor based on the entropy of distribution may amplify the influence of the wrong assignment and mislead the model. Have you ever considered about this and what about your explanation?\n4. Is there any evidence to validate the contribution of reverse cross-entropy in the ablation study? This may validate the effectiveness of the adaptive factor.\n5. The annotations and explanations should be reconsidered. For example, if the adaptive and reverse cross-entropy loss is not added, does the method assign gradients to the pseudo-label generation process? If the gradients are assigned, what is the loss? reverse cross-entropy loss or others? This should be annotated clearly and added an explanation in the ablation study part.\n6. The ResNet50-based models achieve 100% on some experiments on Office-31, Office-Caltech dataset outperformed the methods which are based on ResNet101. What are the results if your use ResNet101 on ADNT on the Office-31, Office-Caltech, and Office-Home\ndataset?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear. However, the idea is not good enough, it is like leveraging current information for domain fusion.",
            "summary_of_the_review": "The paper is a borderline paper for this conference.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2575/Reviewer_javN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2575/Reviewer_javN"
        ]
    }
]