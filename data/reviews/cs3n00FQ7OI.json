[
    {
        "id": "N0Shaym7eSI",
        "original": null,
        "number": 1,
        "cdate": 1666535058277,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666535058277,
        "tmdate": 1666535058277,
        "tddate": null,
        "forum": "cs3n00FQ7OI",
        "replyto": "cs3n00FQ7OI",
        "invitation": "ICLR.cc/2023/Conference/Paper5887/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose low-precision training techniques to lower the number of bits to represent parameters, gradients, and moments. SGDM optimizer is assumed while low-precision training can improve the model accuracy by using two main principles: unbiased stochastic quantization and microbatching. Overall, memory consumption can be reduced by over 70% when ResNet-18 and ResNet-50 (on ImageNet) are trained.",
            "strength_and_weaknesses": "*Strength\n- Convergence of the proposed low-precision training is analyzed.\n- The impact of microbatching is investigated with details.\n\n*Weakness\n- Stochastic rounding has been introduced by lots of previous works. What is the major difference compared to those related works? For example, the paper titled \"Training deep neural networks with 8-bit floating point numbers\" already introduced while the first paper introducing stochastic rounding is probably \"Deep Learning with Limited Numerical Precision\" which is missing in the reference.\n- Properties of microbatching seem to be very similar to chunk-based additions of the above reference.\n- Comparisons with previous low-precision training are missing in the experiments.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Two major ideas of the manuscript seem to have been already introduced previously. What is new compared to those previous works?\n- Improvements and novelty are incremental at best.\n- Only two models (ResNet-18 and ResNet-50) are analyzed. A lot more models and dataset need to be investigated to claim the generality of the proposed techniques.",
            "summary_of_the_review": "- Ideas are incremental (or not new at all)\n- Experiments need to be more extensive. Only two models are not enough to prove the claim.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5887/Reviewer_wNyQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5887/Reviewer_wNyQ"
        ]
    },
    {
        "id": "2PVtJ4MwyLd",
        "original": null,
        "number": 2,
        "cdate": 1666662373617,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662373617,
        "tmdate": 1666662373617,
        "tddate": null,
        "forum": "cs3n00FQ7OI",
        "replyto": "cs3n00FQ7OI",
        "invitation": "ICLR.cc/2023/Conference/Paper5887/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the issue of large model memory as a bottleneck to model scaling. It proposes a framework to compress model parameters, gradient accumulations, and momentum, during training. Achieves iso-accuracy on ResNet/Imagenet models down to 12/8/8 bits, respectively, and minimal degradation down to 8/8/8 bits. Underflow is addressed by combining stochastic rounding with microbatching (accumulation of subsequent gradients). Convergence is mathematically guaranteed in the presence of stochastic quantization.",
            "strength_and_weaknesses": "Strengths:\n- combination of stochastic rounding (SR) and microbatching achieves remarkable model memory compression with no (or limited) accuracy degradation\n- good theoretical support on LPMM-SGDM convergence\n\nWeaknesses:\n- the paper heavily relies on known techniques (SR and microbatching) which are applied as-is to the particular case study\n- the main insight is claimed to be identification of underflow as the core issue to be solved. However, the impact of delays in the parameters update (or lack thereof) is a well known quantization-related issue, both in the case of having access or not to a full precision copy of the quantized parameters (see for example the already cited [1], which discusses the conceptually-similar swamping issue as well as SR, or [2], which discusses mitigation strategies for updates delayed by quantization). So, it's hard to accept the claim that it has been \"discovered\" in this particular scenario\n- demonstration is limited to two CNN models so it's unclear if findings may generalize (e.g., is this compression level suitable for other models?)\n- what is the runtime (or training time overhead) of this algorithm? how practical is it to apply this training technique beyond the two examples shown?\n- there is a mention of a training algorithm implementation at the level of CUDA kernel in PyTorch but no details about it\n\nOther corrections:\n- section 3.2: part of sentence missing: \"...is far from the Extremely, there could be\"\n- algorithm 1: should the quantization in line 5 be carried out _after_ all the micro-batch gradients have been accumulated? Doesn't applying the quantization at every step defeats the purpose of accumulation if gradients are individually small?\n\n[1] N. Wang et al. \"Training Deep Neural Networks with 8-bit Floating Point Numbers\" (section 3.2)\n[2] M. Nagel et al. \u201cOvercoming Oscillations in QAT\u201d",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well structured, clear, and easy to follow.\nWhile the techniques used to quantize the model parameters and address underflow are known, there is novelty in how they are jointly applied to this case study (CNNs) and how convergence is theoretically demonstrated.\nImplementation is explained in details so results should be reproducible.\n\n",
            "summary_of_the_review": "The paper applies known techniques to successfully compress model memory (i.e., model parameters, gradient accumulations, and momentum) and achieve strong compression and iso-accuracy in two CNN models. I would like to see more details about the algorithm implementation and its runtime (possibly an additional appendix). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5887/Reviewer_Dafx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5887/Reviewer_Dafx"
        ]
    },
    {
        "id": "NXLHpuqY0O",
        "original": null,
        "number": 3,
        "cdate": 1666861792390,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666861792390,
        "tmdate": 1669657815670,
        "tddate": null,
        "forum": "cs3n00FQ7OI",
        "replyto": "cs3n00FQ7OI",
        "invitation": "ICLR.cc/2023/Conference/Paper5887/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose low-precision model memory (LPMM), which quantizes a model's parameters, gradients, and optimizer state to low precision. Prior works have focused only on params/gradients or optimizer state separately. The main challenge is that params, gradients, and momentum may have very different dynamic range, leading to underflow (typically of the gradient updates). LPMM addresses this via unbiased stochastic rounding and increasing the batch size to increase the magnitude of the updates. The paper uses the term \"microbatching\" but this is just accumulating multiple batches of gradients before computing a weight update (some frameworks call this gradient accumulation).\n\nThe authors experiment with ResNet-18 and ResNet-50 on ImageNet, showing that LPMM with 8-bits for params, grads, and momentum can get within 2% of the full precision accuracy. If the params are kept in 12 bits then they can match full precision.\n\nEDIT: raised score from 3 to 5 after rebuttal.",
            "strength_and_weaknesses": "Strengths:\n - Appears to be the first paper to quantize params, gradients, and optimizer state jointly\n \nWeaknesses\n - Experiment section is weak. ImageNet is no longer a SOTA benchmark and ResNet is an 8-year old model that is known to be easy to quantize and train. Experiments focus only on SGD with momentum, but currently ADAM is necessary for Transformers.\n - The techniques applied are well known. For example, the Deepmind Gopher paper [1] used stochastic rounding during parameter updates. Microbatching is already implemented as a toggle option in HuggingFace and Megatron Transformer frameworks. These techniques are not really used in a novel or insightful way.\n - The technique seems difficult to implement and may incur run time overhead. The authors had to rely on non-linear quantization for the momentum, which is quite complex. Unbiased stochastic quantization requires generating random numbers and comparing them to a high-precision rounding probability for each element. These two issues may cause run time overhead on GPU and present difficulties for dedicated hardware. I would like the authors to discuss the actual memory saved on GPU (not just bitwidth) and whether LPMM reduces wall-clock time for training.\n\n[1] https://arxiv.org/pdf/2112.11446.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is well-written and clear.\n\nOriginality: the work mostly applies well-known quantization techniques\n\nReproducibility: there should be sufficient detail to reproduce the results.",
            "summary_of_the_review": "The paper applies stochastic quantization and microbatching to quantize parameters, gradients, and optimizer state. These techniques are well known in DNN quantization and thus the paper is a bit lacking in novelty. The experiment section is weak with only results for ResNets on ImageNet. Lastly, the techniques may be difficult to implement and/or incur run time overhead, which is currently not discussed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5887/Reviewer_UjMp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5887/Reviewer_UjMp"
        ]
    },
    {
        "id": "d0acpNReGzF",
        "original": null,
        "number": 4,
        "cdate": 1666915146173,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666915146173,
        "tmdate": 1666915146173,
        "tddate": null,
        "forum": "cs3n00FQ7OI",
        "replyto": "cs3n00FQ7OI",
        "invitation": "ICLR.cc/2023/Conference/Paper5887/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes low-precision model memory (LPMM) as a quantization training concept. In LPMM, all parameters, including weights, activation, gradient, and momentums are quantized to low-bit, which achieves theoretical low-bit training that saves significant memory footprints on the hardware. ",
            "strength_and_weaknesses": "Strengths\n\n1. This paper proposes the low-bit version SGD optimizer, which is a good approach towards low-bit training on the device that support low precision computation.\n\n2. The batch size experiments in figure 3 is very interesting. The author also provide convergence analysis on the training algorithm.\n\n3. The paper organization is clear, which makes it easy to read. \n\n\nWeaknesses\n\n1. There are many existing work on all low-bit training. And the author didn\u2019t compare the proposed method to them. Such as WAGEUBN (Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers), DoReFa-Net (Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients), FXPNet (Fxpnet: Training a deep convolutional neural network in fixed-point representation), GradScale (Ultra-Low Precision 4-bit Training of Deep Neural Networks) and (Towards Unified INT8 Training for Convolutional Neural Network). They are very important research on all low-bit training and has been published in top conference/journals. Although some of them are cited in related works, but the author didn\u2019t discuss nor compare with them.\n\n2. Because there are many similar works, then the novelty of this work is not that significant. And I don\u2019t see the author discuss about the problems of existing works, which makes the motivation of this work relatively weak.\n\n3. One important part is missing, which is how batch-norm layers are used. During training, batch-norm is keeping the training, but it will change the low-bit number into fp number. Could  the author discuss this problem? \n  \n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to strength and weaknesses",
            "summary_of_the_review": "To sum up, the clarity and quality of this paper need to be improved. The author of the paper did some interesting works on model quantization but fails to demonstrate them with thorough discussion and experiments. Please refer to strengths and weaknesses for more information.\n\nI think this paper needs major revise, both on the technical contribution and experiments. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5887/Reviewer_wG9u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5887/Reviewer_wG9u"
        ]
    }
]