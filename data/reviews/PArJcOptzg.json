[
    {
        "id": "asc4FbHO3q",
        "original": null,
        "number": 1,
        "cdate": 1665975171913,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665975171913,
        "tmdate": 1665975171913,
        "tddate": null,
        "forum": "PArJcOptzg",
        "replyto": "PArJcOptzg",
        "invitation": "ICLR.cc/2023/Conference/Paper3686/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors discuss how to defend against adversarial transfer attacks when the model structure is leaked. Considering that weight encryption techniques are relatively mature and strong, the authors consider the threat model that an attacker can exactly extract the neural architecture of deployed models on a device but not the weight parameters. Therefore, the author proposes DeepGuiser converts the trained model to a functionally-equivalent \u201cdeploy model\u201d with a disguising architecture in a hardware-agnostic and post-training way, which can be applied no matter what hardware platform is used and incorporates no retraining or fine tuning of the trained model. According to experimental results, DeepGuiser can effectively impede adversarial transfer attacks.",
            "strength_and_weaknesses": "Strength:\n1. It is a potential idea to camouflage the neural network structure to reduce the success rate of the transfer attack.\n2. When disguising the neural network structure, the clean accuracy remains almost unchanged.\n\nWeaknesses:\n1. The paper says to defend against transfer attacks, but it does not compare state-of-the-art migration attacks, such as MI-FGSM [1], TI-FGSM [2], PI-FGSM [3] and VMI-FGSM [4]. Both AutoAttack and C&W are common white-box attacks.\n2. The assumptions of this scenario are too strict and may limit the generalizability of the method.\n3. In this setting, the defense performance of the method appears to be inferior to the adversarially trained model. The latter not only defends against transfer attacks, but is also effective against white-box attacks.\n4. The experimental model (ResNet20) is not common enough, can you consider ResNet50 or Wide-ResNet?\n\n[1] Dong, Yinpeng, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. \"Boosting adversarial attacks with momentum.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9185-9193. 2018.\n[2] Dong, Yinpeng, Tianyu Pang, Hang Su, and Jun Zhu. \"Evading defenses to transferable adversarial examples by translation-invariant attacks.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4312-4321. 2019.\n[3] Gao, Lianli, Qilong Zhang, Jingkuan Song, Xianglong Liu, and Heng Tao Shen. \"Patch-wise attack for fooling deep neural network.\" In European Conference on Computer Vision, pp. 307-322. Springer, Cham, 2020.\n[4] Wang, Xiaosen, and Kun He. \"Enhancing the transferability of adversarial attacks through variance tuning.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1924-1933. 2021.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The expression of this paper is relatively clear and the method has some novelties, It should be able to reproduce well.",
            "summary_of_the_review": "We tend to reject this paper because of its more restrictive assumptions and lack of more convincing experiments. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3686/Reviewer_Tj1c"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3686/Reviewer_Tj1c"
        ]
    },
    {
        "id": "IbS4xdMZYL1",
        "original": null,
        "number": 2,
        "cdate": 1666053555769,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666053555769,
        "tmdate": 1669259983163,
        "tddate": null,
        "forum": "PArJcOptzg",
        "replyto": "PArJcOptzg",
        "invitation": "ICLR.cc/2023/Conference/Paper3686/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a defense against neural architecture extraction attacks, which would effectively hurt the model through transfer attacks. The presented method seeks a replacement architecture that has the same operation results as the original model but with low adversarial transferability, which is estimated by a transferability predictor with policy gradient optimization. The TransAdvBench used to pre-train the predictor also constructs a new benchmark.",
            "strength_and_weaknesses": "Strength\n+ The idea to deploy a different architecture with the exactly same operation results is novel, and naturally hardware-agnostic and retrain-free.\n+ The method to minimize the adversarial transferability is technically sound with careful design.\n+ The TransAdvBench contributes to the community in research of attack transferability.\n+ Solid experiments validate the superiority of the proposed method in mitigating transfer attacks.\n+ The paper is well-motivated and easy to follow with available codes.\n\nWeakness\n+ The architecture is not the only factor affecting adversarial transferability. Although it is an important factor, the author needs to answer whether \u201cthe same pair of architecture with different parameters have similar transferability\u201d. If modifying parameters (but maintaining the architecture) could greatly change the ranking of TransAdvBench, then its significance would be questioned.\n+ The proposed method largely increases the inference cost. Although it is understandable that great changes in the architecture tend to decrease the transferability more, transfer attacks via architecture extraction attacks may not be that popular in real-world scenarios. A better choice is to consider the model size or FLOPs when searching the architecture, enabling the deployer to trade-off.\n+ Does the Acc_clean in Tables 1 & 3 stand for clean test accuracy? If so, why is lower accuracy better, and why does a CIFAR-10 baseline have only 91.5 accuracy (which should be around 95, the same problem exists in CIFAR-100 and Tiny-ImageNet)? If not, what is it? Also, the related bolding is confusing and incorrect.\n",
            "clarity,_quality,_novelty_and_reproducibility": "See Strength And Weaknesses.",
            "summary_of_the_review": "It is a good and original paper on motivation and design. But the experiments have some issues needed to be addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3686/Reviewer_PPSy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3686/Reviewer_PPSy"
        ]
    },
    {
        "id": "9Oo1DlzVDw",
        "original": null,
        "number": 3,
        "cdate": 1666576222519,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576222519,
        "tmdate": 1666576222519,
        "tddate": null,
        "forum": "PArJcOptzg",
        "replyto": "PArJcOptzg",
        "invitation": "ICLR.cc/2023/Conference/Paper3686/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a disguise architecture method to search for a better functionally-equivalent architecture that leads to a lower transferability attack success rate even if the model is leaked through side-channel attack. To solve the disguise architecture search problem, a reward function is built for evaluating network architectures, and a policy learning method is proposed to solve the architecture search based on the reward. Expeirments are conducted on cifar-10, cifar-100, and tiny-imagenet.",
            "strength_and_weaknesses": "Strengths\n\n-[motivation] the topic is of significance in terms of AI security. the disadvantages of existing methods are clearly discussed.\n\n-[design] the design of searching functionally equivalent architectures to defend against side-channel attacks is interesting.\n\n-[experiments] the proposed induces less overhead compared to previous methods.\n\nWeaknesses\n\n-[clarity, major] I find Eq.3 confusing. Since A and B are functional equivalents, the output of A with an adversarial example created from B should produce exactly the same output, making the same error if there is any. Since changing different disguises would not change the fact that A/B are functionally equivalent, the loss degenerates into a constant (the function output cannot be changed in the disguise space). And, how is the problem formulation in section 4.1 relevant to side-channel attacks?\n\n-[clarity, minor] for functional equivalent operations such as fig2.(b) (c), are the padded zeroes frozen during the training process? If not, the disguise cannot be claimed as functionally equivalent.\n\n-[evaluation, major] The evaluation of disguise architecture is not very straightforward. The evaluation is not directly based on neural network architecture extraction, but is based on the transferability attack after the extraction. Since the disguise model is functionally equivalent, an example created using model B should lead to the same output in model A. But in the experiments, the models are not equivalent due to re-initialized parameters, which makes the models not equivalent to each other. Then, that means the effectiveness of the proposed method is tightly entangled with randomly initialized model parameters. We don't know whether the transfer attack success rate stems from such parameter differences -- because (emphasize again) when A and B are the same mapping, they produce the same result and will not make a difference. In that case, does that mean what the model has learned is to some extent the differences in different parameter initializations instead of good disguise architecture choice? What if we fix the model parameter and convert it to a series of mathematically equivalent functions? The claim/practice inconsistency makes the evaluation confusing and not convincing. Apart from that, is the proposed defense really effective against side-channel attack itself?\n\n-[suggestion] it is suggested to discuss adaptive attack where the attacker has full knowledge of the disguise algorithm.",
            "clarity,_quality,_novelty_and_reproducibility": "-[clarity] the task in this paper is clearly stated (disguise attacks through side channel). The task is of significance in terms of AI security.\n\n-[clarity] fig2 (b) and (c) are clear examples on functionally equivalent replacement of an operation. This helps the reader better understand the section 4.1.\n\n-[quality] important relevant works are comprehensively reviewed in section 2.\n\n-[novelty] the idea of finding functional equivalent to defend against side-channel attack is novel. I think this method would work, but the evaulations are not confusing since they are indirect evaluation, and the side-channel attack evaluation is left unrevealed.\n\n-[reproducibility] there is python code.",
            "summary_of_the_review": "\nThis paper aims to defend against side-channel attack, but the evaluation is based on the transferability attack, with the side-channel attack effectiveness hidden within the transferability attack, which makes the evaluation obscure. Although the idea of finding a functional equivalent is very intersting, the inconsistency where the functional equivalence is actually not guaranteed in the experiments further make the method and evaluation confusing and less convincing -- it's is hard to attribute the effectiveness to the disguise architecture or parameter difference. There is still a room for improvement in terms of clarity and evaluation. I'd recommend weak reject currently.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3686/Reviewer_moEm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3686/Reviewer_moEm"
        ]
    },
    {
        "id": "TIniiH7FEsY",
        "original": null,
        "number": 4,
        "cdate": 1667253910530,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667253910530,
        "tmdate": 1667253910530,
        "tddate": null,
        "forum": "PArJcOptzg",
        "replyto": "PArJcOptzg",
        "invitation": "ICLR.cc/2023/Conference/Paper3686/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a model disguising method. The key motivation is that: when the model architecture is obtained by the attacker, they can rebuild the weight parameters through a transfer learning attack. Then the attacker will generate adversarial examples exploiting the rebuilt model.\n\n By smartly adding some redundant operations (through RL), the paper can hide the model architectures while increasing the adversarial accuracy when being attacked.\n",
            "strength_and_weaknesses": " 1. Strength:\n- The problem is new and interesting.\n\n2. Weakness:\n- The overhead is huge. I am not sure if the method is applicable to other types of models (discussed in the questions).\n- The solutions are not new. Predictor+RL are very standard methods in NAS. Predicting adversarial accuracy is hard. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Above average.\nQuality: Above average. \nNovelty: Discussing a new problem but the solution is not new.\nReproducibility: Not sure.",
            "summary_of_the_review": "1) You claim that your disguised models are quality neutral in terms of model accuracy against the original models. Is there any evaluation to prove it?\n\n2) If the attacker is aware of this protection method, they can simply conduct random pruning (or NAS) to find the DNN architecture in the reduced search space. \n\n3) Also, in my opinion, this method can only be applied to NASNet or similar models. I don't think it is applicable to recent advances like EfficientNet or ViT. That is because these layers models are wired in sequence, the attacker would simply remove your disguised modules \n before starting the transfer learning.\n\nOverall, I think the paper proposes a very interesting idea. We need to have two versions of our models; the original model and another disguised model for deployment purposes.  However, the paper does not dig into the problem enough. There are many easy ways to neutralize this defense mechanism. At least, none of them are discussed. \n\nMinor: Acc_clean is not explained anywhere. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No Ethics Concerns",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3686/Reviewer_DYHG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3686/Reviewer_DYHG"
        ]
    }
]