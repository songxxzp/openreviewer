[
    {
        "id": "OlOT0Q130b7",
        "original": null,
        "number": 1,
        "cdate": 1666247035798,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666247035798,
        "tmdate": 1666247035798,
        "tddate": null,
        "forum": "xE-LtsE-xx",
        "replyto": "xE-LtsE-xx",
        "invitation": "ICLR.cc/2023/Conference/Paper3027/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a novel approach based on two transformers to reconstruct NeRF. The view transformer leverages multi-view geometry (epipolar geometry) to guide the attention-base scene representation. Next, the ray transformer renders novel views using attention to decode from the representation learned by the view transformer. The proposed GNT outperforms other state-of-the-art methods in both single-scene and cross-scene scenarios.",
            "strength_and_weaknesses": "Strength\n1. View transformer enables the learning of a neural representation generalized across scenes.\n2. Replacing explicit volume rendering formula with a learned ray transformer. This might contribute to better render refractions and reflection.\n3. Outperforming other state-of-the-art methods in both single-scene and cross-scene scenarios.\n\nWeakness\n1. GNT includes two transformers. It is not clear how stable is the training with respect to random seed or changing some hyperparameters.\n2. Some clear artifact at the bottom parts in the Horns example.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written with good motivation and clear descriptions of methods and experiments.\nDetails are included in the implementation subsection, with a good chance for the method to be reproduced.\n",
            "summary_of_the_review": "This is one of the first papers using the attention mechanism in most components for the task of NeRF.\nTwo novel transformers are introduced to learn a neural representation generalized across scenes and better render refractions and reflection. Finally, The proposed GNT outperforms other state-of-the-art methods in single-scene and cross-scene scenarios.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3027/Reviewer_xoKn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3027/Reviewer_xoKn"
        ]
    },
    {
        "id": "kL85uhGGW5F",
        "original": null,
        "number": 2,
        "cdate": 1666589524402,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589524402,
        "tmdate": 1666589524402,
        "tddate": null,
        "forum": "xE-LtsE-xx",
        "replyto": "xE-LtsE-xx",
        "invitation": "ICLR.cc/2023/Conference/Paper3027/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents the first NeRF reconstruction method, Generalizable NeRF Transformer (GNT), based on transformer. It introduces the view transformer which predicts coordinate-aligned features by aggregating information from epipolar lines on the neighboring views with attention and the ray transformer which renders novel views using attention to decode the features from the view transformer along the sampled points. Experimental results show that the GNT can successful reconstruct a NeRF model with more details.",
            "strength_and_weaknesses": "Strength: This is, to my knowledge, the first transformer based NeRF reconstruction method. While the proposed method is not groundbreaking, it is a reasonable design and an advancement to study how NeRF reconstruction can be achieved by transformer.\n\nWeakness: I would expect more ablation study and comparisons of the proposed GNT with some more recent methods for NeRF reconstruction not limited to the vanilla NeRF and the MipNeRF. Indeed, there are many recent advancement in NeRF for speeding up the reconstruction, or use partial depth, or spherical harmonic coefficients for NeRF reconstruction. These methods are not referenced nor compared in the paper. Such addition references, comparisons and ablation study should be able to further strengthen the contribution of this work. In the current submission, this paper just read like this is another alternative for NeRF reconstruction, but the advantages of GNT over other recent NeRF reconstruction methods are not obvious.",
            "clarity,_quality,_novelty_and_reproducibility": "Paper writing is clear, and the design of view transformer and ray transformer are novel.",
            "summary_of_the_review": "I am happy with this submission and I think the technical novelty of this work is sufficient to be accepted in ICLR. At the same time, I feel that this paper can be even better with further ablation study and comparisons (check my comments in weakness).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3027/Reviewer_TeHV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3027/Reviewer_TeHV"
        ]
    },
    {
        "id": "PLxjUF8EdtD",
        "original": null,
        "number": 3,
        "cdate": 1666690172122,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690172122,
        "tmdate": 1670117191149,
        "tddate": null,
        "forum": "xE-LtsE-xx",
        "replyto": "xE-LtsE-xx",
        "invitation": "ICLR.cc/2023/Conference/Paper3027/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a neural architecture GNT that uses transformers for novel view synthesis from multi-view images. Specifically, a view transformer aggregates feature from feature maps of input images to obtain features of sampled 3D points in the scene. Then a ray transformer learns to aggregate features of points along each ray and decode code from them. Experiments have shown that GNT can achieve comparable results for single-scene overfitting settings. And achieve better results on cross-scene generalization settings compared with other baselines.",
            "strength_and_weaknesses": "+ The paper demonstrates that pure transformer architecture with geometry prior can learn to perform the task of novel view synthesis.\n+ The model has a better performance compared with other baselines in the cross-scene generalization settings.\n\n- My major concern about the paper is the lack of technical novelty compared with previous works. There are a number of works that leverage transformers to learn the pipeline of volume / light field rendering such as [1][2][3][4]. Specifically, NeRFormer [1] also uses the transformer for aggregating features of every 3d point from feature maps of multi-view images using epipolar constraints. GNT simply replaces the volumetric rendering part with a transformer, which introduces limited novelty.\n- The benefits of the proposed architecture is not empirically convincing. GNT performs worse than NLF[4] in the single-scene settings. On cross-scene settings, the improvement is also very marginal. I would like to know the computing cost (training time inference time) of GNT compared with GNT with volumetric rendering ablation.\n- The authors claim that the GNT can better handle challenging view-dependent effects. However, no quantitative results (e.g. on the Shiny dataset) can support this point. And in Figure 4, results of the strongest baseline NLF is not shown. This makes it hard to evaluate the effectiveness of GNT for view-dependent effects.\n\n[1] Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction. ICCV 2021.\n[2] Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. CVPR 2022.\n[3] IBRNet: Learning Multi-View Image-Based Rendering. CVPR 2021.\n[4] Light Field Neural Rendering. CVPR 2022.\n\nA few questions:\n1. In Table 3, why the Epipolar Agg. \u2192 View Agg variant is significantly worse than NLF when they share a similar two-stage aggregation strategy?\n2. What is the training/inference time of GNT in single-scene and cross-scene settings?\n\nMinor:\n- Section 4.3 Datasets section, \u201cIBRNetto\u201d -> \u201cIBRNet to\u201d\n- In Figure 2, symbols of input should have explanations. Otherwise it is very hard to understand the figure.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall writing is Okay. Figure 2 is confusing because of lack of explanation. The originality of the paper is limited.",
            "summary_of_the_review": "Overall, the submission presents marginal improvements over existing method. However, the main concern is the lack of technical contribution, and that the empirical evaluation is not convincing.\n\n---------\nUPDATED: after rebuttal I think my concerns have been partly addressed. I changed my score to weak accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3027/Reviewer_qn8f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3027/Reviewer_qn8f"
        ]
    },
    {
        "id": "XcPQfXdpOL",
        "original": null,
        "number": 4,
        "cdate": 1667575085049,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667575085049,
        "tmdate": 1669993133896,
        "tddate": null,
        "forum": "xE-LtsE-xx",
        "replyto": "xE-LtsE-xx",
        "invitation": "ICLR.cc/2023/Conference/Paper3027/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": " This paper addresses the problem of generalizable Nerf using a view transformer for generalization and a ray transformer for ray-based rendering.  For generalization, the view transformer uses multi-view geometry inductive bias to predict coordinate aligned features by aggregating information along the epipolar lines on other views. For rendering, the ray transformer renders novel views using features from the view transformer along the ray points during ray marching. Such designs allow authors to generalize the Nerf in the previously unseen scenes.",
            "strength_and_weaknesses": "++ The addressed problem in this paper is a problem of high interest.\n\n++ The proposed method is intuitive and meaningful.\n\n++ The paper is well written and easy to follow.\n\n++ The provided experimental results are comparable, and in fact exciting.\n\n++ The experimental evaluations provided in the supplementary material further illustrate the effectiveness of the proposed method.\n\n-- I do not find any major problem with the proposed method. However, a discussion regarding the failure cases and limitations is missing. The paper can benefit from demonstrating the examples where the proposed method lacks to generalize. ",
            "clarity,_quality,_novelty_and_reproducibility": "Please, refer to the strengths above.",
            "summary_of_the_review": "Please, refer to the weakness regarding the failure cases and limitation of the proposed method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerning ethical issue as far as can be seen.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3027/Reviewer_i8zE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3027/Reviewer_i8zE"
        ]
    }
]