[
    {
        "id": "HtblfnMdN0",
        "original": null,
        "number": 1,
        "cdate": 1666588803549,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588803549,
        "tmdate": 1666588803549,
        "tddate": null,
        "forum": "eG14tR9lssZ",
        "replyto": "eG14tR9lssZ",
        "invitation": "ICLR.cc/2023/Conference/Paper2003/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studied data poisoning attacks against online reinforcement learning, where an attacker perturbs the reward signal over time to mislead the victim agent into learning a sub-optimal policy. The author provided a generic mathematical formulation of their poisoning attacks (equation (1)), which has three important parameters, including the poisoned policy value V, the total number of poisoning steps C, and the per-step maximum perturbation on the reward signal C. Instead of analyzing the feasibility of attack with respect to V, C, and B, the authors proposed to design attack algorithms and then analyze the corresponding V, C, B that the attack algorithm can induce. In total, the paper proposed four attack algorithms. For each algorithm, the authors analyzed an upper bound on the difference between the perturbed reward function and the original reward function as a function of the desired policy value V. Given this upper bound, the corresponding B can be easily derived. To complement the theoretical analysis, the paper performed empirical study on four deep RL task and several classic RL algorithms, and demonstrated that the proposed attacks can significantly reduce the performance of the best learned RL policy.",
            "strength_and_weaknesses": "Strength:\n\n(1). This paper studied an important topic that lies in the intersection area of reinforcement learning and security, which has important implications in uncovering vulnerability of real-world RL systems and how to design effective defense against attacks.\n\n(2). The paper provided interesting and strong theoretical analysis on the theoretical properties of the proposed attack algorithms.\n\n(3). The paper has solid experiments, and the results clearly demonstrate that the proposed attack algorithms are capable to reducing the performance of the learned RL policy.\n\nWeaknesses:\n\n(1). The presentation of the theoretical results is very confusing.The authors mentioned lower bounds on B, C, and V. However, I am very confused what lower bounds actually mean for equation (1). Does it mean B, C, V has to be at least some number so that equation (1) can be feasible? If so, I think the lower bounds would not make sense because regardless of how small B, C, and V are, one cannot rule out the possibility that there exists some smart and super efficient attacker who can easily corrupt the performance of the learned RL policy with little attack budget. I guess by lower bounds, the authors really mean that B, C, and V, although being as small as some value, the attack can still be successful. I believe the authors really need to rethink how to present their theoretical results in a more clear and succinct manner. Right now it's very confusing and hard to understand.\n\n(2). The attack formulation is not aligned with the attacker ability. The authors mentioned that there is an upper limit E on the per-episode corruption, but this parameter E does not appear in equation (1).\n\n(3). While the authors claimed that the attack does not require knowledge of the underlying environment. In LPE attack, the attacker really needs to first estimate some good policy pi dagger beforehand. This actually requires access to some simulator of the underlying environment. That means, the attack proposed in this paper is not fully black box. The authors need to clarify that in the paper.\n\n(4). My major concern is about the novelty and the significance of contribution. There has been plenty of prior works that studied poisoning attacks on RL, including online RL. The attack of changing the underlying reward function has been investigated in [1] below. Also there was a prior work on attacking online RL [2]. I find it hard to justify how much technical contributions are really significant compared to prior works. Also in the experiment, this paper does not compare their attack algorithms against [2], which is very related work. I think the authors may want to spend more effort discussing how this paper advanced the state-of-the-art attacks against RL and what is really unique about the theory and method of this paper.\n\nAmin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching via environment poisoning: Training-time adversarial attacks against reinforcement learning. In International Conference on Machine Learning, pp. 7974\u20137984. PMLR, 2020.\n\nXuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks against reinforcement learning. In International Conference on Machine Learning, pp. 11225\u2013 11234. PMLR, 2020b.",
            "clarity,_quality,_novelty_and_reproducibility": "The theoretical results are not presented in a very clear manner, and needs to be improved.\n\nI am not fully convinced that the paper is technically novel enough.",
            "summary_of_the_review": "I work on related areas.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2003/Reviewer_VFr8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2003/Reviewer_VFr8"
        ]
    },
    {
        "id": "TTPaHgvmiL",
        "original": null,
        "number": 2,
        "cdate": 1666641518738,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641518738,
        "tmdate": 1669281815299,
        "tddate": null,
        "forum": "eG14tR9lssZ",
        "replyto": "eG14tR9lssZ",
        "invitation": "ICLR.cc/2023/Conference/Paper2003/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider poisoning attacks on RL agents (i.e. adversarially modify the training data to harm performance) in a setting where the poisoner can only corrupt the reward, and with an ability to do this that is budget-limited in various ways. The authors frame this problem as the goal of finding a new MDP (by changing the reward function only) whose optimal policy will be the one desired by the attacker. Although this is computationally hard, the authors propose some good heuristics for solving it, and empirically experiment with them.",
            "strength_and_weaknesses": "Strengths: the proposed methods for coming up with attacks are intuitive yet clever, and the general idea and empirical evaluation seem convincing.\n\nWeaknesses: the most obvious weakness is that it seems totally unsurprising that, if an adversary is able to change the rewards in an MDP, they should have a lot of power to make the agent learn the wrong policy. ~~On the one hand, this makes it sort of a less interesting example of an adversarial attack (it's like corrupting labels would be for supervised learning).~~ On the other hand, maybe it's actually more realistic because adding some adversarial noise to the reward signal might be very possible in some circumstances.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: there are no problems with clarity, although it's also not an example of great presentation.\n\n- Quality: quality seems fine. In some sense it should be very non-surprising that if an adversary can corrupt reward in an MDP, then they can make the policy learn the wrong thing.\n\n- Novelty: adversarial attacks in RL are not a hugely explored area, and this work seems to take a novel perspective on it.\n\n- Reproducibility: enough information seems to be given to reproduce the results. A quick review of the code in the supplementary material seems reasonable.",
            "summary_of_the_review": "Overall, the paper seems fine; the ideas in it are argued and presented reasonably but don't seem very surprising or profound. As such, I think it seems like a fine paper but probably not enough to get accepted to ICLR. Because I do not know the area well, my confidence is low and I could be persuaded in either direction.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2003/Reviewer_de9h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2003/Reviewer_de9h"
        ]
    },
    {
        "id": "tu7fjvLxKnI",
        "original": null,
        "number": 3,
        "cdate": 1666671650953,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671650953,
        "tmdate": 1666671799190,
        "tddate": null,
        "forum": "eG14tR9lssZ",
        "replyto": "eG14tR9lssZ",
        "invitation": "ICLR.cc/2023/Conference/Paper2003/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The work designed a set of black-box adversarial attacks to corrupt a small proportion of training time rewards, and make the agent learn a low-performing policy. The goal and contribution of this work is on efficient poisoning attacks on DRL via reward poisoning, assuming the attacker has no knowledge of the exact DRL algorithm and does not have detailed knowledge about the agent's environment, and the attacker has limits on the amount of the reward corruption it can apply. The formulation of the problem is to find a training time reward perturbation such that the resultant learned policy has a bad performance, and also under the limit of the times of reward corruption and magnitude of reward corruption. However, realizing that directly solving the optimization problem is computationally infeasible. The work proposes to evaluate the limit of any reward poisoning attack algorithm on its attack performance, limits on reward perturbation times and magnitude. The trade-off between limits on reward perturbation times and magnitude as well as resultant policy value affects the number of available adversarial MDP that satisfies the constraints. The work then proposes several practical adversarial attacks on rewards. Uniformly random time attack as a baseline, where the attacker randomly perturb reward with a fixed probability and fixed amount. Learned policy evasion attack: the attacker penalizes the learner whenever it chooses a good action that are shared between several good policies. Random policy inducing attack: a fixed reward perturbation is added whenever the agent behaves differently from a random policy. Random policy promoting attack: rewards positively when the agent chooses actions the same as a random policy. Experiment study on these attacks for DQN, Double DQN, DDPG, TD3, SAC, PPO are presented to demonstrate the performance of these attacks.  ",
            "strength_and_weaknesses": "The work presents an interesting study on reward poisoning attacks where the attacker does not know the training algorithm of the victim agent. The main novelty is that it does not require knowing the victim agent's policy training details, in particular the training algorithm and related hyper parameters, while other previous works may require this. However, there are several weakness of the work. \n\nFirst, some descriptions over-claims the contribution of the work. For example, the author claimed the attacker does not know the training algorithm of the victim agent, however, in the experiment section, the work only evaluates the case where the attacker and the victim agent use different training algorithms, without evaluating the case when they use the same algorithm. The author also claims the attacker knows no detailed knowledge of the agent's environment, but can observe the full observation, actions, rewards generated during training at each time step, and these information already leaks more than enough information of the training environment, and the attacker may use this knowledge to perform other kinds of adversarial attack, even when the attacker does not know the training algorithm of the victim agent. Similarly, in the experiment section, the attacker has access to environment to learn attack policy offline, which means the attacker has access to the environment, and this is not consistent with the claim that the attacker does not have detailed knowledge of the environment. \n\nSecond, the organization and/or thought flow of the paper is not carefully designed. The author first proposes a very general optimization problem to solve to find the feasible attacks, but then transits to discuss several weakly-related attacks. For example, the uniformly random time attack can serve as a good baseline to attack the victim with randomly proposed reward perturbation, however, it is not clear why the author chooses to talk about learned policy evasion attack as the immediate next attack. For example, the paper can talk about the limitation of UR attack, and then propose LPE attack that addresses part of the limitations. Similarly, why the RPI attack is proposed following LPE attack, and so on. \n\nThird, the experiments are limited and not thoroughly discussed. The paper spends the majority of its pages discussing the theoretical insights of the different methods, however, does not discuss intuitively the pros and cons of the various proposed algorithms. This making it unclear why these methods are proposed and under what circumstances should the attacker use what algorithm. If one algorithm strongly outperforms the other three, why proposing the other algorithms, etc. The data in figure 1 is also not discussed well, and ideally should have error bar. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is clear about its goal but less clear about its thought flow. For example, why the authors propose the four different attacks, and the pros and cons of these methods. The figure 1 is also not very clear in terms of what the authors want to imply and what conclusion should we draw. It's clear from the figure that all the attacks are successful to bring the performance down, and the case without attack has the best policy return. However, the used markers to represent different classes are hard to distinguish (e.g. LPE2 and LPE3), and the points need to have error bars, and the fonts need to be larger to be readable.\n\nQuality: the presented algorithms are working well, but are not discussed thoroughly, both in terms of the algorithms themselves and in terms of the results on these algorithms. The authors did say algorithm 1 is better/worse than algorithm 2, but did not provide convincing explanations. \n\nNovelty: since the claims in the paper are kind of over-claiming their contributions, it is not clear how much novelty the work has.\n\nReproducibility: code is provided, not sure if the results would match because we don't have the error bars in the figure. Also, the y-axis in figure 1 says policy value, is that the total episodic rewards or really the policy value? Usually the value is associated with a starting state, what is the starting state?",
            "summary_of_the_review": "To sum, this work presents a relatively novel set of attack methods on reward poisoning for training time attack on DRL. The contribution of the work is limited and unclear. The experiments require a lot of training and work, but can still be improved and probably should have more space in the paper. Overall, this is an interesting work with a good potential but probably needs more work to improve the writing and experiments. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2003/Reviewer_oGPF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2003/Reviewer_oGPF"
        ]
    }
]