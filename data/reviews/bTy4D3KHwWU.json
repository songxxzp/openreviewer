[
    {
        "id": "DzdO-TimGf",
        "original": null,
        "number": 1,
        "cdate": 1666675262270,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675262270,
        "tmdate": 1666675262270,
        "tddate": null,
        "forum": "bTy4D3KHwWU",
        "replyto": "bTy4D3KHwWU",
        "invitation": "ICLR.cc/2023/Conference/Paper2714/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers an interesting problem in interventional causal inference, which is estimating the distribution of a potential outcome corresponding to an intervention. It proposes a fully parametric deep learning model for this task called Interventional Normalizing Flow that uses 2 flows. \n\nThe teacher flow computes the propensity score $\\pi\\_{a}(X)$ from $X$ and parameters of a conditional normalizing flow network (CNF) from $(X, A)$. This conditional flow network maps Gaussian noise to $P(Y|X, A)$.\n\nThe student flow is a set of unconditional normalizing flows for each intervention $A=a$. Each unconditional flow maps Gaussian noise to $g(Y, \\beta\\_a)$ where $\\beta\\_a$ is the set of parameters corresponds to $A=a$. Each unconditional flow w.r.t. $A=a$ is learned by mimicking $P(Y|do(A=a))$ obtained from the teacher flow. This problem is transformed into an equivalent problem of solving moment conditions. This paper also introduces an (optional) one-step bias correction for doubly robust estimation of $\\beta\\_a$ by leveraging the propensity score prediction from the teacher flow.\n",
            "strength_and_weaknesses": "### Strengths\n1) The problem considered in the paper is interesting and the method is novel.\n2) The paper is well-written with a good discussion of related works especially those related to interventional density estimation and normalizing flows for causal inference (Appendix A). I also appreciate the authors' attempt to make things clear for the readers by using colors in their formulas.\n3) I think the experiment is quite extensive with various baselines for density estimation. The authors also compare different variants of their method.\n\n### Weaknesses\n1) The main weakness is the complexity of this method as in my opinion, it can be much simpler. For example, the student flow seems redundant as it just approximates $P(Y|do(A=a))$ though I know the authors want to avoid taking an average over $X$ during testing. In the ablation studies, the authors also consider a variant of the proposed method without the student flow, and when looking at the results in Table 2, I saw that this variant is not too bad compared to the full model.\n2) Please correct me if I am wrong but I think we can use simple methods like Balancing Linear Regression [1], TARNet [2], DragonNet [3], \u2026 for interventional density estimation (IDE) problem in the paper. These methods are mainly designed for ITE estimation but we can easily modify their outputs to model $P(Y|X, A)$. Once we can model P(Y|X, A), we can simply estimate $P(Y|do(A)) = \\mathbb{E}_{p(X)}[P(Y|X, a)]$ via backdoor adjustment. I would like to hear the authors\u2019 opinions about this.\n3) The authors should compare their method with existing ITE estimation methods to support their argument in the paper saying that \u201cestimating the potential outcome density is better than just estimating the average potential outcome\u201c.\n4) In the TeacherFlow, I would like to understand why did the authors use $X, A$ to compute the weight $\\theta$ of the conditional normalizing flow? Why don\u2019t just model $P(Y|X, A)$ directly via a neural network with $X, A$ are inputs to this network? Since the distribution of outcomes $Y$ is usually simple. Do we really need a normalizing flow with hyper-network for its parameters?\n5) The authors should provide more insights and explanations about why each component is useful for the method to work. Currently, they only use one metric, which is the log-likelihood, for comparison, which I think is not enough.\n\n[1] Learning Representations for Counterfactual Inference, Johansson et al., ICML-2016\n\n[2] Estimating individual treatment effect: generalization bounds and algorithms, Shalit et al., ICML-2017\n\n[3] Adapting Neural Networks for the Estimation of Treatment Effects, Shi et al., NIPS-2019\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, has good quality and is novel. I am not sure about the reproducibility",
            "summary_of_the_review": "Overall, this is a good paper which proposes a novel method to address an interesting problem. The only problem is that the model is more complex than it should be. This can limit the practical applications of the work. I give this a paper a weak accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2714/Reviewer_quXx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2714/Reviewer_quXx"
        ]
    },
    {
        "id": "W6bLZk-6_-",
        "original": null,
        "number": 2,
        "cdate": 1666741939215,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666741939215,
        "tmdate": 1669831924459,
        "tddate": null,
        "forum": "bTy4D3KHwWU",
        "replyto": "bTy4D3KHwWU",
        "invitation": "ICLR.cc/2023/Conference/Paper2714/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces an optimization framework with a model consisting of a teacher and student normalizing flows for estimating the interventional distribution for the case of backdoor adjustment. The developed optimization uses one-step bias correction and utilizes influence functions for doubly robust estimation of the interventional density. Results on multiple synthetic and semi-synthetic datasets are reported showing that INFs outperform nonparametric and ablated methods.",
            "strength_and_weaknesses": "**Strengths**\n* The presented methods fill a gap in the literature on estimating interventional densities using parametric models. Therefore the problem considered is timely and significant.\n* The motivation is clear as the example in Fig. 1 is representative of why the problem considered is important.\n* I found the schematic in Fig. 2 very helpful, it provides a nice summary of the components of the model.\n* The presentation and the logical flow of the paper are clear, I specifically appreciated the use of colors in the text for determining what corresponds to nuisance parameters and what does not.\n* The background contains a nice summary of the existing literature for estimating interventional effects for the case of backdoor adjustment. The literature has focused on average treatment effect (ATE) and the non-ATE literature is mostly non-parametric or non-algorithmic.\n\n**Comments**\n* The problem of estimating $\\mathbb{E}_{X \\sim P(X)} \\big[ P(Y|X, A) \\big]$ given samples from $X, Y, A$ sounds like a well studied estimation problem in statistics regardless of the context of interventional distributions (here, the interventional or counterfactual data is not used and we are interested in the above estimation problem). To me, it is surprising that Normalizing Flows are not used for this purpose before so I cannot confirm the novelty of the methodology. Given that (conditional) normalizing flows, the theory of semi-parametric estimation of the interventional distribution, doubly robust estimation tools existed before the main contribution seems to lie in the addition of the student network, and framing and solving the optimization problem.\n\n* Given that the work is applicable only to the backdoor adjustment, even for the cases in which we can identify the effect of intervention we cannot use this method. However, the extension to the more general case sounds feasible. This would largely broaden the impact and scope of the paper. Can the authors discuss the possibility of this extension?\n\n* What happens if $Y$ is not one-dimensional? It seems that the method relies on creating a grid on $\\mathcal{Y}$. Is this applicable and does this scale to higher dimensional $Y$ variables?\n\n* Can the authors explain why in the case of the synthetic dataset in Fig. 3 KDE is doing very well (even better than the proposed method) for $a=0$ but fails for $a=1$? Where does this asymmetry come from?\n\n* Can the authors include the SCMs used for other examples in the supplementary? \n\n* I think the contributions of this paper by themselves are enough for the acceptance of the paper if these techniques are not used in the context of causal inference and interventional density estimation before. I am happy to change my score to acceptance if the authors include the above information.\n\n* Can the authors include the famous tobacco control example in the experiments?\n\n* To make this a standalone paper, I encourage the authors to include a short summary of the influence functions and doubly robust estimation. More specifically, to me, it wasn't quite clear how we go from Eq. 5 to 7, 8.\n\n\n**Post rebuttal**\n\nI thank the authors for clarifying their contributions and distinguishing between the original estimation theory and the semi-parametric giving rise to the optimization framework and the suggested student-teacher architecture. Now it's clear why the classical density estimation techniques aren't as powerful as semi-parametric methods and why finite-dimensional estimands are not applicable. I think the new additions (adding more background on semi-parametric theory and SCMs, including the SCMs associated with the experiments, adding results on the tobacco control experiment) make the paper more standalone and powerful. Hence I changed my score to acceptance.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear, some background on influence functions and doubly robust estimation can make the paper stronger. The theoretical framework of the presented method existed before, the main novelty lies in the algorithmic instantiation and the performance of the presented methods. The information in the paper sounds sufficient for the reproduction of the results, however, I did not try to reproduce the results myself. ",
            "summary_of_the_review": "The paper has good quality, including further information (summarized under comments) can make it stronger.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2714/Reviewer_uUEZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2714/Reviewer_uUEZ"
        ]
    },
    {
        "id": "pkHvXvaWAE",
        "original": null,
        "number": 3,
        "cdate": 1666860224171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666860224171,
        "tmdate": 1666860224171,
        "tddate": null,
        "forum": "bTy4D3KHwWU",
        "replyto": "bTy4D3KHwWU",
        "invitation": "ICLR.cc/2023/Conference/Paper2714/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a fully-parametric, deep-learning method for interventional density estimation, called Interventional Normalizing Flows (INFs). INFs provide a properly normalized density estimator. The authors further develop a two-step training procedure with a one-step bias correction for efficient and doubly robust estimation.",
            "strength_and_weaknesses": "To improve the readability, the authors may want to: \n1. Add more introduction to the structural causal model.\n2. In the example provided in Introduction Section,  explain interventional distributions, observational distributions, and counterfactual distributions. Explicitly define the notation Y [a].\n\n\nQuestion: Why does Table 3 compare % of runs with best performances, while other tables compare the log-probability? And why are the columns for Table 3 named log-prob?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors did extensive experiments and showed their methods could outperform other methods. But the authors can improve the introduction to audiences unfamiliar with causal inference and the normalizing flow method. ",
            "summary_of_the_review": "The method proposed in this work is novel. The authors estimate the density of potential outcomes after interventions from observational data and propose a fully-parametric deep learning method INF. The authors also provide extensive numerical studies to support the performance of INF. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2714/Reviewer_A7uk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2714/Reviewer_A7uk"
        ]
    }
]