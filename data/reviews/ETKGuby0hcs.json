[
    {
        "id": "BHRdOoutSPE",
        "original": null,
        "number": 1,
        "cdate": 1666410010977,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666410010977,
        "tmdate": 1668840977222,
        "tddate": null,
        "forum": "ETKGuby0hcs",
        "replyto": "ETKGuby0hcs",
        "invitation": "ICLR.cc/2023/Conference/Paper3080/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a novel approach to address a fundamental problem in supervised machine learning: misalignments exist between a training objective and the truth. Instead of telling the model \"what is true\" by human annotations, they proposed to learn the truth by the model itself in a purely unsupervised way. Specifically, they introduced Contrast-Consistent Search (CCS), a method that learns a linear projection of the hidden states that satisfies logical consistency (consistent across negations). Given a yes-no question, they first construct two instances with the binary answers concatenated to the question. They then train a classifier to predict the binary labels using the hidden states of a pretrained language model. To train the classifier, they add several constraints to insure logical consistency. For example, a statement and its negation should have probabilities that add up to 1. They conducted experiments using 6 different models and the empirical results on 10 question answering datasets showed they can improve the zero-shot performance by 4% compared to the baselines.",
            "strength_and_weaknesses": "Strengths: \n- The proposed approach is novel to my knowledge.\n- Strong empirical results across many datasets and models.\n- The problem this paper trying to tackle is fundamental \n\nWeaknesses:\n- My major concern about this paper is that the claims made are not well supported by the experiments.\n\t- In section 3.2.2, \"Recall our motivating goal: to discover latent knowledge in a language model even when its training objective causes the model to output false text.\" I don't see how this claim is related to the experiments in this section. The experiment is conducted in a zero-shot setting in which no training objectives are used. What they really do is use a set of contrastive prompts to mislead the model and the experimental results show that **CCS can improve the robustness of the model**. That's the takeaway I can conclude from the experiment.\n\t- In section 3.3.1, \"CCS FINDS A TASK-AGNOSTIC REPRESENTATION OF TRUTH\". I am not super convinced by this claim, as all of the models (without fine-tuning using the CCS objective) used for this experiment can already make zero-shot generalizations on those datasets to some extent. It is unclear to me what is added by the CCS objective. Does it just make the model more robust against domain shifts? Or it captures something else? Moreover, do you have a more formal definition of TRUTH here?\n\t- In section 3.3.2 \"CCS CAN FIND INTERMEDIATE REPRESENTATIONS OF TRUTH\". I don't quite get the logic behind this experiment. How can we conclude by looking at the results that lower layers perform worse? Can you elaborate on this a bit more?\n- The baseline seems a bit weak to me. How about you just fine-tune the models used in the paper using the original training objective on those datasets used for testing?\n- The \"purely unsupervised\" setting seems to be a bit overclaiming. In the experiments, they still need a non-trivial number of training examples to make the proposed approach work. It would make the paper much stronger if they could use examples from a more \"wild\" setting, e.g., sentences from a random corpus, to train CCS and show it could still work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: In general, this paper is well-written and organized. There are several parts that confuse me:\n- Why do you use just 1000 examples per dataset? Can you give a justification for this design choice?\n- In table 3, how do you add the contrastive prompts to the encoder-only and encoder-decoder models? To me, these models are not designed to work with prepended prompts.\n\nNovelty: To my knowledge, the approach is novel and intriguing and I liked the idea.\n\nQuality: The experiments are comprehensive and have extensive discussions about the results. However, I don't think all of the claims are well supported. See weaknesses for details.\n\nReproducibility: The results should be reproducible by using the code provided.",
            "summary_of_the_review": "In general, the proposed approach is novel and the empirical results seem strong. However, at the current stage, several claims are not well supported by the experiments and it is not super clear what the takeaways are. At the current stage, I lean toward rejection but I think improvements can be made to make it a very strong publication.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3080/Reviewer_32x3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3080/Reviewer_32x3"
        ]
    },
    {
        "id": "bq0oGA6O8h",
        "original": null,
        "number": 2,
        "cdate": 1666582193757,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582193757,
        "tmdate": 1666582193757,
        "tddate": null,
        "forum": "ETKGuby0hcs",
        "replyto": "ETKGuby0hcs",
        "invitation": "ICLR.cc/2023/Conference/Paper3080/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors introduce an unsupervised probe for knowledge in large language models to tackle a perceived challenge in verifying truthfulness of predictions/generated text. The authors' proposed method involves classifying/ranking model activations between binary labels/framings of a yes-no question. The authors demonstrate that their CCS probe is resilient to prompt-based attacks.",
            "strength_and_weaknesses": "Strengths:\n- The paper targets an important issue with language models: verifying truthfulness of outputs and whether knowledge is learned within the model parameters.\n- There is an interesting result in 3.2.2 for how CCS is robust to adversarial prompts. I would have liked to see more discussion on why this particular design of \"misleading prefix\" was chosen, however.\n- CCS is demonstrated to be usable with multiple models across a wide range of tasks.\n- Good discussion of limitations and possible future challenges.\n\nWeaknesses:\nI'm having a bit of a challenge reconciling the challenge of verifying truthfulness with the focus on results of how CCS out-performs zero shot approaches on accuracy - as mentioned in the paper, it *does* indicate that there is a disconnect between inferences made via one modality (ranking of activations) vs. direct model outputs, but it is difficult to see how that shows that one method is \"better\" than the other for probing knowledge.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper reads clearly and is well-written.\n\nQuality: The experiments and analysis seem of high quality.\n\nNovelty: The CCS probe seems to be a novel way to probe for knowledge by posing knowledge as a binary classification task.\n\nReproducibility: Code is provided but I have not independent verified if it is runnable / reproducible.",
            "summary_of_the_review": "This paper tackles an important challenge with verifying truth and knowledge in language models, but I have some questions about the basic premise of how it shows what it claims (a better way to probe for knowledge).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3080/Reviewer_eJhP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3080/Reviewer_eJhP"
        ]
    },
    {
        "id": "OncAEbHwGh",
        "original": null,
        "number": 3,
        "cdate": 1666631736752,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631736752,
        "tmdate": 1668769179865,
        "tddate": null,
        "forum": "ETKGuby0hcs",
        "replyto": "ETKGuby0hcs",
        "invitation": "ICLR.cc/2023/Conference/Paper3080/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to understand language models' internal knowledge about truth values of the text that it outputs.\nGiven a set of yes-no questions, they train a truth classifier in an unsupervised way by first extracting the hidden states of the language model when it gives a positive or negative answer, respectively. They train the classifier to be consistent, i.e., the probabilities of the two outcomes should sum to one, and confident (one label should receive high probability).\nThe model is tested on zero-shot binary classification on a large set of tasks. It performs reasonably well, outperforming standard LM-based zero-shot classification on average by a substantial margin. They further discuss and test several alternative hypotheses for their model's success, aiming to show that their model indeed captures internal information that is qualitatively different from the information in the model's textual output.",
            "strength_and_weaknesses": "Strengths:\n* very strong motivation\n* simple and effective idea\n* very valuable empirical results to the community\n* discusses relevant alternative hypotheses\n\nWeaknesses:\n* **update post rebuttal**: my biggest concerns have been addressed\n* statistical validity is unclear due to small test sets\n    * why are the datasets subsampled to include only 1000 examples, of which 400 are the test set? \n    * these are unneccessarily small test sets, causing greater uncertainty and therefore confidence in the results\n    * can you please justify why you believe that differences are unlikely to be due to randomness? Note that stds in Table 1 are relatively large.\n* evidence for disproving alternative hypotheses is weak in several cases:\n    * Section 3.2.2: CCS Is Robust To Misleading Prompts\n        * goal: \"discover latent knowledge in an LM even when its **training objective** causes the model to output false text\"\n        * method: change **test time** prompts and observe difference\n        * method is not well suited for the goal, because you don't touch the training objective of the model at all.\n        * result is found for only one out of 5 models. Combined with the statistical uncertainty above, I am not convinced that this is sufficient evidence that this is a valid, general finding.\n    * Section 3.3.2: CCS Can Find Intermediate Representations of Truth\n        * motivation: output text might encode truth, and CCS may merely correlate with output text\n        * method: show that CCS applied to intermediate layers also works well.\n        * implicit assumption: lower layers don't correlate with the produced output text (as much?). This needs evidence of some kind, e.g. a reference.\n        * results: lower layers perform worse, but according to the paper, they still perform well enough. It isn't clear what constitutes well enough.\n        * (their) conclusion: lower layers do well despite not correlating with the output text, so they have to encode truth qualitatively different from the output text.\n        * Lower performance at lower layers may be entirely explained by decreasing (but not vanishing) correlation with the output text. Since there is no quantification of the latter, we cannot make the above conclusion.\n\nAlternative method to prove that internal truth values are present independent of the input:\n* if truth values are encoded independent of textual output, you should be able to extract them by:\n    1. replacing positive and negative labels with completely uninformative, random text. Since you aim to normalize out the effect of \"Yes\" and \"No\", the appended text shouldn't matter either way, or\n    2. don't alter x_i, but train the model with two different classification heads.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of generally high-quality writing and the method and insights appear to be novel.\nDegree of reproducibility appears to be high from the textual description and supplementary material.\n\nRegarding clarity, there are some questions/suggestions:\n* Why do you subsample only 1000 examples from each dataset? Should be justified in the text.\n* What do you mean by \"we optimize 10 times using AdamW\"? Are these 10 different seeds? If so, what is the impact of seed tuning?\n* What is the added benefit of Figure 2 in comparison with the average statistic? It does not appear very helpful to support the argument, which already pretty strong (for this one model).\n* Figure 3: The absolute task performance (alongside its color coding) is not very meaningful with respect to what you want to show, which is that transfering from one task to another is doable. The figure is therefore not easy to grasp. I suggest to change figure 3 such that each cell represents the difference in performance with respect to \"No Transfer\". That way the reader would immediately understand from the colors that transfering almost never degrades the performance.",
            "summary_of_the_review": "I applaud the technical and empirical novelty of the paper. I think it will be of significant interest to many in the community. The main claims are fairly well supported. As noted in the rebuttal, some aspects of the methodology are confusing and could maybe be executed and/or explained/justified better.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3080/Reviewer_r2Hq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3080/Reviewer_r2Hq"
        ]
    },
    {
        "id": "N7mfjQpnOn3",
        "original": null,
        "number": 4,
        "cdate": 1666640441583,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640441583,
        "tmdate": 1666641516497,
        "tddate": null,
        "forum": "ETKGuby0hcs",
        "replyto": "ETKGuby0hcs",
        "invitation": "ICLR.cc/2023/Conference/Paper3080/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Contrast-Consistent Search (CCS), that tries to identify latent knowledge in language models in an unsupervised manner. The process is to convert each question into binary-answer pairs, then train the LM in an unsupervised way via optimizing a consistency loss (probabilities of opposite answers should sum to 1, and a confidence loss to avoid degeneration.\n\nThe proposed method outperforms (calibrated) zero-shot accuracy by 4% on average over 10 question-answering tasks. The authors did some additional analysis like CCS's robustness to misleading prompts and the transferability of CCS.",
            "strength_and_weaknesses": "Strengths:\n- To the best of my knowledge, exploring the internal consistency of language models in an unsupervised way is a novel contribution. The authors proposed an interesting method via optimizing the LM with a consistency and a confidence loss to figure out the more likely choice.\n- The empirical results are significant across 4 out of 6 models, and across 10 tasks.\n\nWeakness:\n- The proposed approach applies only to questions that can be converted with binary answers (i.e., the answer choices are fixed and are from a very limited set, otherwise the inference cost would increase substantially). \n1) This largely constrains the applicability of this method to many tasks, e.g., math tasks that have numbers as answers, which would require generating an infinite number of binary-answer questions.\n2) for some tasks the conversion might be non-trivial and requires additional human processing.\n\n- Although the proposed method doesn't use any labels, it still uses input examples and requires a fairly large number of training examples to perform well (Figure 5) due to the optimization procedure. This could pose a challenge compared to true zero-shot learning which doesn't require any examples at all. For any new task, how to obtain a large number of training examples (even without labels)?\n\n- Table 1 presents average performance. Is there a more detailed breakdown of performance gains across different tasks? Are there categories of tasks that benefit from the method more vs less?\n\n- Relatedly, as the authors have stated, CCS relies on the assumption that true/false inputs can be separated reasonably well in the activation space. Are there analysis regarding what are tasks that satisfy this property and what tasks are less easily separated?",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarify/Quality: The paper is very clear and is overall well-written.\n- Novelty: The contribution on exploring the internal consistency of language models in an unsupervised way is novel.\n- Reproducibility: code and prompts for zero-shot are provided in the paper / supplementary materials.\n\nMinor:\n- Section 3.3.1, 2nd paragraph, \"transfer wells\" -> \"transfers well\"\n- Table 1, what is the difference between \"CCS\" vs \"CCS (All data)\"? I thought CCS already used all 1k balanced subsamples of the training set?",
            "summary_of_the_review": "Despite its limited setting (tasks need to be converted to questions with binary answers, and the need of a large number of unsupervised samples), this paper's contribution is quite novel with relatively strong empirical results, thus I recommend weak accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3080/Reviewer_YocT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3080/Reviewer_YocT"
        ]
    }
]