[
    {
        "id": "lUrneMfn7b",
        "original": null,
        "number": 1,
        "cdate": 1666458395368,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666458395368,
        "tmdate": 1666474030398,
        "tddate": null,
        "forum": "hmpjFiUly1",
        "replyto": "hmpjFiUly1",
        "invitation": "ICLR.cc/2023/Conference/Paper2750/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces the idea of injecting human knowledge into the active learning process by First order logic using Triangle Norms. By formulating domain expert knowledge as loss functions (acquisition function), unlabeled points can be selected based on the number of violated knowledge. The paper tested on 4 different datasets and compared with a bunch of existing AL classification tasks and showed that KAL is surpassing average traditional AL methods.",
            "strength_and_weaknesses": "Strength:\n\nThis paper proposed an interesting idea: using domain knowledge to improve active learning performance. The benchmarked traditional AL methods are plentiful, and experimental results show the promise of KAL. The ablation study shows the amount of knowledge added to the AL progress. The graphic looks and clean.\n\nWeaknesses:\nApart from some clarity issue (below section), I would like to question the experimental design of this work, especially on the XOR illustration that serves the main purpose to conveying how this method work to the audience:\n\nThe initial batch of samples of the AL process is not random, or at least not representative of random. If we look at Figure 1, the first iteration where the blue points are previous samples (and therefore the initial samples that might be the same across experiments), they are heavily biased towards the top left corner. This is an important issue as the majority of the traditional AL algorithm relies on a randomly sampled initial set like BALD. If we then look at Figure 3, the BALD and Margin result, non of the queried points are in the bottom right corner, which indicates the model has not even seen any of those points. The only explanation that made sense to me is that their initialization is biased towards the top left corner, just like Fig 1 shows. This raises strong concerns with respect to the experimental setup. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Not coming from a first-order logic background, I needed to read extra literature to understand the paper. This is totally fine for reviewers but if the paper is oriented towards a general AL audience, putting more background information of how to convert FOL through T-norm and the limitations of such conversion is crucial.\n\nAlso, the computation time advantage is heavily highlighted; I would suggest making room for a more detailed explanation about how and what specific knowledge is injected into each of the datasets and how new researchers can come up with similar knowledge if they would like to use KAL. The main reason I think time complexity is not an issue for Active Learning is because of AL's innate assumption of expensive labeling (time/money wise) and therefore, ten-fold increase in the sampling process might still not be the bottleneck.\n\nQuality: The overall quality of the paper is good\n\nNovelty: This paper is novel to the best of my knowledge\n\nReproducibility: Code is provided with good documentation.",
            "summary_of_the_review": "Given the concerns I have around the experimental setup, along with the presentation issue of the storyline for AL audiences, I recommend borderline reject for this manuscript.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "This work potentially brings (or, equally, mitigates) significant bias towards the machine learning system due to the subjective knowledge injection into the model with First order logic. If such a system is used in an unfair way, e.g., biased judgment injected, in high-stake systems, it might pose potential ethical concerns to society. Also, the combination with XAI further complicates the \"bias\" as XAI is usually used in higher stakes systems due to its explainability (often trading off pure performance compared to black-box models) and humans might not notice what the influence is injected through the \"knowledge\" injected by XAI to KAL.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2750/Reviewer_fw63"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2750/Reviewer_fw63"
        ]
    },
    {
        "id": "_lESe5hY1sF",
        "original": null,
        "number": 2,
        "cdate": 1666538844084,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666538844084,
        "tmdate": 1669812833348,
        "tddate": null,
        "forum": "hmpjFiUly1",
        "replyto": "hmpjFiUly1",
        "invitation": "ICLR.cc/2023/Conference/Paper2750/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposed a new methodology for active learning. In particular, they propose to exploit the background knowledge available about a problem to better select the datapoints to be labelled. The background knowledge must be expressed as logical constraint as it is then mapped into the loss function via the usage of the t-norms. An extensive experimental analysis has been done to evaluate the strengths and weaknesses of the model.",
            "strength_and_weaknesses": "**Strengths:**\n1. The proposed approached is very novel. As far as I know, nobody has proposed the usage of logical constraints for the task of active learning. \n2. The authors propose synthetic experiments that help in understanding the strengths of the work. \n3. The problem has been analysed from a variety of different angles. \n\n**Weaknesses**: \nEven though I really liked the paper, I found that at times it was written in an imprecise/misleading way. Please see the box below for more details.",
            "clarity,_quality,_novelty_and_reproducibility": "**Quality**: \nThe paper is in general of good quality, however, as stated above, I found some of the statements imprecise/misleading. More in detail: \n1. Expressivity of the constraints: since the constraints considered in the framework are transformed using the t-norms the framework proposed in the paper cannot handle full FOL (with functions and infinite domains). Since all the constraints studied involve relationships between data and classes the authors should talk about propositional logic constraints. \n2. The authors should state since the introduction that they are confining their method to single-label or multi-label classification problems (they cannot handle regression). \n3. The authors write that they can have constraints over the input data, however this makes the assumption that the features are binary (as they are then transformed with the sigmoid function). This is then reflected in the XOR-like problem, where the authors write the constraints where the atoms are algebraic functions. This can be misleading as in general their framework does not support such expressivity. (Please correct me if I am wrong, suppose I have the feature \"Age\" can your framework incorporate the constraint \"if Age > 100 then...\"? )\n4. The authors talk about the Monte-Carlo Dropout version  of their model, without ever explaining exactly what it does. There is a reference to another paper, but each work should be intra-contained, so it would be useful to have some lines explaining the differences between KAL and $\\text{KAL}_D$.\n5. The authors write: \"By always selecting the data that violate this corpus of rules, KAL ensures them that the trained model respects the provided knowledge also at test time.\" I don't see how this can be true. In general it is not enough to just \"provide a datapoint\" to a model to guarantee the satisfaction of the constraints. In this respect, in the recent years there have been a some  models that incorporate the constraints into the loss/topology of neural models to guarantee their satisfaction (see e.g., [1,2,3]). It would be nice to have a discussion on how they could be used together with KAL to actually get the satisfaction of the constraints by design. \n6. Why is the ablation study reported in table 2 done only on CUB200? Could you do it for all datasets?\n7. Table 3: Instead of reporting the violations computed as the increased percentage over the violation of a model trained to respect this knowledge, could the number of violations be directly reported? It would be more informative and less misleading. *As a side node:* why is KAL here called $\\text{KAL}$ with *small* as subscript? What is the difference between the this and normal KAL? Why is the set of constraints called $\\mathcal{K}$ with *CUB-S* as subscript$? What does the \"-S\" stand for?\n8. In Figure 5 KAL is the only one that is provided with orange datapoints around the point (1.0,0.0) at iteration 0 why is that the case? Were those datapoints provided by the authors? Shouldn't all the models start with the same training set? Why is it not the case? Can the authors ensure that all the models were provided with the same set of labelled datapoints at iteration 0?\n\nAnother major point that undermines the findings done by the authors is that they considered one way to include the logical constraints in the training process (i.e., transforming the constraints using the t-norms). However, there has been quite some development in the recent years in the field of \"deep learning of logical constraints\" (see survey [4] ) and it would have been useful to at least discuss why the t-norm method has been chosen over others. However, most of the recent methods are not even mentioned. For example, in [6] the Semantic Loss has been proposed. The semantic loss has the advantage (over the t-norms) of being syntax dependent. Why are the t-norms convenient wrt such method? Are you planning to try this out in the future? \n\nAnother concern that I have is what happens when we have partial knowledge? Suppose that in the XOR example we have only the domain knowledge: $$\ny(x) = 1 \\text{ if } x_1 > 0.5 \\wedge x_2 \\le 0.5\n$$\nmy guess is that KAL would only learn to output one only for the datapoints such that $ x_1 > 0.5 \\wedge x_2 \\le 0.5$. Would you mind actually providing the results for this case? My guess is that for each iteration we would need to also select some datapoints at random. However, it would be interesting to study what the model learns when we vary the percentage of such randomly chosen datapoints.\nSince it is not a resource heavy experiment, would you mind running it and reporting the results?\n\n**Clarity:**\n\nThe paper is very clear and easy to follow.\n\n**Novelty:**\nThe paper is very novel and potentially very relevant\n\n**References:**\n\n[1] Paolo Dragone, Stefano Teso, and Andrea Passerini. Neuro-symbolic constraint program- ming for structured prediction. In Proc. of IJCLR-NeSy, 2021.\n\n[2] Eleonora Giunchiglia and Thomas Lukasiewicz. Multi-label classification neural networks with hard logical constraints. JAIR, 72, 2021.\n\n[3] Nicholas Hoernle, Rafael-Michael Karampatsis, Vaishak Belle, and Kobi Gal. MultiplexNet: Towards fully satisfied logical constraints in neural networks. In Proc. of AAAI, 2022.\n\n[4] Eleonora Giunchigla, Mihaela C. Stoian, and Thomas Lukasiewicz. Deep learning with logical constraints. In Proc. of IJCAI, 2022.\n\n[5] Jingyi Xu, Zilu Zhang,Tal Friedman, Yitao Liang, and Guy Van den Broeck. A semantic loss function for deep learning with symbolic knowledge. In Proc. of ICML, 2018.\n",
            "summary_of_the_review": "TL;DR I think this paper has all the potential to be a great paper. However, in the current form I have too many concerns. If all of them are addressed I would happily fully accept the paper",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2750/Reviewer_JCBB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2750/Reviewer_JCBB"
        ]
    },
    {
        "id": "TZ6OozcDhZO",
        "original": null,
        "number": 3,
        "cdate": 1666670044794,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670044794,
        "tmdate": 1666670044794,
        "tddate": null,
        "forum": "hmpjFiUly1",
        "replyto": "hmpjFiUly1",
        "invitation": "ICLR.cc/2023/Conference/Paper2750/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an approach for active learning that selects examples using logic constraints specified as domain knowledge. The proposed approach is evaluated in a series of empirical experiments compared to traditional active learning baselines.",
            "strength_and_weaknesses": "**Strengths**\n\n* This paper presents an interesting and flexible framework for active learning. The proposed approach allows users to specify domain information in the form of FOL to determine selection criteria.\n* The authors demonstrate empirical effectiveness via a series of empirical experiments . The experiments are extensive and used to validate the approach.\n\n**Weaknesses**\n\n* I think that the paper could be improved if there was more succinct understanding as to where and when the proposed approach is more effective or efficient than baseline methods in active learning\n* Table 1 leads to a somewhat confusing landscape of empirical success, where baseline methods of kcenter and kmeans are highly effective. I think the presentation can be improved by highlighting reasons to select a particular method more.",
            "clarity,_quality,_novelty_and_reproducibility": "I believe that clarity and novelty are adequate.",
            "summary_of_the_review": "An interesting approach for using domain knowledge in active learning. The proposed approach is extensively evaluated against active learning methods, however, in places where the best method is not abundantly clear, the paper could be improved to specify why one approach is preferred to another more succinctly. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2750/Reviewer_KFBz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2750/Reviewer_KFBz"
        ]
    },
    {
        "id": "PalxG0pj-oM",
        "original": null,
        "number": 4,
        "cdate": 1666683578444,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683578444,
        "tmdate": 1666684299689,
        "tddate": null,
        "forum": "hmpjFiUly1",
        "replyto": "hmpjFiUly1",
        "invitation": "ICLR.cc/2023/Conference/Paper2750/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Summary: Authors propose an active learning pipeline that incorporates domain knowledge into the pipeline for more efficient active learning & yielding models more consistent with known domain rules. Specifically, the model employs first-order-logic (FOL) based encoding to employ domain rules to inform examples to be selected for active learning. Authors demonstrate the effectiveness of their generic FOL pipeline on variegated tasks like Object recognition, classification and compare their model to various state-of-the-art uncertainty based active learning approaches and other clustering based methods. Overall the proposed method yields good results. \n\n\n \n ",
            "strength_and_weaknesses": "## Strengths: \n\nAuthors propose a light-weight model for active learning incorporating domain knowledge using the T-Norms based rules introduced in [1]. \n \n\nResults of the proposed model are convincing as a result of the multiple tasks and variegated baselines employed for state-of-the-art comparison. \n \n\nPaper is well written and well organized. \n\n \n\n## Weaknesses: \n\nThe proposed method is applied to classification problems and in somewhat balanced data contexts. It would be interesting to check the power of domain knowledge in data imbalanced contexts.  \n\n Proposed model is somewhat novel as the crux of the FOL encoding is based on the previously proposed T-Norm idea.\n\nThe discussion about contexts in which the proposed method would out-perform baselines would be especially insightful to readers.\n\nIncorporating a little more background regarding FOL might help readers fully comprehend the paper better.\n\n## References: \n\n1. Klement EP, Mesiar R, Pap E. Triangular norms. Springer Science & Business Media; 2013 Apr 17. ",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is clear and source code investigation does not raise many questions regarding reprodubibility.",
            "summary_of_the_review": "The proposed model is somewhat novel and the core idea (i.e., encoding domain knowledge as FOL using T-Norms) is based on previously published paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2750/Reviewer_r58Q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2750/Reviewer_r58Q"
        ]
    },
    {
        "id": "FtjLM3um8T",
        "original": null,
        "number": 5,
        "cdate": 1667261615918,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667261615918,
        "tmdate": 1667261615918,
        "tddate": null,
        "forum": "hmpjFiUly1",
        "replyto": "hmpjFiUly1",
        "invitation": "ICLR.cc/2023/Conference/Paper2750/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a novel idea of incorporating domain knowledge into first-order logic  (FOL) for active learning. Consistency between the predictions made on unlabelled data and the quantified domain knowledge encoded in the form of FOL is assessed, and the examples defying make the candidate examples to query their label. Experiments are performed in multiple data sets and compared with a wide range of existing methods. \n",
            "strength_and_weaknesses": "Active learning is a machine learning paradigm where a subset of examples is selected from a large pool of unlabelled data to query their labels. Most of the active learning literature works are based on either uncertainty or diversity, or both. I find the work novel because it introduces the idea of injecting domain knowledge. \n\nThe paper is well-motivated, and the previous works are discussed adequately. However, some of the recent works on model-based active learning are missing:\n\nSinha, Samarth, Sayna Ebrahimi, and Trevor Darrell. \"Variational adversarial active learning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.\n\nYoo, Donggeun, and In So Kweon. \"Learning loss for active learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n\nCaramalau, Razvan, Binod Bhattarai, and Tae-Kyun Kim. \"Sequential graph convolutional network for active learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\nThe experiments are performed for both image classification and object recognition. The experimental results are either comparable to better than the existing method without incurring much computational cost.  Although the experiments are done on a small scale, I think this should not be considered a weakness. However, I wondered how domain knowledge could be applied to active learning for regression tasks. \n\n\nThe paper is well presented, and explaining with the toy example makes the idea easy to understand. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-motivated, easy to understand, and novel. ",
            "summary_of_the_review": "I find the idea of injecting domain knowledge into active learning very interesting. Although the experiments are done on a small scale, they are convincing. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2750/Reviewer_NXVX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2750/Reviewer_NXVX"
        ]
    }
]