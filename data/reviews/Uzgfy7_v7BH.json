[
    {
        "id": "_mfk7JMUsyz",
        "original": null,
        "number": 1,
        "cdate": 1666446312255,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666446312255,
        "tmdate": 1666446312255,
        "tddate": null,
        "forum": "Uzgfy7_v7BH",
        "replyto": "Uzgfy7_v7BH",
        "invitation": "ICLR.cc/2023/Conference/Paper4408/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Multi-agent interactions result in non-stationary environment for reinforcement learning algorithms, which harms the performance of mean field method, which reduces high number agent's problem to pairwise interactions approximations. \n\nTo make the algorithm more robust, the authors propose learning counterfactual effects to generalized representation of merged agents. \n\n\n\nMean field method reduces the heterogeneous agents interactions to be centered to a  common agent interaction with its merged neighbors. Concretely, the value-action function involves the state of the agent in question and all its neighbors' interaction, which is decomposed to be the sum of pair wise value-action and approximated to be interaction with merged agent state-value function. \n\n\nTo quantify the importance of one specific neighbor to the current agent in question, the author proposes to quantify using the counterfactual policy distance by replacing the merged agent with that specific neighbor. \nThis importance weight is then used to average the contributions of the pairwise state-value function to the state value function with respect to all its neighbors. \n\nThe author further use Taylor expansion and assume state-value function is L-smooth thus the Lagrange remainder could be eliminated. The policy is conditioned on a weighted sum of previous step actions. \n\n\nTo evaluate the performance empirically,   IQL, MFQ(mean field) and attention MFQ is used as baselines.  Test environment has more agents than training environments. ",
            "strength_and_weaknesses": "Strength:\n\n1. I think the proposed method is quite novel and addresses an important issue\n2. Empirical studies are good. \n\n\n\nWeakness:\n1. I think Figure 2need more explanation, I could understand the mechanism intuitively, but are there some more insightful explanation? Fig 2(b) is essentially comparing the KL divergence between two policies by using a concrete agent or a merged agent, the result in used in Fig 2(a) to generate the weight, why would this be a good mechanism? \n2. Could you add more explanation to the L-smooth justification of the Q function with respect to action? I thought in your game, action is discrete? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:  Good\n\nQuality: Good\n\nReproducibility: Not sure",
            "summary_of_the_review": "I think the paper addresses an important issue in multi-agent reinforcement learning, where the number of agents are big and existing mean field method tend to ignore the different contributions of neighbors to the fate of the agent in question. The proposed method is well explained, though lack more in depth analysis. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4408/Reviewer_v11b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4408/Reviewer_v11b"
        ]
    },
    {
        "id": "yTsI_EjLPm",
        "original": null,
        "number": 2,
        "cdate": 1666744101737,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666744101737,
        "tmdate": 1666744101737,
        "tddate": null,
        "forum": "Uzgfy7_v7BH",
        "replyto": "Uzgfy7_v7BH",
        "invitation": "ICLR.cc/2023/Conference/Paper4408/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes causal mean-field Q-learning (CMFQ) to address the scalability problems in non-stationary environments. Results show better scalability in mixed cooperative-competitive game and a cooperative game. ",
            "strength_and_weaknesses": "strengths:\n-the paper address the limitation of ignoring pairwise interactions in mean field Q-learning using by using a weighted combination through causal inference\n- factorizing joint-q function into local pairwise Q-functions is the basis to address scalability issues \n\nweakness:\n-more experimental scenarios need to be evaluated \n-the figures are hard to read and need to be explained more clearly especially 5a\n-how the proposed method improves scalability needs to be demonstrated in the experimental results when compared to the other approaches\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and builds on mean field q-learning. The code and settings are made available for reproducibility. \n\n",
            "summary_of_the_review": "The paper builds on mean field q-learning hence the contribution may be limited. More experimental scenarios can be considered to showcase the superiority of the proposed approach. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4408/Reviewer_bK4T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4408/Reviewer_bK4T"
        ]
    },
    {
        "id": "g8gmt9ViIim",
        "original": null,
        "number": 3,
        "cdate": 1667162386998,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667162386998,
        "tmdate": 1667162386998,
        "tddate": null,
        "forum": "Uzgfy7_v7BH",
        "replyto": "Uzgfy7_v7BH",
        "invitation": "ICLR.cc/2023/Conference/Paper4408/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the scalability problem in MARL, scaling to environments containing more agents during testing, and proposes a causal mean-field Q-learning (CMFQ) algorithm. This paper solves the non-stationarity challenge by focusing on crucial pairwise interactions identified by Causal inference. And Mean Field Theory is beneficial to scalability since it turns a many-agent problem into a two-agent problem. They evaluate CMFQ in the cooperative predator-prey game and mixed cooperative-competitive battle game. The results illustrate that the scalability of CMFQ significantly outperforms all the baselines. Furthermore, results show that agents controlled by CMFQ emerge with more advanced collective intelligence than those controlled by other baselines.\n",
            "strength_and_weaknesses": "Strength\n\n1. Adequate experiments show better performance on reward and scalability than other baselines and surprising visualization analysis shows the CMFQ has better ability to cooperate.\n\n2. This work employs the causal model to infer the weight of pairwise Q functions replacing previous average-based mean-field and attention mean-field methods. I think this approach is original.\n \nWeaknesses:\n1. In the introduction section, the potential shortcoming of previous mean-field Q-learning (MFQ) is discussed, however, the discussion about attention-MFQ, e.g. the limitation of attention-MFQ seems missing.\n2. equation (9) seems to need more explanation, and what the $Q^j$ and $Q^i$ indicate respectively?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nFor writing, the logical flow of this paper is clear. However, there are also some minor problems.\n1. The meaning of F and fVj(.) in section 3.3 is not explained. And the definition of F is not clear and is nested with fVj.\n2. What do the Q^j and Q^I indicate in section 4.2 respectively?\n3. Why does the larger TE(KL divergence ) result in the larger weights in equation 11? I think that a smaller KL divergence would correspond to a more important action thus with a larger weight.\n4. I am confused about how fig 6(a) shows that \u201cCMFQ learns the tactic of besieging, while MFQ tends to confront frontally\u201d.\nQuality: This work proposes a new causal mean-field Q-learning algorithm, improving the traditional mean-field Q-learning by assigning different weights when combining Q functions. Experiments results are sufficient to prove the effectiveness of the casual mean-field algorithm on test scalability and reward performance.\nNovelty: I think this approach is original. It introduces the causal inference to measure the importance of Q functions, which is different from the previous method and empirically effective.\nReproducibility: The primary procedures to computing casual mean-filed Q-function are given by equations, it\u2019s clear. It would be clearer if more details and pseudocode of the algorithm CMFO are added.\n",
            "summary_of_the_review": "This work proposes a new causal mean-field Q-learning algorithm, it is original to apply causal inference in mean-field Q-learning. This paper addresses the scalability problem,  experiments results are sufficient to prove that it can scale to environments containing more agents. The writing of this paper is easy to follow, and the structure is clear.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4408/Reviewer_3dJB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4408/Reviewer_3dJB"
        ]
    }
]