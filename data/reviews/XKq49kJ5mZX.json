[
    {
        "id": "yxcV8nunA1h",
        "original": null,
        "number": 1,
        "cdate": 1665973950747,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665973950747,
        "tmdate": 1665973950747,
        "tddate": null,
        "forum": "XKq49kJ5mZX",
        "replyto": "XKq49kJ5mZX",
        "invitation": "ICLR.cc/2023/Conference/Paper4795/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces the approach of incorporating an uncertainty estimator into the PPO model. The authors define the uncertainty of RL models by the standard deviation of logits from neural models and propose capturing these uncertainties by utilizing a previously proposed model named Masksembles. To evaluate the model, this paper introduces a benchmark that includes OoD datasets in the previously studied RL environment, such MojoCo, and Atari. The empirical evaluation shows the RL model with Masksembles layers can outperform previous work in terms of collecting more rewards and detecting OoD samples.",
            "strength_and_weaknesses": "### Strength\n\n1. This paper explores an intriguing research topic about how to enable uncertainty in RL models.\n2. The empirical studies demonstrate the leading performance of proposed models under multiple environments.\n\n### Weaknesses\n\n1. **Writing.** The writing style is cumbersome, including many incompatible expressions. I list some problems in the abstract, but there are more issues in the main paper. Reviewing based on the current version of this paper is difficult. It seems the paper introduces a fairly easy-to-follow model but the writing makes it hard to comprehend. Many details require further clarification. I strongly recommend using some help from the writing service from your school and other institution or companies in order to improve your writing. \n\n\nTo overcome these issues -> To resolve these issues\n\nhave not seen widespread adoption -> have not been extended to\n\nTo overcome the first gap -> To fill this gap\n\npossible applicable -> potentially applicable\n\nThe second point -> The second issue or problem\n\nWe show experimentally -> Our experiment shows\n\namong the survey methods -> among the previously-proposed method or in a survey paper?\n\ndetection while matching -> that matches\n\nvia a custom evaluation benchmark  -> via a customized evaluation benchmark\n\n2. **Uncertainty Estimation.** It seems the authors misunderstand the concept of uncertainty in RL. An RL task commonly has two kinds of uncertainties, including epistemic uncertainty and aleatoric uncertainty. At the beginning of this paper, the authors emphasize the problem of the OoD dataset, which is a typical source of epistemic uncertainty, but then in the benchmark, the authors mention \"perturbations to the parameters of the physics simulation\", which, on the other hand, defines aleatoric uncertainty. I recommend looking into the definition of uncertainties in a common machine learning class. The works from Yarin Gal's group could serve as nice examples, for instance \n\nJishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip H. S. Torr, and Yarin Gal.\nDeterministic neural networks with appropriate inductive biases capture epistemic and aleatoric uncertainty. CoRR, abs/2102.11582, 2021.\n\n3. **Comparison Methods.** The paper argues \"In contrast, our work specifically targets for on-policy and actor-critic methods.\" I believe the uncertainty of Q values can define policy uncertainty since both Q-learning methods and policy gradient methods rely on the Q values for determining or updating policy. The uncertainty of cumulative rewards can directly capture the uncertainty in an RL task since the cumulative rewards term is the objective of RL agents. Compared to policy, it directly reflects the agent's preference under different states and is more compatible with the goal of MDP. More importantly, **I do not understand why we can not use the uncertainty of Q values to detect OoD samples.** What's the problem and why not compare with previous methods?\n\n4. **Additional Issues**\n\n- In Formula (3). The logits have not been normalized. Their ranges can significantly deviate from one to another. Their stds do not have a consistent meaning.\n\n- In Figure 2. What does the dashed line mean in this plot?\n\n- In section 5. What's the problem with using a D4RL dataset? We can create an OoD dataset by simply excluding the optimal trajectories or sub-optimal from the D4RL dataset.\n\n- \"Two-Objective Optimization\" in Section 6. It seems the Maskembles method does not require an objective to optimize. It is in fact a neural architecture that can be integrated into any of the deep models. The deep model does not have to be for solving the RL problem. The only objective the paper optimized is the traditional RL objective.\n\n- In Figure 3. The legends and labels are impossible to read. Please resize them.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality.** The clarity and the quality of the paper need to be further improved, as I mentioned above\n\n**Novelty.** Using std of neural logits for representing uncertainty and adding Maskembles layers into the neural model define the main contributions in these works. These ideas are incremental and marginal, especially for the submissions to this top-tier cs conference.\n\n**Reproducibility** Reproducibility should be satisfying, but my understanding could be biased since the paper is composed in a rush and is difficult to review.",
            "summary_of_the_review": "This paper has some critical issues in writing. The contributions are marginal, and I have listed some concerns about their major contributions, especially about the concept of uncertainty. It seems the paper is composed in a rush. I recommend revising the paper for a reader-friendly format. I vote for a rejection based on the current format.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4795/Reviewer_5Wtm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4795/Reviewer_5Wtm"
        ]
    },
    {
        "id": "ubCOV2_cuP",
        "original": null,
        "number": 2,
        "cdate": 1666681180574,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681180574,
        "tmdate": 1666681324096,
        "tddate": null,
        "forum": "XKq49kJ5mZX",
        "replyto": "XKq49kJ5mZX",
        "invitation": "ICLR.cc/2023/Conference/Paper4795/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focues on quantify the uncertianty of online RL and detect out-of-distribution (OOD) states. Specifically, the definitions of uncertainty and OOD was presented for proximal policy optimization. The cocepts of value and policy uncertainty were discussed. Moreover,  this paper formulates a Pareto optimization problem to overcome the trade-off between reward and OOD detection. Empirically, they found Masksembles enables high-quality uncertainty estimiation and OOD detection while matching the performance of original RL agents. ",
            "strength_and_weaknesses": "## Strength\n\n+ Introduce the concept of uncertainty fir on-policy actor-critic RL, specifically, introduce policy and value uncertainty in section 3. There is limited work study uncertainty for OOD detection in RL. Unlike iid samples in supervised learning, defining indistribution and out-of-distribution data for RL is challeging. Also the definition of uncertainty for RL should be different than that in supervised learning. Previous uncertainty definition in supervised learning does not scale to RL domains. This work smartly present the uncertainty of RL policy and value as the disagreement between submodels. The disagreement was performed as the standard deviation between submodels and alternatively as Jensen-Shannon divergence between output distributions (in Appendix D). With the definition of policy and value uncertainty, existing uncertainty estimation methods can be intergrated for uncertainty estimation in RL setups. Then, the OOD detection was framed as a binary classification problem, using the computed uncertainty scores as a threshold to classifiy ID and OOD states. \n\n+ The empirical study was conducted in both discrete action space and continuous action space. The limitations of this work has been discussed.  \n\n## Weakness\n\n+ The main contribution of this work is providing definitions for policy and value uncertainties in RL. However, the practical uncertainty estimation implementation directly uses existing uncertainty estimation methods. If a novel uncertainty estimation method can be proposed  for RL would further increase the quality of this work. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\nThe writting is clear.\n\n## Quality\nThe quality is good.\n\n## Novelty\nThe proposed definition of policy and value uncertainty for RL is novel. \n\n## Reproducibility\nThe provided details should be sufficient for reproducibility.\n\n",
            "summary_of_the_review": "Overall, I found this work is a intreresting work and novel in providing the definition of policy and value uncertainty. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4795/Reviewer_dCQj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4795/Reviewer_dCQj"
        ]
    },
    {
        "id": "FDwey-Kolq",
        "original": null,
        "number": 3,
        "cdate": 1666758114037,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666758114037,
        "tmdate": 1666758114037,
        "tddate": null,
        "forum": "XKq49kJ5mZX",
        "replyto": "XKq49kJ5mZX",
        "invitation": "ICLR.cc/2023/Conference/Paper4795/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a study into epistemic uncertainty estimation and OOD detection for reinforcement learning (RL), focusing on proximal policy optimization (PPO). The paper frames the OOD detection problem for RL, the differences and challenges encountered for RL as compared to supervised learning, and proposes value and policy based uncertainty measures. Several methods were considered for uncertainty estimation: Masksembles, MC Dropout, MC Dropconnect, and ensembles across many simulation settings. Masksembles was found to maintain the performance of RL (in terms of reward) while outperforming other methods in uncertainty estimation. ",
            "strength_and_weaknesses": "Strengths:\n* I really enjoyed reading this paper. It is well written and the ideas are easy to follow. \n* The topic is timely in terms of OOD detection in RL for safety-critical applications.\n* The problem is well-motivated and the literature review does a good job of contextualizing the paper in prior work.\n* The methods section introduced concepts eloquently, clearly, and sequentially - excellent exposition!\n* The figures are informative and effectively illustrate the benefits of the proposed approach.\n* The experiments consider a good number of baselines and many simulation settings, as well as include a thorough ablation study.\n* The future work and limitations sections were interesting and pertinent to the research community. Specifically, some important insights and open problems to address are: \n    * The OOD detection and reward trade-off, which includes robust uncertainty estimation methods potentially not being performant for on-policy RL;\n    * Training data is constantly changing for on-policy RL causing the OOD states to evolve and catastrophic forgetting causing ID states to become OOD;\n    * Since there is not necessarily a singular correct action, a low probability for a certain action does not necessarily correspond to high epistemic uncertainty;\n    * Framing of OOD makes sense for RL as valid game states that the agent has not yet encountered rather than, for example, random perturbations to images.\n\nWeaknesses:\n* The Pareto optimization problem is only explicitly mentioned in the abstract. I am assuming this is referring to the Fig. 4 discussion?\n* The paper lacks polish as there are a number of formatting and referencing inconsistencies (see below).\n* A more in-depth discussion of how the proposed approach differs from Clements et al. (2019) and Charpentier et al. (2022) would be prudent.\n* The authors highlight that since there is not necessarily a best correct action in on-policy RL inducing high aleatoric uncertainty, this does not necessarily correspond to high epistemic uncertainty. Does this not mean that methods that disentangle aleatoric and epistemic uncertainty (e.g., Charpentier et al. 2022) would work better in the RL setting? Similarly, the discussion that disagreement between sub-models causes a decrease in reward appears to lead to the benefit of one-shot uncertainty estimation models, such as Charpentier's evidential deep learning architecture. It seems that baselining against such an approach would make the paper stronger.\n* Since Masksembles are a critical part of the paper, it would be helpful to include a more detailed explanation of the method in Sec. 4.1.\n* The performances in Tables 1 and 2 were not particularly decisive, although the Masksemble did seem to outperform in the majority of the settings. It would be helpful if standard errors across the 3 seeds were reported.\n* I was missing a takeaway conclusion for the experiment in the rewards paragraph in Sec. 6. Why is it important to note that multiple predictions does not necessarily lead to better rewards? \n* As the authors state, it is very surprising that ensembles performed so poorly in the PPO setting given many prior successes across domains. Do the authors know of any other papers that support poor performance of ensembles in on-policy settings? Perhaps 4 models in the ensemble is not sufficient?\n* The experiments section would benefit from additional qualitative and quantitative analysis. Qualitatively, it would be interesting to look at some example states that were marked as OOD by Masksembles. Quantitatively, it would be helpful to look at perturbation magnitude (in continuous perturbation settings) versus estimated epistemic uncertainty for different methods to examine if an increase in perturbation leads to higher estimated uncertainty.\n* There was no clearly better approach in terms of value versus policy uncertainty estimation and no associated discussion.\n\nSome typos and points of confusion are listed below:\n1. Sec. 1, paragraph 1: 'safety-critic' -> 'safety critical'.\n2. The reference citation format is inconsistent throughout the paper, often with citations being placed with or without parentheses arbitrarily (e.g., first example is second line on page 2).\n3. OOD and ID acronyms are not defined in the body of the paper (should be defined in paragraph 1 of page 2).\n4. Sec. 2, paragraph 1: 'aleatoric' -> 'epistemic'.\n5. 'Bayesian' should be capitalized.\n6. Sec. 3, second paragraph: 'Markov decision process is set $\\mathcal{M}$ is incorrect grammar.\n7. Sec. 3, third paragraph: should all the $a$ and $s$ in the sentence 'Formally...' have $t$ subscripts?\n8. Eqs. 1, 4, 5: missing periods at the end of sentences.\n9. Eq. 2, I do not believe $\\tilde{\\pi}$ was defined formally.\n10. Sec. 4.1, second paragraph: 'determine flexibly determine'.\n11. Sec. 4.1, third paragraph: missing space in 'Fig.2'.\n12. Fig. 2 caption seems like the semi-colon should be a period?\n13. Sec. 5, second paragraph: 'varying the dimensions of body bodies'.\n14. Sec. 5, second paragraph: 'These modifications create OOD states when the agent acts in it.' is a bit grammatically awkward.\n15. Sec. 5: referencing 'section 6' instead of 'Sec. 6'.\n16. Sec. 6, reward paragraph, line 2: extra parentheses added in two places.\n17. Sec. 6, reward paragraph: 'in 4.2' -> 'in Sec. 4.2'.\n18. Sec. 6, reward paragraph: 'in the Tab. 1' -> 'in Tab. 1'.\n19. Tables 1 and 2 were not properly formatted. Try using the booktabs package for formatting tables.\n20. MS, MCDO, MCDC acronyms were not defined in Table 1, but were defined in the caption of Table 2.\n21: Sec. 6, OOD detection paragraph: 'These results' -> 'The results', \n22: Fig. 4 axis labels are too close to the sub-figure labels.\n23. Fig. 4 caption: 'The configuration are sampled' -> 'The configurations are sampled'.\n24. Table 2: V. U./P. U./Value U. were not defined. 'MC DO', 'MC DC' have a space in this table, but not in Table 1.\n25. Appendix should start on a new page after references list.\n26. Appendix sections should consistently either start on a new page or not.\n27. Appendix F, paragraph 2: extra space in 'Eq.  5'.\n28. The references should be proofread (e.g., to ensure the year is not entered twice in a citation, the conference venue is listed instead of ArXiv when available, the conference name formatting is consistent, etc.).",
            "clarity,_quality,_novelty_and_reproducibility": "The exposition in the paper was high quality and clear. Although there was not a particular focus in this paper on novelty, but rather framing and exploration of OOD detection in an RL setting, the authors brought up important points of discussion for this area of research and empirically studied a good number of baselines across many simulation settings. Aside from the points of clarification listed above, I found the paper to be clear. Code is provided in the supplemental material, which aids reproducibility. ",
            "summary_of_the_review": "Overall, this is an interesting, clearly written paper that considers a timely problem of epistemic uncertainty estimation in on-policy RL. The authors do their due diligence in considering several baselines across many datasets. The paper frames important considerations for OOD detection in the RL setting. However, further insight gleaned from the experiments in terms of statistic significance and policy versus value uncertainty, as well as baselining against a one-shot uncertainty estimation method (e.g., Charpentier et al., 2022) would strengthen the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4795/Reviewer_vngt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4795/Reviewer_vngt"
        ]
    },
    {
        "id": "ZIlSwRKYtA",
        "original": null,
        "number": 4,
        "cdate": 1666801380738,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666801380738,
        "tmdate": 1666801380738,
        "tddate": null,
        "forum": "XKq49kJ5mZX",
        "replyto": "XKq49kJ5mZX",
        "invitation": "ICLR.cc/2023/Conference/Paper4795/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper investigates OOD detection, with different ensembles amd dropout methods, of policy and value networks in PPO. Experiments report RL-performance and OOD-ROC curves on 4 Mujoco and 4 Atari benchmarks. All methods decreased perrformance in (almost all) environments, but Masksembles have been reported to work somewhat better for OOD.",
            "strength_and_weaknesses": "**Stength**\n\n- OOD detection, as a first step to react somehow to it, is an important topic in RL research.\n\n**Weaknesses**\n\n- Unless the reviewer misunderstood something fundamental, there seems to be almost no novelty here. The authors empirically compare known uncertainty estimators, most of them known to work well for OOD in RL (see missing references below), and come to the conclusion that the newest method (Masksembles) works the best. The only other novelty is how OOD samples are generated, which did not convince this reviewer (see below).\n- The evaluation is in this reviewers eyes insufficient and conceptually flawed. First, every uncertainty method seems to reduce the performance. Why is that? Second, none of the tables report standard deviations over the 3 seeds, which are very few to make a strong argument anyway. And last, the way the OOD data is generated is very random and does not allow to say how similar evaluated samples are to the training data. Some samples will therefore be very similar and others dissimilar, without a way to correct for this confounding factor.\n- The authors also seem to misunderstand epistemic and aleatoric uncertainty. Most of Section 3.2 concerns methods that estimate the *aleatoric uncertainty* of $\\pi$, which does not affect OOD! For example, the confidence or entropy of the policy network can be extrapolated to be extremely certain, even if the state is OOD. Only the ensemble methods (including dropout) estimate actually estimate the required epistemic uncertainty.\n- The definition of policy uncertainty remains questionable: in RL multiple policies can have the same (or near identical) values. The same problem can therefore have multiple optimal policies that assign different probabilities to the same actions. Due to the training procedure this will probably not happen very often, but it still makes the metrics somewhat questionable.\n- The introduction of the evaluated methods omits many (possibly important) details. Furthermore, MC Dropconnect is neither defined nor cited in the main paper.\n- The paper does not cite a large body of works on exploration in RL that uses OOD detection with ensembles (and other methods) to produce intrinsic rewards (see below).\n- Finally, the reviewer is not certain what the takeaway from the paper is. The authors present two OOD detectors based on value and policy, but how should those be used and what is the meaning if they should disagree? To warrant publication, the authors should introduce some novel idea or analysis and demonstrate them on tasks that are of practical interest, like e.g. exploration.\n\n**Missing references**\n\nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and R\u00e9mi Munos. \"Unifying count-based exploration and intrinsic motivation.\" In Advances in Neural Information Processing Systems (NIPS) 29, pages 1471-1479, 2016. URL https://arxiv.org/abs/1606.01868\n\nYuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. \"Exploration by random network distillation.\" In 7th International Conference on Learning Representations (ICLR), 2019. URL https://openreview.net/forum?id=H1lJJnR5Ym\n\nMeire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. \"Noisy networks for exploration.\" In International Conference on Learning Representations (ICLR), 2018. URL https://arxiv.org/abs/1706.10295\n\nXiuyuan Lu and Benjamin Van Roy. \"Ensemble sampling.\" In Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017. URL https://papers.nips.cc/paper/2017/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html\n\nBrendan O\u2019Donoghue, Ian Osband, R\u00e9mi Munos, and Volodymyr Mnih. \"The uncertainty Bellman equation and exploration.\" In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 3836\u20133845, 2018. URL https://arxiv.org/abs/1709.05380\n\nIan Osband, Benjamin Van Roy, and Zheng Wen. \"Generalization and exploration via randomized value functions.\" In Proceedings of the 33rd International Conference on International Conference on Machine Learning (ICML), pages 2377\u20132386, 2016. URL https://arxiv.org/abs/1402.0635\n\nIan Osband, John Aslanides, and Albin Cassirer. \"Randomized prior functions for deep reinforcement learning.\" In Advances in Neural Information Processing Systems (NeurIPS) 31, pages 8617\u20138629. 2018. URL https://arxiv.org/abs/1806.03335\n\nGeorg Ostrovski, Marc G. Bellemare, A\u00e4ron van den Oord, and R\u00e9mi Munos. \"Count-based exploration with neural density models.\" In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 2721\u20132730, 2017. URL https://arxiv.org/abs/1703.01310\n\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. \"Curiosity-driven exploration by self-supervised prediction.\" In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. URL https://arxiv.org/abs/1705.05363\n\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. \"#Exploration: A study of count-based exploration for deep reinforcement learning.\" In Advances in Neural Information Processing Systems (NIPS) 30, pages 2753\u20132762. 2017. URL https://arxiv.org/abs/1611.04717\n\nM. Sensoy, L. Kaplan, and M. Kandemir, \"Evidential deep learning to quantify classification\nuncertainty.\" arXiv preprint arXiv:1806.01768, 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written, but seems to get a lot of details wrong. The paper should also make clearer what the contributions are and what is already well known (like \"We, therefore, hypothesize that we can interpret these average standard deviations as a measure of the agreement/disagreement\", which already has been well evaluated). Although the extensive appendices make reproducibility appear very feasible, the paper has almost no novelty that would warrant publication.",
            "summary_of_the_review": "The paper does, in the eyes of this reviewer, not have enough novelty to warrant publication. While well written, the paper misrepresents some key aspects and does not yield a particularly actionable result. The reviewer therefore strongly argues for rejection.\n\nTo change this recommendation, the authors would have to convince the reviewer that there is significant novelty in the paper that has been overlooked in this review.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4795/Reviewer_rBt3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4795/Reviewer_rBt3"
        ]
    }
]