[
    {
        "id": "9esNHNM-k4",
        "original": null,
        "number": 1,
        "cdate": 1666601502573,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601502573,
        "tmdate": 1666601502573,
        "tddate": null,
        "forum": "XC_yGI-0j9",
        "replyto": "XC_yGI-0j9",
        "invitation": "ICLR.cc/2023/Conference/Paper5277/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a novel method for modeling neural correlations\nbased on sum-product networks. The idea is that inference of the\nstatistical structure of neural population activity is made easier by\nthe assumptions encoded in the choice of this type of network, which\nenforce a hierarchical structure of alternating mixtures and\nfactorizations. Within this class of models, the model structure is\nchosen with a procedure based on hierarchical clustering computed from\nan information metric. Once the model structure is fixed, model\nparameters are learned by EM. The paper introduces sum-product\nnetworks, describes the proposed method, and compares it with existing\nmethods on synthetic data as well as on neural recordings.",
            "strength_and_weaknesses": "## Strengths\n1. The proposed method is novel and highly original.\n2. The comparisons with synthetic and recorded data convincingly show\n   that at least in the examples analyzed here the method outperforms\n   some notable baselines.\n3. The paper also does a good job of introducing sum-product networks\n   to a potentially broad audience.\n\n## Weaknesses\nIn my view this is a generally solid paper; however, here are some\nsuggestions for strengthening the paper further by increasing coverage\nof related literature.\n1. When discussing comparisons to existing approaches to model neural\n   correlations, it could be appropriate to mention recent works on the\n   statistical complexity of maximum entropy models for binary data\n   (Beretta et al 2018, De Mulatier et al 2020). These have shown that\n   the complexity of a model depends not so much on the order of the\n   interaction but more on their mutual arrangement, and also that it\n   is possible to efficiently search through the space of \"simple\"\n   maximum entropy models. I wonder if anything is known about\n   potential efficient approximations of the Bayesian evidence of\n   sum-product networks that could be used in a formal model-selection\n   context.\n2. When discussing neural correlations from a neuroscience perspective\n   and its relationship with brain state, consider citing Cohen and\n   Maunsell 2009, which has shown that attention modulates pairwise\n   correlations in monkey visual cortex, and that this modulation of\n   population structure has substantial behavioral implications.\n3. Another class of approaches aimed at uncovering the statistical\n   structure of population data that seem worth mentioning is that\n   based on GLMs (Pillow et al 2008). Despite their limitations, these\n   methods scale well to large populations and are applicable to\n   complex experiments involving rapidly-varying external variables\n   (see e.g. Runyan et al 2017).\n4. Moreover, it would be great if the authors could add the K-pairwise\nmodel (Tkacik et al 2014) to their comparisons in Figure 3 and 4. That\nmodel, which is already cited in the present paper, is considered a\nparticularly powerful (while practical) higher-order extension of\npairwise maxent models and seems like a good candidate to benchmark\nagainst. However this is just a suggestion that I think could make the\npaper's message even stronger, not something necessary for acceptance.\n\n### Typos:\n- page 5, bottom: \"if and only OF all neurons are independent...\"\n  (of\u2192if)\n- page 7: \"The RBM performs better with SMILE size populations.\"\n  (smile\u2192small)\n- page 7: \"their average log-likelihood difference (...) increases\n  with \u03b3\". I think here \u03b3 should be replaced by \u03bb.\n- please check for consistency of the symbol used for \"milliseconds\"\n  (or seconds). Sometimes it's ms, sometimes msc, sometimes msec. The\n  correct SI abbreviation is ms (s for seconds).\n\n### References:\n\nBeretta, Alberto, Claudia Battistin, Cl\u00e9lia De Mulatier, Iacopo Mastromatteo, and Matteo Marsili. 2018. \u201cThe Stochastic Complexity of Spin Models: Are Pairwise Models Really Simple?\u201d Entropy 20 (10): 739. https://doi.org/10.3390/e20100739.\n\nMulatier, Cl\u00e9lia de, Paolo P. Mazza, and Matteo Marsili. 2021. \u201cStatistical Inference of Minimally Complex Models.\u201d arXiv. https://doi.org/10.48550/arXiv.2008.00520.\n\nCohen, Marlene R., and John H. R. Maunsell. 2009. \u201cAttention Improves\nPerformance Primarily by Reducing Interneuronal Correlations.\u201d Nature\nNeuroscience 12 (12): 1594\u20131600. https://doi.org/10.1038/nn.2439.\n\nPillow, Jonathan W., Jonathon Shlens, Liam Paninski, Alexander Sher, Alan M. Litke, E. J. Chichilnisky, and Eero P. Simoncelli. 2008. \u201cSpatio-Temporal Correlations and Visual Signalling in a Complete Neuronal Population.\u201d Nature 454 (7207): 995\u201399. https://doi.org/10.1038/nature07140.\n\nRunyan, Caroline A., Eugenio Piasini, Stefano Panzeri, and Christopher D. Harvey. 2017. \u201cDistinct Timescales of Population Coding across Cortex.\u201d Nature 548 (7665): 92\u201396. https://doi.org/10.1038/nature23020.\n\nTka\u010dik, Ga\u0161per, Olivier Marre, Dario Amodei, Elad Schneidman, William Bialek, and Michael J. Berry. 2014. \u201cSearching for Collective Behavior in a Large Network of Sensory Neurons.\u201d PLOS Computational Biology 10 (1): e1003408+. https://doi.org/10.1371/journal.pcbi.1003408.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, and the work is of high quality and\nnovel. The method seems explained at a sufficient level of detail.",
            "summary_of_the_review": "This is a good paper. The proposed technique is novel and original,\nand is shown to be effective at capturing the structure of neural\npopulation activity. The theory is well explained and the concrete\nbenchmarks are convincing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5277/Reviewer_gQaa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5277/Reviewer_gQaa"
        ]
    },
    {
        "id": "3lBv_c6Ni9q",
        "original": null,
        "number": 2,
        "cdate": 1666626829771,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626829771,
        "tmdate": 1669322957163,
        "tddate": null,
        "forum": "XC_yGI-0j9",
        "replyto": "XC_yGI-0j9",
        "invitation": "ICLR.cc/2023/Conference/Paper5277/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes the use of Sum-Product Networks (SPNs) to study spike train recordings of populations of neurons. It compares SPNs against latent variable and energy-based methods on a synthetic problem: Correlations are estimated for a network of homogenous exponential integrate-and-fire neurons. It also applies SPNs to experimentally obtained neural recordings from the Allen Brain Observatory.",
            "strength_and_weaknesses": "**Strengths:** The use of SPNs for neural population analysis of spike train data is, to the best of my knowledge, a novel application. On problems and metrics considered in the paper, results overall show promising improvements over the baselines considered.\n\n**Weaknesses:** To me, the biggest weakness of the paper is its narrow empirical evaluation -- the authors only consider one example in which ground-truth is known. I would have hoped for more extensive evaluation on problems with synthetic data, in particular, considering heterogenous networks with known ground truth. In addition, I found a discussion of SPNs limitations for neural data analysis to be missing. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Novelty**: As stated above, to my knowledge, the paper explores a novel application of SPNs. SPNs have been developed in prior works, a technical novelty of this paper is the method for learning correlations in neural populations.\n\n**Reproducibility:** Training details for baseline methods are currently missing, e.g., it is unclear how RBMs were trained exactly. The authors write that the code will be made available upon publication; unfortunately it was not possible to look these details up.\n\n**Clarity**: For the most part, the paper is clearly written. \n\n**Minor comments:** Adding citations in brackets would improve readability. \"smile size\" -> \"small size\".",
            "summary_of_the_review": "The application of SPNs to neural population analysis is interesting and I am surprised this has not been explored previously. The results are promising, but empirical evaluation on synthetic examples with known ground truth is currently limited to a just a single problem. I think that a more thorough empirical analysis and a discussion of limitations would be critical for practioners considering application of SPNs to their data analysis problems.\n\n### Update after rebuttal\n\nI appreciate the addition of new experiments and updated my overall recommendation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5277/Reviewer_Ae2s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5277/Reviewer_Ae2s"
        ]
    },
    {
        "id": "l678qAzCkU",
        "original": null,
        "number": 3,
        "cdate": 1666661443209,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661443209,
        "tmdate": 1669530714752,
        "tddate": null,
        "forum": "XC_yGI-0j9",
        "replyto": "XC_yGI-0j9",
        "invitation": "ICLR.cc/2023/Conference/Paper5277/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Learning the correlation structure of neuron population is tuff. Usually, only pair-wise linear correlations can be measured. It is hard to compute higher-order correlations in the population. The authors design a Sum Product Network (SPN) to learn higher-order correlations of a neuron population. Overall, the paper is well written, however, I feel not convinced that the current model could have a larger application. ",
            "strength_and_weaknesses": "Strength: \nCompare to existing methods such as pairwise maximum entropy models, the SPN model is more computationally efficient and fits neuron spiking better. \nWeakness: \n1.\tThe interpretability of the current model is low.  \nThe authors show that the SPN models could use correlation information in predicting population neuron activities, as the current model performs better than the na\u00efve Bayesian model which does not model neuronal interactions. However, it is not clear to me how to interpret higher-order correlations using the current model. Which part of the model learns the higher-order correlation? Can we read out pairwise correlation from the model parameters? \n2.\tNot well-supported interpretation\nThe authors suggest that the delta_ll value is a measure of \u2018correlation\u2019 (Fig. 5), which I found not well supported. First of all, the relationship between delta_ll and higher-order/linear correlations is not well established. In particular at the physiological neuron population. Second, it is a very low-resolution measure as one value (delta_ll) is computed for one dataset while the biological physiology (eg. pupil dynamics) changes much faster. Third, the fact that a control model (MPF) does achieve similar results on the majority dataset makes me confused even more about this measurement.  \n3.\tModel is only tested with simple data\nThe simulation data seems to only have a simple correlation structure (eq. 3). The correlation of the simulated neuron population is generated through common input, with no synaptic interactions or coupling. Also, it seems the whole population is receiving the same common time-varying input, which would probably make the correlation structure super simple. Plus the correlation between the simulation set is pretty high (>0.1). In addition, the real data set is also a simple dataset: visual neuron response evoked by four drifting gratings. Such a dataset is expected to have a low-dimensional correlation structure. It is not clear to me whether the model performance on these simple datasets would translate easily to more broad applications. Plus, other existing approaches like GLM might work as well on these simple data. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. The quality of the work is marginal. The application of SPN models to fitting correlated neuronal data is novel. The data set used in the current study is open source. It is not clear to me whether the model of the current work would be released. ",
            "summary_of_the_review": "I feel excited reading the first half of the paper, as the authors set up the problem and direct the audience very well. However, the result section is a bit disappointing. The keyword \u2018higher order correlation\u2019 is repeatedly mentioned in the paper, however, only indirect measurement (delta_ll) is provided to suggest that the model learns the correlation. Without more information about the model and better validation of the model, it is really hard to generalize what has been presented in the paper. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5277/Reviewer_Lrts"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5277/Reviewer_Lrts"
        ]
    },
    {
        "id": "YNYulPMQawa",
        "original": null,
        "number": 4,
        "cdate": 1666737026852,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666737026852,
        "tmdate": 1669123818324,
        "tddate": null,
        "forum": "XC_yGI-0j9",
        "replyto": "XC_yGI-0j9",
        "invitation": "ICLR.cc/2023/Conference/Paper5277/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a method to model correlations between neurons in large populations which is based on sum-product networks (SPNs). They show that it outperforms two baselines (pairwise maximum entropy models and restricted Boltzmann machines) on a synthetic dataset and the Allen Brain Observatory neuropixels dataset.\n",
            "strength_and_weaknesses": "### Strengths\n\n + Estimating correlations between neurons from limited data is a long-standing, but not completely solved problem\n + Approach seems promising and novel (to my knowledge), at least within the application domain\n + Explicit reproducibility statement and promise of code being published\n\n\n### Weaknesses\n\n 1. Important baselines for modeling neuronal correlations missing\n 1. Relationship to prior work on SPNs somewhat unclear\n\n\n### 1. Baselines\n\nAlthough the two baselines chosen by the authors (PME & RBM) are sensible choices, several alternative methods are missing. For instance, the literature on noise correlations has shown that common mode or low-rank fluctuations are abundant. To model such effects, latent variable models have been developed and yield state of the art performance. It is not clear to me why the authors consider an RBM \"the best latent variable model\" (p.2) - at least Koster et al. (2014) don't show that. Examples for alternative latent variable models include Poisson Linear Dynamical Systems (Macke et al. NeurIPS 2014) or deep gamma dynamical systems (Guo et al. NeurIPS 2018). Also relevant: Pfau et al., NeurIPS 2013, Ramesh et al., NeurIPS 2019 Neuro-AI workshop or very simple baselines like exponential family PCA or Factor Analysis. As it stands, it is not possible to tell from the evidence presented whether or not the authors' model is competitive.\n\n\n### 2. Relationship to prior work on SPNs\n\nOn p.3 the authors comment that \"there exist few algorithms for structure learning Gens & Pedro (2013); Vergari et al. (2015),\" but they developed their own method. Why not use existing methods? How does the method developed here differ from existing methods and how does it compare in terms of performance? While I am not familiar with the literature on SPNs, the present paper seems to mostly do knowledge transfer from other fields and apply an existing method in a new context. It doesn't become clear, however, where the authors make novel contributions because existing approaches from other fields are not applicable and need to be modified, and what these modifications are.\n\n\n### Other\n\n- On p.7 the authors comment that their \"model's main target is to capture the largest correlations with its available resources (free parameters).\" Why is this a reasonable thing to do in a model where all pairs are expected to have the same correlations due to the common input?\n\n- Related to the previous point, it is not clear to me why in a model where all correlations are driven by a single common input, the authors model should outperform an RBM by two orders of magnitude. What's the mechanism of this huge improvement? In such a simple data-generating process, I would expect any model with a single common mode to perform quite well. What additional inductive bias does the authors' model have that other baselines don't have?\n\n- I could not follow the description of how the authors partition the data on the bottom of p.7 / top of p.8. What does it mean to \"combine[d] the data of 5 consequent presentations with the same direction\"? How does this \"keep the state of the animal [...] as unchanged as possible\"? Why does this processing result in 15 blocks? Blocks of what? Relatedly, I couldn't follow the bottom of p.8 and what exactly Fig. 4, left, shows (e.g. what's the difference between \"SPN\" and \"SPN CV\"?)\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nDespite being very familiar with the topic of modeling neuronal correlations, I had a fairly hard time following the paper, mostly because I'm not very familiar with SPNs and the paper didn't guide me through the material in an intuitive way. \n\n### Quality\nI have no doubts that the modeling is technically solid, but I find it hard to conclude whether the approach is indeed as strong as claimed by the authors because of the missing comparisons with alternative strong approaches.\n\n### Novelty\nJudging novelty is somewhat difficult, mostly because I'm not familiar with the SPN framework, and neither the limitations of existing work in that direction nor how the authors overcome them are clearly discussed in the paper.\n\n### Reproducibility\nOn the positive side, the paper contains an explicit reproducibility statement and promises to published the code.\n",
            "summary_of_the_review": "Potentially interesting and novel (within application domain) approach, whose performance is somewhat difficult to judge due to limited comparisons with existing work.\n\n### [Update after rebuttal]\n\nThe authors have included an additional baseline and clarified their contributions w.r.t. SPNs. I now support the paper's publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5277/Reviewer_S4f9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5277/Reviewer_S4f9"
        ]
    }
]