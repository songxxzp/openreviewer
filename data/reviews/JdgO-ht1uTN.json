[
    {
        "id": "45zTdSbjyTP",
        "original": null,
        "number": 1,
        "cdate": 1666636721955,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636721955,
        "tmdate": 1666636721955,
        "tddate": null,
        "forum": "JdgO-ht1uTN",
        "replyto": "JdgO-ht1uTN",
        "invitation": "ICLR.cc/2023/Conference/Paper152/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper is well motivated, citing the rather broad literature on learning logical and probabilistic rules for KG completion, and proposing a method that can learn non-chain-like rules. The method makes sense. They show good results on several KG completion datasets, comparing against relevant baselines.",
            "strength_and_weaknesses": "Love the examples of logical rules discovered from the family dataset. These kind of examples are great for getting a sense of the method. Maybe in the supplementary material, a diagram of the networks that express some of those rules could be helpful to see.\n\nThe description of the model itself could due with being a little more computationally grounded. The probability / alpha / beta parameters are constrained to the simplex, which presumably is accomplished with a softmax. Being more explicit about this could be helpful.\n\nMore importantly, listing the entire # of parameters each method uses for each of the datasets would be helpful for comparison.\n\nThere is a lot of work on soft logic, it would be nice to have a bit more detailed literature review here.  While the distinction between horn clauses and the richer clauses you learn here is clear, there is certainly work outside of KG completion that learns more complex probabilistic rules, e.g. graphical model structure learning, probabilistic soft logic, natural logic, Markov logic networks, etc.\n\nWhen combining with TransE, do you retrain your own TransE for the baseline for apples-to-apples? The original results in the TransE paper are quite a bit lower than what you can get with a well trained TransE using modern methods.\n\nNits:\n\n- For \"Embedding-Based Neural Graph Reasoning\", could cite methods based on box embeddings (query2box, BoxE, etc)\n\n- 5.1 \u201cknowldge graph completion\u201d should be \u201cknowledge graph completion\u201d",
            "clarity,_quality,_novelty_and_reproducibility": "There are some nits with the presentation that I note above. The novelty in the context of knowledge graph completion is there, though the paper could benefit from more literature review in e.g. graphical models. Generally the paper is well written, and details for reproducibility are included. ",
            "summary_of_the_review": "The paper presents a clearly motivated model for knowledge graph reasoning that solves problems with existing methods and compares against relevant baselines. The examples of learned rules are greatly appreciated, and the experimental analysis is thorough. The KG completion community would benefit from this paper\u2019s publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper152/Reviewer_i7dT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper152/Reviewer_i7dT"
        ]
    },
    {
        "id": "8nO7TXsCf8Y",
        "original": null,
        "number": 2,
        "cdate": 1666681374579,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681374579,
        "tmdate": 1666681374579,
        "tddate": null,
        "forum": "JdgO-ht1uTN",
        "replyto": "JdgO-ht1uTN",
        "invitation": "ICLR.cc/2023/Conference/Paper152/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for learning complex logical rules considering semantics hidden in subgraphs rather than simple chain-like rules. The authors conduct extensive experiments on both large KG datasets and relatively small statistical rule learning datasets. ",
            "strength_and_weaknesses": "### Strength\n1. The method can learn logical rules more complex than traditional chain rules. \n2. The rules learned by this method are interesting (table 3). \n\n### Weakness\n1. The novelty seems limited considering the existing works NLIL[1], which is highly-related but missing in this paper. NLIL[1] is a related work proposing tree-like rules, which needs to be analyzed and compared. (Although the defined tree-like rules are somehow diffferent)\n2. In page 5, the physical meanings of the above equations about $\\mathbf{v}$ require more explanations. It the value in $\\mathbf{v}$ always in (0, 1) (even after summation and multiplication)? Otherwise $1- \\mathbf{v}$ would be weird. \n3. As mentioned in paper, $p$, $\\lambda$, etc. are learnable parameters. I wonder whether the authors have considered use some model to generate such parameters? For example, DRUM use RNN to generate the weight $\\alpha$. \n\n[1] Yang Y, Song L. Learn to Explain Efficiently via Neural Logic Inductive Learning[C]//International Conference on Learning Representations. 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this paper is good and codes are provided. ",
            "summary_of_the_review": "This paper is somehow interesting, but considering key related works are missing, and the novelty may be weakened compared to the authors claim. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper152/Reviewer_F1BJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper152/Reviewer_F1BJ"
        ]
    },
    {
        "id": "8pDhgj59O7V",
        "original": null,
        "number": 3,
        "cdate": 1666898082577,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666898082577,
        "tmdate": 1666898082577,
        "tddate": null,
        "forum": "JdgO-ht1uTN",
        "replyto": "JdgO-ht1uTN",
        "invitation": "ICLR.cc/2023/Conference/Paper152/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes LERP (Logical Entity RePresentation), a model that\nuses logical rule learning, but which embeds information about the\nobjects represented by the logical variables in the form of a vector\nof logical functions.",
            "strength_and_weaknesses": "Pros:\n+ It explores a new alternative to learn probabilistic logical rules\n+ Paper is well written and organized\n\nCons:\n- Related work on logic rule learning does not cite many important\nreferences about the main method for learning first order rules:\ninductive logic programming. The same happens with the related work on\nProbabilistic logical rule learning where authors omit refs to\nProbLog, CLPB(n), SlipCover, ProbFoil, SKiLL, among others.\nThere are also several works that are neurosymbolic like RNN, and\nothers which try to capture the neuronal network structure from the\nrelations in the data (e.g., Kaur et al. and Sourek et al).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good\nNovelty: minor contribution\nReproducibility: very good",
            "summary_of_the_review": "This paper proposes LERP (Logical Entity RePresentation), a model that\nuses logical rule learning, but which embeds information about the\nobjects represented by the logical variables in the form of a vector\nof logical functions.\n\nRelated work on logic rule learning does not cite many important\nreferences about the main method for learning first order rules:\ninductive logic programming. The same happens with the related work on\nProbabilistic logical rule learning where authors omit refs to\nProbLog, CLPB(n), SlipCover, ProbFoil, SKiLL, among others.\n\nThere are also several works that are neurosymbolic like RNN, and\nothers which try to capture the neuronal network structure from the\nrelations in the data (e.g., Kaur et al. and Sourek et al).\n\nThe work looks quite interesting as it presents an alternative way and\noptimization to find interpretable models. Just out of curiosity,\nauthors say that RNNLogic uses 100 to 200 rules and DRUM uses 1 to\n4. I don't know if I understood that well. ILP systems such as Aleph\nor Foil can learn thousands or millions of rules in a decente amount\nof time. How much time does your method take to learn the\nprobabilistic logical rules? The search for subgraphs for entities\n(logical variables) is naturally implemented in ILP systems. It would\nbe interesting to compare the probabilistic logical rule learning\nmethod used in this work with a probabilistic ILP system like, for\nexample, ProbFoil.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper152/Reviewer_N1P8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper152/Reviewer_N1P8"
        ]
    },
    {
        "id": "aj9gdZEONm",
        "original": null,
        "number": 4,
        "cdate": 1666900274670,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666900274670,
        "tmdate": 1666900274670,
        "tddate": null,
        "forum": "JdgO-ht1uTN",
        "replyto": "JdgO-ht1uTN",
        "invitation": "ICLR.cc/2023/Conference/Paper152/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes LERP, a differentiable ILP method that mines FOL rules from knowledge graphs. Compared to the prior backward-chaining methods which only learn chain-like paths in the graph, the authors propose to learn more expressive family of rules by considering the local subgraphs along the main path. In the experiment, LERP is evaluated with several ILP and graph embedding methods and shows better performance and interpretability.",
            "strength_and_weaknesses": "Strength \n- Approaches an important problem of learning more expressive FOL rules with ILP\n- LERP learns a more expressive family of rules than prior ILP methods.\n\nWeaknesses\n- Presentation should be largely improved. The current draft, especially section 4, is difficult to follow and misses some important technical details.\n- The learnable rule family seems to be still far from being considered as local subgraphs.",
            "clarity,_quality,_novelty_and_reproducibility": "### Novelty\n\nThis work approaches an important problem in ILP, which is learning more expressive rules from the KGs. The authors propose to extend the standard backward-chaining methods, which only learn chain-like rules, by also learning the \"branches\" rooted from the intermediate entities in the main path. While this family of rules is not as general as \"local subgraphs\" as has been claimed by the authors, it is already an interesting extension.\n\n\n### Quality\n\nDue to presentation issues, I'm unable to assess the proposed method in detail, but the general methodology is sensible and computationally feasible. Some of my concerns are as follows:\n\nThe complexity of adjacency matrix multiplication should be O(Kn^3) instead of O(Kn^2)\n\nIf my understanding is correct (see clarity), while one can set depth $T$ and control the complexity of the functions in Fig (2), the number of hops permitted for each $L(z_i)$ is always set to 1. This means the LERP effectively learns a \"branched\" chain-like path, where the \"main\" path is of length K, and the branches of length 1. This is rather limited to be considered as \"local subgraphs\". Nevertheless, it is still a nice extension to the original chain-like ones, but this aspect should be stated clearly in the paper.\n\n\nThe 6 types of functions introduced on page 5 require more justifications as they are not very intuitive. For example, it is unclear what $v_{i,j}$ represents: judging from the chaining function, $v_{i,j}$ seems to be the random walk feature vector, but I'm not sure why we set $v_{i,j}$ to be 1 for the first column. Also, if $v_{i,j}$ is the random walk vector that contains counts of unique paths to the entities then why one should perform conjunction and disjunction on it?\n\n\n### Clarity\n\nThe presentation, especially section 4, should be largely improved. Right now\n\nI'm confused about the notion of \"columns\" referred to in Fig 2 and section 4.1.\n- Why does one need a matrix of $f_{i,j}$? How is it related to the \"tree-like\" function in Def 1?\n- The ranges of $i, j$ are confusing. I suppose they are $m$ and $T$? If so, how do you pick values for $m,T$ and why?\n- It is unclear why the first columns are set to true; what do the entries of the first column represent? Is there any graphical interpretation of Fig 2? Or in other words, what subgraph does Fig 3 represent?\n- Does the second column always use the chaining function? \n\n\n\nNotation issues:\n- $w_i$ used in Def 1 without definition. How is it different from $z_{i,j}$?\n- What is $T$ in \"There are a total of T+ 1columns of intermediate functions\"?\n",
            "summary_of_the_review": "While this work presents an interesting extension to the existing ILP methods, there are some serious presentation issues to be fixed. At this stage, I cannot recommend for acceptance but I will be happy to raise my score if the authors could address my concerns and I'll be looking forward to reading the revised draft.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper152/Reviewer_dYXp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper152/Reviewer_dYXp"
        ]
    }
]