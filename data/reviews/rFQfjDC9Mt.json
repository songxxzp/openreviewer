[
    {
        "id": "eVLV-RVVsa",
        "original": null,
        "number": 1,
        "cdate": 1665711785005,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665711785005,
        "tmdate": 1665711785005,
        "tddate": null,
        "forum": "rFQfjDC9Mt",
        "replyto": "rFQfjDC9Mt",
        "invitation": "ICLR.cc/2023/Conference/Paper2431/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a backdoor attack which does not require modifying the inputs and only requires modifying labels.  The way it operates is by trigger selection where a trigger is an object already in the training images.  The work also tests against several defenses.",
            "strength_and_weaknesses": "I generally like the idea of this paper, and the execution is decent.\n\nCan this really be called a backdoor attack?  Conventional backdoor attacks focus on triggers selected by the attacker that can be placed on an image or on a physical object (e.g. a sticker on a stop sign or a pair of sunglasses).  What is called a trigger in this paper is not what is typically considered a trigger.  As is, this paper is essentially the same as standard backdoor attacks except they just consider an object already in the image to be the trigger, which in some ways makes the paper more akin to other data poisoning attacks (where there are other attacks that only modify labels).\n\nAlong the same lines, this paper cites some related work as backdoor attacks which are not actually about backdoor attacks.  For example, the authors cite Shafahi et al. as a backdoor attack which it is not.  Please correct this issue.\n\nExisting backdoor literature focuses largely on classification tasks, but some of these backdoor attack methods might be directly applicable to the poisoned labels only setting as well.  For example, Sleeper Agent backdoor attack is a gradient-based backdoor attack, and one could simply compute gradients with respect to the annotations.\n\nIt would be good to mention label flipping data poisoning attacks in the literature review.  They are not competing with the proposed method, so they are not required as baselines.  You can find some of them in the survey paper, \u201cDataset security for machine learning: Data poisoning, backdoor attacks, and defenses\u201d.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear enough.",
            "summary_of_the_review": "I like the concept behind this paper.  I think the description and the marketing as well as the related works could use some improvement as discussed above.  I lean towards acceptance of this paper even though some things in the paper are technically incorrect, for example in the literature review.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2431/Reviewer_xF4p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2431/Reviewer_xF4p"
        ]
    },
    {
        "id": "AZw4h-jgIFS",
        "original": null,
        "number": 2,
        "cdate": 1666540977203,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666540977203,
        "tmdate": 1666540977203,
        "tddate": null,
        "forum": "rFQfjDC9Mt",
        "replyto": "rFQfjDC9Mt",
        "invitation": "ICLR.cc/2023/Conference/Paper2431/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes the first clean-image backdoor attack, which only poisons the training labels without touching the training samples. Experimental results demonstrate that the proposed method can achieve an attack success rate of up to 98.2% on the images containing the trigger pattern.",
            "strength_and_weaknesses": "Strength:\n1. Generally speaking, this work uses a new direction to generate backdoor attacks. It is different from previous works. The multi-label learning task is also a new task for the poisoning attack.\n2. The results are good. Besides, the proposed clean-image backdoor can evade existing state-of-the-art defenses\n3. It also proposes three target label selection strategies to achieve different goals.\n\nWeaknesses:\n1. The background seems to be too simple. What are the differences between the poisoning attack and the backdoor attack\uff1f\n2. Maybe you should highlight your contributions in the introduction. Besides, please introduce your weaknesses in the paper.\n3. Compared with previous works, maybe changing the labels is easier to be noticed.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is original, but the clarity is not so clear. The quality is overall up borderline.",
            "summary_of_the_review": "The paper designs a novel clean-image backdoor attack, which manipulates training annotations only and keeps the training inputs unchanged. Specifically, it designs a trigger pattern exploration mechanism to analyze the category distribution in a multi-label training dataset. The paper is interesting and original. However, maybe it is easier to be noticed compared with previous works.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2431/Reviewer_WM62"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2431/Reviewer_WM62"
        ]
    },
    {
        "id": "vJ3YVBOkl-x",
        "original": null,
        "number": 3,
        "cdate": 1667136438816,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667136438816,
        "tmdate": 1669355268491,
        "tddate": null,
        "forum": "rFQfjDC9Mt",
        "replyto": "rFQfjDC9Mt",
        "invitation": "ICLR.cc/2023/Conference/Paper2431/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a data poisoning attack for a multi-label model that only poisons\nthe training labels without modifying the training images. This work presents three\ntypes of attack, object disappearing, appearing, and misclassification. Extensive\nexperiments show that the method works on various datasets and bypasses previous\ndefense strategies.",
            "strength_and_weaknesses": "Strengths:\n\nThe authors consider various scenarios of attack evaluation. However, it is still a lack of justification that the problem setting is significant enough.\n\nWeaknesses:\n\n1. In supervised learning, labels are critical, which makes the author's goal trivial. Hence, the novelty of this work is limited. The clean label setting will be more astonishing. The authors also mention that malicious behaviors are hard to be distinguished from common labeling mistakes.\n\n2. Under the same threat model (third-party service provider), NeuRIPS 2021 \"Manipulating SGD with Data Ordering Attacks\" provide less capability to attackers (only batch reorder, no modification of label or image). This work needs to cite and compared.\n\nQuestions:\n\nWhy is this attack a targeted attack? Given an arbitrary image, how can the attacker specify the model's behavior during inference?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing and organization of this work need to improve. Moreover, the contribution of this work sounds pretty minor.",
            "summary_of_the_review": "I have two major concerns.\n\n1. The goal of this work is easy to achieve during training.\n\n2. The trigger in this work is not the conventional one. Hence, the attacker cannot use this trigger to control the behavior of the model during inference.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2431/Reviewer_XWsQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2431/Reviewer_XWsQ"
        ]
    },
    {
        "id": "wdjVbvrp1R",
        "original": null,
        "number": 4,
        "cdate": 1667444660744,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667444660744,
        "tmdate": 1667444660744,
        "tddate": null,
        "forum": "rFQfjDC9Mt",
        "replyto": "rFQfjDC9Mt",
        "invitation": "ICLR.cc/2023/Conference/Paper2431/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel backdoor attack, clean image backdoor that only modifies the label information and does not modify the training samples.This paper targets a multi-label learning task. The trigger is the specific annotation of training samples consisting of a specific set of classes. The evaluation is done on object detection tasks. The evaluation shows that the attack can achieve 98% attack success rate with similar performance on clean data. The evaluation also shows it can bypass existing detection methods.",
            "strength_and_weaknesses": "Pros:\n+ This paper proposes a new angle for backdoor attacks where the attackers poison the image labeling process. Given the fact that a lot of data labeling work is being out-sourced, I think the threat model is realistic.\n\n+ This proposed attack shows a high attack success rate while has high clean performance.\n\nCons:\n- The limitation of triggers. This paper uses some classes combination of the existing training images. I think there are many class combinations that are not covered in the coco training data. Is it possible for the attacker to use external images or synthesized images to create any trigger (any class combinations) that he wants?\n\n- All the evaluated attacks are done on object detection. Is it possible to attack another machine learning task? Only being able to attack object detection makes this attack a bit constrained.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear to read. The perspective to attack only the labeling process is novel. I do not know if the details privided.in this paper is enough for reproducing its experiment's result.",
            "summary_of_the_review": "The perspective to attack only the labeling process is novel and evaluation shows this attack has good performance. However, this paper has some limitations on the trigger selection and the evaluated tasks. I recommend weak acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2431/Reviewer_3t9C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2431/Reviewer_3t9C"
        ]
    }
]