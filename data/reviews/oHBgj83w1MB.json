[
    {
        "id": "KnhTRQ--Ewa",
        "original": null,
        "number": 1,
        "cdate": 1665862421081,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665862421081,
        "tmdate": 1665862421081,
        "tddate": null,
        "forum": "oHBgj83w1MB",
        "replyto": "oHBgj83w1MB",
        "invitation": "ICLR.cc/2023/Conference/Paper2097/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents Causal Proxy Models, which are trained to mimic outputs of a given model for original inputs as well as for alternative (counterfactual) versions of those inputs that differ based on modification of a particular (sentiment) feature. They use data from the CEBaB dataset for creating human-annotated and sampled input pairs, and train their models on these data. The models involve either a special token indicating change of a particular feature, or representation intervention on representations trained to store causally-relevant information for a given feature. The authors report that their methods achieve state-of-the-art performance on the CEBaB dataset. They also show that the models can explain their own behavior, have performance on par with the original models, and that the representations trained to capture certain features do show reasonable feature attributions.",
            "strength_and_weaknesses": "Strengths: The paper presents an interesting set of methods, which are explained in the paper reasonably clearly, and which show state-of-the-art performance on the CEBaB dataset. The authors include a couple of follow-up analyses that help to flesh out the overall picture of the methods' outcomes and potential impact.\n\nWeaknesses: My main concern with the paper is that the methods feel very narrowly focused on improving on the CEBaB dataset, which seems like an interesting task, but which casts explanation in a very particular way. So there are two subparts to this concern. First, the way the proxy models are trained feels very specifically targeted toward improving on CEBaB -- aka, leveraging the fact that the task is focused on predicting how models will behave on inputs edited for particular sentiment features -- so although the methods are interesting, I'm not sure how surprising I find it that they were able to improve over other methods, and I'm also skeptical about how generalizable this method would be for explanation on other types of domains/tasks/explainability settings.\n\nThe second subpart of the concern relates to the last point: while the authors are targeting an existing benchmark that has been introduced for explainability, and they show improvement on that benchmark, it's just not clear to me from the paper in what way their method is actually resulting in interpretable/usable insights for explaining the behaviors of the models being approximated. What they show is that their methods allow them to use feature tokens or representational interventions to predict how target model predictions will change if an input has been changed along a particular sentiment feature dimension. But it's not clear to be how this help us to understand what strategies the models are using, what features are influential for prediction, what potential heuristics might cause untrustworthy behavior, etc -- the types of things that I expect from an explanation method. I'm happy to believe that there is more insight here than was made clear, but at the moment I don't feel the paper makes a strong argument for how it is providing insights into the target models.",
            "clarity,_quality,_novelty_and_reproducibility": "On the whole I think the paper was fairly clear -- however, as someone slightly outside this area I did have trouble understanding the motivations and methods on the first pass. One thing that I think would help would be more linguistic examples illustrating the kinds of inputs and counterfactual edits are being used, to anchor the descriptions in the specific phenomena being targeted.\n\nI think the quality and reproducibility of the work are decent, and I don't know any other work similar enough to indicate an issue in novelty. My main issue, as outlined above, is that the methods feel very narrowly applicable, and I'm not seeing a clear indication that they produce the kinds of interpretable insights that I would expect from an explainability method.",
            "summary_of_the_review": "The proposed methods are interesting, and show improvement on the specific benchmark being used. However, I have concerns about the narrowness in applicability of the method (not clear it's useful beyond that specific benchmark) and I'm not clear how much insight it provides as a contribution to explainability.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2097/Reviewer_aAEJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2097/Reviewer_aAEJ"
        ]
    },
    {
        "id": "NysZMa8kgfI",
        "original": null,
        "number": 2,
        "cdate": 1666512084312,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666512084312,
        "tmdate": 1666652538507,
        "tddate": null,
        "forum": "oHBgj83w1MB",
        "replyto": "oHBgj83w1MB",
        "invitation": "ICLR.cc/2023/Conference/Paper2097/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method called Causal Proxy Model (CPM) for explaining a black-box model $\\mathcal{N}$ that makes use of available counterfactual training data.",
            "strength_and_weaknesses": "### Strengths\n1) I think the main strength of the paper is that it is well-written and easy to follow.\n2) Learning a model that enables the explanation of a pretrained black-box model has a major practical impact. However, the proposed method is limited to a particular scenario in which counterfactual data is available.\n\n### Weaknesses\nI) About the motivation and idea:\n1) In the Abstract and Introduction, the authors argue that *\u201cthe fundamental problem of causal inference is that we hardly observe the counterfactual inputs\u201d* but the proposed method, by design, assumes (approximate) counterfactuals are available. This limits the practical application of the proposed method to certain datasets (e.g., the CEBaB dataset in the paper) and cases.\n2) I do not see any discussion about the clear drawbacks of existing methods for explanation in comparison with the proposed method.\n\nII) About the proposed method:\n1) In my opinion, the proposed method is not very novel. Given counterfactual data $\\tilde{x}^{C_i \\leftarrow c'}\\_{u,v}$,  it is quite straightforward to think of matching $\\tilde{x}^{C_i \\leftarrow c\u2019}\\_{u,v}$ with $x_{u,v}; t\\_{C_i \\leftarrow c\u2019}$ for convenient intervention.\n2) A limitation of CPM is that it does not account for the stochasticity of $\\tilde{x}^{C_i \\leftarrow c\u2019}\\_{u,v}$. $x_{u,v}; t\\_{C_i \\leftarrow c\u2019}$ only yield one value but the corresponding counterfactual text $\\tilde{x}^{C_i \\leftarrow c\u2019}\\_{u,v}$ can be abundant.\n\nIII) About the presentation:\n1) The overall writing is good but the descriptions of many (mathematical) terms in the paper are not clear, causing difficulty in understanding the method. I would like to discuss some of them below:\n- I don\u2019t really understand what is the *\u201cdescription of a concept intervention $C_i \\leftarrow c\u2019$\u201d*. Does it mean $C_i$ will take one of three categorical values {negative, unknown, positive} or a text that describes the concept $C_i$? Can we just call $C_i\\leftarrow c\u2019$ a concept intervention for simplicity?\n- What is the token $t\\_{C_i \\leftarrow c\u2019}$ used in the paper? Would the authors please provide some concrete examples of this variable as I could not find such thing in the paper? Is it possible to just write $t\\_{c\u2019}$ instead of $t\\_{C_i \\leftarrow c\u2019}$?\n- The objective $\\mathcal{L}\\_{\\text{mimic}}$ seems to be shared between $\\text{CPM}\\_\\text{IN}$ and $\\text{CPM}_\\text{HI}$ and should be put above the paragraph that describe $\\text{CPM}\\_\\text{IN}$. The authors should also write an overall loss, e.g., $\\mathcal{L} = \\lambda_1 \\mathcal{L}\\_{\\text{mimic}} + \\lambda\\_2 \\mathcal{L}\\_{\\text{counterfact}}$ where $\\mathcal{L}\\_{\\text{counterfact}}$ can be either $\\mathcal{L}\\_{\\text{IN}}$ or $\\mathcal{L}\\_{\\text{HI}}$\n2) The Structural Causal Model (SCM) described in Figure. 1a does not really make sense to me. I cannot figure out what are treatment, covariate, and outcome from this figure. From what shown in the figure, I assume $C_1$, \u2026, $C_k$ are different treatments, X is an outcome, and no covariate. But it turns out X is actually a covariate, and the outcome is a 5-star sentiment score Y which does not appear in the causal graph. I suggest the authors to redraw the SCM to make it correct, adding Y and removing U, V as they do not have any contribution to the model.\n3) For the S-learner baseline described in Section 4.3, I don\u2019t really understand how the authors actually perform intervention with the output of the BERT model $\\mathcal{B}$. In other words, how is  $\\mathcal{B}(x_{u, v})_{C_i \\leftarrow c\u2019}$ implemented?\n\nIV) About experiments:\n1) Please provide references for methods used in $\\text{BEST}_{\\text{CEBaB}}$.\n2) S-learner considered in the paper seems not a strong baseline since the intervention is only done for the output of the BERT model on the FACTUAL text $x_{u, v}$ which means \u201clate fusion\u201d between covariate $X$ and treatment $C_i$. Meanwhile, CPM fuses $X$ and $C_i$ early, which allows the method to model the interaction between $C_i$ and $X$ better.\n3) The authors should compare their method with other causal inference methods such as X-learner [1], TARNet [2], and Balancing Linear Regression [3] in a fairer setting. The authors should also compare their method with other methods for explanation discussed in the paper. \n4) In this paper, the authors consider $\\mathcal{N}$ as a black-box model but in the experiment, they initialize $\\mathcal{P}$ from the weights of a pretrained $\\mathcal{N}$. This means $\\mathcal{N}$ is no longer black-box anymore. I suggest the authors to do experiments with $\\mathcal{P}$ different from $\\mathcal{N}$ (e.g., $\\mathcal{N}$ is BERT and $\\mathcal{P}$ is GPT-2 or LSTM). This will make the method more convincing.\n\n[1] Metalearners for estimating heterogeneous treatment effects using machine learning, Kunzel et al., PNAS-2019\n\n[2] Estimating individual treatment effect: generalization bounds and algorithms, Shalit et al., ICML-2017\n\n[3] Learning Representations for Counterfactual Inference, Johansson et al., ICML-2016",
            "clarity,_quality,_novelty_and_reproducibility": "Please check what I have discussed above.",
            "summary_of_the_review": "This paper is a well-written paper. However, its problem is not very significant and limited to certain cases, and its proposed solution is quite straightforward. Thus, I think the paper is below the acceptance bar of ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2097/Reviewer_MZ4t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2097/Reviewer_MZ4t"
        ]
    },
    {
        "id": "TWnatd9ffGy",
        "original": null,
        "number": 3,
        "cdate": 1666582929781,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582929781,
        "tmdate": 1666582929781,
        "tddate": null,
        "forum": "oHBgj83w1MB",
        "replyto": "oHBgj83w1MB",
        "invitation": "ICLR.cc/2023/Conference/Paper2097/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a robust explanation method for NLP tasks using Causal Proxy Model (CPM). Given a black-box model to be explained, the CPM tries to simulate both the factual and counterfactual performance of that model. With CPM, one can have 1) an explanation of the black-box model, 2) comparable factual performance and 3) learn concept-level representation in the hidden code. The model is evaluated on the CEBaB benchmark. ",
            "strength_and_weaknesses": "Strength:\n1) The paper proposes to use CPM to approximate the factual and counterfactual performance of a black-box model, which is an interesting idea. \n2) The learned CPM can perform comparably to the black-box model in terms of factual performance. \n3) CPM_HI generates a conceptual level of understanding of the black-box model. \n4) The paper provides the insight to directly use CPM as the deployed model and explainer. \n\nWeaknesses:\n1) The causal graph in Figure 1a is particularly for CEBaB data set. Does the method work without knowing the causal graph? My understanding is that one has to know the causal graph so as to use CPM. This can be its limitation. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and the idea is clearly stated. However, I am not an expert in NLP area. Thus, I don't think it is fair to evaluate its novelty.  ",
            "summary_of_the_review": "Overall, the paper is well-written and the story follows naturally. The proposed method seems interesting and novel to me. In particular, the paper builds the insight that CPM can be directly used in the replacement of a black-box model. However, one potential limitation is that it builds on a known causal graph, which is usually unavailable in the world. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2097/Reviewer_LSZt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2097/Reviewer_LSZt"
        ]
    },
    {
        "id": "P10t85doJT1",
        "original": null,
        "number": 4,
        "cdate": 1666657341721,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657341721,
        "tmdate": 1669773857195,
        "tddate": null,
        "forum": "oHBgj83w1MB",
        "replyto": "oHBgj83w1MB",
        "invitation": "ICLR.cc/2023/Conference/Paper2097/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method to fine-tune a black-box model that is counterfactual consistent.\nFirst, they obtain the true counterfactual input via either: (1) from the dataset explicitly by a human annotator or (2) by sampling the nearest sentence from the dataset using intervention to a specific concept. Next, they employ the Causal Proxy Model (CPM) -- which is a copy of the Black-box to (1) mimic the BlackBox for the factual input, (2) create a neural representation to be intervened to approximate the BlackBox for the counterfactual input. While simulating the counterfactuals, they use the true counterfactual text as input to predict the counterfactual output from the BlackBox. However, to mimic the counterfactual output from the BlackBox, they use either of the following strategies: (1) append a token to the factual text using the intervened concept to approximate the true counterfactual input and train CPM, (2) employing interchange intervention training (IIT) by Geiger et al. (2022) to intervene at the hidden neurons of CPM that get affected due to the intervention of the concepts.\n",
            "strength_and_weaknesses": "# Strength\n1. Having a model that is counterfactually consistent is a reasonable goal when the model us to interact with humans. \n 2. the experiments demonstrated that the proposed CPM performs better than the baselines for various architectures.\n 3. Their experiments also showcased that the CPM can be consistent with the BlackBox and can even replace the BlackBox for estimating factual and counterfactual outcomes.\n\n# Weakness \n1. My biggest concern with this paper is motivation and how it is presented. The overall aim is to have counterfactual consistency, meaning the output should change reasonably by changing the counterfactual concept. It is not clear to this reviewer why they call their method an explanation. The so-called CPM is a fine-tuned version of the original model (with the same arch.), hence using the integrated gradient to explain CPM is not the same as explaining the original model.  Overall, I do not agree with the authors that this is an explanation method. They did not compare with some existing explanation strategies like TCAV, demonstrating how these counterfactual explanations serve a better purpose. They should rename the paper if the original aim is to predict good counterfactuals.\n\n 2. There are unnecessary notations in the paper that can be easily simplified. For example, they only use $U$ and $V$ in the causal diagram in figure 1, but they hardly use them in the paper. $x_{u, v}$ can simply be $x$. All the concept level interventions can be shown using $x^{C_i = \\hat{c}}$ and with similar form. They should mention this clearly in the paper if they have any specific reason to show $U$ and $V$. Beyond using the notion of counterfactual, causality also doe not play a role in this paper. Also, the name *proxy* is used for a different thing in causality literature, so it is quite confusing. \n\n 3. For figure 1-d) $\\mathcal{L}_{IN}$: It seems that the authors append a token to the factual text using the intervened concept to approximate the true counterfactual input. This concatenation can be problematic because of the conflicting concept in the input text and the generated token from the intervention. It is not clear to me why appending the counterfactual token is a good idea. If the authors are not appending a token and doing something else, it is not clear from the paper.\n\n 4. The notation in equation 8 is ambiguous. The authors should specify what does $C_i \\leftarrow c'$ indicate? If they meant intervened concept $C_i$ is appended to $\\mathcal{B}(x_{u,v})$, they should use consistent notation in equation (2) or figure 1-d. Do they mean to append using a special concept token?\n\n 5. The authors should clarify how they intervene in the intermediate layers of the transformer-based models. Did they intervene at the concept level in the CLS token of the intermediate layers? This is not clear from the paper.\n\n6. One of my main criticism is their choice of evaluation metric. Yes, it makes sense to ensure that the fine-tuned model does not hurt the performance of the original model (and there are experiments to show that in the paper) however, correct estimation of the ITE neither proves a better explanation nor a more useful fine-tuned model. So it is not clear to me why they chose to report ITE.\n\n\n 7. In many places (like table 4), they term the original BlackBox as 'Finetuned'. Can they specify the reason?\n\n 8. In fig 4, they should use a color bar.",
            "clarity,_quality,_novelty_and_reproducibility": "The methodology and experiments proposed in this paper, are not well geared toward X-AI research. Nonetheless, the authors will release the code. They articulated the hyperparameters and training strategies clearly in the paper.\n",
            "summary_of_the_review": "As mentioned in the first point in the weakness, the authors generated consistent counterfactuals. However, they did not substantiate with their experiments how these counterfactuals will help in explanation. Some of the choice experiments are not well justified and may not be appropriate for the paper's goals. Overall, I disagree with the authors (1) about calling this method an explanation method , (2) calling the approach *causal* .\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2097/Reviewer_9V3t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2097/Reviewer_9V3t"
        ]
    }
]