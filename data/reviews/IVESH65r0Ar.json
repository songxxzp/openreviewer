[
    {
        "id": "1dEr50fB8HT",
        "original": null,
        "number": 1,
        "cdate": 1666068633840,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666068633840,
        "tmdate": 1666068633840,
        "tddate": null,
        "forum": "IVESH65r0Ar",
        "replyto": "IVESH65r0Ar",
        "invitation": "ICLR.cc/2023/Conference/Paper2715/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper combines two ideas: snapshot ensembles and sampling-based (uncertainty and mutual information) active learning. Snapshot ensembles are shown to empirically perform better than standard (deep) ensembles and Monte Carlo dropout, when used in conjunction with uncertainty sampling (and BALD). Furthermore, warm-starting (instead of training from scratch) is shown to work well empirically for data collection with active learning as long as the model is evaluated with a model trained from scratch.",
            "strength_and_weaknesses": "Strengths:\n - The combination of techniques used in the experiments dramatically improves runtime with equal or better runtime.\n - The observation that warm-starting works for data selection (though not for the final model) is very interesting and important.\n - This paper's combination of recent ideas in parameter sampling with old ideas in active learning is very timely.\n\n\nWeaknesses:\n - It seems that some rows are missing for the experiments. For example, for CIFAR10, BALD with SE+FT, ME with MCDO, and ME with SE+FT are missing. Furthermore, it appears that MCDO and SE+FT are missing for the pre-trained models.\n - I'm curious why margin uncertainty sampling (best vs second best or BvSB) is not used. In the literature, this method usually outperforms entropy uncertrainty sampling.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- This paper is quite clear.\n- Figure 3 is a bit hard for me to understand. Perhaps cut it, or add more explanation.\n- The right side of Figure 2 is a bit mysterious. What are the percentage values in the bottom?\n- The experimental work is original, though all techniques appear in previous work (not necessarily in active learning).",
            "summary_of_the_review": "While \"only\" combining existing ideas, this paper experimentally finds two very practically useful observations that I think the research community will appreciate.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2715/Reviewer_qYVA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2715/Reviewer_qYVA"
        ]
    },
    {
        "id": "4-sQmCTwzi",
        "original": null,
        "number": 2,
        "cdate": 1666215881313,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666215881313,
        "tmdate": 1666215881313,
        "tddate": null,
        "forum": "IVESH65r0Ar",
        "replyto": "IVESH65r0Ar",
        "invitation": "ICLR.cc/2023/Conference/Paper2715/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a more time efficient active learning method based upon epistemic uncertainty by incorporating snapshot ensembles rather than using the traditional method of deep ensembles. The experimental results demonstrate comparable recognition performance with a significantly lower runtime.",
            "strength_and_weaknesses": "The strength of the paper is that is well written, presents a reasonable somewhat novel approach for active learning, and the experimental results do show the potential of the approach.  The weakness of the paper is that both the deep ensemble and snapshot ensemble approaches are controlled by many various hyperparameters (e.g. number of samples, burn in time N_thresh, jumps between snapshots J, etc.).  These particular hyperparameters do not seemed to be discussed in the context of any hyperparameter search method in the paper or the appendices.  Choosing these hyperparameters judiciously seems important to appreciate the reduction in runtime. Also, the SE with fine tuning seems to be a significant advance over SE, but it is not fully evaluated in the experimental results.  It seems that the authors ran out of time to run the experiments, and the results are incomplete. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the approach has novelty and the significant details of the approach are presented so that the paper appears reproducible.\n\nI may quibble that some of the maximum entropy and variation ratio are not selecting the samples with highest epistemic uncertainty. In these cases, this reviewer believes these are estimates of aleatoric uncertainty while BALD is indeed a measure of epistemic uncertainty.  Nevertheless, these issues are not central to the main result of the paper which is the use of SE rather than DE.\n\nThis reviewer would like to see more assessment of the reduction in run-time via SE+FT over SE. This reviewer would like to see some discussion of selection of the SE specific hyperparameters. \n\nIt also seems that what really matters is the runtime to select the next m samples to label rather than the total runtime?  What really matter is that this runtime is comparable (less than) the time for the annotators to actually label the batch of m samples.\n\nIn the bottom paragraph of page 6, SE and DE are discussed in relation to baselines.  What are these baselines?\n\nThe table captions should explain what is being shown.  Are the results the probability of correct classification?  Are the percentages in the column header the percentage of unlabeled data that are eventually labeled?\n\nI could not see where Figure 1 is referenced in the narrative. \n\nI did not follow Figure 3.  It seems that the upper left in lowest VR and highest VR scores is equally confused between cat and dog for all 5 snapshots. In that case, it would seem that max p would be comparable.  If so, why the discrepancy between VR scores. I am also not clear what the probability map actually represents.\n\nNevertheless, I do like how the paper tries to show how the SE method provides ensembles with more diverse opinions.\n\nIt would seem that MCMC methods such as Langevin dynamics would provide more diverse ensembles in a single pass than SE methods.  Perhaps some discussion of why this might not be the case or if this should be explored as future work would seem appropriate.\n\n\n",
            "summary_of_the_review": "The work appears novel and the experiments demonstrate the effectiveness of the method. The results seem somewhat incomplete at the SE+FT seems like the significant results that could be explored more. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2715/Reviewer_s1VV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2715/Reviewer_s1VV"
        ]
    },
    {
        "id": "QqQwqhh-UB",
        "original": null,
        "number": 3,
        "cdate": 1666319168738,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666319168738,
        "tmdate": 1666319168738,
        "tddate": null,
        "forum": "IVESH65r0Ar",
        "replyto": "IVESH65r0Ar",
        "invitation": "ICLR.cc/2023/Conference/Paper2715/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a simple idea of using the snapshot ensembles instead of deep ensembles for active learning for uncertainty based methods. The paper compared their method with traditional DE and MCDO on three different dataset (CIFAR and Tiny ImageNet) with three differenty acquisition functions and showed that the SE + FT method achieved both a high accuracy as well as a low computation requirement.",
            "strength_and_weaknesses": "Strength:\n\nThe idea is simple. Combining SE to AL is not a new idea as I believe a lot of AL practitioners (like myself) training multiple models (DE) would think of possibly training less, like using FT. The problem is majority of us stop short when we see the fine-tuned accuracy is much lower than retraining from scratch, not realizing that the final accuracy is what we want....\n\nJust like the title, this simple idea is powerful as demonstrated in the experiments. The paper is well-written and easy to understand. The details are sufficient for the readers to follow and reproduce the results.\n\nAlso, just want to point out that Figure 1 is crucial for helping the reader as it conveys an important point that before the final episode, the model is at low accuracy. I couldn't understand why other fine-tuning efforts failed until I see Figure 1.\n\nWeakness / Suggestions:\nThe final analysis part is a bit weak (possibly due to the limitation of article length and the desire to squeeze things into 9 pages). I would suggest prioritizing Fig 2 and put Fig 3 (and more analyses like this) into the appendix.\nThe table 1 and 2 might require some caption to explain the 10%, 15%, 20% meaning. \nbest-performing results should be highlighted in Tables\nMinor typo on page 4: \"are to acquire acquire samples\"\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, and novel in the sense that it points out SE should be carried out with the final few episodes retrained from scratch. Reproducibility is good with enough details disclosed.",
            "summary_of_the_review": "I think this is a very interesting paper that made a simple idea that other people have tried actually working. The experiments are sound and the clarity is good, overall a good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2715/Reviewer_qtq1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2715/Reviewer_qtq1"
        ]
    },
    {
        "id": "Q34APMcXB9",
        "original": null,
        "number": 4,
        "cdate": 1666694190386,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694190386,
        "tmdate": 1666694190386,
        "tddate": null,
        "forum": "IVESH65r0Ar",
        "replyto": "IVESH65r0Ar",
        "invitation": "ICLR.cc/2023/Conference/Paper2715/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose to use snapshot ensemble (SE) to replace deep ensemble (DE) for modeling uncertainty in active learning. SE takes predictions from different epochs of training of a single model and thus is more efficient than DE. The authors compare SE with DE and MC-dropout on several basic AL acquisition functions including Variation Ratio, Max-entropy and BALD, and show that SE is comparable. They also use SE with continual learning where the model is not trained from scratch but is continuously finetuned after each acquisition. Such training strategy achieves comparative result with vanilla AL when using VR as acquisition.",
            "strength_and_weaknesses": "Strength\n\n- The authors propose a simple modification of ensemble-based AL learning by using Snapshot ensemble and continual learning, which achieves comparable or better result compared to deep ensemble while reducing the computational cost, when used with simple uncertainty-based acquisition functions. \n- The finding that snapshot ensemble produces more diverse predictions than deep ensemble is interesting. \n\nWeakness\n\n- The novelty of the work is limited, as both the snapshot ensemble and continual active learning have been introduced previously, and there is no novel acquisition function or AL algorithm. \n- The authors only consider very basic AL methods (VR, Entropy, BALD) which are relatively old and has shown to be inferior than recent SOTAs. Moreover, the empirical experiments are incomplete, with full comparison among MC-dropout, DE, SE and continual-SE only available for VR, making the conclusion on superiority of SE unconvincing. \n- The authors use a batched AL setting; however, the chosen acquisition functions are not suitable for batched AL as it just greedily takes the top m examples (in fact BALD can be worse than random in batched setting). It would be more compelling if result on algorithms designed for Batched AL is shown.\n- The claim that prediction diversity is more important than model performance could be misleading, as overly diverse predictions could result in under-confidence, making choices of samples almost random. The right amount of uncertainty should be calibrated and informative of actual model uncertainty. No theoretical analysis is provided on why SE always produce better diversity. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clear and easy to follow. The major limitation is on the novelty and the comprehensiveness of the empirical results. ",
            "summary_of_the_review": "The paper proposes a simple modification to ensemble-based AL by relacing DE with SE and shows some preliminary results using basic AL strategies. Although the result on VR is promising, more thorough comparison with recent AL techniques is needed to prove the significance of the result. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2715/Reviewer_WKSq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2715/Reviewer_WKSq"
        ]
    }
]