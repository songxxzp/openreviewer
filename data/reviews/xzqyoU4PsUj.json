[
    {
        "id": "xp2v2WJ_x0M",
        "original": null,
        "number": 1,
        "cdate": 1666040141517,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666040141517,
        "tmdate": 1666040141517,
        "tddate": null,
        "forum": "xzqyoU4PsUj",
        "replyto": "xzqyoU4PsUj",
        "invitation": "ICLR.cc/2023/Conference/Paper4777/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to use parametric transforms such as exponential function or stick-breaking transformation on the outputs of Bayesian neural network (BNN) and turn them into positive random vectors or simplex vectors. This new transformed BNN module can be plugged into probabilistic graphic models as a replacement prior. Sampling from transformed BNN is also possible, with the priors on the weights and biases. An additive mixture model is introduced for signal background separation with synthetic experiments. ",
            "strength_and_weaknesses": "It is an interesting idea to study combining Bayesian neural networks with probabilistic graphic models, and get the best of both worlds.  However, the current paper seems not in a form that can be accepted yet. The authors also quite frankly listed a few limitations of the current approach in Section 7. \n\nThere are many aspects that require further development, listed below,  \n\nFrom the probabilistic modeling point of view, it is unclear what is the benefit of bringing in the Bayesian neural network piece, compared to other conventional choices such as Dirichlet prior or Gamma vectors, or log-normal vectors.  The potential advantage might include being easier to incorporate covariates, with more flexible dependencies, and/or nonlinearity from neural networks, but this requires more supporting evidence.  \n\nThe model is tied to a variational model in Pyro, inference is conducted by maximizing the ELBO with an additional penalty term of relative entropy to encourage the separation of components. A key question unaddressed: how does the proposed framework compare to the standard VAE framework, with a probabilistic graphical model as the generative model, and NN or transformed NN as the recognition model? \n\nFrom the application point of view, it is unclear how the simple additive mixture model is compared to other choices of non-parametric models such as Gaussian processes.  The non-parametric aspect for unsupervised learning is especially interesting. Is it possible to explicitly show some differentiation achieved with the proposed approach? \n\nThe argument of building a random field rather than point distributions also needs more development, e.g., what can be achieved beyond Gaussian random fields?  Although a few potential applications are listed in Sec. 7., the paper would certainly benefit from evaluations on some real-world datasets. \n",
            "clarity,_quality,_novelty_and_reproducibility": "\nQuestions to clarify\n\n1. In Figure 2, the relative entropy r is treated as an observed variable. I found this confusing. \n2. What is the observation model for y in the additive mixture model? At the end of Section 4, it says \u201cthe observations are sampled using a small fixed variance (0.002)\u201d, but it would seem a little bit inconsistent if the intensities are assumed to be non-negative. \n3. How to balance the ELBO term and relative entropy term in the objectives? \n4. Is the citation to \u201cgradient clipping\u201d precise?\n5. What exactly is the stick-breaking transformation used? Citations missing? Is there any implicit bias in constructing a simplex vector in this way? \n6. The \"training of unit BNNs\" part reads very confusing. \"(i) generate a data sample emulating desired shapes (ii) fit a unit BNN...obtaining guide parameters (ii) using the \"inferred guide parameters\" as prior for more complex model\". What are the motivation and benefits of these procedures? \n",
            "summary_of_the_review": "Overall, I think this paper needs further development to be accepted, including both a more formal justification of the proposed framework, comparisons to other methods such as VAE or GPs models,  and experimental validations on real-world datasets. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4777/Reviewer_6vPR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4777/Reviewer_6vPR"
        ]
    },
    {
        "id": "UcaF5Dx-a8",
        "original": null,
        "number": 2,
        "cdate": 1666622918290,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622918290,
        "tmdate": 1666622918290,
        "tddate": null,
        "forum": "xzqyoU4PsUj",
        "replyto": "xzqyoU4PsUj",
        "invitation": "ICLR.cc/2023/Conference/Paper4777/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors incorporate Bayesian neural networks as non-parametric alternatives for standard distributions within a probabilistic graphical model, and demonstrate the approach in a toy example of separating two independent signals from a mixtures.",
            "strength_and_weaknesses": "Strengths:\n- The basic approach is sound and well in line with the modern literature where flexible function approximations are being used to expand standard model families\n\nWeaknesses:\n- The specific contribution is unclear and the paper looks more like a case study of a well-known principle, where the case being studied is also quite simple\n- The writing is a bit naive and known concepts (treating neural networks as probabilistic elements) are introduced as if they were new",
            "clarity,_quality,_novelty_and_reproducibility": "The main idea of the paper seems to be replacing standard distributions within a PGM with Bayesian neural networks whose outputs satisfy the properties of a distribution, to obtain some sort of non-parametric alternatives for conditional distributions. This standard approach is described as a novel idea and it is decorated with some additional terminology ('transformed BNN' that seems to mean standard BNN with specific activation function for the output layer), but I still fail to see where the actual contribution of the paper lies. Probabilistic programming libraries, like Pyro that the authors also use, are specifically designed to make building this kind of models easy and using neural networks as function approximations within those models is a routine practice. The idea itself is hence by no means bad, but it also has no novelty and the basic idea of using neural networks within a PGM and variational inference dates at least back to 2005 (Harva et al. \"Bayes blocks: an implementation of the variational Bayesian building blocks framework\", UAI 2005). The case study in Section 4 is something most Pyro users should be able to write if given the task and the solution they would build is roughly the same that is proposed here (including even the choice of the inference method), to the extent that it could perhaps even be used as an exercise on a MSc level course.\n\nThe story is also a bit problematic. I think the authors are building towards a general-purpose approach, but in practice only present two very simple examples (stick-breaking and exponentiation) and then proceed to study a very simplified case example. The paper would be more interesting if more challenging examples (perhaps normalising flows?) were characterised as the building blocks and the case-study would be for a problem that is clearly challenging and could not be easily addressed with the standard tools.",
            "summary_of_the_review": "Extended case study of how probabilistic graphical models can use neural networks within the model, assuming they are implemented using the standard libraries.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4777/Reviewer_YQeZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4777/Reviewer_YQeZ"
        ]
    },
    {
        "id": "MI-io0JnzG6",
        "original": null,
        "number": 3,
        "cdate": 1666672957013,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672957013,
        "tmdate": 1666672957013,
        "tddate": null,
        "forum": "xzqyoU4PsUj",
        "replyto": "xzqyoU4PsUj",
        "invitation": "ICLR.cc/2023/Conference/Paper4777/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes to add non-linear transformation to Bayesian neural networks and uses them in an additive mixture model. They should that this can solve signal separation problems on 2 synthetic tasks.",
            "strength_and_weaknesses": "## Strength\n- The paper is easy to follow\n\n## Weakness\n- The techniques mentioned in the paper are not new and the contributions are over-claimed.\n    - Having a transformation in the output layer of neural network or for any probability distribution is a common technique used in the community. Both transformations used in the paper are standard.\n    - Additive mixture is another common trick, in both deep learning community and graphical modeling community.\n    - Inference method used (SVI) is also standard.\n- The experiment is too simple and if signal separation is the focus, some real-world datasets that used for benchmarking purpose should be considered. ",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is simple and clear.\n- Quality: The method is too simple.\n- Novelty: All methods mentioned in the paper are known in the literature.\n- Reproducibility: The URL to GitHub doesn't work.",
            "summary_of_the_review": "The proposed method is not novel at all and the contribution is over-claimed. The empirical study is also solely based on simple simulated data.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4777/Reviewer_SrCA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4777/Reviewer_SrCA"
        ]
    },
    {
        "id": "Tl1tOyytRGF",
        "original": null,
        "number": 4,
        "cdate": 1666681548369,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681548369,
        "tmdate": 1666681548369,
        "tddate": null,
        "forum": "xzqyoU4PsUj",
        "replyto": "xzqyoU4PsUj",
        "invitation": "ICLR.cc/2023/Conference/Paper4777/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The incorporates Bayesian neural networks into probabilistic graphical models and apply their model to signal/noise decomposition.",
            "strength_and_weaknesses": "The considered problem of combining data-driven models with probabilistic models is clearly a timely one. Using Bayesian neural networks in that context promises to provide a seamless integration because of the joint probabilistic interpretation. While I would really like to see some fundamental progress being made in that direction, I am afraid that this paper does not do so. The main problem I see is that \nthe authors simply put those two parts together without accounting for the subtleties that usually arise when considering practical scenarios. This does not become too obvious as the paper makes some strong implicit assumptions as e.g., the consideration of simple mixture distributions with few components and experiments with well separable signal and background noise.\n\nThere are also some technical inaccuracies that are sloppy at best and might suggest that the authors did not put sufficient work into understanding the probabilistic graphical models framework at worst. For example, in Figure 1 it is written that the observed random variable is denoted as x with a latent variable z, i.e., we have the underlying data generating process $P_\\theta (x|z)$. Yet the left figure clearly corresponds to a data generating process of  $P_\\theta (y|z)$.\n\nSimilarly, in Figure 2 the relative entropy is described (and illustrated) as a random variable, although it is clearly a function thereof.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well motivated and easy to follow. \nYet it hides some detail that are important for understanding the contribution. In particular, it overpromises by stating to unify the domains of BNNs and PGMs, while elegantly omitting some of the underlying assumptions.\nI understand that this is a hard problem for which it is generally not straightforward to minimize the KL divergence. At the very least, the paper could explain this and make an argument for their assumptions.\n\nThe paper also has a very slim list of references with respect to work that has been done on the intersection of those two domains. For example, it does never mention the work of [1] that considers a similar setting.\n\n[1] Johnson, Matthew J., et al. \"Composing graphical models with neural networks for structured representations and fast inference.\" Advances in neural information processing systems 29 (2016).",
            "summary_of_the_review": "The paper tackles an interesting problem of combining data-driven models with probabilistic ones. The contribution of the paper is limited in that it makes strong assumptions on the probabilistic model and does not discuss how both aspects can be merged together effectively in a more general setting. Also, the authors seem to miss some important prior work on that topic.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4777/Reviewer_CEDA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4777/Reviewer_CEDA"
        ]
    }
]