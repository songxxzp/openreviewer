[
    {
        "id": "jNTTziil-VU",
        "original": null,
        "number": 1,
        "cdate": 1666634400427,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634400427,
        "tmdate": 1666634644008,
        "tddate": null,
        "forum": "04OL67rm6ok",
        "replyto": "04OL67rm6ok",
        "invitation": "ICLR.cc/2023/Conference/Paper4234/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors study Distributed Mean Estimation problem (DME) where $n$ clients communicate a representation of a $d$-dimensional vector\nto a parameter server which estimates the vectors\u2019 mean.\n\nThis paper aimes to resolve the decoding time slowdown from these recent state of the art DME\ntechniques Vargaftik et al. (2021; 2022). To control quantization erorr, tthe authors send an expected $p$-fraction of the rotated coordinates exactly for some small $p$ and apply client-specific shared randomness. ",
            "strength_and_weaknesses": "Strength: The paper is relatively well-written. It also interesting that the authors provide simple examples such as those in Sections 3.3 and 3.4 to help the readers understand the method. \n\nWeaknesses: Unfortunately, I think the main weakness is that there are major technical issues in the paper, which will be discussed in the following: \n\n\n------\n\n1- \"transmitting a \u201cbounded-support\u201d normal random variable $Z$, $Z\\in[-T_p, T_p]$ using $b$ bits\"\n\nGiven the bounded support, the distrubution will not be normal anymore. It will be \"truncated normal\" with different probability distribution function and moments. I am not sure about the correctness of the analyses in this paper since the authors assume normal distribution for coordinates after the rotation.  \n\nFor example, the expected squared error in page 4 and optimal unbiased quantization problem in page 5 are not correct because the distribution of $Z$ is truncated normal distribution. \n\n------\n\n2- Regarding, the dimension-indpendent upper bounds on the normalized mean square error in this paper, how can these upper bounds be explained based on the known lower bounds (Mayekar 2020, Ramezani-Kebrya 2021, Acharya 2021)? \n\nPrathamesh Mayekar and Himanshu Tyagi. Limits on gradient compression for stochastic optimization. In IEEE International Symposium on Information Theory (ISIT), 2020.\n\nAli Ramezani-Kebrya, Fartash Faghri, Ilya Markov, Vitalii Aksenov, Dan Alistarh, and Daniel M\nRoy. Nuqsgd: Provably communication-efficient data-parallel sgd via nonuniform quantization.\nJournal of Machine Learning Research, 22(114):1\u201343, 2021.\n\nJayadev Acharya, Clement Canonne, Prathamesh Mayekar, and Himanshu Tyagi. \"Information-constrained optimization: can adaptive processing of gradients help?\"\u00a0Advances in Neural Information Processing Systems\u00a0(NeurIPS), 2021.\n\n\n\n------\n\nSeveral closely related work have not been cited/compared:\n\nHantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. ZipML: Training linear models with end-to-end low precision, and a little bit of deep learning. In International Conference on Machine Learning (ICML), 2017.\n\nWei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. TernGrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017.\n\nFartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel M. Roy, and Ali RamezaniKebrya. Adaptive gradient quantization for data-parallel SGD. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nPrathamesh Mayekar and Himanshu Tyagi. Limits on gradient compression for stochastic optimization. In IEEE International Symposium on Information Theory (ISIT), 2020.\n\n\n------\n\nThe expected number of bits to transmit should be added to Table 1. For example, for the same $b$, the expected number of bits to transmit for QSGD is much less than QUIC-FL since only one real value for the norm is sent precisely. \n\n\n------\n \nWhen $b$ and $p$ very small,  QUIC-FL becomes essentially rand-$K$ sparsification method.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is relatively well-written. The techniques are mostly based on Vargaftik et al. (2021; 2022).  To improve reproducibility, the authors should elaborate on the hyperparameters used for baselines in Section 4. ",
            "summary_of_the_review": "Unfortunately, I think there are major technical issues in the paper. So this paper is not ready for publication. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4234/Reviewer_V1XF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4234/Reviewer_V1XF"
        ]
    },
    {
        "id": "tcRu-qTYNi",
        "original": null,
        "number": 2,
        "cdate": 1666679607451,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679607451,
        "tmdate": 1666682885137,
        "tddate": null,
        "forum": "04OL67rm6ok",
        "replyto": "04OL67rm6ok",
        "invitation": "ICLR.cc/2023/Conference/Paper4234/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an unbiased stochastic quantization method called QUIC-FL for rotation-based mean estimation, in order to reduce the communication by transmitting full-precision vectors. The method uses a 'shared randomness' approach which reduces the computation cost at the server to recover the compressed signals. The theoretical foundation is based on a Gaussian approximation argument, and the algorithm is realized by relaxing a continuous problem to a discrete problem. Detailed algorithm design and error analysis are presented. Experiments on simulated and real FL datasets shows the effectiveness of the proposed QUIC-FL.",
            "strength_and_weaknesses": "1. In general, the paper studies a meaningful problem of design unbiased stochastic compression for a vector. However, I have some concerns about the theoretical development. For a vector $x$, after rotation, the norm of the rotated vector $x^TR$ is fixed (equal to $||x||$). Thus, the argument that $x^TR$ is treated as iid Gaussian for designing the quantizer is not very promising. What's the convergence rate of the iid Gaussian approxmiation? I think this could be a serious issue regarding the correctness of the paper.\n\n2. The performance of QUIC-FL only has marginal improvement over prior methods, which is not very impressive.\n\n3. The presentation can be improved. The text on pages 3-6 is too dense. It might be helpful to add some definitions, claims, or theorems to highlight the key results. \n\n4. In Figure 4, why there is a huge sudden drop of the test accuracy curves, including the baseline full-precision model? This seems strange and needs to be re-implemented.\n\n5. Quantization for Gaussian distribution is a classical topic in literature. There should be a related work section on quantization of Gaussian (and other) random variables. The novelty of the proposed quantization method is not very clear.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity can be improved by polishing the presentation. The paper has some novelty in it, but the theoretical analysis is not very rigorous. Reproducibility: no code is provided but I think the results can be reproduced.",
            "summary_of_the_review": "The paper presents meaningful results on unbiased stochastic quantization. However, the analytical assumption on the Gaussian approximation, which is the building block of the paper, might be problematic. Thus, this issue needs to be addressed properly. Besides, the empirical gain is not very significant. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4234/Reviewer_GcFN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4234/Reviewer_GcFN"
        ]
    },
    {
        "id": "N9NLmdrz2p",
        "original": null,
        "number": 3,
        "cdate": 1667509117886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667509117886,
        "tmdate": 1667509117886,
        "tddate": null,
        "forum": "04OL67rm6ok",
        "replyto": "04OL67rm6ok",
        "invitation": "ICLR.cc/2023/Conference/Paper4234/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work introduces an unbiased compression pipeline for Distributed Mean Estimation (DME) and achieves good performance over several typical datasets compared with competitive baselines. ",
            "strength_and_weaknesses": "Strength:\n1. Comprehensive experiments over different datasets are conducted, and QUIC-FL outperforms conventional baselines and is comparable with EDEN. Different ablation studies are provided, such as different bit budgets, number of clients, and dimensions. \n\nConcerns:\n1. This paper is hard to follow. Besides, the math symbols should be highly reduced and unified. In this case, I can not understand exactly what the algorithm is doing and why it works. Moreover, I would suggest Alg. 1 be better described with more details, then more extended descriptions to be given based on Alg. 1. It gives me a feeling of the current Alg. 1 is only an unreadable sketch. This work should also be more centralized and have a better structure.  \n2. In some figures, such as CIFAR-10 in Fig. 5, both the training accuracy and the testing accuracy for Float32 is lower than EDEN and the proposed QUIC-FL. I doubt the results here. More analysis should be provided as well. \n3. It's unclear what is the convergence speed of QUIC-FL. Only the vNMSE upper bound is provided, but I assume it's insufficient. This paper is called \\textit{Quick} compression; I'm not quite sure whether the convergence speed of QUIC-FL is promising. Besides, are the operators of QUIC-FL computationally heavy?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThis paper is unclear and should be refactored to be more readable.\n\nReproducibility:\nThe code is provided, but I haven't checked the details. ",
            "summary_of_the_review": "I'm highly confused with the writing and structure of this paper\u2014also have some doubts about the results. The details are provided above. \n\nI would like to receive some comments from other reviewers and feedback from the author(s).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4234/Reviewer_Whkw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4234/Reviewer_Whkw"
        ]
    }
]