[
    {
        "id": "mLOQzt7_nZV",
        "original": null,
        "number": 1,
        "cdate": 1666550067368,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666550067368,
        "tmdate": 1666550067368,
        "tddate": null,
        "forum": "4rXMRuoJlai",
        "replyto": "4rXMRuoJlai",
        "invitation": "ICLR.cc/2023/Conference/Paper3825/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims at understanding and improving the ability of current LMs on reasoning over some basic laws of physics. A new text-based automatically generated physics QA dataset, UTOPIA, is proposed, and the authors showed that current LMs struggle on it. The authors then propose Mind's Eye to improve the physics reasoning ability by incorporating physics simulation results in the prompting. Specifically, a separate LM is trained to generate a simulation config from the original question, and the simulation result is appended to the prompt as a hint. With this method, LM zero-shot and few-shot performances are improved significantly across all scales reported and the improvement scales as the model size. ",
            "strength_and_weaknesses": "Strength:\n- The proposed method is simple and effective at improving the zero-shot and few-shot physics reasoning performances of LMs.\n- Thorough experiments at various scales are performed.\n- The proposed dataset can be generated automatically from simulation and potentially extensible to more complicated scenarios.\n\nWeaknesses:\n- The current form of Mind's Eye seems very specifically designed for the proposed dataset UTOPIA, which only contains simple physics questions that compare 2 objects and are generated by physics simulations to begin with. The generalization to other datasets probably will be bottlenecked by training a good text-to-code converter and the complexity/ambiguity of the physics scene.\n  -  In UTOPIA it is easy to obtain text code pairs for training the text-to-code converter since the dataset itself is generated by simulation. However, for other datasets (like PiQA or MMLU Physics) how can sufficient training data be obtained?\n  - There are also complex physics scenes (e.g. liquid and deformable bottles etc in PiQA) that could be hard or lengthy to fully specify to a simulator. And the resulting physics metric to read out could also be more specific than only a binary comparison.\n- The proposed method of injecting a physics simulator is conceptually very similar to the injection of calculator used in GSM8K dataset (Training Verifiers to Solve Math Word Problems). The specific idea of using physics simulation to help solve textual physics QA is also already explored by previous works (e.g. Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning; the physics here is simulated via a trained neural network to answer the more complex counterfactual questions with video input). These related previous works should also be cited and acknowledged. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and of high quality. As mentioned in the previous section, the proposed method is conceptually a combination of two previous lines of work. Nonetheless, this combination is novel and reveals unique insights about using LM in a domain constrained by a lack of knowledge.\n",
            "summary_of_the_review": "Overall, this paper presents a proof-of-concept way to combine external physics knowledge and the general reasoning ability of LM in a very simple setting. The demonstrated performance gain across scales is impressive and reveals important insights. However, the generalizability of the specific proposed method to more complex settings is unclear.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3825/Reviewer_46te"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3825/Reviewer_46te"
        ]
    },
    {
        "id": "Ii7my9JhxI",
        "original": null,
        "number": 2,
        "cdate": 1666653952346,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653952346,
        "tmdate": 1666653952346,
        "tddate": null,
        "forum": "4rXMRuoJlai",
        "replyto": "4rXMRuoJlai",
        "invitation": "ICLR.cc/2023/Conference/Paper3825/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new paradigm \u2013 adding a computational physics engine to aid language modeling on physical reasoning tasks.  \n",
            "strength_and_weaknesses": "*Strength:*\n\nThe idea proposed by this paper is pretty novel and effective; \n\nThis paper is well written and easy to understand;\n\nThis paper provides a new benchmark to test physic related reasoning tasks;\n\nThis paper has completed comparisons with current reasoning enhanced techniques, such as Chain of Thought, \u201cLet\u2019s think step by step\u201d with zero-shot priming, Self-consistency decoding. \n\n*Weaknesses:*\n\nI hope this method could be open sourced, which will promote the development of the whole community.\n",
            "clarity,_quality,_novelty_and_reproducibility": "*Quality:*\n\nThis paper has high quality. See above strengths. \n\n*Novelty:*\n\nThis paper is pretty novel\n\n*Reproducibility:*\n\nThis paper doesn\u2019t provide source code / interaction website.\n",
            "summary_of_the_review": "This article greatly improves language models\u2019 reasoning abilities on physical related tasks. This idea is pretty novel and this method consists of human physical reasoning steps: carry about simulation then write sentences about it. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3825/Reviewer_TaNC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3825/Reviewer_TaNC"
        ]
    },
    {
        "id": "wvO__tiEcWC",
        "original": null,
        "number": 3,
        "cdate": 1666794721400,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666794721400,
        "tmdate": 1666794721400,
        "tddate": null,
        "forum": "4rXMRuoJlai",
        "replyto": "4rXMRuoJlai",
        "invitation": "ICLR.cc/2023/Conference/Paper3825/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper attempts to create a pipeline that combines inputs of a language model with a computation physics engine to simulate possible outcomes, attempting to provide external world information into a text-only language model to assess whether this allows them to reason better.  They create a range of such tasks (as a benchmark dataset) and evaluate the improvement in performance across a range of model sizes. They see that this approach of giving in extra information from a physics engine as part of a language models context allows better performance, and additionally allows smaller language models to achieve performance at-par with the larger text-only ones.\n",
            "strength_and_weaknesses": "Strengths\n\n1. This paper is well-motivated and attempts a simple and succinct approach towards adding in additional world information into a language model for physical reasoning tasks. \n2. They create and release a dataset that focuses on physical reasoning tasks and use this to evaluate the models to see how they perform with augmented physical information\n3. They illustrate and discuss how this is an attempt at a method that is both scalable and \u201cgrounded\u201d---although it is worth noting that the inputs/outputs are still only ever in text format and the base LM has still never experienced input apart from text; however the additional context given by the physics simulator can serve as a proxy for non-textual information.\n\n\nQuestions / Weaknesses\n\n1. Are there any insights on which reasoning abilities might be easier to gain information from from text (e.g., looking at the categories of motion, friction, free-fall etc.). This could be verified via co-occurrence statistics e.g., looking at which related words occur more in the training datasets, as well as which related answers might occur more in correspondence with the question in the prompt.\n2. It might be good to have a memorisation control to test whether or not the answers generated by the model might have simply existed somewhere in it's training data (which means it's not actually using the outputs from the physics simulator to reason/generate the correct response).\n3. Can the authors report error bars and variance in all the accuracy results?\n4. Some missing citations in grounded reasoning for text-only LMs: Li et al., 2021; Patel and Pavlick, 2020.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and seems fairly straightforward to reproduce assuming the authors release the dataset.\n",
            "summary_of_the_review": "Interesting evaluation paper and release of dataset.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3825/Reviewer_pY5b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3825/Reviewer_pY5b"
        ]
    }
]