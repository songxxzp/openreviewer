[
    {
        "id": "hk8xz6KEm-t",
        "original": null,
        "number": 1,
        "cdate": 1666388897489,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666388897489,
        "tmdate": 1666388897489,
        "tddate": null,
        "forum": "JZMR727O29",
        "replyto": "JZMR727O29",
        "invitation": "ICLR.cc/2023/Conference/Paper6123/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the problem of inserting combinatorial solvers into neural network architectures. Since combinatorial solvers have discrete outputs the gradient is zero or non-existent, meaning that an alternative \"gradient\" must be used on the backwards pass in order to provide directional information to update the model parameters. \n\nThis paper proposes a method that uses the identity function for the gradient, with an additional projection matrix applied in cases where these is a known invariance in the solution space. The projection is motivated by arguing that irrelevant gradient components interfere with adaptive methods such as Adam during training. This point is also empirically checked in several experiments. Other experiments show that the overall method is competitive and sometime better than representative methods in the related literature, e.g., the work of Vlastelica et al.",
            "strength_and_weaknesses": "**Strengths**\n\n- Paper is nicely written, an fairly clear\n- A number of experimental setups are considered, I am happy to see a good variety of problems considered. \n- The method is computationally cheap, and general purpose\n- The argument that the identity is guaranteed not to find worse solutions is solid (e.g., Theorem 1), although of course the \"there exists. gradient step such that the loss is better\" doesn't say anything about how long before that useful step happens.\n\n**Weaknesses**\n- the identity method does appear to be very similar to the straight through estimator. Efforts are made in the paper to explain the difference but I currently fail to understand the differences. \n- the projection idea is discussed in detail, and is motivated via the connection to adaptive methods. But no experiments are provided empirically validating this connection.",
            "clarity,_quality,_novelty_and_reproducibility": "**Contribution**\n\nFrom what I can tell the main contribution is this: 1) if it is known that certain updates do not change a solution quality, then the component of the gradient update in this direction can be safely removed without hurting the loss by using $P \\cdot (dl/dy)$, instead of $dl/dy$, and 2) actually it is a good thing to remove this redundant when using an adaptive optimizer like Adam since otherwise the learning rate will be adjusted downwards due to the spuriously big gradient size. \n\nThis is an interesting point, and is intuitive yet novel so far as I am aware. But the paper doesn't focus enough attention on this detail. There re a number of experiments showing the benefits of the projection, but no attempt is made to empirically validate the connection to adaptive training - it would be great to see plots showing the gradient update sizes with and without the projection, and plots showing the learning rate size and so on and so forth. Another interesting experiment would be to run training using non-adaptive SGD and show that there is no difference between the projection and no-projection methods. This would be good evidence to suppose the stated motivation for the projection method. \n\n**Clarity**\n\n- the paper is sufficiently clear so as to be quite quickly understood \n\n**Reproducibility**\n\n- the authors commit to releasing code when the paper is accepted. This is adequate in my view. ",
            "summary_of_the_review": "\n\n\nMy main concern for rebuttal is this: can the authors please provide further clarification on the differences between their identity method and the Straight-Through estimator? I appreciate the efforts given in Appendix A to explain the differences, but unfortunately I still do not see what the difference is? For instance, Figure 5, which is used to explain the differences instantiates the proposed method for differentiation through a sampler. But the only non-differentiable part is the argmax, which uses the identity. Isn't this exactly what the straight through estimator would use too? I am very keen not to mis-accuse this work of being \"the same\" as the straight through estimator, but until I understand the difference I cannot advocate acceptance. \n\nThis paper has some interesting ideas, namely the projection method, but the emphasis of the paper is on the identity update, and secondarily on the projection. Consequently there is too little time spent discussing and studying the consequences of the projection idea. I really think this manuscript will make a nice paper eventually, but my suggestion to the authors would be to focus on the projection idea and the connection to adaptive methods. If you can show in more detail that this connection exists and is important to performance then this could be really helpful to practitioners. \n\nAlthough I see a lot of promise in this work, in it's current state I cannot advocate for acceptance. I will pay very close attention to your rebuttal to make sure that I haven't made some terrible misunderstanding about the identity idea vs the straight through estimator. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6123/Reviewer_NUvG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6123/Reviewer_NUvG"
        ]
    },
    {
        "id": "jBcgHg_s9R",
        "original": null,
        "number": 2,
        "cdate": 1666630832724,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630832724,
        "tmdate": 1668794818674,
        "tddate": null,
        "forum": "JZMR727O29",
        "replyto": "JZMR727O29",
        "invitation": "ICLR.cc/2023/Conference/Paper6123/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a surrogate gradient for differentiating through combinatorial solvers with linear objective function, for which gradient is not informative (either null or does not exist). ",
            "strength_and_weaknesses": "Strengths:\n- simplicity and computational efficiency of the \"negative identity\" approach compared to e.g. Vlastelica's approach, which requires two calls to the solver per gradient iteration\n- consideration for solver invariants is a plus\n- empirical evaluation on a wide range of problems is a plus\n\nWeaknesses:\n- intuition and theory in 3.2 and 3.3 shows that the update leads to a lower-loss solution. It would be helpful to have some estimate/indication of how does the update compare with the best possible update (of some limited magnitude)\n- empirical evaluation is rather limited in terms of competing methods. It would be interesting to see how the proposed method compares with e.g. cvxpylayers and similar approaches on combinatorial problems equivalent to convex optimization problems, where these methods can be applied \n- it would be helpful to have some complexity/run-time information provided for all the methods/runs in Section 4, to allow for judging the tradeoff between accuracy and speed.\n- the relative comparisons show somewhat mixed results, e.g. no improvement over Softsub and small over I-MLE in Table 1, similar results to BB in Table 2, lower performance than BB in Table 3, similar performance but higher robustness than BB in Fig. 4. \n- minor: graphical illustration for the more challenging case (Section 3.3) would increase the accessibility of the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and proposes a simple new way of providing an update to the cost vector of a combinatorial problem that aims at achieving lower loss. Ample information for reproducibility is provided in the appendix. ",
            "summary_of_the_review": "The paper presents a novel way to perform updates over combinatorial solvers. The approach is simpler compared to prior approaches, yet in some cases empirically competitive. While well-argued and well-written, the paper could benefit from a more in-depth theoretical analysis, and from more information in the experimental section. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6123/Reviewer_MfHU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6123/Reviewer_MfHU"
        ]
    },
    {
        "id": "7_tp9KVgWI5",
        "original": null,
        "number": 3,
        "cdate": 1666710346473,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666710346473,
        "tmdate": 1669041514892,
        "tddate": null,
        "forum": "JZMR727O29",
        "replyto": "JZMR727O29",
        "invitation": "ICLR.cc/2023/Conference/Paper6123/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a straightforward hyper-parameter-free approach to embed discrete solvers as differentiable layers into neural networks. In detail, during the backward pass, the Jacobian $\\frac{\\partial y(\\omega)}{\\partial \\omega}$ is simply treated as a negative identity matrix and a theoretical justification is provided. As the identity method alone can result in unstable learning behavior, two transformations (i.e. mean and norm) are applied to the cost vector $\\omega$ to exploit invariants. Noise is added to the cost vector to improve the robustness. Experiments on discrete samplers, deep graph matching, and image retrieval show the proposed method is competitive against previous more complex methods.",
            "strength_and_weaknesses": "## Pros\n- The motivation of this work is clear and the proposed method seems simple and efficitive. It is suprising that the identity method, which is similar to Straight-through estimator for samples drawn, can be applied to optimization operation.\n- Extensive experimental results on discrete samplers, deep graph matching, image retrrival, etc. show the effectiveness of the proposed method.\n\n## Cons\n- The paper is not easy to read and some place is hard to decipher. Some derivations are not very clear due to omissions or inappropriate notation usage\uff1a\n1. I think  that Equation (2) should be something like $\\frac{d}{d\\omega}\\langle\\omega, y^* - y(\\omega^0)\\rangle$ where $y(\\omega^0)$ is a fixed vector rather than a function of $\\omega$.\n2. In Equation (6), the linearization $f(y)$ can be viewed as the Taylor expansion of $l(y)$ around $y(\\omega)$, which means $f(y)$ can only approximate $l(y)$ in the small region around $y(\\omega)$. However, for the more wide region $y \\in Y$, a lower linearized loss doesn't mean a lower true loss. For instance, assume $y \\in \\mathbb R^2$ and the feasible region is the polygon with extreme points $\\{(0, -1), (0,10),(-10,0), (10,0)\\}$, and the nonlinear loss $l(y) = ||y||^2$. If $y(\\omega) = (0, -1)$, by the defination of Equation (6) $Y^*(y(\\omega)) = \\{(0,10),(-10,0), (10,0)\\}$. All points in this set have a lower linearized loss but higher true loss than $y(\\omega)$. \n3. In the Linear Transforms part of Section 3.4, $\\Delta^I\\omega$ is decomposed into $\\Delta^I\\omega = \\Delta^I\\omega_1 + \\Delta^I\\omega_2$, where $\\Delta^I\\omega_1 = P\\Delta^I\\omega, \\Delta^I\\omega_2 = \\Delta^I\\omega - \\Delta^I\\omega_1$. It is clear if $P$ is a projection matrix (e.g. the $P_{mean}$ in Equation (13)), then $\\Delta^I\\omega_2 \\in \\text{ker } P$. But for general $P$, which is not a projection matrix, whether $\\Delta^I\\omega_2 \\in \\text{ker } P$ still holds? This may affect the derivation of Nonlinear Transforms.\n- Some additional ablation studies about invariant mappings should be conducted: In Table 1 and Table 3, $P_{norm}$ cooperate with $P_{mean}$ to improve the performance. In Table 2 and Table 4, the performance of Identity with $P_{norm}$ alone is worse than that without projection. Therefore, the ablation study is needed to find out whether $P_{norm}$ is useful. For example, in Table 1, besides $Id. (P_{std})$ and $Id. (\\text{no } P)$, $Id. (P_{mean})$ and  $Id. (P_{norm})$ should also be evaluated to find out whether these two mappings alone help to improve model performance.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper said: a curated Github repository for reproducing all other results will be published upon acceptance.",
            "summary_of_the_review": "I would to reconsider my rating based upon authors' feedback and clarification. Currently it is hard for me to fully understand the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6123/Reviewer_bT3d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6123/Reviewer_bT3d"
        ]
    },
    {
        "id": "QZ9F8nY1H9",
        "original": null,
        "number": 4,
        "cdate": 1667477135882,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667477135882,
        "tmdate": 1667477135882,
        "tddate": null,
        "forum": "JZMR727O29",
        "replyto": "JZMR727O29",
        "invitation": "ICLR.cc/2023/Conference/Paper6123/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a principled method to approximate the gradient of a combinatorial solver, which could be readily integrated into an end-to-end learning pipeline. To achieve this, the authors studied the behavior of linear combinatorial solvers and proposed \"Identity\" as the surrogate gradient over the coefficients, with further added perturbation to avoid the cost collapse. Experiments were conducted on various combinatorial tasks, showing its superior performance over existing gradient estimators. The idea of utilizing identity as a gradient is interesting and novel, with some theoretical guarantee.",
            "strength_and_weaknesses": "In general, I like reading this paper since it was well-written and very easy to follow. All the necessary details were presented and the idea was insightful. To me, the strength is as follows:\n1. The idea of utilizing identity as a gradient estimator for linear combinatorial solvers is very interesting and neat. This is without any extra calling of the solvers and can be easily extended to a large variety of downstream tasks.\n2. Although the technical part of the proof of Theorem 1 is not so complicated, I think Theorem 1 can support the claims about the applicability suggested by the authors.\n3. The authors provided an initial but insightful discussion on utilizing projection as solver relaxation.\n\nDespite the aforementioned strength, this paper can be further improved:\n1. The discussion of projection as solver relaxation seems not so thorough and solid. More detailed and theoretically guaranteed discussion can greatly strenghened the contribution of this paper.\n2. There have been several previous works approximating gradient of linear combinatorial solvers. I suggest the authors to give some discussion on the non-linear cases (say, quadratic).\n3. Aside from using bar graph in Table 2 and 3, it could be more readable if numbers could be provided.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper is good. I feel very pleased and smooth reading this paper.\nNovelty of this paper is also above the average of ICLR.\nThe authors also provided the code for reproducing part of the experiments.",
            "summary_of_the_review": "In general, I vote an acceptance for this paper, taking into account its readability, novelty, theoretical soundness and empirical performance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6123/Reviewer_cnD6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6123/Reviewer_cnD6"
        ]
    }
]