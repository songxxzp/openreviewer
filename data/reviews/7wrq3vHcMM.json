[
    {
        "id": "Jo9JhwZx7n",
        "original": null,
        "number": 1,
        "cdate": 1666647887708,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647887708,
        "tmdate": 1666810868467,
        "tddate": null,
        "forum": "7wrq3vHcMM",
        "replyto": "7wrq3vHcMM",
        "invitation": "ICLR.cc/2023/Conference/Paper5323/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the problem of understanding how models generalize to new domains from a theoretical perspective. Specifically, they seek to answer the core question: \n\"Under what conditions on the source distribution P , target distribution Q, and function class F do we have that any functions f, g \u2208 F that agree on P are also guaranteed to agree on Q?\"\nThis paper extends existing results on this problem to a broad class of nonlinear functions, without requiring the assumption that the support of Q lies within the support of P. They show a number of related results for different types of data (discrete, continuous gaussian-distributed) relying on the assumptions that a) the support of the *marginals* of Q lie within the support of the marginals of P, and b) the covariance-matrix is non-degenerate, then the function class $F={\\sum_i f_i(x_i)}$ along with P and Q satisfy the above property.",
            "strength_and_weaknesses": "The paper addresses a central and important question in the theory of generalization, and extends new results to a broad class of nonlinear functions without making assumptions about the support that fail in the context of domain generalization. Their arguments are rigorous and well-presented, and make a convincing case for why this result is powerful and novel. Their argument convincingly shows how the ability to extrapolate to new domains (for the class of functions they consider) is tied to the conditions of non-degenerate covariance and overlapping marginal support.\n\nMy primary hesitation with the paper is in the assumptions made about the data. My understanding is that the results shown in this paper hold for three settings: 1) when x takes on discrete values, 2) when $x=(x_1,...,x_d)$ such that each pair $(x_i, x_j)$ is pairwise gaussian distributed, and 3) when x can be divided into two subsets $x_1,x_2$ and $(x_1,x_2)$ is gaussian distributed. \n\nFirst, the results in the discrete setting are proved only for the 2-dimensional setting as far as I can tell. In the paper, it is stated that \"Theorem 2 can be easily extend to the case when k > 2. We choose to present the current version because of simplicity, and because the kernel matrix KP coincides with the Laplacian of a bipartite graph when k = 2, which provides further insights and better intuitions.\" I think the extension to higher dimensional x should at least be shown in the appendices.\n\nSecond, the assumptions on the data in real-valued settings seem quite restrictive. I am not an expert on learning theory, so it is possible these assumptions are more standard than I assume, but the first set of assumptions (e.g. each pair of features is pairwise gaussian distributed) seems unlikely to be true in general cases, and the note in the latter case (where features are partitioned into subsets) that \"we want to handle the general version where the inputs are divided into multiple subsets, but due to technical reasons we can only use our proof techniques to two subsets\" also seems like a significant constraint. Perhaps the authors could give some motivation for whether there is a reason to expect either of these assumptions about the form of the data to hold in cases that are of particular practical or theoretical interest.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is largely clear, well-written, and easy to follow. The results build off existing work but generalize it to a new and novel setting that is very relevant to the field. ",
            "summary_of_the_review": "This paper tackles a very interesting theoretical question and attempts to provide answers in a setting that is a) of direct practical interest and b) has not yet been satisfactorily resolved in the literature. It is well-written and high-quality, but the specific assumptions made about the form of the data may be overly restrictive and limit its usefulness. For now I would consider this paper borderline, but I hope that the authors or other reviewers might be able to provide more context on how limiting those assumptions really are. From an empirical perspective they seem quite severe, but from the perspective of learning theory it may still be a significant result, and as such I may update my score pending further discussion.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5323/Reviewer_1mvE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5323/Reviewer_1mvE"
        ]
    },
    {
        "id": "ujT9C3oT7Vg",
        "original": null,
        "number": 2,
        "cdate": 1666802272137,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666802272137,
        "tmdate": 1666802272137,
        "tddate": null,
        "forum": "7wrq3vHcMM",
        "replyto": "7wrq3vHcMM",
        "invitation": "ICLR.cc/2023/Conference/Paper5323/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies extrapolation of models from source to target distribution. Unlike prior work that made restrictive assumptions about the distribution shift, this work considers any shift with shared marginals but arbitrary shifts in the joint. However, as a compromise, they only study a class of functions that are the sum of nonlinear functions on each coordinate (the last theorem relaxes the class of models to some extent though). ",
            "strength_and_weaknesses": "Strengths.\n1. Theoretical understanding of distribution shift is much needed, with the closest related work from 2010. Any progress toward better understanding is welcome. \n2. Writing is mostly clear, assumptions are clearly stated and claims are argued well. \n\nWeakness/questions.\n1. I found the writing a bit dry because it does not anchor itself with interesting empirical findings that the theory can explain. They do not also have any section that explains how the theoretical results are of practical consequence. \n2. I expected some empirical validation of their claims with either continuous or discrete inputs. \n3. I find the considered class of functions (which are the sum of functions on each dimension) too restrictive. Does the bound on extrapolation measure $\\tau$ depend on d because of the assumed class of models? How does the upper bound change with a more general class of models? For the continuous case, when does the bound on $\\tau$ lead to a non-trivial bound? The paper can gain from some commentary around the developed theoretical claims, such as how tight the bounds are, what is the worst and best case shift, etc. \n\nMinor comments.\n1. Around eq (8), (9), r is swapped with k.\n2. Section 3.3 talks about f1, f2 without introducing them. \n3. \u201cconsidered theorem 5\u201d -> \u201cconsidered in \u2026\u201d\n4. Page 5: sparest cut -> sparsest cut.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper rates highly on each of clarity, quality and novelty. I only found minor typos in the writing. ",
            "summary_of_the_review": "The paper makes a good effort toward developing theoretical bounds for extrapolation from a source to a target distribution for a specific class of models. The theoretical results are interesting but their relevance and impact value is left to the reader. The paper can improve by commenting on tightness of its result, discussing broader impact, and with engaging commentary in connection to known empirical results. Without such, it is difficult to assess the significance of its claims.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5323/Reviewer_JEBk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5323/Reviewer_JEBk"
        ]
    },
    {
        "id": "f0Qj1WMdjjM",
        "original": null,
        "number": 3,
        "cdate": 1666888620478,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666888620478,
        "tmdate": 1671260092669,
        "tddate": null,
        "forum": "7wrq3vHcMM",
        "replyto": "7wrq3vHcMM",
        "invitation": "ICLR.cc/2023/Conference/Paper5323/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies domain shift between distributions whose marginals have good overlap by analyzing the maximum of the ratio between the two distributions of the corresponding squared-distances between any two models. They bound this ratio for a model class that includes models additive over individual features and some additional gaussianity assumptions.",
            "strength_and_weaknesses": "This work has some potentially interesting ideas, but overall the studied setting is limited making it questionable how informative or insightful the results really are. \n\nThe lower bound (Prop. 6), although not surprising, is helpful to the story as it motivates the idea of imposing structural assumptions between P and Q. Yet, the specific structural assumption assumed (identical marginals) is rather strong and at least it should be motivated. Besides that, the Gaussianity assumption also appears limiting in that setting. Even more concerning, there is no explicit discussion on how informative the bounds are (eg Thm 4 and 5). The result of Thm 4 is compared to the linear case but the comparison reads loose: (i) In (14) we have d/\\lambda_min compared to 1/\\lambda_min for linear models; why is \"d\" treated as a \"constant\" here? (ii) the bound in the linear case depends both on Q and P. (eg this is also the case in Thm, 3).\n\nminor:\n- Paper would benefit from a careful read as there are several typos here and there\n- r_i not defined end of page 4. Why is wlog to assume i\\in[2]\n- Should it be f^* in (3)?",
            "clarity,_quality,_novelty_and_reproducibility": "The results per se appear original to the best of my knowledge (proofs use rather standard ideas but nevertheless appear to be non-trivial). In terms of clarity, the paper would potentially benefit from better structuring. Despite some attempts of the authors to provide interpretations of the results more effort in this directions seems to be needed to make the contribution less \"dry\".",
            "summary_of_the_review": "See above. I have not read the appendix due to review time constraints. At this point my feeling is that the paper is somewhat below the acceptance threshold. While the results could be of some interest, some more effort appears that is needed to justify the assumptions and draw concrete conclusions that are useful before the theory itself.\n\n\n---------------------------------------------------\n**** Raised score 5-->6. See response to authors ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5323/Reviewer_CdV3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5323/Reviewer_CdV3"
        ]
    },
    {
        "id": "cDiA4yUQJQ",
        "original": null,
        "number": 4,
        "cdate": 1666993428817,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666993428817,
        "tmdate": 1666993428817,
        "tddate": null,
        "forum": "7wrq3vHcMM",
        "replyto": "7wrq3vHcMM",
        "invitation": "ICLR.cc/2023/Conference/Paper5323/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes domain generalization results for squared error that apply to cases where the support of the target distribution exceeds the support of the source distribution. There data model is nonlinear but the function has to be nearly realizable with a certain hypothesis class. ",
            "strength_and_weaknesses": "The most important addition to this paper would be a synthetic experiment where the data satisfies the assumption. This would both make it clearer how easy it is to construct such data and how tight the bounds are.\n\nDiscussing the impossibility results in previous work where disjoint support leads to unbounded divergence will also help with the motivation. Do Ben David et al already provide examples of this situation?\n\nMinor comments:\n- In Eq 1 tau is defined as a function of three variable and then, in the text below it, the variables are dropped. This change in notation should be clarified.\n- I believe epsilon in Eq 2 needs a quantifier such as \"there exists epsilon and f* such that...\".\n- What does stylized mean in the abstract?\n- The title of the paper is a bit misleading. Ben David et al's result already applies to nonlinear models. The submission is providing results for certain classes of nonlinear models and data where the previous results would not apply and so is not \"a first step\" for the general case of nonlinear models.\n- [1] also discusses issues with domain adaptation models and its relationship to support overlap. The discussion in [1] as far as I know is different from the submission but perhaps the authors can see some connections. There is no need to include this paper in related work if the authors do not find it relevant.\n\n[1] Stojanov, Petar, et al. \"Domain adaptation with invariant representation learning: What transformations to learn?.\" Advances in Neural Information Processing Systems (2021)",
            "clarity,_quality,_novelty_and_reproducibility": "See previous section",
            "summary_of_the_review": "The approach taken in this paper is vastly different from previous domain adaptation works I've read and I lack the background to carefully evaluate correctness. I did my best to verify the math in the main paper but it's likely that I've missed some errors. The assumption on the data is restrictive but the paper is clear about it. I'm leaning towards acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5323/Reviewer_fhp8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5323/Reviewer_fhp8"
        ]
    }
]