[
    {
        "id": "7tkaeKb6psX",
        "original": null,
        "number": 1,
        "cdate": 1666380951006,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666380951006,
        "tmdate": 1666715794140,
        "tddate": null,
        "forum": "NQuCQoHqqSY",
        "replyto": "NQuCQoHqqSY",
        "invitation": "ICLR.cc/2023/Conference/Paper3450/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method, TECO, for the task of video prediction. TECO learns compressed representations to effectively and efficiently condition on long-form videos by modeling long-term dependencies.In order to achieve that, they leverage the MaskGit prior for dynamic prediction and propose a few new architectures based on VQ-GAN. As last, they also introduce several new long-form video prediction benchmarks in this work.\n",
            "strength_and_weaknesses": "This paper is well-written. The discussion on the previous video prediction works is mostly well done. Their main contribution is extending the short video generation to the long-form video generation while maintaining the temporal consistency. Thus, the motivation is well justified. \n \nThey show a quantitative comparison between TECO and other baseline methods in long temporal consistency and sampling speed, demonstrating TECO could achieve competitive results with fast sampling speed. \n \nBesides, this paper introduces three challenging video benchmarks to better measure the long-range consistency in video prediction, which I believe could be a significant contribution to the community. \n \nAt last, this paper demonstrates promising performance in these challenging datasets and outperforms the previous state-of-the-art methods.\n \nHowever, there are also a few points that need to be further explained and addressed.\nIn Figure 2, it seems that the $\\hat x_t = D(z_{t}, h_{t-1})$, which is different from the text in the paper. It seems the prediction of the future time-step from the current time-step include the feature embedding from the future? To make it clear, will be good to show how $\\hat x_t $ is computed.  Also, I am not sure how the output of MaskGit ($\\hat z_t$) is leveraged in the equation. It will be good to generate notation for $\\hat z_t$.\n \nIn Encoder, it is not well explained how the proposed algorithm could leverage/benefit from the temporal redundancy in video data. Also the temporal redundancy should be datasets-dependent. There should be some justifications showing there are temporal redundancies with different levels in each dataset.\n \nIn addition to these, there are a few minor points:\nIt will be good to show the formulation of $L_{LPIPS}$ in Eq 3\nfirst row below Eq(6): the decoder is a unsampling -> is an unsampling..\nIn Figure 3 right, what is the timestep for this figure? As the LPIPS of each method is dynamically changed with timestep (left figure).\nIn the main paper, some explanation refer the result in the Appendix, which is not suggested.\n \nSuggestion:\nIn terms of modeling the long-term dependencies, there are a few recent works that are not included in the related work. For example, the MeMViT[1] and ViS4mer[2]. Both of them focus on capturing long-term dependencies in an efficient and effective way. Would be good to at least mention them in the related work.\n\n[1]Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition, CVPR 2022\n\n\n[2] Long movie clip classification with state-space video models, ECCV 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "The Clarity and Novelty of this paper need to be further justified, which is detailed in the above section. In terms of Reproducibility, this paper provides several resources including code, baseline and dataset.\n",
            "summary_of_the_review": " \nOverall, I would vote weak reject at this point due to some unclear points mentioned in the above section. But I tend to increase my rating after seeing the feedback from authors. As is mentioned in this paper, the long-form video topics have not been well-explored in the community and I believe this paper is able to make contributions in this area.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3450/Reviewer_zBcT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3450/Reviewer_zBcT"
        ]
    },
    {
        "id": "-KQe5OOUVr8",
        "original": null,
        "number": 2,
        "cdate": 1666680432465,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680432465,
        "tmdate": 1669742152836,
        "tddate": null,
        "forum": "NQuCQoHqqSY",
        "replyto": "NQuCQoHqqSY",
        "invitation": "ICLR.cc/2023/Conference/Paper3450/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to improve the discrete sequence model for long-term video prediction. The proposed method operates on (patch-wise) quantized representation of videos extracted by VQ-GAN, and models the temporal dynamics through Transformers. To handle the quadratic complexity of Transformers thus enabling training with long sequences, the authors proposed to incorporate an additional CNN-based encoder and decoder and model the latent dynamics with smaller tokens using temporal and bidirectional Transformers (MaskGit). The proposed method is evaluated on three challenging video benchmarks and demonstrated compelling long-term prediction performance over baselines.",
            "strength_and_weaknesses": "Strength:\n- The paper is generally well-written and easy to follow.\n- The ideas presented in the paper are sound. The extensive experiment results support that training with longer sequences by the proposed method is indeed helpful in generating long videos better than the baselines in terms of general fidelity. \n\nWeakness:\n1. It is unclear from the paper which factors play a major role in improving the efficiency of video prediction. Based on my understanding, the major improvements are from the spatial downsampling in both the encoder and temporal Transformer, while the model retains the same quadratic complexity as the prior works. Despite the practicality, this is a simple trick that is also applicable to other Transformers, and having it as a major component for enabling long-term prediction limits the significance of the work. The same arguments also apply to DropLoss, given that there are also many hierarchical Transformers that apply the prediction at multiple scales (i.e., I feel that DropLoss is a simplified version of a hierarchical Transformer where the loss is coarsely defined over time). \n\n2. To demonstrate the significance of each component in improving efficiency, it would be great to have ablation studies on complexity (e.g., FLOPS and peak memory) together with fidelity. Also, it would be great to have a comparison with baselines in terms of complexity to demonstrate the full advantages. \n\n3. Since one of the main goals of this work is to improve the long-term consistency in prediction, the evaluation should also clearly demonstrate that part. Current protocols are measuring general fidelity of the predicted frames, and are not specifically focused on long-term consistency. One way to focus on the latter is to condition all video prediction models on ground-truth action trajectories and measure the loss on the part of the trajectories revisiting the observed places. This will isolate how well the model can reconstruct the known places from how good the fidelity of the predicted frames is.\n\n4. When predicting the next frame \\hat{x}_t, the decoder takes both the temporal encoding of the Transformer (h_t) and the prior (z_t). It creates a trivial autoencoding path through the prior (x_t \u2192 z_t \u2192 x_t), which makes it unclear how the model will learn to leverage the temporal encoding h_t.\n\n5. Figure 4 is not quite intuitive to interpret. Based on my understanding, the authors intended to show that TECO is the only method that preserves the size of the environment (which is fixed and bounded), which serves as evidence that it understands the long-term dependency. However, since they all rely on different action sequences, it is hard to isolate its impact. I feel that it would be more intuitive to condition all generative models with the same ground-truth action sequence and show the reconstructed map. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written but it would be great if the authors clarify some parts more. Please read the above review.",
            "summary_of_the_review": "I believe that the paper tackles the interesting and important problem of long-term video prediction, but the technical and empirical significance of each part is unclear from the current draft. It would also be great to have additional experiments to analyze the performance in terms of long-term consistency. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3450/Reviewer_7Nbj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3450/Reviewer_7Nbj"
        ]
    },
    {
        "id": "DYl2ZxhSoju",
        "original": null,
        "number": 3,
        "cdate": 1666720630259,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666720630259,
        "tmdate": 1666720630259,
        "tddate": null,
        "forum": "NQuCQoHqqSY",
        "replyto": "NQuCQoHqqSY",
        "invitation": "ICLR.cc/2023/Conference/Paper3450/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper propose TECO (Temporal Consistent Video Transformer) for long-term video prediction. To make video prediction on long videos feasible, TECO trains on pretrained VQ-GAN codes, using transformer for temporal modeling, and on the top is a decoder (with an expressive prior). Experiments are done on various benchmarks (some are generated from game engine or embodied-AI simulation, e.g., Habitat, one realistic benchmark is also included, e.g., Kinetics-600). Various baselines are also provided. The results show that TECO performs better than other baselines on multiple video prediction metrics. TECO also provides faster sampling while maintain temporal coherence. Written presentation is fair and readable. ",
            "strength_and_weaknesses": "# Strength\n- The proposed TECO has better performance when compared with baselines on standard video prediction metrics.\n- Experiments on various benchmarks, baselines are provided.\n\n# Weakness\n- It is unclear how significant those improvements (metrics) will render on predicted frames. It would be interesting to conduct a user study to test if those improvements make any difference.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Novelty: novelty may be moderate, since TECO is a combination of VQ-GAN, MaskGit, Transformer, CNN encoder and a decoder. The technical contribution may be a whole TECO system that can generate videos (predict future frames conditioned on the current ones) with long-term consistent.\n* Quality: Everything come down to Table 1 which report comparisons of TECO with baselines on 4 benchmarks. TECO provides better metric-results. The question is \"are these results significant enough to justify publication?\" or \"do we need user study to confirm the significant improvement brought by TECO\"\n* Clarity: some minor clarification may be needed. In Fig. 2, the decoder takes z_2 and h_1 as input and predict \\hat{x}_2, while in text (at teh end of page 4), it writes \\hat{x}_t = D(z_t,h_t), is that be a mismatch of subscripts? \n",
            "summary_of_the_review": "Overall, the current submission has moderate novelty and fair experimental results. I currently rate this paper with a borderline rating and would love to hear the authors/other reviewers to discuss about the significance of this work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The is the risk of generating inappropriate content with generative models. But that risk is applied for all generative models, not just only this proposed approach.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3450/Reviewer_HpHv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3450/Reviewer_HpHv"
        ]
    },
    {
        "id": "VBdKFF_kw3n",
        "original": null,
        "number": 4,
        "cdate": 1666932819763,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666932819763,
        "tmdate": 1666932819763,
        "tddate": null,
        "forum": "NQuCQoHqqSY",
        "replyto": "NQuCQoHqqSY",
        "invitation": "ICLR.cc/2023/Conference/Paper3450/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a method for long-term (up to 300 frames) video prediction. The technical method includes \n1) encoding latent code from consecutive frames, \n2) a temporal transformer with autoregressive sampling,\n3) a MaskGit for decoding the dynamic prior, and \n4) a CNN decoder to decode the frame latent code.\nTo validate the effectiveness of the proposed approach, the paper creates and evaluates several datasets, e.g., DMLab, Minecraft, Habitat, and Kinetics-600. The results show that the proposed method outperforms the state-of-the-art video prediction algorithms. ",
            "strength_and_weaknesses": "Strength:\n+ The paper is well-written. The exposition is generally clear. \n+ The method is technically sound. \n+ The evaluation is thorough and convincing. The paper tested two long-term video prediction tasks (DMLab, Minecraft) and showcases generation results on complex natural video (Kinetics-600). The ablation study in Section 4.5 validates the contributions of various technical components. \n\nWeakness: \n- The novelty of the work is somewhat limited. The core difference over prior work on video generation is to first compress the frame latent tokens using an encoder to map concatenated consecutive frame latent code into a new \"Vector-Quantized latent dynamics\". The main efficiency comes from the strided convolution for downsampling discrete latent z_t and a transposed convolution for upsampling after modeling with a transformer. The resolution downsampling and upsampling operations for transforms have been extensively used in methods like VQVAE and VQGAN. \n\n- In Figure 2 shows the Decoder takes h_1 and z_2 as input. Yet, this contradicts Equation (5) where the decoder models p(x_t | z_t, h_t). Which one is correct?\n\n- Section 3.1 presents the core method. I can follow it up with the temporal transform part since they are common operations. But I did not get the motivations and rationale for designing MaskGit iterative decoding for z_t and a CNN for decoding h_t and z_t to frame latent code x_t. The paper only describes what was done, but does not provide reasons. In particular, I think it would make the exposition clearer if the paper shows a simple baseline method (e.g., from frame latent x_t, run a temporal transformer to predict h_t, and then decode h_t back to x_t) so that the readers understand the core differences and modifications are. \n\n- It would be helpful to show how the sequences of frame tokens are generated in the prediction process. It's hard to imagine what the autoregressive steps are with h_t, z_t, and x_t. I can only guess how it might work but it would very helpful if the paper can show it explicitly. For example, during autoregressive sampling, do we use the decoded \\hat(z_t) as input to the decoder? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: As discussed in the weakness section above, I think several parts of the method descriptions are not well motivated. \n\nQuality: The video prediction results are impressive. The quantitative evaluation is solid.\n\nNovelty: The method is technically sound, but the novelty is somewhat limited. Compared to the standard temporal transformer for modeling the video frame latent code, the main differences are \n- learning a CNN to extract features in consecutive frames\n- using MaskGit to decode z_t\n\nReproducibility: I think the paper includes all sufficient details for reproducibility. ",
            "summary_of_the_review": "I think the paper demonstrates great results in long-term video generation. The evaluation is strong. But I am not sure if the technical novelty is sufficient for conferences like ICLR. As discussed earlier, the main source of efficiency for modeling long videos is 1) encoding consecutive frames and 2) downsampling before using a temporal transformer. I thus rate the paper as marginally above the acceptance threshold at this point.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I don't find specific ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3450/Reviewer_JK8Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3450/Reviewer_JK8Y"
        ]
    }
]