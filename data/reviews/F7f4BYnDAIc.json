[
    {
        "id": "-0R2Szi-3GB",
        "original": null,
        "number": 1,
        "cdate": 1666526806311,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666526806311,
        "tmdate": 1671070199605,
        "tddate": null,
        "forum": "F7f4BYnDAIc",
        "replyto": "F7f4BYnDAIc",
        "invitation": "ICLR.cc/2023/Conference/Paper3815/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an efficient sampled transformer with O(n) complexity. The developed method splits the point set into subsets, each one being processed by a shared Hamiltonian self-attention mechanism. The proposed strategy is well-supported theoretically. The method is evaluated on ModelNet40 under self-supervised learning and few-shot learning. ",
            "strength_and_weaknesses": "=== strength ===\n\n1. this paper is well-written and organized clearly. \n\n2. the proposed method is efficient, according to Tab.3. the proposed method is well supported theoretically.\n\n=== weaknesses ===\n\n1. From Tab.3, we can observe that the proposed method is efficient. I would like to see the performance on large-scale point cloud, such as S3DIS and SemanticKitti for segmentation. In addition, the performance on other datasets, like ScanObjectNN is also expected to show the effectiveness. \n\n2. As the proposed method splits point sets into subsets, maybe it would be better if the authors could show the difference from the window-based transformer, like Stratified Transformer [1].\n\n[1] Lai et al., Stratified Transformer for 3D Point Cloud Segmentation.",
            "clarity,_quality,_novelty_and_reproducibility": "See above\n\nReproducibility: code is not available in the submission, but the implementation details might be useful for reproducibility.",
            "summary_of_the_review": "This paper is well-written and the proposed method is interesting. However, more experiments are needed.\n\n== Post-Rebuttal ==\n\nIn the response, the authors respond that they do not conduct experiemnts on the large-scale data, and only on the 2048 points, due to the limited time and computational resources. Alothugh the reported results can show that the proposed method yields comparable performance in comparison with previous methods while requiring less resources, I still think it would be better to conduct comparison on large-scale data, which is important for the application of transformer on large-scale data. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3815/Reviewer_V8rM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3815/Reviewer_V8rM"
        ]
    },
    {
        "id": "WdMOL_5qxfe",
        "original": null,
        "number": 2,
        "cdate": 1666845730646,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666845730646,
        "tmdate": 1666845730646,
        "tddate": null,
        "forum": "F7f4BYnDAIc",
        "replyto": "F7f4BYnDAIc",
        "invitation": "ICLR.cc/2023/Conference/Paper3815/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an O(n) self-attention layer based on the sparse transformer that is permutation equivariant. The paper shows that their proposed architecture is a universal approximator, and present experiments on set-input datasets.",
            "strength_and_weaknesses": "The sparse transformer requires only O(n) computation but is not permutation invariant. This paper proposes a randomized linear-compute method that is permutation invariant in expectation and is a universal approximation of set-to-set functions.\n\nThe narrative around inductive bias is not clear to me. As the authors point out, existing works (Zhao et al, Lee et al) propose O(n) operations that are permutation invariance. Both papers prove the universal approximation property. The paper claims in the introduction that these methods \u201cintroduced additional inductive biases\u201d and \u201ca research question naturally arises to avoid introducing unneeded inductive bias\u201d. Can you elaborate more on (1) why the inductive bias imposed by nearest neighbor search or learned inducing points is unneeded and (2) what evidence or intuition you have for your sampled attention having less of an inductive bias than both of these approaches?\n\nI skimmed through the theory and proofs, and I did not find any major errors.\n\nFor some of the experiments such as Tables 3 and 5, you could also consider the learned inducing point method from Lee et al as a baseline, which I think can be better suited to this problem setting compared to kNN attention.\n\nFor Table 1, 3, 5: do you have error bars?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear. To my knowledge, the proposed architecture is new.",
            "summary_of_the_review": "The main narrative around \"inductive biases\" is unclear to me, and the experiments lack a comprehensive comparison to O(n) attention mechanisms.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3815/Reviewer_AbVu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3815/Reviewer_AbVu"
        ]
    },
    {
        "id": "Yl06mU1ePIM",
        "original": null,
        "number": 3,
        "cdate": 1666913686566,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666913686566,
        "tmdate": 1666913686566,
        "tddate": null,
        "forum": "F7f4BYnDAIc",
        "replyto": "F7f4BYnDAIc",
        "invitation": "ICLR.cc/2023/Conference/Paper3815/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed an O(n) complexity sampled transformer that can process point set elements directly without additional inductive bias. The sampled transformer introduces random element sampling, which randomly splits point sets into subsets, followed by applying a shared Hamiltonian self-attention mechanism to each subset. The overall attention mechanism can be viewed as a Hamiltonian cycle in the complete attention graph, and the permutation of point set elements is equivalent to randomly sampling Hamiltonian cycles. This mechanism implements a Monte Carlo simulation of the dense attention connections. The paper shows that it is a universal approximator for continuous set-to-set functions.  Experimental results for classification and few-shot learning on point-clouds show comparable or better accuracy with significantly reduced computational complexity compared to the dense transformer or alternative sparse attention schemes.",
            "strength_and_weaknesses": "+: The sampled attention mechanism which maps the random permutation of set elements to the random sampling of Hamiltonian cycle attention matrices, permitting the direct processing of point sets.\n+: It proves that the proposed sampled transformer is a universal approximator of continuous set-to-set functions.\n+: Compared to previous transformer architectures, the empirical results show that our proposed sampled transformer achieves comparable (or better) performance with less inductive bias and complexity.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is rather theoretical and a little hard to read especially the proofs. Overall it does seem to be a good paper that is technically sound.",
            "summary_of_the_review": "This paper proposed an O(n) complexity sampled transformer that can process point set elements directly without additional inductive bias.  Experimental results for classification and few-shot learning on point-clouds show comparable or better accuracy with significantly reduced computational complexity compared to the dense transformer or alternative sparse attention schemes.\n\nOverall I feel this is a good paper and hopefully can be very useful for a lot of follow up works.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3815/Reviewer_cTVx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3815/Reviewer_cTVx"
        ]
    },
    {
        "id": "5zWH_MKAtCA",
        "original": null,
        "number": 4,
        "cdate": 1667104824367,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667104824367,
        "tmdate": 1667106000176,
        "tddate": null,
        "forum": "F7f4BYnDAIc",
        "replyto": "F7f4BYnDAIc",
        "invitation": "ICLR.cc/2023/Conference/Paper3815/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a novel transformer model for dealing with point set data, with the main focus on achieving linear time complexity in the attention mechanism. By utilizing the idea of random sampling-based approximation to the vanilla attention from sampled Hamiltonian cycle attention, the authors show that the proposed model can be a universal function approximator of set-to-set functions. Experiments on point cloud data are studied with comparison to several baseline methods, showing that the proposed model can achieve comparable performance with better efficiency.",
            "strength_and_weaknesses": "- This paper is well written and easy to follow. The motivation is clear and the overall structure is well organized.\n- It seems like the theoretical discussion on the universal approximation property is heavily based on the work of [1,2], while the authors also briefly touched on the role of contextual map. However, the definition 3.1 and the paragraph followed do not seem very self-explanatory to me and it's in fact a bit confusing about what are the novel part exactly in the proof. I wonder if the authors can provide more details on this part.\n- One major contribution of this work focuses on achieving linear time complexity on set data. However, the experiments are only conducted on point cloud data, which is a special case of set data. I wonder if the authors can provide more experiments on other set data, such as graph data, to show the generalization of the proposed model. Also, there exists a lot of works on efficient attention in the past few years, but in recent benchmarks and scaling studies, the vanilla transformer is still the best performing one. In a similar spirit, how do the authors think about the contributions of the proposed model in this context?\n\n[1] Yun, Chulhee, et al. \"Are transformers universal approximators of sequence-to-sequence functions?.\" arXiv preprint arXiv:1912.10077 (2019).\n[2] Yun, Chulhee, et al. \"O (n) connections are expressive enough: Universal approximability of sparse transformers.\" Advances in Neural Information Processing Systems 33 (2020): 13783-13794.",
            "clarity,_quality,_novelty_and_reproducibility": "Please see above comments.",
            "summary_of_the_review": "I believe the problem studied in this work is interesting and this work made some contributions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3815/Reviewer_YJ8Q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3815/Reviewer_YJ8Q"
        ]
    }
]