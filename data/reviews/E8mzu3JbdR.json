[
    {
        "id": "5D5Z4QYZfT",
        "original": null,
        "number": 1,
        "cdate": 1666026309327,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666026309327,
        "tmdate": 1670369137955,
        "tddate": null,
        "forum": "E8mzu3JbdR",
        "replyto": "E8mzu3JbdR",
        "invitation": "ICLR.cc/2023/Conference/Paper3013/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose ChordMixer: a simple neural network building block that is able to mix tokens within a sequence. Each ChordMixer block consists of a rotation layer followed by 2-layer MLP. Compared to previous methods, ChordMixer allows attention over variable input length and performs favourably on very long sequence tasks. The authors show results on three toy tasks: synthetic addition problem, long document classification and DNA sequence-based taxonomy.\n\n======== Post-rebuttal update ========\n\nAfter reading other reviewers' comments and the authors' rebuttal, I decided to upgrade my score to \"8: accept, good paper\". The model is simple and clever and the experimental results are convincing. Hopefully the code will be available. Congratulations!",
            "strength_and_weaknesses": "### Strengths\n- The paper is well written and easy to follow. The authors did a good job on related works and provide a clear explanation of the proposed model.\n- The problem tackled on this paper\u2014modelling attention in very large sequences\u2014is of extreme practical importance in many fields of machine learning.\n- The proposed idea is very simple and intuitive. With a relatively simple and straightforward approach based on a P2P protocol, the authors are able to overcome some issues of current methods (eg, variable length and good performance in very long sequences).\n\n### Weaknesses\n- The paper only show results on very toysh tasks. Although I understand that even though standard models fail on those tasks, it would be much more interesting to see results in more realistic tasks (and/or on unsupervised pre-training).\n- It is unclear to me why some models appears in the evaluation of some tasks, but not others. Eg, S4, Luna appear on the \u201cAdding problem\u201d but not on others and the \u201clong document classification\u201d only has 4 models. ow does the omitted models perform on those tasks? are they comparable or better than ChordMixer?\n- As pointed by the authors, this approach also uses the Chord protocol. However, Paramixer does not show on any experimental comparison. It would be nice to compare ChordMixer and Paramixer model, in particular what are the pros and cons of each architecture.\n- As mentioned by the authors, training requires that every batch contains sequences of the same length. Is this an issue? Can this bias the model in some particular way?",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clean and straigthforward to read.\n- The main idea of the paper is very simple and intuitive to follow.\n- The proposed approach is somehow novel. The idea of using Chrod in the context of sequence modelling has already been explored in Paramixer. Nevertheless, ChordMixer is simpler and allows for variable length sequence.\n- The authors propose details about the model and the data used to train/eval it. Moreover, they provide the source code to train the model.",
            "summary_of_the_review": "Based on the comments above, I am rating this paper as \"marginally above acceptance threshold\". The mode is simple, somewhat novel and they show better results than current models on toy datasets with very long sequence lengths. It would be nice if the authors could clarify the points I mentioned on the \"weaknesses\" section.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3013/Reviewer_Bvdo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3013/Reviewer_Bvdo"
        ]
    },
    {
        "id": "MsFKH1EI3eY",
        "original": null,
        "number": 2,
        "cdate": 1666669223929,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669223929,
        "tmdate": 1670111117834,
        "tddate": null,
        "forum": "E8mzu3JbdR",
        "replyto": "E8mzu3JbdR",
        "invitation": "ICLR.cc/2023/Conference/Paper3013/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a method to extend neural attention to longer text with limited computations. With inspiration from the P2P Chord protocol, it rotates the token indices to allow the model to attend to tokens in different scales of distances. It is proved to have a large receptive field compared to traditional transformers while preserving sublinear space and time complexity. Experiments are conducted on both synthetic arithmetic problems, long document classification, and DNA taxonomy tasks. The proposed method, named ChordMixer, is shown to be more effective than its baseline models.",
            "strength_and_weaknesses": "Strengths:\n\n1. ChordMixer is theoretically guaranteed to have sublinear time and space complexity, which makes it efficient on long sequence tasks.\n2. The proposed method is simple and can be efficiently implemented with modern hardware.\n3. ChordMixer is shown to be effective on the tasks presented in the paper.\n\nWeaknesses:\n\n1. Experiments should be conducted on more standard datasets, e.g. Long-Range Arena. Given that there are tons of work on long sequence transformers, comparing them one by one is unrealistic, thus testing ChordMixer on such a dataset is a preferable way. I would expect the ChordMixer to be a point in fig 3 of Tay et al. (2021) (the LRA paper). Working with a new dataset makes it hard to judge the performance of ChordMixer.\n2. In section 4.2, I don't know why other models all need zero padding to align with the maximum length. Some models, e.g. Reformer and Linformer do not have any constraints on the input length and do not need the inputs to be of equal lengths. \n3. ChordMixer might be viewed as a pattern-based method in the taxonomy of Tay et al. (2020), then some baselines seem necessary. E.g. random pattern can similarly achieve a long receptive field with a high probability, and longformer (sliding window + dilated pattern + global token) can also achieve a large receptive field for relatively long sequences, although I think a larger receptive field does not necessarily lead to better interactions between distant tokens.\n4. The presentation needs to be improved. I would give more details in the next section.\n\nQuestions:\n\n1. In section 2, I don't think Longformer and ETC use learnable side memory modules (or do you mean the global token?).\n2. ChordMixer is designed to have sparse routes, while some sequences, such as natural language, and adjacent tokens could be very important contexts. Do you think ChordMixer may be less efficient to capture this information?",
            "clarity,_quality,_novelty_and_reproducibility": "The math in the paper isn't clear. For example, symbols \"x\" and \"z\" are used without definition or description in eq 1. The meaning of the indices i, j, and k also needs to be inferred from the context, which makes it hard to understand the work.\n\nThe novelty of this paper is limited as some pattern-based methods are pretty similar to this work.\n\nI noticed that the codes of this paper have been released, along with all the data used in the paper, thus I do not doubt the reproducibility of ChordMixer.",
            "summary_of_the_review": "The paper proposes a method that mimics the Chord protocol to allow distant attention for long sequence encoding. Although shown to be effective on newly proposed data, more popular datasets should be used so the results can be more convincing. Also, the novelty of this paper is limited given many pattern-based methods have already been proposed with similar ideas. I would suggest a rejection for this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3013/Reviewer_pZas"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3013/Reviewer_pZas"
        ]
    },
    {
        "id": "ApzLxpdMmb",
        "original": null,
        "number": 3,
        "cdate": 1666788070902,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666788070902,
        "tmdate": 1666788070902,
        "tddate": null,
        "forum": "E8mzu3JbdR",
        "replyto": "E8mzu3JbdR",
        "invitation": "ICLR.cc/2023/Conference/Paper3013/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new neural attention model called \"ChordMixer\" to enable long-range interactions for sequences with variable lengths. The proposal takes inspiration from the Chord protocol in P2P networks. The ChordMixer consists of a series of ChordMixer blocks where in each block, an input token with $d/M$ dimensions (M = $log(N) + 1$) first interacts with $k=log(N)$ peers/tokens via the rotation layer. The rotation layer concatenates the representation from peers to its own to get a $d$ dimensional representation for the token. Next, each token is processed by a mixer layer comprising a two-layer MLP which enables sharing of information. A series of $log(N)$ blocks ensure a full-receptive field, i.e., all output tokens can be influenced by each input token.\n                                                                                                      \nExperiments on tasks involving synthetic and real data show that ChordMixer is scalable to very long sequences while outperforming RNNs, CNN, and other efficient attention methods.",
            "strength_and_weaknesses": "**Strengths** \n    \n- The paper is clearly written. The design of the ChordMixer block is novel, conceptually simple, and well-motivated from the P2P network. The figures on the Chord protocol and the rotate layer make it easy to understand.  \n- The experimental results on the artificial and real-world sequences showcase that ChordMixer outperforms recent efficient attention models when dealing with long sequences. The code in the supplementary and the details should make the work easily reproducible.\n    \n\n**Questions and Weaknesses**:\n\n- Page 1 paragraph 2: \"Although numerous architectures such as Transformer and its variants have been proposed, they usually assume constant input length.\" The assumption/requirement stems from efficient GPU processing, thus, it would be better to rephrase this as \"For efficient batch processing, architectures such as....\"\n \n- The paper mentions \"We believe after the mixing, elements at every position summarize good information from all input positions.\". While at the end of $log(N)$ steps, each output token receives information from each input token, the mixing doesn't seem uniform. For instance, consider the case of $i$-th token. Tokens $i+1$ and $i+2$ would mix with it in a single step. In the next hop/block $i+1$ token would again mix with $i+2$ token carrying information received from $i$-th token. In contrast, $\\exists$ a token that mixes with the $i$-th token only at the last hop. Could it affect the performance or introduce inductive biases? It would be good to discuss this case. Note that the paper covers reaching probabilities in Appendix, however, I am not sure if that answers the question completely. \n\n- The paper says \"A ChordMixer network consists of $log(N_{\\text{max}})$ blocks, where $N_{\\text{max}}$ is the length of the longest sequence.\" Does each input get processed with the same number of blocks or do you use fewer blocks for shorter sequences? Another question is what happens if you obtain a sequence of length $2 \\times N_{\\text{max}}$ during inference? Though this may not be likely for datasets with long sequences.             ",
            "clarity,_quality,_novelty_and_reproducibility": " The paper is well-written and easy to follow. The idea of using the Chord protocol to enable long-range interactions is novel and also theoretically well-motivated. The experiments compare against recent attention as well as LSTM and CNN to showcase the efficacy. Code and details are provided for reproducibility.  \n",
            "summary_of_the_review": "As explained above, the idea of ChordMixer is novel and the experiments are thorough. Thus I recommend acceptance for this work. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3013/Reviewer_9e1K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3013/Reviewer_9e1K"
        ]
    }
]