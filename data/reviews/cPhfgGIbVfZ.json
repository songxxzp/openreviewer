[
    {
        "id": "DxvCTKiM3_D",
        "original": null,
        "number": 1,
        "cdate": 1666603189844,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603189844,
        "tmdate": 1670704372214,
        "tddate": null,
        "forum": "cPhfgGIbVfZ",
        "replyto": "cPhfgGIbVfZ",
        "invitation": "ICLR.cc/2023/Conference/Paper5408/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The work tackles the problem of error detection in semantic segmentation. The naive approach of training a binary pixel-wise classifier on the image and the corresponding semantic map (produced by a segmentation network) is extended by perturbing the semantic map with a three types of straightforward transformations. The experiments indicate that such a binary classifier would surpass previous best methods on standard benchmarks of error detection.",
            "strength_and_weaknesses": "Strenghts:\n\n- The idea to use data augmentation is straightforward, easily implementable and seems to be more effective than previous works. \n- I like that there are cross-domain experiments (training on Cityscapes, testing on BDD).\n- There is a fair amount of qualitative results and comparisons which improve the presentation quality.\n\nWeaknesses:\n\nI\u2019m not sure where to position this work.\n\nOn one hand, there are uncertainty estimation methods, such as MC-Dropout, that strive to provide calibrated confidence. One could binarise the confidence to obtain an \u201cerror detection\u201d method. Although there is one comparison to MC-Dropout, the methodology of training this baseline and its architecture specifics (e.g. one needs to place Dropout layers carefully) are unclear. I am curious if the predictions of the error detection network can be used for uncertainty estimation. For example, how would expected calibration error compare to that of MC-Dropout? I also wonder how this approach compares to test-time augmentation, i.e. augmenting the input image and averaging the semantic maps to obtain calibrated confidence. These would be two off-the-shelf baselines.\n\nOn the other hand, there are anomaly detection methods that focus on identifying out-of-distribution phenomena in the test data. Here, the paper lacks appropriate comparisons as well, since these methods are typically evaluated on anomaly detection benchmarks, e.g. StreetHazards, which is missing in the experiments. Running the method on BDD after training on Cityscapes goes in this direction, but both are the datasets of traffic scenes, after all.\n\nThe notation could be improved (e.g. missing Iverson bracket in Eq. 1, 3)\nThe quality of presentation can be improved significantly. There are numerous typos (e.g. \u201cexamples by adjust our [\u2026]\u201d, \u201cwidiscuss\u201d, etc.), missing cross-references (e.g. Sec. 3.2).\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Both the clarity and quality of this work are good, but could be improved further (see a few points on this above).\nThe method should be straighforward to reproduce given its simplicity.\nThe technical novelty is rather modest, since it's an application of the classical data augmentation paradigm to a new domain. The empirical results showing that it's quite effective can be interesting to some audience interested in uncertainty estimation and out-of-domain generalisation.",
            "summary_of_the_review": "Overall, I like this work. However, there is lack of experimentation both in terms of uncertainty estimation and anomaly detection, which needs to be addressed.\n\n**Post-rebuttal comment**\n\nI thank the authors for their response. As before, I like the simplicity of this approach and the consistency improvement over the baseline.\nHowever, I downgrade my rating as I do agree with the other reviewers that the experimentation is a bit too conservative:\n\n* Missing MC-Dropout and TCP in the out-of-distribution setting [R-G1qP]\n* The increase in model capacity is not discussed [R-G1qP]. In fact, as R-2JRb points out, Table 3 suggests that around half of the improvement comes from architectural changes (increasing the number of parameters) rather than from erro augmentation.\n* Missing comparisong to FSNet. I am not sure this would be an apple-to-apple comparsion by just copying the values due to the differences in the segmentation accuracy and the validation set. These points should be discussed, nevertheless.\n* There is not sufficient analysis of this work in the context of anomaly detection and/or uncertainty estimation, as typical in comparable works (e.g. in SynthCP).\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5408/Reviewer_iFWU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5408/Reviewer_iFWU"
        ]
    },
    {
        "id": "2w8e51hJCB",
        "original": null,
        "number": 2,
        "cdate": 1666616550359,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616550359,
        "tmdate": 1666616678478,
        "tddate": null,
        "forum": "cPhfgGIbVfZ",
        "replyto": "cPhfgGIbVfZ",
        "invitation": "ICLR.cc/2023/Conference/Paper5408/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper introduces a set of data augmentation techniques for predicting dense error maps in semantic segmentation.",
            "strength_and_weaknesses": "Strengths:\n\n1. The empirical observation that two separate encoding networks (one for the input image and the other for the prediction map) can boost the prediction performance is a plus.\n\nWeaknesses:\n\n1. The paper looks like a rush one with not-hard-to-spot typos on each page.\n\n2. The reviewer is not a fan of augmentation, as it is generally not transferrable to unseen augmentations. To correct the reviewer's biased view, the authors are encouraged to conduct cross-model experiments (e.g., training on FCN-8s / DeepLab v2 and testing on other segmentation models).\n\n3.  How to do boundary handling in ShiftSwap and how to control the similarity level between swapped map and the original one in MapSwap?\n\n4. The primary motivation for developing a failure predictor, either global or local, is to spot hard examples (in an online fashion) to prioritize the training process or to identify catastrophic failures (in an offline fashion) to actively fine-tune the model for improved performance. These results however are not obtained in this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Simple to understand despite somewhat sloppy presentation.\n\nQuality and Novelty: Somewhat limited due to Point 4 in the above section.\n\nReproducibility: Most parts of the paper are readily reproducible. ",
            "summary_of_the_review": "With better experimental designs (including prioritizing the training process and active fine-tuning) and more comprehensive evaluations (in cross-model and cross-dataset settings), this paper may have a chance to get in. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5408/Reviewer_2Ds6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5408/Reviewer_2Ds6"
        ]
    },
    {
        "id": "Rm-Poa4AVGn",
        "original": null,
        "number": 3,
        "cdate": 1666877518514,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666877518514,
        "tmdate": 1666877554538,
        "tddate": null,
        "forum": "cPhfgGIbVfZ",
        "replyto": "cPhfgGIbVfZ",
        "invitation": "ICLR.cc/2023/Conference/Paper5408/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an error augmentation neural network to identify the errors made by semantic segmentation models by manipulating the predicted labels fed to the error detection network. \n\nThree transforms are tested - shifting the prediction, swapping a class or completely changing the prediction labels. Along with the RGB image, one of these augmented prediction labels is selected at random and fed to dual-branch error detection NN that yields the final error map.\n\nThe authors claim state-of-the-art performance, with +~7.7 % AUPR error (49.99 vs 53.84) on Cityscapes in-distribution results. Furthermore, even out-of-distribution tests (BDD100K) yield +13.6 % AUPR improvement.",
            "strength_and_weaknesses": "### Strengths\n\n- solid performance improvements over predecessor (SynthCP)\n- straightforward error augmentation strategy - ShiftSwap, ClassSwap, and MapSwap involves shifting the image, swapping a class or the whole prediction\n- fairly intuitive insight - if mixing unlabelled samples works for semi-supervised learning [1*] and domain adaptation [2*], variations of the method for error detection should also improve error detection performance\n- can be added on top of any segmentation network\n\n### Weaknesses\n\n- generally poorer results compared to FSNet[3*] -- please cite and compare with\n   - considering the (arguably most relevant metric) AUPR, the method yields 59.84 vs 67.83 on FCN8 ( _+22.15%_ for FSNet compared to +7.8% ErrorAug vs SynthCP) and 53.84 vs 57.84 on DeepLabv2 ( _+15.70%_ for FSNet compared to +7.7% ErrorAug vs SynthCP)\n- ~50% of improvements are due to the architecture input change\n  - even _without any augmentation_ , we are at ~+3.7 over SynthCP -- this means ~half of the error reduction is due to the different way of training the architecture, and only ~4% are due to the augmentation itself\n  - I would speculate that a more powerful architecture and more augmentations would yield better results; nevertheless, the ablation study is frugal (I would add tick marks for each augmentation, I believe they are incremental?) and there are not many options explored (e.g., what happens if all three labels are concatenated, instead of the 1/3 chance of picking one)\n\n\n\n___\n[1*]Tranheden, W., Olsson, V., Pinto, J., & Svensson, L. (2021). Dacs: Domain adaptation via cross-domain mixed sampling. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 1379-1389).\n\n[2*]Olsson, V., Tranheden, W., Pinto, J., & Svensson, L. (2021). Classmix: Segmentation-based data augmentation for semi-supervised learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 1369-1378).\n\n[3*] Rahman, Q. M., S\u00fcnderhauf, N., Corke, P., & Dayoub, F. (2022). Fsnet: A failure detection framework for semantic segmentation. IEEE Robotics and Automation Letters, 7(2), 3030-3037.",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty:\n\nThe paper uses augmentation concepts that have proven valuable in semi-supervised learning for error detection. The contributions consist of mixing multiple augmentation strategies together and a different use of the architecture for error detection. While the latter looks very similar to SynthCP (ResNet-18), only the way the data is passed differs (concatenate RGB + prediction vs separate branch foreach - probably the biggest single insight of this paper).\n\nClarity:\n\nThe paper has only minor issues, but could use some help. E.g., Figure 2, please show somehow it's a dual branch network. I would shrink related work and add 3 rows from Fig 6 supp, it make the transforms clearer. Or make a mix of Fig 2+4+6, it would help understanding the common artifacts. \n\nNot sure on how the testing on BDD100K was carried out, the whole training set was evaluated? FSNet claimed they picked a random 1k sample set.\n\nQuality:\n\nAlthough the problem and results are well described and mostly well reported, I believe the current pipeline is under-investigated. If a 6-year-old dual-branch NN with separate RGB / prediction yields half of the improvements, why not try a newer one? Why not concatenate the prediction augmentations? why not also cut the RGB image in random / semantic segmentation pieces? I would be very nice to see an edge label contamination discussion, similar to the ClassMix paper).\n\nReproducibility:\n\nMost information is available in the paper for reproducing the results. \n\nMisc\n\nMinor paper issues, such as wediscuss (p4), Figure reference issue (p5),  uncertainity (p7)",
            "summary_of_the_review": "Although the paper yields good results on error detection from semantic segmentation, a (mostly) better performing / arguably SOTA method is left out. Disregarding the omission, I believe the augmentation techniques are under-exploited and could benefit from a better investigation. Otherwise, I believe the novelty is fairly limited, since shifting/pasting other labels have been tried and tested for better/domain adapted semantic segmentation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5408/Reviewer_2JRb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5408/Reviewer_2JRb"
        ]
    },
    {
        "id": "whaRuWpCmGs",
        "original": null,
        "number": 4,
        "cdate": 1667338265130,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667338265130,
        "tmdate": 1667338265130,
        "tddate": null,
        "forum": "cPhfgGIbVfZ",
        "replyto": "cPhfgGIbVfZ",
        "invitation": "ICLR.cc/2023/Conference/Paper5408/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "ErrorAug proposes to use data augmentation for direct prediction of errors in semantic segmentation models. By augmenting errors in a model's predictions by swapping classes, shifting predicted mask or swapping segmentation from a different image, ErrorAug is able to train a more robust direct error prediction model. This is shown by showing better transfer of error prediction from Cityscapes to BDD100K, which has much more diverse imagery along with a domain shift from Europe to North America. The paper proposes to use a siamese architecture for direct error prediction instead of concatenating image and label in the channel dimension, which already beats synthesize and compare (SynthCP, ECCV 2020) on area under the precision-recall curve (Ap-Err, error prediction is treated as binary classification) by ~2 points for an FCN-8s model. The proposed data augmentation strategies additionally improve Ap-Err by ~2 points on Cityscapes semantic segmentation using FCN-8s. The paper also shows results using Deeplab-v2 as the base segmentation model. ",
            "strength_and_weaknesses": "# Strengths\n\n- The simplicity of the proposed method over SynthCP that requires image synthesis in the loop is a big strength\n- The proposed error augmentations are sound and well motivated. They cover class prediction errors and segmentation errors. The proposed map-swap augmentation smartly takes advantage of the fact that autonomous driving scenes often have correlated scene structure\n- While the paper does not suggest this, error augmentation cou\n\n# Weaknesses\n\n- Evaluation follows SynthCP (from 2020) and only uses FCN-8s and Deeplab-v2 as the base segmentation models. At this point in time, these are far from state of the art models, which make much more fine-grained errors in Cityscapes semantic segmentation. It'd be important for the community to understand whether ErrorAug can be used to recover these finer errors in state of the art models such as ViT-Adapter\n\n- The proposed siamese architecture for direct prediction uses double the parameters from the \"OldArch\" baseline (the naming convention could be much better here). It is therefore unclear if it's the siamese architecture or the increased parameter count that improved error prediction performance. A baseline for OldArch with comparable parameters would be ideal\n\n- How does MCDropout and TCP do on BDD transfer? The transfer results do not show these\n\n- The paper compares with SynthCP as is, which uses SPADE (CVPR 2019) as its conditional image synthesis backbone. In the 3 years since, there have been multiple improvements in image synthesis and it is entirely possible that a better conditional image synthesis backbone improves SynthCP's performance considerably",
            "clarity,_quality,_novelty_and_reproducibility": "- I perceive the proposed method as an improved baseline method for future work on error prediction of semantic segmentation methods, which is an important contribution\n- To the best of my knowledge, I believe the idea is novel\n\nThere are a few writing and clarity issues that a revision would easily solve\n- At the end of page 3, the \"existing model architectures struggle to maintain consistent performance\" is repeated twice\n- Eq. 1,2 and Eq. 3,4 are pretty much the same and I believe the data augmentation could be folded into the equation to make it clearer, rather than leaving the definition of $e'$ and $\\hat{y}'$ in text\n- End of page 5 has a missing figure reference\n- It is unclear which model was used for the BDD transfer experiment\n- Training details are completely missing in the paper, which is a big detriment to reproducibility",
            "summary_of_the_review": "Overall, ErrorAug shows that data augmentation can make direct error prediction from a model achieve better results than the state of the art method SynthCP, which synthesizes images conditioned on a predicted segmentation map and uses a learned comparison module to predict pixel-wise errors (and detect anomalies / predict IoU, which ErrorAug does not focus on) in a segmentation prediction. This should serve as an improved baseline and help the sub-community move forward. \n\nHowever, ErrorAug is evaluated with at this point quite outdated semantic segmentation models and it is unclear whether state of the art segmentation model errors could be predicted well with these models. Moreover, due to the progress in conditional image synthesis, it is entirely possible that SynthCP, when implemented with newer image synthesis techniques would improve its performance. \n\nDue to this, even though I believe the contribution and idea are valuable, I cannot recommend acceptance of the paper in its current form since the findings from the paper might not hold up when using state of the art methods from 2022. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5408/Reviewer_G1qP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5408/Reviewer_G1qP"
        ]
    }
]