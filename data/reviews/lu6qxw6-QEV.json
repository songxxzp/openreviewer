[
    {
        "id": "--EnFN7k3E",
        "original": null,
        "number": 1,
        "cdate": 1666056812878,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666056812878,
        "tmdate": 1668914656899,
        "tddate": null,
        "forum": "lu6qxw6-QEV",
        "replyto": "lu6qxw6-QEV",
        "invitation": "ICLR.cc/2023/Conference/Paper3053/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is on safe reinforcement learning (RL) with unknown stochastic environments. Under the safety definition in terms of reachability, the authors aim to enforce hard constraints rather than soft constraints that are often the main target of constrained Markov decision processes (CMDP) literature. In this paper, the authors introduce a notion of generative model based soft barrier functions to encode the hard safety constraints. The authors propose a safe RL algorithm to jointly model the environment and optimize the agent policy while satisfying the safety (reachability) constraints. Finally, this paper demonstrate the effectiveness of the proposed method in several benchmark problems.",
            "strength_and_weaknesses": "### Strength\n- Important and relatively-unexplored problem settings on safe RL with **hard constraints**.\n- New algorithms based on shielding and generative models\n- Good experimental results compared to CMDP methods and FOCOPS*.\n- Overall, this paper is well-organized and easy to follow.\n\n### Weakness\n- Safe model-based RL\n    - There are significantly few references to safe model-based RL. I know there is the following sentence, but still I consider the relevant methods should be mentioned in related work section and compared to the authors' proposed method.\n    - > \"However, these methods do not apply to our setting, where the environment is uncertain and stochastic without explicit dynamics, backup safe policy, or human intervention.\"\n    - Hard constraints in safe RL has been studied in unknown (stochastic) environment. For example, Moldovan and Abbeel (2012) studied safe exploration in unknown environment in terms of ergodicity. Also, Turchetta et al. (2016) and Wachi et al. (2018) studied safe exploration problems in unknown environment while guaranteeing reachability and returnability under regularity assumptions. I think the authors should have surveyed wider safe RL methods (the three papers I mentioned are all hard safety constraint; that is, the agent is required to satisfy the safety constraint at every time step).\n    - Though the above papers are indeed different from the authors' one in terms of problem formulation or assumptions, Yarden et al. (2021) can be mentioned in related work section and compared to the authors' method in the experiments. If I understand correctly, the method in Yarden et al. (2021) can be fairly compared to the authors' one, and source code is also open-sourced (https://github.com/yardenas/la-mbda),\n\n- Experiments\n    - Benchmark problems are very easy. Hopefully, Safety-Gym should be used for testing safe **deep** RL methods.\n    - Computational time and sample efficiency should be also compared. I \"guess\" the authors' proposed method requires much more data until convergence.\n\n### Minor comments and typos\n- [Section 3.1. Equation regarding $J$] I think $t$ should be $i$ (or $i$ should be $t$).\n- [Definition 2] $\\tau(t)$ is not defined. Is $\\tau(t) := [ s(i) ]_{i=t}^T$?\n\n### References\n- Moldovan, T., and Abbeel, P. 2012. Safe exploration in Markov decision processes. In International Conference on Machine Learning (ICML).\n- Turchetta, M.; Berkenkamp, F.; and Krause, A. 2016. Safe\nexploration in finite Markov decision processes with gaussian processes. In Advances In Neural Information Processing Systems, 4305\u20134313.\n- Wachi, Akifumi, et al. \"Safe exploration and optimization of constrained mdps using gaussian processes.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018.\n- As, Yarden, et al. \"Constrained Policy Optimization via Bayesian World Models.\" International Conference on Learning Representations. 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThis paper is overall organized and easy to follow. I feel Section 3.1 is a bit messy and relatively hard to follow, but I don't have an idea for improving it.\n\n### Quality\n- As I mentioned in Weakness, the quality of related work section should be improved by surveying and referring more papers (especially on safe model-based RL). Also, in experiments section, the authors' proposed method should be compared with more baselines (e.g., Yarden et al. (2021) in more complicated environments (e.g., Safety-Gym).\n\n### Novelty\n- As far as I know, this proposed method is novel, but generative model as a form of world models has already exists; hence, several sentences should be modified.\n\n### Reproducibility\n- The source code is not attached and it is not sure whether it is open-sourced upon publications, so I need to say that reproducibility is low for now.\n",
            "summary_of_the_review": "I think this paper has several merits in terms of 1) important problem settings, 2) new algorithm, 3) good experimental results (easy benchmark and limited baselines), and 4) good writing.\n\nHowever, I have several concerns in terms of 1) how to deal with existing safe model-based RL literature and 2) empirical experiments, as stated in Weakness.\nTherefore, I recommend rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3053/Reviewer_H8ou"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3053/Reviewer_H8ou"
        ]
    },
    {
        "id": "wKBkMe5lAS0",
        "original": null,
        "number": 2,
        "cdate": 1666385514377,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666385514377,
        "tmdate": 1666385514377,
        "tddate": null,
        "forum": "lu6qxw6-QEV",
        "replyto": "lu6qxw6-QEV",
        "invitation": "ICLR.cc/2023/Conference/Paper3053/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The focus of this work is on learning safe MDP policies, in the sense that a safety constraint holds across the entire trajectory of the policy. The distinction from the widely-studied CMDPs is that the constraint in the CMDP holds over a cost aggregated across the entire trajectory, and not at each point in the trajectory. The approach in this work is to learn a barrier function that is a supermartingale over the filtration defined by transitions, and thus with high probability is not violated during the execution. There is a (weak) theorem showing that in the asymptotic limit, the policy achieves safety. More compellingly, experiments on a few control benchmarks suggest that the method indeed avoids unsafe regions.",
            "strength_and_weaknesses": "The main strengths of the paper are (1) the approach to learning the barrier function seems to be fairly novel in this setting and (2) the experiments do seem to be pretty promising. In particular, on the technical side, I did appreciate the use of the (locally-defined) supermartingale constraint on the barrier function to obtain the safety bound.\n\nThe main weakness is that the paper is a bit of a mess -- I'll discuss this below.\n\nAn asymptotic theoretical guarantee is included, which is better than nothing, but certainly much weaker than we'd want to have if the objective was to ensure safety. Remark 1 proposes a \"practical safety probability lower bound\" that is used to obtain bounds reported in the experiments, that are indeed less than the empirical fraction of safe trajectories (1), but it isn't immediately obvious that the bound is valid.",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned above, the clarity is a substantial issue with this paper. For example, it seems that the notation changes at various points in the proof of the one theoretical claim of the paper (that or the objects under discussion appear without a proper introduction).  Along these lines, I am really not sure what l(tau,\\hat{\\tau}) means in the generative model loss function -- a likelihood is obtained for a realization against a distribution (model), but this seems to be a likelihood between two realizations. I am guessing that \\hat{\\tau} represents the distribution of the model, but that's just a wild guess.\n\nLikewise, I can't make any sense of the plots in Figures 4-5---what's on the x-axis? And, what method is used to obtain the \"gene\" trajectory in Figure 3?\n\nOtherwise, the quality and novelty of the work seem fairly strong, as noted above.",
            "summary_of_the_review": "The paper's presentation is rough, but the approach has some significant technical novelty and seems to be effective empirically.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3053/Reviewer_MNvg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3053/Reviewer_MNvg"
        ]
    },
    {
        "id": "SndXg4a_p0",
        "original": null,
        "number": 3,
        "cdate": 1666679669138,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679669138,
        "tmdate": 1666688274950,
        "tddate": null,
        "forum": "lu6qxw6-QEV",
        "replyto": "lu6qxw6-QEV",
        "invitation": "ICLR.cc/2023/Conference/Paper3053/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed ways to do safe RL in an unknown stochastic environment. Specifically, they model the problem in hard constraints instead of the standard CMDP constraints and derived a novel barrier function method to incorporate the hard constraints into the RL algorithm. The authors manage to show with theoretical results that such an approach asymptotically achieves safety in policy optimization in RL, and they demonstrate the effectiveness of the proposed method on several safe RL benchmark problems for which the proposed method outperform SOTA in terms of safety guarantees and return performance.\n",
            "strength_and_weaknesses": "Strengths:\nThe barrier-based method for guaranteeing safety  is novel and is related to the work of guaranteeing safety in the form of stability in model-based RL algorithms. \nThe problem of enforcing RL policy to be safe w.r.t. to hard constraints is interesting and important (in areas e.g., robotics).\nThe algorithms proposed is justified with theoretical studies (which utilize martingale properties of the barrier function to obtain a safety results in the asymptotic sense) and its benefits are also illustrated with some experiments over standard CMDP methods. \n\nWeaknesses:\nIt seems the safety bound only holds in asymptotic, and it is unclear whether this method ensures the property of safety during training. \nThe experiments seem to be overly simple. Running this algorithm on more realistic benchmarks (e.g., Safety gym for safe RL experimentations) and doing more comparisons with other model-based safe RL methods will be better in terms of demonstrating the effectiveness of this method.\nIt is unclear whether the safety bounds provided in the theory can be justified by the experimental results \nThe quality on the writing of the proofs is currently quite low, it requires more work to explain the notations and intuitions clearly in order to make it readable.",
            "clarity,_quality,_novelty_and_reproducibility": "Currently the paper is quite hard to follow. There are lots of notations in various parts of the proofs that have not been formally defined and well explained. Also some notations seem to be repetitive and not well defined (e.g., \\tau), and for example the generative model loss is not clearly defined.  This makes checking the theoretical proofs of this work rather difficult. Several descriptions at the plots are also unclear or missing, which makes parsing the results difficult.\n\nWhile some experimental setup has been discussed, it does not seem sufficient for readers to reproduce the results without the source code pf these methods. Thus this affects reproducibility.",
            "summary_of_the_review": "This paper studied an important safe RL problem and proposed a novel barrier-function based method. It also showed this method's effectiveness both theoretically (in asymptotic sense) as well as empirically on several safety RL benchmarks. However, it'd be great to verify the benefits of this method by 1) testing in more complex domains and 2) comparing with more model-based safe RL methods (e.g., Turchetta, M. et al Safe exploration in finite Markov decision processes with gaussian processes. NeurIPS 16). The writing of this paper also needs significant improvements for it to be readable/understandable by the broader ML audience. The current paper is quite hard to follow.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3053/Reviewer_YtE9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3053/Reviewer_YtE9"
        ]
    },
    {
        "id": "B80fd_6byDW",
        "original": null,
        "number": 4,
        "cdate": 1667554235651,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667554235651,
        "tmdate": 1667554235651,
        "tddate": null,
        "forum": "lu6qxw6-QEV",
        "replyto": "lu6qxw6-QEV",
        "invitation": "ICLR.cc/2023/Conference/Paper3053/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the problem of safe reinforcement learning with hard constraints in an unknown stochastic environment. The key idea (and the contribution) is the proposed generative model based soft barrier functions that relax the hard safety constraints adapting to the unknown environment. This is further incorporated into the presented safe RL framework as a bi-level optimization process for jointly learning the optimal control policy, the soft barrier functions, and the initially unknown environment model, with avoidance of unsafe region in a probabilistic setting. Experimental results with two baseline approaches such as PPO-L and FOCOPS are presented to show the effectiveness of the proposed algorithm. ",
            "strength_and_weaknesses": "Strength:\n+ The general idea of this paper is well written with clear motivation and good overview about the high level idea of the algorithm.\n+ The bi-level optimization framework that jointly learns the policy and soft barrier functions is interesting.\n \nWeakness:\n- While the reviewer found the comparative analysis with CMDP algorithm is sound and solid, it is unclear how the algorithm could compare with other model-based safe RL algorithms, especially methods that combine RL and control theoretic approaches such as control barrier functions.\n- The related work discussion needs to be improved, as some claims seem inaccurate given existing approaches that do not require explicit known model nor an initial safe policy, e.g. [Berkenkamp et al 2017].\n- The proof of safety (Lemma 1) is substandard in absence of supplementary materials. For example, it is unclear how the conclusion of bounded probability for the ENTIRE trajectory could be reached with the presented proof, especially given the stochastic environment.\n- The experimental results are reported with few trials and no quantitative performance in terms of mean and variances are given to justify the robustness of the performance.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The studied problem of safe RL with bounded constraints satisfaction is an interesting topic and has gained significant attention in the learning and control community. The paper is in general well written to highlight the motivation, existing challenges, and the adapted approach of the safe RL framework with soft barrier function learning. With that being said, however, the technical details seem insufficient to justify the theoretical guarantee of the probabilistic safety and it is unclear how the algorithms would compare with those existing approaches that are not based on CMDP framework. Please see detailed comments as follows.\n\n- Contribution and Novelty: the contribution of the proposed soft barrier functions and the bi-level optimization based safe RL framework is overall well presented, and the authors have made a good point in comparison to other CMDP based approaches. However, as mentioned it is unclear how the algorithms would compare to model-based learning with control theoretic approaches for safety considerations. Although it is claimed in the related work that \u201cthese approaches either require explicit known system models for barrier or shielding construction, or an initial safe policy\u2026\u201d, the following work did not require either of them:\n[1] Berkenkamp, F., Turchetta, M., Schoellig, A., & Krause, A. (2017). Safe model-based reinforcement learning with stability guarantees. Advances in neural information processing systems, 30.\n\t[2] Taylor, Andrew, Andrew Singletary, Yisong Yue, and Aaron Ames. \"Learning for safety-critical control with control barrier functions.\" In Learning for Dynamics and Control, pp. 708-717. PMLR, 2020.\n\n- Quality: one major concern is the technical quality in terms of proof of safety for the proposed bi-level optimization process. Given the unknown and stochastic environment, it is challenging to learn both barrier functions and the optimal policy safely without making further assumptions, e.g. bounded Lipschitz continuity, etc. With the presented analysis, it is unclear to the reviewer that (a) if the barrier functions are always learnable before violating a safety constraint, and (b) if the probability of the entire trajectory is always bounded with a satisfying probability due to unbounded stochastic noise. In the carpole example, a valid safe control policy might not exist if the cartpole is already at the boundary of a safety constraint with some initial velocity. And when the time horizon is very large, it seems intractable to bound the safety probability given the cascading effect of the step-wise unsafe probability. More discussions on this point could be helpful to make the safety conclusion more convincing. \n\n- The experimental results are reported with few trials and no quantitative performance in terms of mean and variances are given to justify the robustness of the performance.\n",
            "summary_of_the_review": "The paper presents an interesting topic of safe RL in an unknown and stochastic environment. While the general idea is clear, the reviewer feels that the technical part should be substantially improved given the insufficient details regarding safety guarantee, which is the core part of the presented approach. The experimental results also seems substandard without extensive trials under different environment settings. Authors are encouraged to address the comments in the future version of the paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3053/Reviewer_kxrF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3053/Reviewer_kxrF"
        ]
    }
]