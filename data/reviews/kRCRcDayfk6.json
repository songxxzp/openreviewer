[
    {
        "id": "xqZQytH5ayV",
        "original": null,
        "number": 1,
        "cdate": 1666624067483,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624067483,
        "tmdate": 1666624067483,
        "tddate": null,
        "forum": "kRCRcDayfk6",
        "replyto": "kRCRcDayfk6",
        "invitation": "ICLR.cc/2023/Conference/Paper4412/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presented an interesting idea for accelerating the training procedure of SNNs. The proposed SNN is considered to reduce consumption by enabling SNNs to maintain at most one firing spike. \n\nThe organization of this paper is clear, and citations are also appropriate. The experiments conducted on various datasets verify the effectiveness of the proposed method.\n\nMajor Issuses:\n1. As claimed in this paper, the single-spike model can speed up the training procedure of SNNs due to a decreasing number of firing spikes. Thus, it is natural to verify the computational complexity of the single-spike and multi-spike models, instead of running time. In other words, the running time is not sufficient, and the computational complexity is very likely to be the same order of magnitude. \n2. As shown in this paper, when the authors reduce the number of firing spikes, the single-spike neural network not only has a faster training procedure, but also performs other comparative models, even avoiding the problem of \"dead neurons\". According to Occam's Razor, what is the price of superior performance? It is significant to be discussed in this work.",
            "strength_and_weaknesses": "mentioned above.",
            "clarity,_quality,_novelty_and_reproducibility": "mentioned above.",
            "summary_of_the_review": "Overall, I believe this is an interesting paper that focuses on an energy saving way of SNNs. However, I still find that there is something has been hidden by the authors, which urgently need to be analyzed and discussed in depth. Therefore, I tend to accept this paper if the authors fixed these issues in the next phase.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4412/Reviewer_5huF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4412/Reviewer_5huF"
        ]
    },
    {
        "id": "rY4cX4UVo9",
        "original": null,
        "number": 2,
        "cdate": 1666668636469,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668636469,
        "tmdate": 1668937938447,
        "tddate": null,
        "forum": "kRCRcDayfk6",
        "replyto": "kRCRcDayfk6",
        "invitation": "ICLR.cc/2023/Conference/Paper4412/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper unrolls the update formula of the LIF model along the time axis and uses the vectorized variables for parallel acceleration. It proposes a simple method to determine when the first spikes are fired. In this manner, both the goal of speeding up training and lowering the spike counts are accomplished.",
            "strength_and_weaknesses": "Strength\uff1a\n1. This material is generally well-presented and easy to read.\n2. It\u2019s a good idea to speed up the SNN training using parallel acceleration.\n\nWeakness:\n1. When comparing accuracy, acceleration, and spike reduction, specific corresponding baseline models must be provided.\n2. The simulation time steps in the experiments are usually higher than the required value. E.g., MNIST datasets may only need 8 simulation time steps, the same as conv MNIST. When the simulation time steps are small, the training acceleration of this method in the paper is not so significant as the authors claim.\n3. The article's novelty and contribution are modest, and it offers few enthralling or motivating insights.\n\nQuestions:\n1. Which part of the paper's algorithm is robust enough to declare in the title?\n2. In Table 1, most of the experimental results are worse than the existing methods, especially on more complex convolutional networks. So, I doubt whether the authors' method can be implemented into mainstream deeper convolutional to take full advantage of the training efficiency.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. It provides a novel approach to decrease the spiking rate. ",
            "summary_of_the_review": "Overall, this paper proposes an interesting idea to speed up the training SNN  through parallel acceleration. Based on the description, I think the acceleration is real but the deficit of representation power of large models remain concerned as well as its advantage of the quantized model. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4412/Reviewer_cHb7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4412/Reviewer_cHb7"
        ]
    },
    {
        "id": "Xd1Wo3uaGpF",
        "original": null,
        "number": 3,
        "cdate": 1666704107304,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666704107304,
        "tmdate": 1666704107304,
        "tddate": null,
        "forum": "kRCRcDayfk6",
        "replyto": "kRCRcDayfk6",
        "invitation": "ICLR.cc/2023/Conference/Paper4412/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors use spiking neural networks for image classification. They use time-to-first-spike (TTFS) coding. After a neuron has fired, any eventual subsequent spike is blocked. Training is done with a surrogate gradient.",
            "strength_and_weaknesses": "STRENGTHS:\n\nTTFS is appealing because it consumes few spikes, therefore little energy\n\nWEAKNESSES:\n\nThe authors limit themselves to extremely simple toy datasets (not even CIFAR!), and even on these datasets, the method is not competitive w.r.t. other proposals (see Table 1).\nZhou et al AAAI 2021 should be included in Table 1. They have decent results on CIFAR, and even on ImageNet, using TTFS!\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Nothing to declare.",
            "summary_of_the_review": "There is a new method for training TTFS-SNNs, but it's unclear what the advantages are w.r.t. existing methods.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4412/Reviewer_HU9D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4412/Reviewer_HU9D"
        ]
    },
    {
        "id": "i1mLmsbYBKI",
        "original": null,
        "number": 4,
        "cdate": 1666919644985,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666919644985,
        "tmdate": 1666919644985,
        "tddate": null,
        "forum": "kRCRcDayfk6",
        "replyto": "kRCRcDayfk6",
        "invitation": "ICLR.cc/2023/Conference/Paper4412/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed a new modal for training single-spike SNNs, which offers a 13.98x training speedup compared to a multi-spike counterpart. The speedup is achieved by avoiding all sequential dependence on time and exclusively relies on GPU parallelizable non-sequential operations. The effectiveness of the proposed method is reflected by five different datasets, performing on par with multi-spiking counterparts. ",
            "strength_and_weaknesses": "Strength:\n+1. The submission is well written. \n\nWeaknesses:\n-1. Single spike SNN itself does not make sense to me, which is against the bio-plausibility of SNN. Essentially, the single-spike scheme dramatically impacts the effectiveness of SNN's temporal information. The authors also pointed it out in the section \"Single-spike neurons solve challenging temporal problems using neural heterogeneity\" and remediated the problem by learning time constants. But, the accuracy is much worse than Perez-Nieves'. \n\n-2. The only motivation for using single-spike SNN is to enhance energy efficiency. But, the authors did not show any quantized experimental results to support the motivation. How much energy could it save on neuromorphic hardware?\n\n-3. For LIF, we normally set the reset potential to 0. Is \"calculate membrane potentials without reset\" still valuable if that is the case? \n\n-4. The motivation for training speedup is unclear to me. Yes, the 17x looks awesome. But how long does it take to train a multi-spike SNN model for the tested tasks? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good.\n\nReproducibility should be good as the authors provided the code.\n\nI did not see much novelty.\n",
            "summary_of_the_review": "To me, the valuable part of SNNs is their bio-plausibility, especially in the temporal domain. Single-spike itself does not make sense to me, and the authors did not show the quantized experimental results on energy saving. Thus, the motivation for this work is unclear to me. In addition, the current neuromorphic hardware can barely mimic the simplest biological neuron. The fundamental research in the SNN domain should not be limited or favor current neuromorphic hardware. Instead, fundamental research should lead the development of neuromorphic hardware. \n\nAt the moment, I would not recommend this work. But I would like to see the authors' response.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4412/Reviewer_1hwt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4412/Reviewer_1hwt"
        ]
    }
]