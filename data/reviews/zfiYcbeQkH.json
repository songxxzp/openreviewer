[
    {
        "id": "NZac4_IDVY",
        "original": null,
        "number": 1,
        "cdate": 1666473746927,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666473746927,
        "tmdate": 1666473746927,
        "tddate": null,
        "forum": "zfiYcbeQkH",
        "replyto": "zfiYcbeQkH",
        "invitation": "ICLR.cc/2023/Conference/Paper6140/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces SciRepEval, which is a novel benchmark for training and evaluating scientific document representations. Additionally, the struggles of multi-task learning with regards to generalization in these tasks is explored. An alternative that explicitly encodes the task type is offered and evaluated.\n",
            "strength_and_weaknesses": "Strengths\n\n- Not only are we treated to a single task in this benchmark, but 25, including classification, regression, and recommendation, each of which have appropriate measures for evaluation (though other measures may also have been appropriate).\n- Given the variety of tasks, a \u2018multi-format\u2019 method of representation learning is proposed which is not entirely novel on its own, but is additively useful within this paper more ostensibly about the benchmarking suite.\n- Suitable baselines (SciBERT, SPECTER) and several extensions (e.g., CTRL-code and embedding adapters) provide a suitable evaluation suite.\n\n\nWeaknesses\n\n- Although much of the information is found in the Appendices, more detail on the tasks themselves are expected within the main body, especially with regards to the source of the data, which could likely be added to Table 1, in some form.\n- Tables 2 and 3 and their discussion in the text require some additional context to evaluate these results, especially with regards to the units of measure and highlighting that the first two columns are not exactly comparable with the third (SciDocs). Given the relative similarities of the performance between SPECTER and SCiNCL (especially considering the variants of each), statistical significance tests should be performed.  \n\n\nMinor\n\n- The use of task-specific control codes is reminiscent (somewhat) of ELMo, and a comparison or acknowledgement should be made to some of the literature around that model.\n- Some further explanation around the deficiencies of SciDocs in Sec 2 (especially with regards to the correlations and easiness of negative candidates) really should be provided, especially as SciRepEval\u2019s superiority is meant to be evaluated in contrast to the former.\n- There is a higher-than-usual amount of repetitiveness, especially around the 25 tasks in SciRepVal in the first few pages, but this is a very minor complaint\n- Some deeper dive into how performance is affected by covariates such as the field of study (Table 8) after one filters out the effect of data set size would be interesting.\n- Be sure to check the formatting of your references\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-written. It does not go into very much technical depth, though it doesn\u2019t need to as a benchmarking paper, although the nature of the data could have been explored more fully. It is an extension of a previous benchmark in this space; while it is a substantial extension, this still limits its novelty. \n",
            "summary_of_the_review": "This paper extends an existing benchmark substantially, and offers a new approach to multi-task learning meant to deal with the multitude of tasks in a way that is not wholly novel, and beats the baseline, although only slightly. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6140/Reviewer_tJad"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6140/Reviewer_tJad"
        ]
    },
    {
        "id": "POSOW6sPRC",
        "original": null,
        "number": 2,
        "cdate": 1666536187881,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666536187881,
        "tmdate": 1666536187881,
        "tddate": null,
        "forum": "zfiYcbeQkH",
        "replyto": "zfiYcbeQkH",
        "invitation": "ICLR.cc/2023/Conference/Paper6140/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors of this paper introduce a benchmark for scientific representation learning consisting of 25 tasks in 4 formats (classificaiton, regression, ranking, search) and evaluate multiple general-purpose scientific representation learning methods (SciBERt, SPECTER, SciNL) alongside adaptation methods to learn task-specific and format-specific representations. The results suggest that fine-grained scientific document representations significantly out-perform general-purpose representations.",
            "strength_and_weaknesses": "Strengths:\n- Introduces a more comprehensive benchmark for scientific document representation learning\n- Clearly distinguishes SciRepEval from previous benchmarks (SciDocs) and justifies why a new benchmark is needed (low-power of the recommendation task, a need for more realistic task settings, and lack of training corpus)\n- Comprehensive ablation setup targeting fine-grained representation learning, multi-task learning with a strong base model\n\nWeaknesses:\n- It would have been nice to see an explicit section detailing what additional challenges remain in benchmarking scientific representation learning and how these limitations apply to the results seen with multi-task approaches.\n- Would have liked to see more discussion about task relatedness when discussing benchmarks, especially since SciRepEval seems to be designed in order to improve breadth and coverage of common real-world applications of the representations. For example, it would be interesting to see the relationship as applied to the task of choosing which sub-tasks to group for multi-task learning (see Efficiently Identifying Task Groupings for Multi-Task Learning; Fifty et al. NeurIPS 2021).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is written clearly and the logic flows well.\n\nQuality: I believe this is a high quality paper with interesting results and strong justification.\n\nNovelty: The authors identify a challenge with existing benchmarks and release a larger and more diverse benchmark. This is an important and novel contribution.\n\nReproducibility: Data is promised to be released; no mention of code release.",
            "summary_of_the_review": "The authors identify an area of opportunity relating to current datasets and methods to evaluate scientific representation learning models and propose a well-grounded, larger benchmark in SciRepEval. The ablation study is comprehensive and there is some interesting discussion of cross-task analysis, although I would have liked to see that section expanded.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6140/Reviewer_d6LN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6140/Reviewer_d6LN"
        ]
    },
    {
        "id": "196nCVxVANN",
        "original": null,
        "number": 3,
        "cdate": 1667264560068,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667264560068,
        "tmdate": 1667264560068,
        "tddate": null,
        "forum": "zfiYcbeQkH",
        "replyto": "zfiYcbeQkH",
        "invitation": "ICLR.cc/2023/Conference/Paper6140/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a new benchmark for scientific document representations, including 25 tasks across classification, regression, ranking and search. The paper also includes investigations on performances of existing models on the introduced tasks. ",
            "strength_and_weaknesses": "Strengths:\u00a0\n- The paper consolidates useful tasks to make one benchmark for scientific document representations\n\nWeaknesses:\u00a0\n- The models mentioned in the paper are mostly created by others in the community\n- The benchmark, while useful, is a consolidation of tasks that were already existing \n",
            "clarity,_quality,_novelty_and_reproducibility": "I did not find the paper to be novel, but the quality and clarity of the writing was sufficient",
            "summary_of_the_review": "While the paper provides a useful benchmark, there is a strong component of novelty missing from the paper. ICLR does not seem like the correct venue for this paper to be presented. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6140/Reviewer_RNDk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6140/Reviewer_RNDk"
        ]
    },
    {
        "id": "AKYRUOCmpRh",
        "original": null,
        "number": 4,
        "cdate": 1672985070771,
        "mdate": 1672985070771,
        "ddate": null,
        "tcdate": 1672985070771,
        "tmdate": 1672985070771,
        "tddate": null,
        "forum": "zfiYcbeQkH",
        "replyto": "zfiYcbeQkH",
        "invitation": "ICLR.cc/2023/Conference/Paper6140/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new benchmark (SciRepEval) for scientific representation learning consisting of 25 tasks in 4 formats (classification, regression, ranking, and search). It shows that learning a separate document representation for each task format would improve the task performance compared to learning a single representation for all tasks.",
            "strength_and_weaknesses": "Strength\n- The authors provide a comprehensive analysis on 25 tasks in 4 formats. \n- The authors use strong baseline models to make the results more convincing.\n\nWeakness\n- There is not enough information on the tasks in the main content. Would be better to provide some high-level info there and left the majority in the Appendix.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with high clarity and above-average quality.\nSome concerns on the Novelty, since the paper doesn't go deep on the technique.",
            "summary_of_the_review": "This paper is written in above-average quality and provides a new benchmark with a comprehensive analysis",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6140/Reviewer_uueE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6140/Reviewer_uueE"
        ]
    }
]