[
    {
        "id": "94dWqgpCeW",
        "original": null,
        "number": 1,
        "cdate": 1666039448864,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666039448864,
        "tmdate": 1666039448864,
        "tddate": null,
        "forum": "cyg2YXn_BqF",
        "replyto": "cyg2YXn_BqF",
        "invitation": "ICLR.cc/2023/Conference/Paper5394/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a two-stage approach to flexibly optimize and test to find\nhyperparameter settings that have provable guarantees with respect to the\noptimized objectives. The authors describe their framework and evaluate it\nempirically on a Transformer model. The presented results demonstrate the\npromise of the proposed method.",
            "strength_and_weaknesses": "+ important and interesting problem\n+ novel angle of analysis\n\n- approach not intuitive to understand\n- performance in practice not clear",
            "clarity,_quality,_novelty_and_reproducibility": "Novel as far as I can tell and the quality seems to be high, but the paper is not very clear.",
            "summary_of_the_review": "The paper studies an interesting and important problem with real-world\nconsequences. It is well written, although the density of math makes it\nchallenging to understand. Intuitively, it is difficult to follow the presented\ntechnique -- the paper seems to imply that I can guarantee e.g. 100% accuracy\nwith 100% probability, which does not make sense in practice. A detailed example\nto illustrate this would be helpful, in particular as the risks for the\napplication are not defined in terms of absolute, but relative performance.\n\nThe empirical results are not easy to interpret either as all results are shown\ndependent on a specific range of alpha values (chosen how?) and it is unclear\nhow this affects other approaches that are not controlled. If I decided to use\nthe proposed method, what results can I expect compared to other hyperparameter\noptimizers, e.g. SMAC, when given the same resources?\n\nThe authors denote hyperparameters with tau, whereas most of the literature uses\nlambda. It would be nice to be consistent with the literature.\n\nIn the \"Model configuration\" section on page 2, the example given for\nperformance guarantees should be _minimal_ error rates. The labels in most\nfigures are too small.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5394/Reviewer_xGYM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5394/Reviewer_xGYM"
        ]
    },
    {
        "id": "EJEvNJajWiX",
        "original": null,
        "number": 2,
        "cdate": 1666892741660,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666892741660,
        "tmdate": 1666892741660,
        "tddate": null,
        "forum": "cyg2YXn_BqF",
        "replyto": "cyg2YXn_BqF",
        "invitation": "ICLR.cc/2023/Conference/Paper5394/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose Pareto Testing, a multiple hypothesis testing strategy based on Learn then test (LTT) for statistically controlling multiple risks. In contrast to prior work, the method uses an additional split of the data to identify hyper-parameter configurations that are Pareto optimal (i.e., on the Pareto frontier). This improves statistical efficiency as less hypothesis tests are required.",
            "strength_and_weaknesses": "Strengths:\n- Writing, while very dense, is generally well structured and includes the main points to follow the paper despite building on extensive work in the Learn Then Test paper.\n- Related work is covered thoroughly and differentiates the contributions of the paper well.\n- The method generally addresses an important problem and improves over previous work by decreasing the number of hypotheses to test.\n\nWeaknesses:\nSome comments on writing:\n-- Because the paper is very dense, it would occasionally be worth resorting to easier sentences. The fourth paragraph in the introduction is a good example for this.\n-- I found the intro figure a bit misleading as it puts too much focus on the architecture. I understand that the experiments use architectural hyper-parameters for risk control. But the figure does not really tell me anything about the risk control itself. It is not a bad figure, but I feel it misses the main contributions.\n-- The notation of minimizing a set of is a bit unclear \u2013 I assume the authors want to emphasize that we jointly want to minimize different risks. But I feel this should be made clearer \u2013 see Eq. (4) and (6) and algorithm 1.\n-- The computation of the p-values \u2013 which is an integral part of the method \u2013 could be made clearer in the main paper. I could only find that in algorithm F.1 or related work and I feel the paper would benefit from making that clearer for readers less familiar with these approaches (e.g., conformal p-values are computed quite differently compared to p-values based on these bounds).\n\nMethod:\n- How exactly is the Pareto frontier found in practice. The experiments include one paragraph on this, but it is actually unclear what the default method of optimization is? Is it right that exhaustive optimization over the grid is used?\n\nExperiments:\n- In general, I feel that the main contribution is improving statistical efficiency and reducing the number of hypotheses to test. I would appreciate some more concrete experiments surrounding this:\n-- More explicitly showcases to what extent Pareto Testing reduces the hypotheses space \u2013 i.e., how big is the Pareto optimal set, how many hypotheses are tested compared to LTT?\n-- How does the D_opt set size impact that and influence the variation or the tightness of the risk control in the end?\n- In plots, I assume the are is the standard deviation, or are the min/max/X% confidence interval? I am trying to understand whether Pareto Testing has some influence on this variation.",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "I think the paper is a good contribution for ICLR. Due to the density, I feel that writing can be improved at places and I\u2019d appreciate comments on my other questions regarding experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5394/Reviewer_R3DQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5394/Reviewer_R3DQ"
        ]
    },
    {
        "id": "BmleSmq-xH",
        "original": null,
        "number": 3,
        "cdate": 1667387900291,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667387900291,
        "tmdate": 1667387900291,
        "tddate": null,
        "forum": "cyg2YXn_BqF",
        "replyto": "cyg2YXn_BqF",
        "invitation": "ICLR.cc/2023/Conference/Paper5394/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work is built upon the results of Learn Then Test framework by introducing Pareto Testing.  The authors develop an efficient method for calibrating models such that their predictions provably satisfy multiple explicit and simultaneous statistical guarantees (e.g., uppper-bound error rates), while also optimizing any number of additional, unconstrained objectives.\n\nThis method contains two stages. Stage1: solving an unstrained  multi-objective optimization problem to recover an approximate set of Pareto-optimal configurations; Stage2: performing rigorous sequential testing over the recovered set to yield tight control of the desired risks.\n\nThe main idea of this proposed method is that  to perform testing ONLY over the Pareto optimal set. And the effectiveness is demonstrated for reliably accelerating Transformers \n",
            "strength_and_weaknesses": "Strength:\n\nThis work is a great attempt by combining the combination of multi-objective optimization and \"Learn and Test\" for risk-control.  Intuitively, it is obvious that applying FST only on Pareto Front will be efficient (because the size of configuration is much smaller). \n\nWeaknesses:\n\nThe setting discussed in this paper is as:  for all c+k objectives,  the first c objectives are controlled by c user-specified risk bounds, and the rest k objectives are free to optimize.  If k =0, i.e., all the risks are controlled, how to decide the final configuration?\n\nOther minor comments:\n\n(1) Equation(3),  minimizing the remaining objective function $Q_c$? ( I guess it should be $Q_{c+1}$)\n\n(2) Algorithm 1 line 11, when k>1, how to get a set of configurations representing different possible trade-offs?\n\n(3) Figure 2 is a bit confusing, from the path showed, Q1 is not controlled under $\\alpha$, instead, the configuration selected with $Q_1>\\alpha$.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written, and the framework is well-organized. ",
            "summary_of_the_review": "Overall, this paper is marginally above the acceptance threshold. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5394/Reviewer_ZXwJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5394/Reviewer_ZXwJ"
        ]
    },
    {
        "id": "aExy3kwQUL",
        "original": null,
        "number": 4,
        "cdate": 1667442412890,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667442412890,
        "tmdate": 1670278548666,
        "tddate": null,
        "forum": "cyg2YXn_BqF",
        "replyto": "cyg2YXn_BqF",
        "invitation": "ICLR.cc/2023/Conference/Paper5394/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers combining multi-objective optimization with multiple-hypothesis testing. More specifically, they consider optimizing some objectives Q_i's while applying risk control on multiple other objectives such that Q_j\\le c with high probability. They consider reducing the dimension of the high dimensional vector of candidates of configurations by only restricting themselves to the Pareto frontier. Effectiveness of their method is considered on large-scale transformer models in NLP.\n\nMain method is based on existing work fixed sequential testing, by adapted it to the Pareto frontier setting.\n\n",
            "strength_and_weaknesses": "Strength. Reducing high-dim configurations is an interesting topic. Experiments are comprehensive.\n\nWeakness:\n1. Writing is not clear.  For example, is equation 3, should the optimization objective be of subscript c+1 instead of c? The discussion of c+1 and c+k is not clear enough, constantly switching between the two cases.\n2. Lack of innovation. The main technique is based on FST. The only contribution is to suggest using the Pareto frontier to reduce the dimensionality of (tau_1, \\cdots, tau_n)'s need to be considered, which is quite natural. Also, there is a discrepancy between the empirical Pareto frontier based on Q.hat and based on the real Q. The one based on real Q is the desired frontier. \n3. Notation sloppiness. In equation 4, argmin is mis-used, which is also pointed out by the author themsevle -- it is possible that no uniformly optimal solution exists.\n\nI suggest the authors significantly polish this writing and consider more extensions.",
            "clarity,_quality,_novelty_and_reproducibility": "Not clear enough. See comments above.",
            "summary_of_the_review": "See comments above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5394/Reviewer_F3Rn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5394/Reviewer_F3Rn"
        ]
    }
]