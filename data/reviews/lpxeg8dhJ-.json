[
    {
        "id": "Z_XKB-UtIS",
        "original": null,
        "number": 1,
        "cdate": 1666443659145,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666443659145,
        "tmdate": 1666443659145,
        "tddate": null,
        "forum": "lpxeg8dhJ-",
        "replyto": "lpxeg8dhJ-",
        "invitation": "ICLR.cc/2023/Conference/Paper4784/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors draw an analogy between memoryless policies in POMDPs and coordination games. In a POMDP, a memoryless policy can be viewed as playing a game with itself in the past---because it can't remember its past action, it is as if it is playing a one-shot game. The authors show that there is a class of POMDPs where this connection is direct---you can convert between the game and the POMDP and the equilibria are the same in each (the authors define a notion of training equilibria in these memoryless POMDPs).\n\nThe authors claim that this observation provides a new justification for why noise in parameter space is valuable (something that had previously been found empirically)\u2014it can remove local maxima.\n\nIn experiments, the authors explore the effect of parameter noise and the effect of adding additional memory in simple settings. Memory is helpful, but the algorithms do not use it efficient, and too much memory degrades performance.",
            "strength_and_weaknesses": "Strengths:\n1. The narrative, mathematics and experiments of the paper are very clear and logical.\n2. Issues relating to training RL in POMDPs are salient to many. The paper gives logical justification for some of these issues.\n\nWeaknesses:\n1. There is not a lot of depth to the observations\u2014the connection between POMDPs and coordination games is fairly surface-level. There is a lot more to POMDPs than those that can be solved optimally with a few states of memory. For example, classic robot control-style with sensor and location uncertainty. I can imagine two ways to improve the paper:\n- Expand the experiments section to more complex environments that still have the interesting properties and measure the impact there\n- Increase the strength of the theoretical connection\u2014a potential avenue for this is to think about connections to evolutionary game theory",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe order of concepts in the paper could be improved. In particular, I would recommend a background section so that the relevant game theoretic concepts are introduced before they are referred to.\nOtherwise, clarity is a strength of the paper.\n\nQuality:\nThe paper perhaps pushes a little hard on the potential impact of its results.\nThe main claims are clearly justified.\n\nNovelty:\nAs far as I know, the authors observations are novel.\n\nReproducibility:\nThe paper provides enough detail. I don't think that a code release is necessary for this kind of paper.",
            "summary_of_the_review": "The paper's main contributions tell us something interesting about current practices in RL for POMDPs, but they are somewhat surface-level. The paper is presented in a clear and accessible manner.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4784/Reviewer_CVMT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4784/Reviewer_CVMT"
        ]
    },
    {
        "id": "3O4HUYDnKGs",
        "original": null,
        "number": 2,
        "cdate": 1666512581634,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666512581634,
        "tmdate": 1666512581634,
        "tddate": null,
        "forum": "lpxeg8dhJ-",
        "replyto": "lpxeg8dhJ-",
        "invitation": "ICLR.cc/2023/Conference/Paper4784/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In partially observable environments, reinforcement learning algorithms such as policy gradient and Q-learning can converge to equilibria that are strictly suboptimal. The authors point the core problem is that in partially observed environments, an agent\u2019s past actions induce a distribution on hidden states. This problem can be solved by unbounded memory, and may be alleviated by bounded memory.",
            "strength_and_weaknesses": "Strength:\nThe authors show theoretically that multiple equilibria can only emerge when the distribution of unobserved states depends on past actions. A sufficient amount of memory resolves the coordination game and provably implies a unique optimal training equilibrium.\n\nWeaknesses:\nMore empirical results are expected to show the practical application of the theory.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The code to reproduce the numerical results is currently unavailable.",
            "summary_of_the_review": "The problem is interesting, and the theoretical analysis is insightful. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4784/Reviewer_A2qp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4784/Reviewer_A2qp"
        ]
    },
    {
        "id": "bTP8BCYWKU",
        "original": null,
        "number": 3,
        "cdate": 1666636877082,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636877082,
        "tmdate": 1666657073694,
        "tddate": null,
        "forum": "lpxeg8dhJ-",
        "replyto": "lpxeg8dhJ-",
        "invitation": "ICLR.cc/2023/Conference/Paper4784/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper points out that under partial observability algorithms like policy gradient and Q-learning may have multiple training equilibriums, though this observation is, in itself, not entirely novel. On top of this, they present a number of other results and proposals including: \n\n* Showing multiple training equilibria can only arise when the observation conditional state distribution depends on the policy and thus the issue can be understood as a consequence of this dependence. \n\n* Suggesting a correspondence between Nash equilibria and training equilibria.\n\n* Showing that by adding sufficient memory to the state implies convergence to the unique optima.\n\nExperiments are also conducted in simple toy environments which demonstrate how adding memory can improve performance in partially observable domains even if there is an optimal memoryless policy. Further experiments demonstrate how adding parameter noise can allow an agent to converge reliably to better training equilibria when multiple are present, at least in some simple environments.",
            "strength_and_weaknesses": "Strengths\n=========\nThe analogy between Nash equilibria and learning equilibria in POMDPs is somewhat interesting and makes intuitive sense.\n\nThe point that memory is useful in POMDP learning even if the optimal policy does not require memory is interesting and valid.\n\nThe use of parameter noise to escape suboptimal training equilibria is interesting, if not surprising.\n\nWeaknesses\n==========\nThe result that algorithms like Q-learning and policy gradient may have multiple training equilibria is not entirely novel. It is essentially implied by the existence of multiple greedy partitions highlighted in section 6.4 by Bertseka (1996), for example. This prior work is not cited, nor is more recent work on the topic by the same author (Bertsekas 2011). For policy gradient, this result is explicitly given in the cited work of Bhandari and Russo (2019). Though as the authors mention it is in the context of a constrained policy class rather than partial observability, the equivalence between these two cases is well known. Young and Sutton (2020) also highlight the possibility of multiple training equilibria explicitly for Q-learning and policy gradient, which is not cited, but given it seems to only appear on arxiv this omission is forgivable.\n\nProposition 3.1 states that when the observation conditional state probabilities are policy independent, Q learning and policy gradient are guaranteed to converge to an optimal policy in a POMDP. I think what is meant is an optimal memoryless policy, but this is very confusing because no clear distinction is made and elsewhere adding memory is shown to allow convergence to the \"optimal policy\".\n\nThe result that including sufficient memory allows convergence to the optimal policy in a POMDP under appropriate circumstances is well known and hence I don't feel it is a significant contribution. This amounts to saying that with sufficient memory a POMDP becomes an MDP and thus normal guarantees for MDPs apply. This is true generally if one conditions on the full history, and true with a small \"length-k history\" essentially by definition if the environment is k-Markov.\n\nThe result in theorem 3.2, that Nash equilibria of two-player games correspond to training equilibria of a related RL problem is unclear to me and I believe incorrect as stated in the appendix. The MDP is defined by running some arbitrary fixed number of iterations of the two-player game where one opponent (i.e. the environment player) is effectively fixed to play the best response to the actions taken so far by the other (i.e. the \"agent\" player). It seems to be assumed that the environment is then the best response to the policy of the agent player, but this is clearly not true in general. For example, it is obviously untrue if the episode length is 1 as the environment's first action is assumed to be random. It may be true in the limit of infinite episode length, but the present argument is not sufficient to show this. Aside from that, I'm not entirely clear what the point of this theorem is as it doesn't seem to help to explain or motivate anything else in the paper aside from the hand-wavy statement \"In practice, the analogy seems to extend to contexts outside of self-play or those covered in Theorem 3.2\". I think more work needs to be done to motivate this connection.\n\nOverall I found the connection between Nash equilibrium and POMDP learning to be pretty vague in terms of how it is presented in the current paper. In particular, prior to introducing the theorem, it's loosely implied that one can understand POMDPs generally in terms of Nash equilibria. Instead what is supposedly shown is that one can construct a POMDP from a given 2-player game in a particular way and then show a correspondence between Nash equilibria of the game and training equilibria of the POMDP which is quite different and in my mind less interesting.\n\nNothing is said about hyperparameters or how they were chosen for the empirical results, which may be important for interpreting them since I would expect, for example, the performance of PPO with larger numbers of memory bits to depend on these details. Similarly, some details of the environments are missing, including the maximum length of the episodes and how exactly the bit-flipping actions are added to the action space where applicable.\n\nMinor Comments, Corrections and Questions\n=========================================\n* In appendix A it says \"(TODO ref figure)\" which presumably should point to a figure instead.\n\n* In section 3 \"continuous action selection\" is kind of vague and could be clarified.\n\n* In 3.1 A(\\pi_{\\theta_k})=\\pi_{\\theta_{k+1}} should probably operate on the parameters rather than the policy if it is meant to be general enough to include Q-learning. For example in Q-learning one may update the parameters in a way that changes the value, but leaves the (epsilon-)greedy policy the same.\n\n* In the abstract, it says \"Prior work blames insufficient exploration\" but I can't see that this is ever mentioned again and no citation to the specific prior work that does so is given, so this could be taken as an unsubstantiated claim.\n\n* In 3.3 it says that a Nash equilibrium is a solution to a \"non-cooperative game\", but later it says \"Nash equilibrium of a cooperative game\" which is a weird inconsistency.\n\n* Missing reference (??) in 3.3.\n\n* Missing period before \"Unfortunately\" in 4.2.\n\n* It would be good to show a line corresponding to optimal performance in Figure 4.\n\n* In Appendix B T is overloaded to mean both the transition kernel and the episode length.\n\n* What episode length is used in the experiments in Section 5.2?\n\n* In the sequential bandit problem is the bit flip added as an extra action, or is the action the composition of the original actions and optionally flipping each bit?\n\nReferences\n==========\nYoung, Kenny, and Richard S. Sutton. \"Understanding the pathologies of approximate policy evaluation when combined with greedification in reinforcement learning.\" arXiv preprint arXiv:2010.15268 (2020).\n\nBertsekas, Dimitri, and John N. Tsitsiklis. Neuro-dynamic programming. Athena Scientific (1996).\n\nBertsekas, Dimitri P. \"Approximate policy iteration: A survey and some new methods.\" Journal of Control Theory and Applications 9.3 (2011): 310-335.\n\nBhandari, Jalaj, and Daniel Russo. \"Global optimality guarantees for policy gradient methods.\" arXiv preprint arXiv:1906.01786 (2019).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is lacking in places, but overall not bad. Certain details are missing.\n\nNovelty is not very high.\n\nReproducibility is not very good due to missing details like hyperparameters and some details of the environments used.\n\nSee above for more details.",
            "summary_of_the_review": "This paper might have the beginning of some interesting directions, but I feel it is far from ready in its current form. Most of the theory is either only vaguely related to the stated point, or already widely known. The empirical results are similarly quite simple and not very robust in their conclusions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4784/Reviewer_C66v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4784/Reviewer_C66v"
        ]
    },
    {
        "id": "Ml_tJRhuQSD",
        "original": null,
        "number": 4,
        "cdate": 1666926954371,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666926954371,
        "tmdate": 1666926954371,
        "tddate": null,
        "forum": "lpxeg8dhJ-",
        "replyto": "lpxeg8dhJ-",
        "invitation": "ICLR.cc/2023/Conference/Paper4784/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies several aspects of RL with non-markovian observations under the assumption there is no exploration issue (assumption 3.1) while focusing on asymptotic guarantees. The authors establish several results which are its key contributions:\n1. Under several (quite harsh) assumptions, Q learning and policy gradient converge to an optimal policy (proposition 3.1.).\n2. A correspondence between POMDPs and Nash equilibrium (theorem 3.2.).\n3. Memory based policies improve the asymptotic convergence (proposition 4.1).\nFurther, the authors conduct some simple experiments to exemplify these observations.\n\n",
            "strength_and_weaknesses": "Strength.\n\n-) The POMDP setting is of importance. To make RL practically useful it is much needed to improve our understanding and tools. This work established the convergence of two of the most popular RL algorithms for POMDDs under assumptions.\n\nWeaknesses.\n\n-) I find the assumptions made in proposition 3.1. quite harsh. Further, the fact there's no finite time analysis nor explicit treatment of the exploration problem weakens the theoretical results in my opinion.\n\n-) The study of this work is not coherent in my opinion. The theoretical results, in my opinion, are not very much related to one another. It seems there's no concrete message, but rather couple of messages that do not necessarily add together to a coherent story/\n\n-) The experimental results are quite basic. The experiments do not show of new capability, or achieve state of the art performance. Hence, the experimental part does not add much more over the theoretical part.",
            "clarity,_quality,_novelty_and_reproducibility": "-) The writing could be improved in my opinion. The different sections do not naturally add up to a single coherent message. Further, the assumptions that were made (in proposition 3.1.) are not properly motivated. The example the authors focus on in figure 1 is quite basic and do not represent a common class fo interesting POMDPs. Are there any interesting POMDPs that satisfy thes assumption?\n\n-) I find the novelty of the paper restricted at this point. The theoretical results are basic (exploration is not tackled, asymptotic analysis, additional hard assumptions) and the bottom line is quite expected (that memory improves the convergence ability). On the other hand, the empirical result are also quite basic.\n ",
            "summary_of_the_review": "Due to the above reasons I believe this work is not yet ready for publications. Specifically:\n1) The theoretical and experimental results are basic.\n2) I currently do not find the different results related to one another.\n3) The main motivation is a toy example. The authors should show (in my opinion) it captures additional interesting problems.\n\nI also attach couple of questions that bothered me while reading thus workL\nPage 2. Nash equilibria in POMDPs\nProposition 3.1:\n1) Shouldn't \\eta have time index? that is, the probability the state at time t is s conditioning on the observation at time t is o?\n2) The assumption here is quite harsh in my opinion and restrict the result to a very specific structure (since \\eta is not a function of pi for all \\pi). Which POMDPs satisfy this assumption? I can see that if the latent state is decodable this assumption is satisfied, but then the environment is not partially observed.\n3) The policy class includes the optimal policy? is it the set of all policies? \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4784/Reviewer_QaRE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4784/Reviewer_QaRE"
        ]
    }
]