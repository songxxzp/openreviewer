[
    {
        "id": "P4_pFOkbR8x",
        "original": null,
        "number": 1,
        "cdate": 1666555702570,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666555702570,
        "tmdate": 1666555702570,
        "tddate": null,
        "forum": "cjavWixtG9f",
        "replyto": "cjavWixtG9f",
        "invitation": "ICLR.cc/2023/Conference/Paper1319/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper the authors propose a neural model to deform a template based on the single-view 3D observations while correspondences between both states are implicitly computed. To that end, the authors exploit a feature constraint deformation network on the single-view point cloud as input. The method uses well-known ideas and modules in literature, providing competitive results with respect to an ample variety of approaches. ",
            "strength_and_weaknesses": "In general, the paper is well written and it is clear enough. The motivation is clearly presented, and the discussion of competing approaches seems to be correct. \n \nq_j needs to be introduced in Eq. (1). \n \nFigure 1 should include a self-contained caption. To be honest, every figure in the paper should consider that. \n \nAccording to the learning and inference pipelines, I think the proposed method is a bit incremental. The idea of deforming a template by considering 2D observations is a well-known problem. Once the template is deformed, the initial human shape is fully estimated. To produce this type of results, the authors assume in training a template at rest and a complete shape in a particular state. In my opinion, the amount of 3D priors to learn the model is very high. At the end, this could be reduced to an alignment task with different sets. However, as the visible points in the set can be observed in the input image, this problem is tractable. In particular, the authors should clarify the differences with Groueix et al. (2018). \n\nSimilar results in terms of a chamfer distance could be derived from very different 3D poses, due to the projection ambiguity on deformable bodies. How can that be handled?  \n\nThe method provides competitive solutions with respect to a wide variety of competing approaches. Both quantitative and qualitative evaluation are well executed. I miss some experiments with large occlusions in the input data, the presence of holes, and so on. That means I would like to see more challenging and realistic scenarios. \n\nWhat about the scale? Could very different bodies be matched? For example, using the current template, incorporating a child as an input data. ",
            "clarity,_quality,_novelty_and_reproducibility": "As it was commented above, the paper is easy to follow, the novelty is a bit limited due to the work being incremental with the literature and the main contributions and differences with some works are not provided in the paper; and the experimental results are enough. Considering the main paper, reproducibility is a bit hard as some details and parameters are missing.  ",
            "summary_of_the_review": "On balance, the method is simple yet effective, the results seem to be promising, but the technical contribution is a bit incremental. As pointed out, Some relevant experiments and details could improve the paper. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1319/Reviewer_KeGj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1319/Reviewer_KeGj"
        ]
    },
    {
        "id": "NdWxJplKru",
        "original": null,
        "number": 2,
        "cdate": 1666615139029,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666615139029,
        "tmdate": 1666615139029,
        "tddate": null,
        "forum": "cjavWixtG9f",
        "replyto": "cjavWixtG9f",
        "invitation": "ICLR.cc/2023/Conference/Paper1319/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors address the task of 3D human mesh registration with key focus on registering a complete template to (incomplete) single view point cloud. Authors show better performance than baselines on the registration as well as correspondence prediction task.",
            "strength_and_weaknesses": "Strengths:\n+ The task of registering partial/ single view data with a common template is quite challenging and interesting.\n+ The proposed methodology is simple and makes intuitive sense.\n\nWeaknesses:\n- I find the experimental setup (Sec 4.1) problematic. The paper claims to work with single view point clouds, but this is actually not the case. Authors use a very wrong way to generate single view data. They remove the back of the SMPL model in canonical pose and repose the remaining vertices to get the training data. This is not true for actual single view data. *The parts that are missing in actual single view data are the ones that are occluded from the camera view due to occlusion not a fixed set of vertices*. The most common source of occlusion are hands as they have high degree of freedom and can cover various parts of body. In this work the hands and hand-related occlusions are totally ignored.\nModel trained with this data would not have to reason about the occlusion at all and can just memorise how to complete a smooth back, which is fixed as fixed SMPL vertices are always removed.\nThis process is used both for train and testing, making the evaluation highly unreliable.\n\n- Technical  novelty appears to be limited. The encoder-decoder formulation and template deformation look very similar to 3D-CODED. The new component is enforcing that the features from the partial point cloud and the complete point cloud should be same. This is enforced via cosine similarity on the encoded features. Can the authors comment on this?\n\n- Authors primarily compare their method with works that deal with full point cloud registration. Works like IPNet, ECCV'20 (not cited) are more suitable baselines as they have shown that they can complete the missing point cloud using implicit functions and then perform 3D registration.\n\n- Why not report results on FAUST Intra?\n\nClarifications:\n- The feature decoding and per-vertex deformation approach is very similar to 3DCODED, which requires several initialisations to handle global orientation. Is this also the case for the proposed approach?\n\n- Do the authors use the official FAUST test set (GT not available, but results need to be submitted to their portal for evaluation)? I've seen a few works overfitting to FAUST train set and reporting results. This is a bit of cheating :P Just want to make sure.\n\n- More recent works like LoopReg, NeurIPS'20 and IPNet, ECCV'20 show registration on dressed human shapes, which is more challenging than the current setup where synthetic SMPL is used. Can the authors comment why are they restricting themselves to this old setting?\n\nMinor suggestions:\n- Please make figure captions more informative.\n- Fig. 2 just shows cosine similarity loss. Is it really necessary?\n- Typo: \"The source and Source M are\" below eq. 5",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is reasonably well written and I could follow it easily. I have some concerns regarding originality (see weakness section). It will be very useful if authors clarify why is their method different than just putting cosine similarity (between features of partial and complete shape) on existing 3dcoded network.",
            "summary_of_the_review": "I have serious concerns with the way authors generate single view data. This is not how single view data is generated and this is the primary contribution of the work (handling single view data). The novelty also appears to be limited (see weakness) as compared to 3dcoded but I'd like to hear from authors on this.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not see immediate ethical concerns arising from the work.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1319/Reviewer_tJR5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1319/Reviewer_tJR5"
        ]
    },
    {
        "id": "DKGHiickF7",
        "original": null,
        "number": 3,
        "cdate": 1666641795923,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641795923,
        "tmdate": 1666641795923,
        "tddate": null,
        "forum": "cjavWixtG9f",
        "replyto": "cjavWixtG9f",
        "invitation": "ICLR.cc/2023/Conference/Paper1319/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a method to estimate human body shape correspondence from both the full scans and from single-view depth data. The authors build on a well established method 3D-CODED but they add a mechanism to support partial depth observations as well. Specifically, the authors train an encoder extracting a global code both on the full scans and on corresponding partial observations and introduce an additional loss function to force the global code to remain the same.",
            "strength_and_weaknesses": "# Strengths\nThe method shows simple yet effective approach to extend a template based human body reconstruction/correspondence estimation method to work on partial observations and report quantitative SotA results.\n\n# Weaknesses\n## Related Work\nThe main focus of the method is to support shape correspondence estimation from single-view (RGBD) observations. While the Related Work section discusses various methods for human shape correspondence estimation, it never makes a distinction between the methods which require a full scan on the input and the ones which can handle single-view input only. The authors should position this paper within the SotA so that it is clear what work was already done and who are the competitors. The authors only mention: \"However, these methods are usually proposed for matching complete 3D human models\". What do the authors mean by \"usually\"? Do some of the  methods in fact support a single-view input? If so, which ones are those? The authors further mention: \"there are relatively few methods applicable to single-view human body data collected by a single RGB-D camera.\" However, the authors never discuss any such method.\n\n## Methodology\n- Fig. 2: The Figure seems redundant as the notion of cosine similarity is well known. I would suggest removing the Figure in favor of more substantial content (e.g. experimental results).\n- The last paragraph - the explanation of correspondence retrieval: The text is quite confusing and it is not clear how exactly the authors find the correspondences. The arrows in Fig. 2 indicate that the points are projected from the template to the observations, but more typical approach would be to project a point from observation A to the template and then from the deformed template to the deformed observation B. This would be clarified if the authors perhaps formalized the process with a mathematical formula.\n\n## Experiments\n- \"Unlike many current human reconstruction methods that rely on a large number of scanned 3D mannequins as training data\" -> Please provide citations for this claim.\n- The paragraph \"evaluation criterion\": Please formulate and clarify the paragraph as it is unclear what your metric is and whether it applies in the same way for all the datasets.\n- Fig. 5: The caption talks about a 3D human correspondence task, but the image shows a reconstruction instead.\n- Figures 5, 6, 7 - If a reconstruction of only a single frame is shown, the color coding of the human does not show any useful information unless it is directly compared to the GT color coded human shape.\n- The authors never show qualitative comparison of correspondences between various body poses despite this application being advertised in the Abstract.\n\n## Presentation\nIn general, the paper is quite difficult to read, due to the obscure formulations the authors choose to use and it is thus often unclear what the authors had in mind. Some examples are listed below but the paper contains many more of these:\nStatements where it is hard to understand what the authors mean:\n- \"The parametric human model has been widely used in various human-related applications and in 3D human mesh correspondence because it provides sufficient scope to modify the resulting model.\" - What do the authors mean by \"sufficient scope to modify the resulting model\"?\n- \"In representation learning, one of the main challenges is to design appropriate loss functions for supervising features with different abilities\" - What do the authors mean by \"supervising features with different abilities\"?\n- \"Meanwhile, the decoder generates a completely transformed template with higher promise\" - What do the authors mean by \"higher promise\"?\n- \"The deformed models have the same vertex definition, definite semantic information, and same face connections as the template.\" - What do the authors mean by \"same vertex definition\"?\n- \"Motivated by Deng et al. (2019), we attempted to penalize the loss in the feature space,\" - Perhaps better formulation would be \"penalize the XYZ by adding a loss ABC\".\n- \"this feature is taken as the observed quantity in Equation (1) and in the human template as an input pair to output the deformed human shape vertices.\" - What input pair?\n- \" then constrain the single-view human shape and intact human shape in the feature space as close as possible, so as to use the same network framework for both the single-view and full-body shapes. \" - Please reformulate (the use of \"so as to\").\n- \"Here, b is a scale coefficient for controlling the influence of the eigenvector similarity on the overall reconstruction effect.\" - The use of any eigenvectors was not either mentioned until this point or after. What eigenvectors do the authors have in mind?\n- \"Below, xi is the serial number \" -> Perhaps index number?\n\nFurthermore, there are plenty of unjustified or unexplained statements which come across as filler sentences rather than content of substance, unless reformulated and/or better cited. Some examples are listed below:\nUnjustified, unexplained or incorrect statements such as:\n- \"The obtained network can better realize the deformation and reconstruction of the single-view human body.\" - Better in what sense?\n- \"The proposed method selects a pa- parameterized human body model with a standard pose as a template,\" - What is the \"standard pose\"?\n- \"deforms it into source and target models with arbitrary poses and body shapes, respectively.\" - It was never explained what are the source and target models.\n- \" which reduces the workload for man- ual labeling. \" - It does not reduce the workload but removes it completely.\n\n## Miscellaneous:\n- The authors use the term \"input model\" to refer to the input data/observations, which might be confusing to the reader. Please reformulate.\npg. 6: \"source M and target M\" - Calling the source and target as source M and target M sounds confusing, perhaps use e.g. M_{s}, M_{t}.\n\n## Questions for the authors:\nEq. 4: Why did the authors choose to formulate the final loss as a convex combination of the two members Lr and Lf, rather than a more common way to combine the individual loss terms involving a linear combination, e.g. Loss = LR + b * LS?\n\nTypos:\n- pg1: \"This makes the correspondence process more challenging.\" -> \"This makes the correspondence estimation process more challenging.\"\n- pg2: \"global feature coded from the input shape.\" -> \"global feature encoded from the input shape.\"\n- pg.4: \"where,pj denotes\" -> \"where pj denotes\"\n- pg.4: \" simplified PointNet network\" -> addd citation\n- pg.5: \"where,M\" -> \"where M\"\n- pg.5: \"whereSc  -> \"where Sc\"\n- pg.5: \"it is will be unable \" -> \"it will be unable \"\n- pg.6: \"(Groueix et al. (2018))to\" -> \"(Groueix et al. (2018)) to\"\n- pg.6: \"EXPERIMENT\" -> \"EXPERIMENTS\"\n- pg.7: \"shown in Equation . \" -> \"shown in Equation 6\"\n",
            "clarity,_quality,_novelty_and_reproducibility": "The main idea is quite simple yet effective as the authors back by the quantitative experiments. The method is simple enough to be reproducible, but it would help if the authors provided all the implementation details (including the choice of optimization strategy, learning rate, choice of the constant b etc.).",
            "summary_of_the_review": "While the authors present a simple and effective method which yields SotA results on the chosen datasets, the paper presentation is of rather low quality. Specifically, the authors often use confusing and unclear formulations, filler statements and unjustified claims. There are quite a few typos (some of them listed above). Some parts are only explained in words in a bit confusing way where a simple mathematical formulation would help (see Weaknesses). The figures presented in the Experiment section use a color coding which does not visually help unless a GT is provided next to them as well. Overall, I believe the quality of the submission is not up to the standard of the ICLR and the paper could use some more polishing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1319/Reviewer_waCJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1319/Reviewer_waCJ"
        ]
    },
    {
        "id": "GD23bcw-PEp",
        "original": null,
        "number": 4,
        "cdate": 1666669434312,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669434312,
        "tmdate": 1666669434312,
        "tddate": null,
        "forum": "cjavWixtG9f",
        "replyto": "cjavWixtG9f",
        "invitation": "ICLR.cc/2023/Conference/Paper1319/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper propose a feature constraint deformation network (FCDNet), the networked has tested on different benchmarks.  No clear contribution for this work.",
            "strength_and_weaknesses": "Some major problems are show below.\n\n1. The main contribution of the paper is unclear. The author did not present it explicitly. Can not find it anywhere.\n\n2. The author did a severe mislead on quantitative analysis on \"MPI Faust dataset\". For most of the experiments, the author did the comparison with 3D-CODED, which is an ECCV 18 paper. I have checked the MPI FAUST dataset for Inter-subject challenge, the SOTA can achieve 1.164cm, at least 5 methods (2 of them are already published, 1 has released the code[1]) has better results than current paper.\n\n3. In the test data, what's the SMPL and SMPL-X mean? From my point of view, they are not dataset, they are just parametric model. Based on SMPL or SMPL-X, the community create several datasets, such as SURREAL or AMASS. \n\n4. The FCDnet do not have clear Novelty. Either end-to-end framework or feature constraints have been widely used.\n\n5. The figures are not clear, please emphasize the difference. \n\n[1] Deep Virtual Markers for Articulated 3D Shapes. ICCV 2021\n[2] Learning from Synthetic Humans (SURREAL). CVPR 2017\n[3] AMASS: Archive of Motion Capture As Surface Shapes. CVPR 2019",
            "clarity,_quality,_novelty_and_reproducibility": "The FCDnet do not have clear Novelty. The overall framework is simple combination.",
            "summary_of_the_review": "Due to the overall major problems, the paper needs to major revision and make the contribution more clear and offer the fair comparison before publication. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The paper use the mature dataset to train and test. It should not have ethic problems.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1319/Reviewer_PTZp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1319/Reviewer_PTZp"
        ]
    }
]