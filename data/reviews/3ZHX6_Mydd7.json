[
    {
        "id": "bP3pSvgcjGV",
        "original": null,
        "number": 1,
        "cdate": 1666240820795,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666240820795,
        "tmdate": 1666240820795,
        "tddate": null,
        "forum": "3ZHX6_Mydd7",
        "replyto": "3ZHX6_Mydd7",
        "invitation": "ICLR.cc/2023/Conference/Paper5162/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies defenses against federated learning backdoor attacks. The paper proposes to force the model optimization trajectory to focus on the invariant directions that are generally useful for utility and avoid selecting directions that favor malicious clients. Particularly, the authors consider the consistency of the client model update as an estimation of the invariance, where the AND-mask and Trimmed mean are used together to measure the sign consistency. The proposed robust aggregator is evaluated on several datasets to defend against backdoor attacks, and show a promising utility-robustness tradeoff.  \n",
            "strength_and_weaknesses": "Strengths\n+ The studied problem is important\n+ Detailed analysis on why AND-mask and Trimmed mean together to enhance robustness\n\nWeaknesses\n-Some details are unclear\n-Theoretical results are narrow \n-Lack of comparison with state-of-the-art defenses",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow in general. The paper novelty is moderate--It shows new theoretical results but are narrow. The experimental setup is clear, and I think the results can be reproducible.",
            "summary_of_the_review": "Detailed comments:\n\nI do not quite understand Figure 1(a): When using the AND-mask, does the result show negative signs, which returns the accurate sign of the benign dimensions?\n\nWhat does expectation ratio indicate in Theorem 4? What is the rough error bound in Equation (5)? Is it too large? Similarly, what is the rough probability in Theorem 5? \n\nTheorem 4 & 5 only relate to a single feature dimension? Can the result be generalized to the feature vector? What is the error bound? \n\nTheorem 8 is for the simple linear model. I am not sure whether the theoretical results can be applicable to general nonlinear model. \n\nI am also curious about the attack scenario that can break both the AND-mask and trimmed-mean. \n\nCan the proposed solution be generalized to defend against model poisoning attacks? \n\n\nWhich features are filtered by AND-mask and which filtered by the Trimmed mean? I would like to see whether they are complementary or have a large overlap.  \n\nWhy no attacks in Table 1 has non-zero backdoor accuracy? \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5162/Reviewer_9BAU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5162/Reviewer_9BAU"
        ]
    },
    {
        "id": "LD1rFB2ygm",
        "original": null,
        "number": 2,
        "cdate": 1666629722470,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629722470,
        "tmdate": 1666629722470,
        "tddate": null,
        "forum": "3ZHX6_Mydd7",
        "replyto": "3ZHX6_Mydd7",
        "invitation": "ICLR.cc/2023/Conference/Paper5162/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to defend backdoor attacks in federated learning. Particularly, it proposes to combine existing two techniques, AND-mask and trimmed-mean estimator, to remove suspicious gradient dimensions. This is based on the assumption that the gradients from benign clients have the same sign while those from malicious clients are different from benign ones. This paper also provides theoretical analysis on the gradient sign consistency and how the combination of the two existing techniques defends backdoor attacks. The evaluation is conducted on three datasets and one backdoor attack. Compared to a few baseline methods, the proposed approach has a better defense performance.",
            "strength_and_weaknesses": "### Strength\n\n+ Important problem of defending backdoor attacks in federated learning\n+ Easy to follow in most places\n\n\n### Weaknesses\n\n- No empirical validation of the motivation\n- Limited novelty\n- No evaluation on more existing backdoor attacks\n- No evaluation on adaptive attacks\n- Missing state-of-the-art defense baselines\n- Missing references and details of baseline techniques\n\n### Detailed comments\n\nDefending backdoor attacks in federated learning is a timely and important topic. This paper aims to address this problem by leveraging the combination of existing techniques. The paper is mostly easy to follows. There are a few aspects not well addressed in the paper.\n\n* In Figure 1, the paper uses two illustrative examples to demonstrate the failure scenarios of the two existing methods respectively. However, there is no empirical result showing that such cases exist in real backdoor attacks. In addition, there could be other attack scenarios. For example, both benign and malicious values are on the same side of the sign. But the malicious value can be far from the benign ones. Other cases may also exist. It shall be empirically studied and validated regarding those cases.\n\n* The novelty is limited. This paper simply combines the two existing techniques, without any modifications. The combination is completely based on the two illustrative examples given in Figure 1, which could be far from comprehensive. The theoretical analysis is conducted on a linear classifier, which is also based on the two assumed attack scenarios. The analysis on the gradient sign does not address the aforementioned case where the malicious gradient shares the same sign as benign ones.\n\n* There are many other backdoor attacks in federated learning such as [1-2]. Particularly, there is an attack setting in [2] where the attacker is selected in each round and continuously participates in the training from beginning to the end, which is a much more aggressive attack. The proposed method shall be evaluated on those backdoor attacks.\n\n* Since the proposed defense is based on the assumption of the malicious gradient sign deviate from benign ones, an adaptive attacker can match the malicious gradient sign with a benign one during the attack. Will the proposed defense still be effective?\n\n* There are other state-of-the-art defense techniques [3-5]. They shall be compared with in the evaluation.\n\n* There are no references and detailed descriptions of those baselines in Table 1.\n\n\n### References\n\n[1] Bagdasaryan, Eugene, et al. \"How to backdoor federated learning.\" AISTATS 2020.\n\n[2] Xie, Chulin, et al. \"Dba: Distributed backdoor attacks against federated learning.\" ICLR 2019.\n\n[3] Pillutla, Krishna, et al. \"Robust aggregation for federated learning.\" IEEE Transactions on Signal Processing 2022.\n\n[4] Fung, Clement, et al. \"The limitations of federated learning in sybil settings.\" RAID 2020.\n\n[5] Cao, Xiaoyu, et al. \"FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping.\" NDSS 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nThe paper is mostly clear and easy to follow. It misses the details of baseline methods in the evaluation.\n\n\n### Quality and Novelty\n\nThe novelty is limited and the motivation needs further validation. The paper fails to evaluate more and stronger backdoor attacks and compare with state-of-the-art defense techniques.\n\n\n### Reproducibility\n\nThe submission does not include the implementation and the details of baselines are missing. \n",
            "summary_of_the_review": "This paper addresses an import problem but lacks novelty and comprehensive empirical results.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5162/Reviewer_f8s8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5162/Reviewer_f8s8"
        ]
    },
    {
        "id": "Vl1-B23wbDt",
        "original": null,
        "number": 3,
        "cdate": 1666701910200,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666701910200,
        "tmdate": 1666701910200,
        "tddate": null,
        "forum": "3ZHX6_Mydd7",
        "replyto": "3ZHX6_Mydd7",
        "invitation": "ICLR.cc/2023/Conference/Paper5162/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a method to mitigate backdoor attacks in federated learning by focusing on invariant directions of gradients and avoiding selecting directions that favor malicious clients.",
            "strength_and_weaknesses": "Strength:\n1. This paper proposed a simple yet effective approach to mitigate backdoor attacks in FL. compared with previous robust aggregation methods, the proposed method can defend against a much stronger attack, i.e., an edge-case attack.\n2. This paper gives robustness analyses for both AND mask and trimmed mean.\n\nWeakness:\n1. Only one backdoor attack in FL (edge-case attack) is tested in the experiments, which cannot validate the proposed method is a general defense method for backdoor attacks in FL. What about other frequently adopted backdoor attacks in FL, such as [1], and the experimental setting adopted in [2].\n2. According to the federated learning setup in the experiments, it adopts an iid client setting. In the case of non-iid clients, will the proposed method of masking inconsistent gradients still work?\n3. Masking inconsistent gradients will also sacrifice a lot of benign accuracies. However, according to the experiment results, the benign accuracy is hardly affected, could you give a more clearer explanation for this? Besides, the benign accuracy is also very low for FedAvg, which is very strange since usually, the benign accuracy for cifar10 in FL is at least over 80%. \n[1] Bagdasaryan E, Veit A, Hua Y, et al. How to backdoor federated learning[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2020: 2938-2948.\n[2] Panda A, Mahloujifar S, Bhagoji A N, et al. SparseFed: Mitigating Model Poisoning Attacks in Federated Learning with Sparsification[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2022: 7587-7624.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper proposes a novel defense method for backdoor attacks in FL, however, the experimental verification of this method is not enough. Thus, we can not determine whether this is a general method or only suitable for some special cases.",
            "summary_of_the_review": "This paper is marginally below the acceptance threshold since although the authors proposed a simple yet effective approach to mitigate backdoor attacks in FL, we can not determine whether this is a general method or only suitable for some special cases.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5162/Reviewer_Pmty"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5162/Reviewer_Pmty"
        ]
    },
    {
        "id": "VRh6Fn8zQw",
        "original": null,
        "number": 4,
        "cdate": 1666833464573,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666833464573,
        "tmdate": 1666833464573,
        "tddate": null,
        "forum": "3ZHX6_Mydd7",
        "replyto": "3ZHX6_Mydd7",
        "invitation": "ICLR.cc/2023/Conference/Paper5162/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new method to defend against backdoor attacks. The proposed method is basically a new aggregation rule. Edge-case backdoor attacks are considered, and multiple simple baselines are evaluated. ",
            "strength_and_weaknesses": "Strengths\n\n- FL is vulnerable to backdoor attacks\n\n- A new aggregation rule is proposed. \n\nWeaknesses\n\n- Many robust aggregation rules have been proposed. This paper's proposed aggregation is just some variant of existing one, e.g., trimmed-mean. Therefore, the paper's novelty is limited. \n\n- Seems like only edge-case backdoor attacks are evaluated. This represents a quite limited backdoor case. Other backdoor attacks should be evaluated. \n\n- Some simple baselines are evaluated. Strong and more recent defenses (appeared in AI, security, and system venues) should be evaluated. \n\n- Adaptive attacks are not considered. ",
            "clarity,_quality,_novelty_and_reproducibility": "See the above strength and weakness. ",
            "summary_of_the_review": "The paper has limited novelty, and experiments are also limited. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5162/Reviewer_uH18"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5162/Reviewer_uH18"
        ]
    }
]