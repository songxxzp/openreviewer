[
    {
        "id": "u8dlScX5Ne-",
        "original": null,
        "number": 1,
        "cdate": 1666593567500,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666593567500,
        "tmdate": 1670144154353,
        "tddate": null,
        "forum": "cHf1DcCwcH3",
        "replyto": "cHf1DcCwcH3",
        "invitation": "ICLR.cc/2023/Conference/Paper3498/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes the Lipschitz continuous transformer, namely LipsFormer, to stabilize the training of vision transformers. The authors study the key components of a transformer layer, including the LayerNorm, self-attention, residual shortcut, and weight initialization. In LipsFormer, the authors derive the Lipschitz continuous counterparts of the aforementioned components, such as the CenterNorm, scaled cosine similarity function, scaled residual shortcut, and spectral-based initialization. A theoretical Lipschitz constant upper bound for LipsFormer is provided in the paper. The authors justify the advantage of the proposed method on the ImageNet classification task.\n",
            "strength_and_weaknesses": "**Strong points:**\n\n1. The paper addresses an important issue in designing and training transformers, which is how to guarantee that the model is Lipschitz continuous.\n\n2. The paper provides an interesting Lipschitz constant upper bound for the proposed LipsFormer.\n\n3. The paper is well-written with illustrative figures.\n\n**Weak points:**\n\n1. Experiments on more tasks and different data modalities are needed. Also, results for applying LipsFormer on other baseline vision transformers are missing.\n\n2. There is no result showing that the training is stabilized. The authors should show the training curves in the paper.\n\n3. The authors say, \u201cwe show that Lipschitz continuity is a more essential property to ensure training stability,\u201d but there is no comparison to other methods that stabilize the training of vision transformers in the paper. The authors seem to try to provide this comparison in Table 3, but this table is just an ablation study of LipsFormer and does not show that \u201cLipschitz continuity is a more essential property\u201d.\n\n4. The authors do not empirically compare their method with other related works that introduce Lipschitz continuity to transformers, such as [1]. How is the upper bound of the Lipschitz constant of LipsFormer compared to the bounds from these works?\n\n5. In both Scaled Cosine Similarity Attention and Weighted Residual Shortcut, the predefined or learnable parameters can be absorbed into the weights, W^{Q}, W^{K}, W^{V} or W, to recover the standard dot-product attention and residual connection. Also, the Weighted Residual Shortcut has been explored in [2]. Thus, these two components are not novel.\n\n**Additional Concerns and Questions for the Authors:**\n\n1. Compared to the baseline CSwin Transformer, the LipsFormer has more parameters and more FLOPs. I am not sure if the advantage of LipsFormer over the baseline CSwin Transformer is from the Lipschitz continuity or just because the LipsFormer has a larger size and does more computation. I mention CSwin Transformer here because the authors adopt a similar training strategy as in CSwin Transformer [3].\n\n2. The authors say, \u201cThe derivation provides a principled guidance for designing LipsFormer networks (e.g. choosing the weight of a residual shortcut according to the network depth)\u201d. Is there any result to support that the value of the weight of a residual shortcut decided by the network depth is better than other values?\n\n**Minor Comments that did not Impact the Score:**\n\n1. What is 11?\n\n**References:**\n\n[1] Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention. In International Conference on Machine Learning, pages 5562\u20135571. PMLR, 2021.\n\n[2] Falong Shen, Rui Gan, and Gang Zeng. Weighted residuals for very deep networks. In 2016 3rd International Conference on Systems and Informatics (ICSAI), pp. 936-941. IEEE, 2016.\n\n[3] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. The quality and novelty of the paper are not high enough. The details on the experiment settings are useful, but there is no submitted code to reproduce the results.",
            "summary_of_the_review": "Overall, I vote for rejecting. I have two major concerns. First, the proposed Lipschitz continuous components are not novel. Second, there is a significant lack of experimental results to validate the advantage of LipsFormer.\n\n*********** After Discussion ***************\n\nThe authors' response largely addresses my concerns. I have increased my original score to 6.\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no ethics concerns for this paper.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3498/Reviewer_H1uW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3498/Reviewer_H1uW"
        ]
    },
    {
        "id": "GsI-1xPsdP",
        "original": null,
        "number": 2,
        "cdate": 1666644153816,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644153816,
        "tmdate": 1666644153816,
        "tddate": null,
        "forum": "cHf1DcCwcH3",
        "replyto": "cHf1DcCwcH3",
        "invitation": "ICLR.cc/2023/Conference/Paper3498/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a new ViT architecture called LipsFormer, which replaces non-Lipschitz or unstable modules with Lipschitz continuous counterparts. Experiments show that LipsFormer obtains better Top-1 accuracy on Imagenet-1k dataset compared with previous works. ",
            "strength_and_weaknesses": "Strength:\n\nThis paper analyzes the Lipschitz bound of each component module of ViT comprehensively, and proposed Lipschitz counterparts for the non-Lipschitz modules. \nExtensive show superior performance compared with previous works, and ablation studies show the importance of each proposed module.\n\nWeaknesses:\n1) This paper replies on the assumption that training stability relies on Lipschitz continuity. My main question is: whether introducing Lipschitz continuity will cause other issues for training stability, since training stability also depends on other aspects other than Lipschitz continuity. For instance, for instance, BatchNorm and LayerNorm are proposed to relieve the \"covariate shift\" problem. The proposed CenterNorm throw out std(y) in the denominator, will this cause other issues due to \"covariate shift\" of neurons from different layers? It may be good to check how does std(y) change across layers with different normalizations.\n\n2) How does scaled cosine similarity perform compared with L2 self-attention proposed in Kim 2021, given these two operations are both Lipschitz continuous? Either theoretical or empirical justification would be fine.\n\n3) Since some of the proposed modules are initialization techniques, I think it'd be interesting to see how the learnable parameters change during training. For instance, does the weight of skipping connection $\\alpha$ stays on a similar order of magnitude as one initializes it?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is mostly well written. The proposed techniques are novel and seem to perform well empirically. \nWhen presenting the definition of each operations, it'd be better to also give the dimension of the input, output and weight matrices. For instance, the definition of LayerNormalization is for $x \\in \\mathbb{R}^D$, and the equation will look different when the input is a high dimensional tensor. ",
            "summary_of_the_review": "This paper proposed LipsFormer, which enforces Lipschitz continuity on the submodules of ViT. The proposed techniques show good empirical performances and seem to be effective In stabilizing the training of ViT.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3498/Reviewer_HiVm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3498/Reviewer_HiVm"
        ]
    },
    {
        "id": "x_BwbcxPhFH",
        "original": null,
        "number": 3,
        "cdate": 1666720054548,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666720054548,
        "tmdate": 1666720054548,
        "tddate": null,
        "forum": "cHf1DcCwcH3",
        "replyto": "cHf1DcCwcH3",
        "invitation": "ICLR.cc/2023/Conference/Paper3498/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a Lipschitz continuous Transformer (LipsFormer), that pursues training stability both theoretically and empirically for vision transformer. LipsFormer replaces unstable Transformer component modules with a series of Lipschitz continuous counterparts. Experiments are performed to validate the proposals.",
            "strength_and_weaknesses": "Strengthen:\n\n++ Writtings are clear in general.\n\n++ The main idea of introducing Lipschitz continuous into vision transformer is novel and interesting.\n\n++ The theoretical analysis indeed strengthens this work.\n\nWeaknesses:\n\n-- Only the performances on ImageNet 1K are reported. It is necessary to report the performances on more downstream tasks like object detection or semantic segmentation.\n\n-- Several recent works of vision transformer are missing in Table 2 for performance comparison:\n\n[A] Crossvit: Cross-attention multi-scale vision transformer for image classification, ICCV. 2021.\n[B] Contextual transformer networks for visual recognition, IEEE TPAMI, 2022.\n[C] Towards robust vision transformer, CVPR. 2022.\n[D] Regionvit: Regional-to-local attention for vision transformers, ICLR. 2022\n[E] Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning. ECCV, 2022.\n\n-- Only Params and FLOPs are reported. However, it is important to add more metrics of computation cost (like memory consumption and latency)  to better understand the trade-offs.\n\n-- Following DeiT, it is necessary to perform evaluation on ImageNet-v2 [F] to measure the level of overfitting.\n\n[F] Do ImageNet Classifiers Generalize to ImageNet? ICML, 2019.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I am satisfied with the quality, and also like this novel and interesting idea of introducing Lipschitz continuous into vision transformer.",
            "summary_of_the_review": "In general, I lean to positive and like this new idea. However, I have several concerns about the insufficient experiments. I hope this issue can be addressed in the next round.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3498/Reviewer_C9zf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3498/Reviewer_C9zf"
        ]
    },
    {
        "id": "Ebvenmkxxu",
        "original": null,
        "number": 4,
        "cdate": 1666816155114,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666816155114,
        "tmdate": 1666816155114,
        "tddate": null,
        "forum": "cHf1DcCwcH3",
        "replyto": "cHf1DcCwcH3",
        "invitation": "ICLR.cc/2023/Conference/Paper3498/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces several modules to stabilize and accelerate the training of the vision transformers. In particular, the CenterNorm, scaled cosine similarity attention, and spectral initialization are proposed, which are the counterpart of the LayerNorm, standard self-attention, and  Xavier/Kaiming initialization respectively. The underlying philosophy is that model's Lipschitz constant plays a critical role in stabilizing the training procedure. The smaller Lipschitz constant enables us to the deeper networks and it is essential to design the structure/modules to control the changes in the Lipschitz constant. The authors verify the efficiency of the proposed modules in the ImageNet-1K classification task and in the ablation study the effect of each individual module is discussed.",
            "strength_and_weaknesses": "Strength: \nSimple but effective structures are proposed which are also well-motivated theoretically.\n\nWeaknesses:\nIn the numerical tests, the model size considered is relatively small (e.g., < 100 M). Usually, in highly overparameterization regimes (e.g., > 300M) the landscape or geometry in terms of optimization is quite different from those in the small model regimes. Many training stabling/accelerating tricks that work well with small models may fail to be effective. It is worth evaluating whether the proposed modules can also improve the training for the large enough model (e.g., >300M) and push a higher SOTA accuracy.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well-written and I enjoy reading it.\n\nQuality: The paper contains well-motivated modules and verifies the efficiency through experiments.\n\nNovelty: Although several models are already considered in several previous works (e.g., ReZero, Swin-V2, and DeepNorm), this paper combines them together and simplifies the structure with theoretical analysis. The better numerical performance verifies that those modules are effective.\n\nReproducibility:  The training details are provided in Appendix C but the code is not provided.",
            "summary_of_the_review": "This paper provides several modules to control the Lipschitz constant of the Transformer models. The modules are theoretically justified and show promising numerical performance in ImageNet-K classification. It would be better if we could see the experiments on the large model (e.g., >300M parameters).\n\nMinor issue:\n\n1. Please add the definition to the $\\odot$ operator in sec. 4.1.1.\n2. Please add a reference for Droppath in the main paper.\n\n\nBased on the current presentation, I tend to accept it and I'm willing to change my evaluation after the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3498/Reviewer_CmFG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3498/Reviewer_CmFG"
        ]
    }
]