[
    {
        "id": "_KSoY-JeGZ",
        "original": null,
        "number": 1,
        "cdate": 1666580140640,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580140640,
        "tmdate": 1666580140640,
        "tddate": null,
        "forum": "NiEtU7blzN",
        "replyto": "NiEtU7blzN",
        "invitation": "ICLR.cc/2023/Conference/Paper3654/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper demonstrates that large language models (LLMs) can be self-improved by generating high-confidence rationale augmented answers for unlabelled questions using an LLM and use that data to fine-tune the same LLM to improve itself. The paper empirically shows that this approach improves the reasoning abilities of the LLMs on various datasets in both in-domain and out-of-domain settings. Further, a broad range of ablations are conducted to show that fine-tuning on reasoning is critical for self-improvement.",
            "strength_and_weaknesses": "\nStrengths: \n1. The paper is well written and easy to follow. \n2. Evaluated the approach of a lot of datasets and showed improvements.\n3. Lot of ablations are conducted which are very useful. \n\nWeaknesses: \n1. The paper presented an approach followed with empirical evidence, but did not provide why such an approach works in the first place (i.e., approach is not backed well enough). Following are my concerns on the approach:\n   - The approach fine-tunes on the rationale-augmented reasoning paths using self-consistency, which means that self-consistency outputs without LMSI (fine-tuning approach) will be on par with CoT-Prompting with LMSI. This is more or less consistent with the observations in main Table-3. \n   - As per my understanding, the model is not learning any new information, the proposed approach is acting like a filtering method where the results achieved by a self-consistency can be achieved by fine-tuned LMSI in CoT setting. I think the self-consistency results for LMSI can be achieved by using more samples for self-consistency without LMSI. These experiments are not there in the paper. \n2. Like I said, there were a lot of ablations in the paper where some of them proved some points presented in previous work (like Table-7) and others are new (Figure-3). On the new ones, the paper failed to provide good discussion on the outcomes/observations. For example, in Figure-3, both step-by-step and few-shot with step-by-step are converging for total samples at 80. Why is that the case? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. Please provide more discussion on whether self-improving a characteristic of large language models or it applies to any model? If yes/no, why? Any meaningful discussion would be helpful. \n2. Can you check if baseline self-consistency with more examples can match LMSI or not?",
            "summary_of_the_review": "Overall, the paper has many interesting experiments but lacks proper evidence and discussion on why this approach works. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3654/Reviewer_nphB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3654/Reviewer_nphB"
        ]
    },
    {
        "id": "PbXQN07LV0",
        "original": null,
        "number": 2,
        "cdate": 1666616478622,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616478622,
        "tmdate": 1666616478622,
        "tddate": null,
        "forum": "NiEtU7blzN",
        "replyto": "NiEtU7blzN",
        "invitation": "ICLR.cc/2023/Conference/Paper3654/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper discusses that training using only unlabelled data can improve large language models' (LLMs) reasoning capabilities. In other words, the authors used an LLM (PaLM in this case), generated multiple outcomes for a given problem, and used the ones where the model was \u201chighly confident\u201d to retrain the model as target outputs to achieve better results than the non-trained version. The authors argue that unlabelled fine-tuning can lead to self-improvement of LLMs. ",
            "strength_and_weaknesses": "### Strengths:\n- The work pushed the state-of-the-art results on multiple datasets including GSM8K which seems to be a challenging dataset even for larger language models.\n- Using what the model knows (in this case by predicting multiple answers to the same question, and doing majority voting to estimate the possible correct output) to train the model further can help in adapting LLM to a specific task and can be used in future to train LLMs without any supervision. \n\n### Weaknesses: \n- Prompting is a popular technique for querying LLMs because of their size and training them is not efficient/possible in most scenarios. Various prompting strategies have been developed in the past that have improved the performance of LLMs significantly, like Chain of thoughts, appending \u201clet's do this step by step\u201d, and so on. An obvious extension was to generate multiple times using the same/different strategies, and select the one with higher confidence or cluster them or do some majority voting. This paper uses all the methods of the past and proposes an obvious extension that takes the confident samples and trains over them. This is not novel at all with many researchers have thought about this but could not execute the idea because of a lack of resources, higher API costs,  and non-availability of open-source LLMs like PaLM.\n- Secondly, proposing an approach and claiming that \u201cThis is similar to how a human brain sometimes learns: given a question, think multiple times to derive different possible results, conclude on how the question should be solved, and then learn from or memorize its own solution\u201d is just random with not theory or study to back this up. In other words, it is saying that humans can look at a problem, and think of a solution many times, and the one that humans conclude the most is the most likely output and use that to think and train themselves to memorize that solution. I don't agree with this point at all (of course I am happy to listen to why authors thought about this). \n- Research where such an important claim is being made (LLMs are able to self-improve) needs to be shown on multiple models (small to very large) and just showing the results on a 540B model does not prove the point that large LMs can self-improve by training on the samples that they produced. When generating outputs with let's say a smaller model like T5-large, where the accuracy will not be that high, the model outputs cannot be used to train them as most of the time, the answers will be incorrect. So my guess is that this approach only works for PaLM or similar-sized models. \n- Finally the limitation of the experiments on one model (and that too not open sourced) is not enough to justify the big claim of \"large language models can self improve\". The reproducibility of the results is a concern to many. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clearly written and very easy to follow. The quality of the paper is very sound and experiments are conducted to support the claim (but restricted to one LLM only). \n- The paper is original in its implementation but not in ideation (very obvious extension that was not conducted by a lot of researchers due to lack of resources). \n- However, the results cannot be reproduced due to the lack of open source policy for PaLM by google and there is no way to test if the results are accurate or not. ",
            "summary_of_the_review": "The authors proposed a way to self improve large language models. This claim is very not supported by the experiments where the results are shown for just one LM and that too is not openly available. So the validity of the method cannot be tested/justified by the community. Moreover, the approach is not novel in its ideation but only novel in its implementation (as the method was not implemented by others due to resource constraints) and hence I would suggest showing the novelty of the idea on more models and datasets to justify the claims. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3654/Reviewer_ombs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3654/Reviewer_ombs"
        ]
    },
    {
        "id": "dfWz-9PsvCI",
        "original": null,
        "number": 3,
        "cdate": 1666668558325,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668558325,
        "tmdate": 1666668558325,
        "tddate": null,
        "forum": "NiEtU7blzN",
        "replyto": "NiEtU7blzN",
        "invitation": "ICLR.cc/2023/Conference/Paper3654/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an avenue to self-improve large language models without explicit supervision. They first generate candidates (in this case high-confidence rationale-augmented answers in their words) for unlabeled questions and further fine-tune the model on those unlabeled and self-generated solutions. They are able to improve performance on a very large 540B model on datasets that the community evaluates on such as GSM8K, DROP, OpenBookQA, and ANLI.",
            "strength_and_weaknesses": "Strengths: \n1) The empirical gains indicate that the method is effective.\n2) The approach is reasonably straight forward, although difficult to estimate the impact of a variety of hyperparameters and choices (some of which are varied and shown in later in the paper) at each stage due to the complexity of fine-tuning such a large model.\n3) One-type of distillation, fine-tuning a smaller model from the generations of the 540B model, is effective.\n\nWeaknesses:\n1) The method is only broadly applied to the 540B model that almost no one else has access to/can train or fine-tune. The distillation approaches show that the method is effective when using the samples from the 540B model. What happens if you don't have access to such a large model? Can you apply the entire method to a smaller model say < 10B? \n2) It'd be great to see if these approaches can be used with language models that are available to everyone and at scales that most of the community can use.\n3) Replicating this is nearly impossible.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper reads well and understanding most of the paper is relatively easy.\n\nQuality/Novelty: The authors integrate chain-of-thought prompting and self-consistency as means for self-improving large pretrained language models. Technically this basically just takes tools from other work and integrates them, but the evaluation and the utilization is reasonably novel.\n\nOriginality: I think many people in the community over the years have approached a self-supervised, self-improvement goal for AI systems and NLP systems, but this specific approach is relatively original and to my knowledge has not been tried before.\n\nReproducibility: Access to the pretrained model is not available, and the sheer size of the models make reproducibility impossible for the vast majority of labs, universities, and companies alike.",
            "summary_of_the_review": "The paper is good. The method is relatively straight forward and empirically the method performs well. The paper is clear and has additional experiments such as distillation that make it more compelling. \n\nThere are limitations in that the models here are of squillions of parameters and not publicly available, so its nearly impossible to replicate. The method has not been tested on models that are of reasonable size or ones that are publicly available, which is a significant limitation. Given the compute resources at the authors' disposal, running a few additional experiments on relatively much smaller and publicly available models would strengthen the paper greatly and would be of significant interest to the community.\n\nIn general this paper is of broad interest to the NLP subcommunity at ICLR, so I vote that it be accepted.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3654/Reviewer_hq7M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3654/Reviewer_hq7M"
        ]
    }
]