[
    {
        "id": "2chQsIMiS3",
        "original": null,
        "number": 1,
        "cdate": 1666588981842,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588981842,
        "tmdate": 1666589790651,
        "tddate": null,
        "forum": "LUOSN8opID1",
        "replyto": "LUOSN8opID1",
        "invitation": "ICLR.cc/2023/Conference/Paper1477/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper builds on recent work in formal methods and ML community on using temporal logic specifications to shape rewards for deep reinforcement learning. The main claim to novelty is the use of differentiable representation of temporal logic specifications. A secondary claim is hierarchical setting where path planner works with low-level control policy. ",
            "strength_and_weaknesses": "Strengths:\n\n- The paper addresses an important problem of using formal/logical specifications for reward shaping in RL\n\nWeaknesses:\n\n- The paper makes a number of claims to novelty that are not accurate when one takes the vast literature on controller synthesis and learning in formal methods, robotics and control literature. The abstraction mentions that \"Prior work, however, has failed to consider the possibility of making these specifications differentiable, which would yield a more informative signal of the objective via the specification gradient.\" This fundamental claim to novelty is inaccurate. Smooth representations of formal specifications (in non-RL) setting have been widely investigated earlier. See for examples: http://robotics.cs.rutgers.edu/wafr2020/wp-content/uploads/sites/7/2020/05/WAFR_2020_FV_55.pdf,  https://link.springer.com/article/10.1007/s10703-019-00332-1 and https://ieeexplore.ieee.org/document/9114883 . When there exists a systematic compilation approach to achieve smoothness, it is important to compare the new approach for differentiability with these existing techniques. The paper needs to provide some justification based on empirical runtime or optimization effectiveness. The reviewer recognizes that this is a new context - DRL, but what is the added challenge in DRL when it comes to having smooth specifications. \n\n- The idea of using a Lagrangian for regularization with constraints is quite common, and it is not clear if the use of temporal logic constraints creates any new challenge.\n\n- The integration of neural network predicates with logic is also widely used. CLIP system from 90s and its recent resurgence in the form of Logic Tensor Networks (LTN): https://www.sciencedirect.com/science/article/abs/pii/S0004370221002009 is an example. Several other mixtures of deep learning and logic use it, including https://proceedings.neurips.cc/paper/2018/hash/dc5d637ed5e62c36ecb73b654b05ba2a-Abstract.html . If the use of this idea in DRL (as against supervised learning applications of LTN) would have involved some special challenge, this claim to novelty would have been justified. At least discussion of these and acknowledgement, the novelty is in adapting a well-known idea to the new setting of DRL would be a more accurate representation. \n\n- It is not clear why the paper views hierarchical decomposition as a novelty. Doesn't the use of temporal logic specification in this paper (and all previous papers doing so and cited in this paper) also implicitly or explicitly end up being hierarchical because temporal logic specifications are compositional? Section 3 does not elaborate on this claim to hierarchy. One is left to assume that the paper is referring to the standard practice of having two steps - planning and control. \n\n- The section on differentiable specification is rather vague, and it is unclear what is the algorithm here (making it difficult to compare conceptually with the literature mentioned above). \n -- The paper points to the neural signed distance function as an example of a differentiable predicate, and then just states that \"our approach .. is not limited to neural SDFs\". What other neural predicates can be learned? \n -- The mention of IRL and claim that they can learn differentiable predicate in this context is confusing. The entire claim in the paper is of the use of temporal logic specifications and hierarchical planning/policy generation, why would IRL to learn reward functions be relevant here? The discussion goes on to say the paper is \"not on investigating the diversity and application of neural predicates.\" which may well be true but one needs more than one example of neural predicates. Also, how these would form a differentiable temporal logic is left vague.\n -- The use of max/min for globally/future in Equation 2/3 are just the standard quantitative semantics of temporal logic that is widely known. If the entire claim to differentiability rests on using this well-known quantitative semantics, then it is alarming because it is widely known that this semantics (due to max/min) is in practice not very smooth (and is the motivation of the literature mentioned in the review above). \n\n- Can authors discuss the similarity and differences in their use of neural-ODE and the TrajODE https://www.ijcai.org/proceedings/2021/0207.pdf  (both conceptually and empirically)? \n\n- The experimental evaluation uses very simple environments, and all of them are essentially path planning examples. Doing an ablation study, where existing smooth representation of temporal logic are considered, would help better explain the value of the proposed approach. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, though it is heavy on just restating widely known concepts in formal methods and control. While there is some value in transferring knowledge from one community to another, the use of temporal logic in DRL is widely recognized in the learning community, and it is not useful to just re-summarize basic concepts around temporal logic and its quantitative semantics in the paper. A reference to a book chapter would be sufficient. Similarly, the use of Lagrangian is textbook and doesn't merit the detailed discussion. The space could be better used in describing some novel contribution in the paper. \n\nPlease see the weaknesses for further comments on clarity and novelty. \n\n",
            "summary_of_the_review": "The paper appears to identify some of the most important needs for making DRL work in practice - hierarchical policies, and the use of temporal logic specification. It seems to be ignoring literature over the last few years on a related topic and makes claims to novelty that are not justified. Further, the presentation of the approach is vague and most of the paper is just re-summarizing known concepts from formal methods and control. Perhaps, presenting this work as taking ideas from different sub-areas and adapting them to DRL would have been much better. But in that case, the paper needs to justify the extra challenges in adapting to DRL and use more involved examples (beyond simple planning/control examples) to show the benefit. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1477/Reviewer_Pa7M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1477/Reviewer_Pa7M"
        ]
    },
    {
        "id": "q3f0df36wyA",
        "original": null,
        "number": 2,
        "cdate": 1666656054235,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656054235,
        "tmdate": 1666656054235,
        "tddate": null,
        "forum": "LUOSN8opID1",
        "replyto": "LUOSN8opID1",
        "invitation": "ICLR.cc/2023/Conference/Paper1477/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper treats the problem of constrained deep reinforcement learning, where constraints are given in the form of signal temporal logic (STL). The paper proposes a (to my knowledge) novel technique for using Lagrangian methods with STL. They evaluate on several domains and illustrate the efficacy of their approach.",
            "strength_and_weaknesses": "# Strength\n\nThe developed algorithm provides a well-motivated construction of a constrained policy gradient algorithm by applying lagrangian methods to a STL formula's quantitative semantics. The result seems technically sound and (in my opinion) superior to other methods I have seen in this area.\n\n# Weakness\n\nThat said, I am suspect of the novelty of using a differentiable STL formula to guide controller synthesis. While I am not deeply involved in this area, I know of at least two works [1],[2] with a quick google scholar search revealing many related articles. I view the comparison with these existing methods.\n\nIf the authors could clarify why these are either not appropriate baselines or provide a comparison, I would be willing to increase my score.\n\nFinally, as with all such methods, it seems that the \"units\" of the STL formula are important for determining the slope of the constraint function. An analysis of changing the STL formula to have the same qualitative semantics, but different quantitative semantics would have been appreciated, i.e., by changing the predicates. The result would have a different gradient landscape.\n\n[1] Pant, Yash Vardhan, Houssam Abbas, and Rahul Mangharam. \"Smooth operator: Control using the smooth robustness of temporal logic.\" 2017 IEEE Conference on Control Technology and Applications (CCTA). IEEE, 2017.\n[2] Leung, Karen, Nikos Ar\u00e9chiga, and Marco Pavone. \"Back-propagation through signal temporal logic specifications: Infusing logical structure into gradient-based methods.\" International Workshop on the Algorithmic Foundations of Robotics. Springer, Cham, 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed approach it clearly laid out and the ablation seems sufficiently thorough.\n\nThe technique builds off existing components in the literature, but does so in a well motivated way.",
            "summary_of_the_review": "My primary concern with this paper is the lack of comparison with other differentiable STL methods that have popped up in the literature. While I believe this approach is well motivated despite this, an evaluation or discussion would help set this paper in context.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1477/Reviewer_Ea5U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1477/Reviewer_Ea5U"
        ]
    },
    {
        "id": "_vfYboarzzl",
        "original": null,
        "number": 3,
        "cdate": 1666852241198,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666852241198,
        "tmdate": 1666852241198,
        "tddate": null,
        "forum": "LUOSN8opID1",
        "replyto": "LUOSN8opID1",
        "invitation": "ICLR.cc/2023/Conference/Paper1477/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper concerns hierarchical reinforcement learning (RL) in safety critical scenarios. The critical aspect is captured via the inclusion of temporal logic constraints, in fact (a variant of) LTL. The key feature of this paper is that a quantitative notion of LTL is defined and used, to get a differentiable logic as opposed to the more binary nature of standard LTL (at a state: formula is satisfied or not). The RL procedure is defined in a hierarchical way, where high-level planning is complemented by low-level execution of a controller. The authors provide all necessary definitions, explain their variant of a quantitative notion, and provide an extensive evaluation of their approach on rather challenging environments. ",
            "strength_and_weaknesses": "+ The paper is easy to understand\n+ The key concept is clear and important, aiming to bringing learning under/with/via temporal constraints differentiable and thereby more scaling\n+ The experimental evaluation is extensive and convincing\n- The connection to probabilistic temporal logic constraints is unclear\n- At parts, the paper could be better polished (mostly in terms of grammar)\n",
            "clarity,_quality,_novelty_and_reproducibility": "As outlined in the strengths/weaknesses, I do believe this is a good paper which is worth to be accepted. The quality is high, the presentation and ideas are clear, and the reproducibility is strong via a strong numerical evaluation and the availability of code. To the best of my knowledge, this is a novel approach, but one weakness I see is that the authors should provide a better comparison/discussion with respect to probabilistic constraints. I will list a few papers below, some already included in the submitted version. In the essence, there has been a lot of work on RL or safe RL against/according to temporal constraints, based on MDP semantics, where one defines the satisfaction in a probabilistic way. Eg: At state s, the probability of satisfying an LTL spec is p. Isn\u2019t this also a differentiable notion? If yes, that should be made very clear. The only other comment I have is that I found the introduction to the LTL variant rather longish and mostly standard.\n\nMohammadhosein Hasanbeig, Yiannis Kantaros, Alessandro Abate, Daniel Kroening, George J Pap- pas, and Insup Lee. Reinforcement learning for temporal logic control synthesis with probabilis- tic satisfaction guarantees. In 2019 IEEE 58th Conference on Decision and Control (CDC), pp. 5338\u20135343. IEEE, 2019.\n\nNils Jansen, Bettina K\u00f6nighofer, Sebastian Junges, Alex Serban, Roderick Bloem:\nSafe Reinforcement Learning Using Probabilistic Shields. CONCUR 2020: 3:1-3:16\n",
            "summary_of_the_review": "Good paper, related work/novelty can be better argued. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1477/Reviewer_KU3V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1477/Reviewer_KU3V"
        ]
    },
    {
        "id": "wxIXnIRvOe",
        "original": null,
        "number": 4,
        "cdate": 1667324568558,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667324568558,
        "tmdate": 1667324568558,
        "tddate": null,
        "forum": "LUOSN8opID1",
        "replyto": "LUOSN8opID1",
        "invitation": "ICLR.cc/2023/Conference/Paper1477/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an approach to constrained deep reinforcement learning using signal temporal logic as a constraint language. The formulas defining the constraints are converted to algebraic constraints using the quantitative semantics of STL, which, in turn are incorporated as Lagrange terms in a dual ascent algorithm. These concepts are explored within the space of hierarchical RL. The results are evaluated on benchmark domains.",
            "strength_and_weaknesses": "Weaknesses:\n1. The general novelty of the approach is not that high, as it combines building blocks that are well understood (there is more to be said on this in strengths section). \n2. This paper is written in a way that is honestly kind of boring and doesn't sell the approach particularly well. 70% of this paper is background and everything interesting is kind of compressed at the end in tables or in the appendix. In some sense, it feels like the goal of this paper is to push benchmarks, not to convince the reader that they could be solving new and interesting problems that can be made more tractable by injecting domain knowledge in the form of constraints. If I were to rewrite this paper, I would put the highlight on concrete problems that are intractable using standard DRL, for example, a focus could be an illustration of how a formal specification provides a principled way out of the nightmare that is reward shaping, in particular with a focus on the additional constructs enabled by STL.\n\nStrengths:\n1. I think the problem tackled here is really important and generally underappreciated (see above).\n2. While none of the building blocks are particularly novel, it has to be noted that they come together rather well in this paper. The proposed solution is rather elegant without any obviously awkward bits and I could see this as an actual tool that one would productively use for tackling a problem where the policy lends itself to an STL specification.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written rather well in terms of flow, but I feel also that it undersells the idea. There are a few nits here and there, such as the fact that the term \"SDF function\" parses out to \"signed distance function function\". Nothing terrible, but a careful read-over would be beneficial. Regarding quality and novelty, my general feelings as expressed in the strengths/weaknesses field are that this is good quality work, presented suboptimally. The suboptimality is mostly optical though, the actual exposition is quite clear and the ideas are easily understood. The relative low novelty is offset by the elegance with which the elements of the framework come together. ",
            "summary_of_the_review": "I think that this paper is likely to be of interest to a non-trivial number of people. The work is technically reasonable and the ideas are quite nice. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1477/Reviewer_urxX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1477/Reviewer_urxX"
        ]
    }
]