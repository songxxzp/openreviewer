[
    {
        "id": "8kyg3zdB4OL",
        "original": null,
        "number": 1,
        "cdate": 1666590934230,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666590934230,
        "tmdate": 1666850188793,
        "tddate": null,
        "forum": "w-x7U26GM7j",
        "replyto": "w-x7U26GM7j",
        "invitation": "ICLR.cc/2023/Conference/Paper1629/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new approach to learning radiograph representation through a joint masked image and language modeling. Specifically, for each pair of radiograph and radiology report pair, both the image and the report are masked for restoration tasks. The masked image patches are encoded and used for both restorations, while the masked text embedding is only used for report restoration. Besides, the input image is downsampled which combines a super-resolution task. The model is pre-trained on the large MIMIC-CXR dataset and then fine-tuned for downstream tasks: classification and segmentation on CheXpert, NIH ChestX-ray, RSNA Pneumonia, COVID-19 Image Data Collection, and SIIM-ACR Pneumonia Segmentation datasets. Experiment results show the proposed method outperforms all comparison methods, including report-supervised, self-supervised, and transfer learning methods. Ablation study experiments also provide an analysis of contributions from individual components in the proposed method.",
            "strength_and_weaknesses": "Strength\n\n- The paper is well-written and easy to follow. The proposed method is explained well with text descriptions and figure illustrations.\n- The proposed method, Masked Record Modeling (MRM), combining masked image and language modeling, is effective for learning radiograph representations. MRM outperforms state-of-the-art both report-supervised and self-supervised / transfer learning methods by large margins in some cases.\n- The ablation study shows the contributions of individual components in the proposed method. Specifically, it shows masked language modeling has a significant impact on improving radiograph representation learning.\n\nWeakness\n\n- Some technical details need more clarification. What is the lookup table for report token embedding? What are the specific configurations of the image encoder, image decoder, and report decoder?\n- In Table 2, MRM outperforms REFERS in most categories, except for \u201cInfiltration\u201d. Why? It is worth conducting a more in-depth analysis.\n- In Section 4.4, \u201cThe cross-entropy and MSE losses are used for masked image and language modeling, respectively.\u201d The order of two losses should be swapped.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper lacks certain clarity on its own, as mentioned above. Some technical details are missing. However, the authors provided their source code. If the missing details are completed, it should be possible to reproduce the results.\n\nThe idea of combining masked image and language modeling is not entirely novel but may be so for radiograph representation learning. There are previous efforts on joint masked image and language modeling [1,2]. However, they are not published yet.\n[1] MLIM: Vision-and-Language Model Pre-training with Masked Language and Image Modeling, https://arxiv.org/abs/2109.12178\n[2] Masked Vision and Language Modeling for Multi-modal Representation Learning, https://arxiv.org/abs/2208.02131\n\nThe quality of this work is high as it made a large improvement to radiograph representation learning compared to previous methods.",
            "summary_of_the_review": "This paper combined masked image and language modeling for radiograph representation learning and showed significant improvement over existing methods for downstream tasks. The weaknesses of this paper could be addressed with some more effort.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1629/Reviewer_35Lo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1629/Reviewer_35Lo"
        ]
    },
    {
        "id": "FgJYfY90fHC",
        "original": null,
        "number": 2,
        "cdate": 1666631419736,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631419736,
        "tmdate": 1666631419736,
        "tddate": null,
        "forum": "w-x7U26GM7j",
        "replyto": "w-x7U26GM7j",
        "invitation": "ICLR.cc/2023/Conference/Paper1629/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper discusses a novel method to enhance the pre-train chest x-ray image embeddings using the associated report. They train two masked auto-encoders (one for reports and the other for images). They then combine the image patch embeddings with report embeddings to decode the masked report tokens. At the same, they also decode a high-resolution image using the same image patch embeddings. They compare their method with existing methods and show that they outperform other methods by a considerable margin on multiple datasets.",
            "strength_and_weaknesses": "Strength(s):\n1. The method is novel and explained well. \n2. The method outperforms other methods by a considerable margin on multiple datasets thereby signifying the method efficiently utilizes the signals from reports to learn better image embeddings.\n3. The ablation experiments of different components nicely demonstrate how each component contributes to the performance.\n\nWeakness(es)/Suggestion(s):\n1. It would be nice to have confidence intervals/ SD around the metrics used to compare different methods.\n2. Why GAP was used to share image information with the masked record decoder? What happens if we use other alternatives like Max pooling, or have a transfer module that takes in embedded patches and returns a global embedding for the image?\n3. How is the lambda parameter in equation 3 decided? \n4. Some ablation studies to investigate how well the model learns for different masking ratios and masking patterns would be interesting for the readers.",
            "clarity,_quality,_novelty_and_reproducibility": "The method is clearly explained and novel. They have shared the code to reproduce the results.",
            "summary_of_the_review": "In summary, I found the method introduced in the paper to be interesting, novel, and elegant. The results across multiple datasets and downstream tasks demonstrate that the method is better than other methods at learning insights from reports to improve image embeddings. There are some important ablation experiments missing from the paper which would only strengthen the paper. I, therefore, give a recommendation of marginally above the acceptance threshold. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1629/Reviewer_JZho"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1629/Reviewer_JZho"
        ]
    },
    {
        "id": "nP36vnUHPmf",
        "original": null,
        "number": 3,
        "cdate": 1666662339603,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662339603,
        "tmdate": 1668800129946,
        "tddate": null,
        "forum": "w-x7U26GM7j",
        "replyto": "w-x7U26GM7j",
        "invitation": "ICLR.cc/2023/Conference/Paper1629/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a novel approach for self-supervised learning from radiographs and associated medical records. The proposed multimodal method learns to predict masked inputs from both record (test) and radiograph (image) data.\n",
            "strength_and_weaknesses": "Strengths\n- Paper is well written and easy to understand \n- A thorough review of literature is provided\n- Extensive experiments validate the proposed approach on the MIMIC-CXR (pre-training) and CheXPert, RSNA Pneumonia, NIH-Chest X-ray and COVID-19 Image Data Collection datasets (fine-tuning).\n- Compelling results are presented showing the computational improvements in AUC with the proposed method\n\nWeaknesses\n- Not clear why masking is necessary for images, or whether the task could be super-resolution only\n- Ablation studies with non-masking, non-super resolution pre-text tasks on records are missing\n- Experimental comparisons to other multimodal (language+image) methods, such as M3AE (Geng et al. 2022) are missing\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is very well written and easy to understand. One part that is missing is a discussion of whether multi-modal self-supervised learning has been tackled in other methods, potentially outside of medical imaging. \nAlso, what is the labelling ratio defined as? \n\nThe approach seems like a novel combination of existing image-based and text-based methods. Code is provided and most of the implementation is well described in the manuscript. It is not clear why the weight (lambda) of the report and the image loss is equal, have the authors experimented with this weight? \n",
            "summary_of_the_review": "The paper seems like a straightforward combination of existing techniques, but generates compelling results on a variety of benchmarks. Novelty of combining MAE with BERT is already proposed by some papers, e.g. M3AE, and there is no clear and demonstrated improvement over these works, aside from the novel application to medical data. However, ablation experiments, discussion, and writing is clear and complete. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1629/Reviewer_Xf1P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1629/Reviewer_Xf1P"
        ]
    },
    {
        "id": "CrfsOkO9DOo",
        "original": null,
        "number": 4,
        "cdate": 1667058581750,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667058581750,
        "tmdate": 1667058581750,
        "tddate": null,
        "forum": "w-x7U26GM7j",
        "replyto": "w-x7U26GM7j",
        "invitation": "ICLR.cc/2023/Conference/Paper1629/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on building better models for unsupervised pretraining of deep neural networks for chest xray analysis.  This is achieved combining multi-modal information, i.e. radiological reports and chest xray images, and defining an unsupervised training procedure that predicts simultaneously masked parts of the images/reports given the non-masked part (masked record modelling). \n\n \n\nThe learned representations can be used to initialize networks for downstream tasks (classification, segmentation), and allow to outperform competing methods even when using a much lower amount of labelled data. \n\n ",
            "strength_and_weaknesses": " \nOne of the biggest issues when developing algorithms for chest xray analysis is access to labelled data, since it is expensive for radiologists to label tens of thousands of images. Most often, labels are  then generated with rule-based NLP algorithms. These labels are however very noisy, both because of the complexity of the text data to analyse, but also because in many cases radiologists might not write in the reports about diseases that are present in the image but are not clinically relevant for the patient. \n\nThe method discussed in this paper allows to consider an alternative approach: learn transferable radiography representations using image+report data, and fine-tune the networks for downstream tasks using a low amount of labels. \n\nThe experimental section is extensive, and shows increased performances compared to related methods. Importantly for practical purposes, thanks to the pretrained model, good classifiers can be built only using 10% of the labels. \n\nAll the ablation studies I had in mind while reading the MRM section were done by the authors, which show that all components of the proposed model are necessary.  \nIt would be interesting for me to see how the model performs with different amounts of masking probabilities. In particular I am surprised that the model performs well even when removing 75% of the patches. \n\nThe structure of the experimental section is a bit confusing the way it is split now in \"Baselines\" vs \"Results\". For example, after reading about the tasks and baselines in section 4.3.1, it would make more sense for the reader to directly see their results (section 4.5.1). \n\n\n_Typos_: \n* Second last paragraph of introduction: base->based \n* First sentence of section 3: learns -> learn ",
            "clarity,_quality,_novelty_and_reproducibility": " \n\nI found the paper very interesting and overall well written. To the best of my knowledge, the presented methodology is novel. \n\n \n\n \n\n",
            "summary_of_the_review": "Overall an interesting paper with practical applications. I believe it can have an impact in the medical imaging community. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1629/Reviewer_UVnZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1629/Reviewer_UVnZ"
        ]
    }
]