[
    {
        "id": "EaJcxWekx4",
        "original": null,
        "number": 1,
        "cdate": 1666472529448,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666472529448,
        "tmdate": 1666472529448,
        "tddate": null,
        "forum": "G2Q2Mh3avow",
        "replyto": "G2Q2Mh3avow",
        "invitation": "ICLR.cc/2023/Conference/Paper748/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose composing multimodal models with language models\nin a process they call \"socratic models\". The key idea is that the\noutputs of VLMs/ALMs can be reformulated in text, which can then be\nfed to a LLM via a prompt. While each instantiation of a socratic\nmodel is slightly different, this family of approaches can serve as a\nsurprisingly strong zero-shot baseline for multimodal tasks. The\nauthors consider several creative compositions and demonstrate that\ntheir general approach is more effective than prior efforts for zero\nand few shot multimodal models.",
            "strength_and_weaknesses": "My favorite part of this work is the creativity of the compositions.\nFor captioning, the authors compose several types of object detectors\n(scene type, person counter, object detector, etc.), format the\noutputs of that in a LLM prompt, which proposes several captions,\nwhich are then re-ranked by CLIP: the resulting model is SOTA for\nzero-shot captioning, and even competitive with models that fine-tune\non upaired captions. The rest of the examples are even more creative,\nranging from video retrieval to egocentric video summarization, to\nmultimodal assistive dialogue, and beyond.\n\nOther positives:\n\n- I think these things make for great baselines, like the authors\n  said. I think most papers on multimodal modeling should include one.\n\n- I like that the features passed between models are in text format,\n  i.e., they are more interpretable compared to visual features from\n  an vision model directly.\n\n- I like that the decoupled nature of these compositions means that\n  performance improvements outside of multimodal modeling (e.g.,\n  better language-only models or better vision-only models) translate\n  directly to improvements in socratic models.\n\nMy biggest technical concerns are:\n\n- The granularity of the outputs of the vision/audio models, when put\n  into text, may not be sufficient for every downstream task ---\n  because information is lost, e.g., when making an object\n  classification vs. the whole image, socratic models may hallucinate\n  moreso than other approaches. The authors are aware of this, and\n  it's interesting to think about how to fix this.\n\n- The design of the prompts, compositions, etc. seem quite ad-hoc,\n  i.e., it seems like there's probably a lot of \"art\" to creating a\n  good prompt/set of attributes to textify from a VLM, etc.\n\nMy biggest presentation concern is:\n\n- Are we really going to call any composition of vision/audio/language\n  model a \"socratic\" model? Even ones that don't have as much\n  \"discussion\"? It seems like the models are not really engaging with\n  a \"dialogue\" as one might imagine given the name of the method.",
            "clarity,_quality,_novelty_and_reproducibility": "see above",
            "summary_of_the_review": "Overall, this is a creative paper with some very promising results.  I\nam hopeful that future multimodal works will include a socratic model\nat least as a baseline. And --- I think the future directions for\nthese types of models (i.e., models that communicate via textual\nrepresentations) are quite promising --- I can't wait to see where it\ngoes!",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper748/Reviewer_NinX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper748/Reviewer_NinX"
        ]
    },
    {
        "id": "mqnqG7ZmtM",
        "original": null,
        "number": 2,
        "cdate": 1666588158505,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588158505,
        "tmdate": 1666588158505,
        "tddate": null,
        "forum": "G2Q2Mh3avow",
        "replyto": "G2Q2Mh3avow",
        "invitation": "ICLR.cc/2023/Conference/Paper748/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies how to incorporate vision language model (VLM), audio language model (ALM) and large language model (LLM) to perform joint predictions for multimodal tasks. The integration of different LMs are based on various language prompts. Results show that the proposed method achieves promising zero-shot and few-shot performance. In addition, the proposed method can be applied to many novel zero-shot multimodal applications.",
            "strength_and_weaknesses": "Strengths\n1. Interesting methods by effectively incorporating LMs from different modalities\n2. Strong results on zero/few-shot learning\n3. Demonstrate interesting examples on many zero-shot applications\n\nWeaknesses\n1. Combining different LMs would make the inference speed much slower, which could be an obstacle for the application.\n2. Section 5 only provides several examples without statistical evaluation results. It it hard to evaluate the model's performance in this application, since the examples could be cherry pick.\n3. In the image captioning evaluation (MS COCO) in Table 1, there is still a large gap between SMs and ClipCap. More few-shot baselines are needed for evaluating the image captioning quality of SMs.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written, the proposed idea is original. The proposed method is novel and well-motivated and this paper provides evaluations on multiple tasks, but lack enough statistical evaluations in many zero-shot applications (Section 5).",
            "summary_of_the_review": "This paper proposed Socratic Models which aim to incorporate VLM, ALM and LLM for zero-shot multimodal tasks. The proposed idea is well motivated and novel and the experiments show promising and strong zero-shot results. However, this method might slow down the inference speed, and this paper lacks enough statistical evaluations in many zero-shot applications.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper748/Reviewer_x3bb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper748/Reviewer_x3bb"
        ]
    },
    {
        "id": "2IlFetpSV6O",
        "original": null,
        "number": 3,
        "cdate": 1666619307905,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619307905,
        "tmdate": 1669960152672,
        "tddate": null,
        "forum": "G2Q2Mh3avow",
        "replyto": "G2Q2Mh3avow",
        "invitation": "ICLR.cc/2023/Conference/Paper748/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores multimodal prompt engineering for zero-shot multimodal reasoning. Using language as the intermediate representation, the authors propose Socratic Models (SM) where outputs from different modalities are composed into textual prompts to provide zero-shot results on image captioning, description generation and video-to-text retrieval. The paper also discusses potential applications of these methods for dialog, QA and robot perception and planning. ",
            "strength_and_weaknesses": "Strength:\n- This paper is well-motivated where zero-shot multimodal reasoning is an important research area with vast industrial and research applications.  \n- The authors promise the availability of corresponding code and also provide anonymous colab notebooks which is essential for reproducibility. \n- Model-generated examples provide detailed insights into what the prompts and generated outputs were. \n\n\nWeakness:\n\n- One might argue that technical novelty is limited in the paper with not much significant research contribution. \n- The vast claims of zero-shot applications go on for 4 pages (Page 6-9) without much experiments and analysis to support them. \n- Continuing on the point above, I believe that the paper needs a major revision where Section 4 could be expanded in more detail with more experiments. For example, Appendix B.2 provides nice examples of ablation where information from VLM is left out and should be a part of the main section compared to unjustified claims in Section 5. Even some applications from Section 5 could be absorbed in Section 4 only when provided with supporting experiments and ablation studies. \n- This paper seems like a mix of position and experimental paper while doing justice to neither. \n- Even though the zero-shot results might be a promising direction, currently the 0-shot results seem far from industry standard applications where there is a huge gap between all the automatic metrics from the baseline method (6.9 vs 40.97 BLEU, 44.5 vs 152 CIDEr)\n- It is not clear how prompts were designed for few shot methods like the ones described in Section 4.1 \n- Similar experiments with few-shot techniques for Section 4.2 and 4.3 would have made the results more compelling. \n- Even with few-shot learning, have the authors experimented with providing more than 3 examples and comparing the evaluation results as the number of examples is increased. \n- Table 1, Bleu-4 of exact 0 value for ZeroCap model seems like a bug in the experiments. Could the authors please recheck?\n- It is not clear how the videos are processed in Section 4.3? Does the model work on a per frame basis? How is CLIP(caption) computed and how does it differ from CLIP(video)? \n- Human evaluation is not provided for the different experiments. \n\n\nQuestions:\n- Could the authors discuss and clarify how they design these multimodal prompts? \n- As the authors also discuss in Section 2 (page 3), simple prompts modifications have a huge impact on performance. Did the authors thus experiment with different wording of the prompts? \n- An ablation on different prompts would be more compelling. How would the results compare when object, place categories or the number of people is omitted from the prompt in Section 4.1? Does the model always generate 3 object and place categories from the VLM? \n- In Section 4.1 (Page 4), have the authors also experimented with nucleus sampling or beam search? How do the results compare against the greedy search? \n- Have the authors also experimented with different prompts for description generation against captioning bot? Example prompting with `I am an intelligent image describing bot` compared to `image captioning bot` to promote longer descriptions and differentiate between the two tasks?\n- Could the authors clarify how the long-transcripts subset of the dataset was created and the statistics of the dataset?\n\n\nSuggestions/Comments:\n- Please take care of using citet compared to citep and vice versa (natbib style) appropriately (especially in Section 7 and Appendix). \n- Section 1 can be improved by providing relevant citations on page 2 of the baselines producing results 11.3 and 40.7 R@1 even though they are discussed much later in Section 4.2 on Page 5. \n- Pictorial depiction of the models either in the main section or in the appendices would increase the understanding and interpretability of the proposed approach.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see strengths and weaknesses for more details. Availability of corresponding code ideally makes the work reproducible while there is limited technical novelty. The paper is written clearly in most parts. ",
            "summary_of_the_review": "The results seem promising; however, the paper could be improved in the presentation and more detailed descriptions. Please see strengths, weaknesses, suggestions and comments. It would be nice to see a revised version of the paper in the future conferences.\n\n--- Rebuttal update ---\n\nI believe the authors have invested fair amount of time to address most of the questions/weaknesses. Even though the 0-shot results are pretty low, they still serve as baseline against other related work. Based on the rebuttal, I am increasing my score. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper748/Reviewer_J1Ki"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper748/Reviewer_J1Ki"
        ]
    },
    {
        "id": "l4AnxMTNs1_",
        "original": null,
        "number": 4,
        "cdate": 1666837832052,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666837832052,
        "tmdate": 1666838311382,
        "tddate": null,
        "forum": "G2Q2Mh3avow",
        "replyto": "G2Q2Mh3avow",
        "invitation": "ICLR.cc/2023/Conference/Paper748/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper expands prompt engineering techniques to multi-modality scenarios. Specifically, the paper sought to find how best to prompt a set of unimodal and multi-modal models to solve multi-modal tasks that are difficult to solve independently or would require large amount of training data to tackle. The paper also creatively demonstrated the power of in-context substitution where information from non-language domain is substituted into an LLM for contextual reasoning. The paper applied the developed techniques to different multi-modal tasks including image captioning, contextual image description, video-to-text retrieval, and others such as egocentric perception, multi-modal assistive dialog, and robotic perception and planning. The paper also introduce a novel zero-shot evaluation techniques that could be adopted for model selection purposes.",
            "strength_and_weaknesses": "Pros:\n\nThe paper is generally easy to follow, well motivated, and shows creative tricks to prompt combination of multi-modal models and LLM to solve multi-modal reasoning tasks. The paper attempted to formulate the multi-modal prompt engineering as traversal of a computational graph which is novel and intuitive. The paper also shows good zero-shot performance over strong baseline.\n\nCons:\nWhile the proposed multi-modal prompting is useful to prompt constrained LLM, VLM, ALM, figuring out the model sequence and the prompt structure to get the best outcome is not trivial. For the cases where the training data is publicly available, could it be possible wouldn't it be best to to just training task specific models. Also, in the case where the multi-modal data is not possible to generate, e.g, egocentric perception, multi-modal assistive dialog, and robotic perception and planning, could it be more useful to use this technique to generate a multi-modal training data that can be used to train task specific model?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. Authors also provided links to Colab where their work could be easily reproduced.",
            "summary_of_the_review": "The paper demonstrated that multi-modal prompt engineering is a viable option for utilizing publicly available multi-modal models at zero-shot without retraining a new model for a specific multi-modal reasoning task. The paper is well motivated and experimental results looks convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper748/Reviewer_3Nxe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper748/Reviewer_3Nxe"
        ]
    }
]