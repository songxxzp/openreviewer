[
    {
        "id": "RLsBJHumK5",
        "original": null,
        "number": 1,
        "cdate": 1666583893865,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583893865,
        "tmdate": 1666583893865,
        "tddate": null,
        "forum": "n-5qp16As_C",
        "replyto": "n-5qp16As_C",
        "invitation": "ICLR.cc/2023/Conference/Paper1139/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new initialization method for ConvNets with residual blocks and ReLU activation, called RISOTTO. It enables training deep networks with residual structures without a normalization layer. This is achieved by maintaining a dynamical isometry at the initialization time, which is implemented as initializing the weight matrices as a block-wise looks-linear structure. The experiments on CIFAR and Tiny Imagenet can support the claim of this paper.",
            "strength_and_weaknesses": "Strength:\n1) The proposed method is simple and effective. It makes ResNet without normalization trainable just by changing the initialization of convolution. Compared to fixup, it does not initialize the output of the residual branch to be 0 (as in Fixup). This is surprising to me.\n2) It takes explicit consideration of ReLU activation and achieves exact dynamical isometry for residual blocks. This is different from several previous works and can be inspiring for future research.\n\nWeakness:\n1) The central theme of this paper is unclear to me. From the introduction, I think the authors want to describe the disadvantage of batch normalization and propose a way to remove batch normalization, which is shown in Table 1. However, the authors also use several paragraphs and Table 2 to show RISOTTO is compatible with batch normalization. It makes me confused because batch normalization can reduce the sensitivity of the initialization method. It is not clear why showing the compatibility between RISOTTO and BN can support the paper\u2019s claim.\n2) In Table 1 and Table 2, it seems the proposed method is not better than the previous method (fixup) without batch normalization.\n3) This paper mentions \u201cseparability of inputs\u201d several times but I didn\u2019t understand what it means. The authors may want to consider adding the reference to that part. Is it equivalent to maintaining the inner product of input?\n4) Missing reference: This paper claims previous work only proposes dynamical isometry in a vanilla network (without residual structures). In [1], the paper proposes a way to achieve \u201capproximate dynamic isometry\u201d both at initialization and training time. There are differences between [1] and the proposed work such as [1] initializing the output of residual blocks to be 0 which is opposite to this work, but it\u2019s worth comparing and discussing.\n5) The proposed method only shows the trainability of a network with 50 layers on CIFAR. In contrast, previous methods usually train 100 layers (fixup and [1]) on ImageNet or 10000 layers on CIFAR. Whether it can be scaled to deeper networks remains unknown. The proposed method is also only validated on CIFAR while many previous methods can train deep resnet on ImageNet without a normalization layer (fixup and [1]).\n[1] Qi et al. Deep Isometric Learning for Visual Recognition. ICML 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "There are several aspects that the paper can improve in writing:\n- It is worth carefully rethinking whether the experiments of adding BN are related to the central story and how to make the point.\n- Sections 2.1 and 2.2 can be made much more concise. I got distracted when I read this section. It would be better if the authors can first clearly state what they propose to change, and then discuss the background and motivation. It would also be helpful to add figures or an algorithm box to emphasize your modifications.\n\nThis paper is novel since the proposed method has not been considered before. It also provides a way of dealing with ReLU non-linearities.\n\nIt provides code. I checked the relevant implementation. I think the proposed method is simple and also easy to reproduce.",
            "summary_of_the_review": "In summary, this paper is interesting as it shows by designing a simple initialization method, deep networks with residual structures can be trained. On the other hand, the content of this paper can be improved a lot. I recommend acceptance for this paper but I expect to see an improved version of this paper in the discussion phase.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not Applicable.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1139/Reviewer_VqzH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1139/Reviewer_VqzH"
        ]
    },
    {
        "id": "VCwBNho9yg",
        "original": null,
        "number": 2,
        "cdate": 1666676538363,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676538363,
        "tmdate": 1666676538363,
        "tddate": null,
        "forum": "n-5qp16As_C",
        "replyto": "n-5qp16As_C",
        "invitation": "ICLR.cc/2023/Conference/Paper1139/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed RISOTTO (Residual dynamical isometry by initial orthogonality) to initialize residual networks. While RISOTTO achieves some success in several datasets. I feel that the experiments are inadequate, and the superiority of RISOTTO is not significant.",
            "strength_and_weaknesses": "Pros:\n\n- Writing is comparably easy to follow.\n\nCons:\n\n- The main difference between the proposed RISOTTO and other initialization (e.g., fixup) is that RISOTTO has analyzed the weight in residual connection (termed as C type in this paper). However, This C type is not the general case in residual nets. In a residual net, there can be a pooling layer for downsampling. And even in a case using C type, the weight is only used for downsampling, thereby not as general as no weight in residual connection. Moreover, Transformer is also a type of residual network, they do not have such a C type. Therefore, I feel that the motivation is not sufficient.\n\n- On experiments:\n  - The superiority of RISOTTO is not significant. For example, in Table 1 and Table 2, all the baselines (i.e., Fixup, SkipInit, He Normal, and He Uniform) can derive better results in some settings. Especially, Fixup achieves all better results on B type, the more general condition.\n  - The ResNets are trained for 150 epochs may not be enough. It is common to train 200 epochs at least.\n  - In Table 2, the results of two He initialization on Tiny ImageNet are incredible. Specifically, on B type, He Normal derives 52.21, and He Uniform derives 55.05. There is around a 3% distance. It is hard to believe the results as these two initialization are both in accordance with He initialization, and the only difference is the sampling strategy.\n  - A learning rate of 0.1 is a common setting. Why did this paper set 0.01 for training ResNet50 on Tiny Imagenet, while 0.1 for cifar10/100?\n  - Figure 2, Figure 3, and Figure 4 are all based on C-type nets. What are the results of B type as it is also an important architecture?\n  - In Figure 3, Fixup seems to derive better results.\n  - It is better to add Transformer as a backbone.\n  - It is better to add experiments on large-scale datasets, like ImageNet.\n\n\nOverall, regarding the above points, I vote for \"rejection\".",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Medium.\n\nQuality: limited.\n\nNovelty: Medium.\n\nReproducibility: Provided codes.",
            "summary_of_the_review": "The paper proposed an isometry dynamic based initialization. However, I still have some concerns as mentioned above. Therefore, I tend to give \"rejection\".",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1139/Reviewer_GPrm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1139/Reviewer_GPrm"
        ]
    },
    {
        "id": "7ArxGo5sER",
        "original": null,
        "number": 3,
        "cdate": 1666872426179,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666872426179,
        "tmdate": 1666872426179,
        "tddate": null,
        "forum": "n-5qp16As_C",
        "replyto": "n-5qp16As_C",
        "invitation": "ICLR.cc/2023/Conference/Paper1139/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose to initialize ResNets with ReLU activations with exact dynamical isometry by combining ideas from delta-orthogonal initialization and looks-linear initialization. The proposed initialization scheme guarantees that every residual block is an exact orthogonal map. A small empirical evaluation shows that the scheme is able to train moderately deep ResNets with/without batch normalization.",
            "strength_and_weaknesses": "Strengths:\n\n1) To my knowledge, this is the first paper to apply looks linear initialization (with orthogonal weights) to a ResNet.\n2) Experiments suggest this scheme achieves good performance.\n\nWeaknesses:\n\n1) Note that Balduzzi et al. already combined orthogonal initialization with looks linear initialization in feedforward networks. Since this scheme already achieves dynamical isometry, it is not clear to me why the skip connection is necessary. However the paper does not explore this question? I would like to see experiments assessing whether or not the skip connection is beneficial in practice.\n\n2) Definition 2.2 (\"Normal ResNet initialization\") is not how practitioners normally initialize ResNets. In particular, practitioners do not normally ensure $\\alpha^2 + \\beta^2 = 1$, indeed almost all implementations do not include either $\\alpha$ or $\\beta$. This leads to some confusing statements later in the text.\n\n3) The experimental section is very weak. No large scale datasets (eg ImageNet) are included, and almost all experiments use the shallow ResNet-18 architecture, which is relatively easy to train with/without BatchNorm simply by ensuring the signal doesn't explode on the forward pass. Initialization becomes more important in deeper networks (eg 100+ layers). \n\n4) No comment is given describing how learning rates and other hyper-parameters are tuned, and I suspect that poor tuning accounts for many of the observed differences between initialization schemes.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is mostly clear. I did not check any theorems or proofs\n\nQuality:\nThe quality of the experimental section is low. The introduction/discussion is mostly clear but has some confusing/misleading statements.\n\nNovelty:\nThe work is somewhat novel. The authors extend Looks Linear initialization from feedforward networks to ResNets, however they do not provide any discussion/experiments to motivate why this is necessary given that deep feedforward networks are already trainable.\n\nReproducibility:\nThe authors do not give full details describing their experiments (eg hyper-parameters, exact architecture).\n\nOther comments:\n1) The authors assert that feature diversity at initialization is important, however they don't provide any evidence for this. Additionally, do schemes satisfying dynamical isometry not by definition have very low feature diversity?\n\n2) As i commented above, it is standard practice to not introduce an $\\alpha$ or $\\beta$ parameter, and consequently the scale of hidden activations on the forward path blows up exponentially without normalization and linearly with normalization. This is the basis of the argument in De and Smith as to how BN suppresses the residual branch.\n\n3) does insight 2.6 assume that the network width is finite?\n\n4) Note that NF-ResNets only require gradient clipping when training with large batch sizes, at small batch sizes gradient clipping isn't necessary as shown in Brock et al. (2021, a). ",
            "summary_of_the_review": "The proposed initialization scheme is quite complicated and it will only be adopted by community if there are clear empirical benefits. Since the experiments provided were not convincing, I recommend rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1139/Reviewer_XaUD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1139/Reviewer_XaUD"
        ]
    },
    {
        "id": "C96ElEw8FF",
        "original": null,
        "number": 4,
        "cdate": 1667376959623,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667376959623,
        "tmdate": 1667376959623,
        "tddate": null,
        "forum": "n-5qp16As_C",
        "replyto": "n-5qp16As_C",
        "invitation": "ICLR.cc/2023/Conference/Paper1139/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a new random initialization scheme Risotto that achieves dynamical isometry for residual networks with Relu activation functions. The key difference from the previous work is that they achieve the dynamical isometry by balancing the signals from both residual and skip branches, unlike the previous works that mainly suppress the signal from the residual branch. They provide theoretical justification for the proposed method and extensive empirical experiments to denote the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strengths:\n1. The derived signal propagation results does not require mean field approximation but highlight input separability issues.\n2. The proposed method considers the contribution from both residual and skip branches unlike most previous works achieving dynamical isometry in residual networks.\n3. Promising empirical results on CIFAIR and Tiny ImageNet datasets.\n\nWeaknesses:\n1. It is not clear why feature diversity is important for the initialization. The authors differ the proposed method from the previous works like Fixup or SkipInit by arguing that the proposed method promotes feature diversity. However, the reason why feature diversity is needed is not well motivated in the paper. It would be good to study how the feature diversity affects the initialization.\n2. The proposed method has limited applicability. Because the method and its derivation highly rely on post-activation ResNet structure, it can not be directly applied to pre-activation ResNet (which could achieve dynamical isometry easily using skip connections). As it is limited to residual networks, it can not be applied to other models such as transformer. \n3. Large-scale evaluations are needed to show the effectiveness of the proposed method. The authors only evaluate the proposed method on CIFAIR and Tiny ImageNet datasets. It would be good to evaluate the method on full ImageNet setting. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. For related works, I suggest the authors to disentangle the proposed method from others such as Fixup and SkipInit more. Currently it is not clear why is the main contribution of your method that differentiates it from the previous works. ",
            "summary_of_the_review": "Overall I believe the paper is above the acceptance level, and I would like to raise my score if the authors address the above concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1139/Reviewer_MPAw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1139/Reviewer_MPAw"
        ]
    }
]