[
    {
        "id": "VfBCGMb30a",
        "original": null,
        "number": 1,
        "cdate": 1666613051967,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613051967,
        "tmdate": 1670091824126,
        "tddate": null,
        "forum": "rdfgqiwz7lZ",
        "replyto": "rdfgqiwz7lZ",
        "invitation": "ICLR.cc/2023/Conference/Paper2790/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a method for detecting covariate shift by training two classifiers to agree on a distribution P and disagree on another distribution Q, and then build a hypothesis test on top of these two classifiers. Their proposed method is able to detect covariate shifts better than several competing approaches. However, there seems to be potential failure cases for the method that the authors do not discuss sufficiently. \n",
            "strength_and_weaknesses": "Strengths: \n- The authors provide very detailed theoretical and empirical analysis of their method, in both the main paper and the appendix. \n- The proposed method works well, beating several other methods for detecting covariate shifts in the experiments. \n\n\nWeaknesses: \n- While the approach provides good empirical performance, the core idea of the method is still unclear to me, in terms of how the assumptions might fail. The main idea of the method is to detect harmful covariate shifts by training two classifiers, forcing them to agree on P while disagreeing on Q. There are several possible scenarios: \n  1. If it's possible to train such a pair of classifiers f and g, it does not necessarily mean the shift from P to Q is harmful. Consider linear model on disjoint supports P and Q. It just means P and Q are different. If we just want to detect shifts between P and Q, why do we not use non-parametric tests such as the KS test, or train a logistic regression model distinguishing P and Q as commonly used in covariate shift correction? \n  2. If it's not possible to train such a pair of classifiers f and g, it might just be that the models are underfitting. Consider fitting linear models to quadratic target y = x^2 between -1 and 1. P is positive region, Q is negative region. It is not possible to fit linear models that agree on P but disagree on Q, but P and Q are distinct.  \n\n  Although these are specific boundary cases on potential failure modes, the fact that the current method does not seem to cover them properly make me uncertain about the the method. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is mostly clearly written, and well-supported by the Appendix. \n",
            "summary_of_the_review": "This paper propose an interesting approach for detecting covariate shifts, and the authors spend a lot of effort in its theoretical and empirical analysis. However, there seems to be potential cases that the method can fail which the authors do not cover sufficiently. A better discussions on these, even listing them as limitations or assumptions, can help improve the paper. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2790/Reviewer_C9Zo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2790/Reviewer_C9Zo"
        ]
    },
    {
        "id": "SNppwAJUduR",
        "original": null,
        "number": 2,
        "cdate": 1666827265293,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666827265293,
        "tmdate": 1670231253599,
        "tddate": null,
        "forum": "rdfgqiwz7lZ",
        "replyto": "rdfgqiwz7lZ",
        "invitation": "ICLR.cc/2023/Conference/Paper2790/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the problem of detecting covariate shifts where a trained model would perform differently. In particular, the authors propose a framework that prepares new models that disagree with the base model and then uses the disagreement rates (or prediction entropy) to build the statistical test.",
            "strength_and_weaknesses": "(+) The problem of detecting performance change with an unlabelled dataset is a practical question for many ML applications.\n\n(+) The proposed framework of training disagreeing models and using the model predictions to build test statistics provides an interesting development to the research problem.  \n\n(-) While this paper has good coverage of related work on drift (covariate shift) detection, the coverage of methods that estimate the model performance on unlabelled datasets is limited. This limited coverage could be crucial as some methods use similar ideas of modelling the disagreement among multiple models and can be used directly to solve the targeted research problem. (See below for details.)\n\n(-) Some details of the paper might need additional justification, such as the objective learning function for model disagreements, the proposed test statistics, and baseline approaches. (See below for details). ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity (4/5): This paper is mostly easy to follow, and readers with similar backgrounds should get the central concept without significant issues. Some mathematical notations might require additional marks to avoid confusion. For instance, $N$ is first used as the number of classes and then as the number of samples. $n$ denotes the dataset size at first but is then used as the number of unseen samples.\n\nQuality (3/5): While the paper provides an interesting solution and good coverage of drift detection approaches, some technical details might require further justifications. Also, the paper doesn't cover some related work that aims to estimate the performance with an unlabelled test set.\n\nOn the technical details: (1) regarding the proposed disagreement-cross-entropy, while the authors provide some intuition, it should be aware that the cross-entropy loss and associated entropy measure both come with theoretical properties (i.e. proper scoring rules). I wonder if the authors can provide any properties of the proposed loss, as it is helpful to know whether the loss can lead to a biased disagreement (other than a pre-defined and consistent one). (2) At the beginning, I was surprised to see that the learned deep-MMD performed poorly in detecting drifts that even affect model performance. Then I realised that the experiments are performed with quite a limited sample size. In this case, I might further consider reducing the complexity of the projection net in methods like deep kernel MMD and classifier two-sample test, as it is just not practical to train a large model with such a limited sample size. (3) Following the discussion on sample size,  it is clear that the proposed method requires repeated training on both P and Q for multiple models. I wonder if this could be a potential reason for experimenting on a small dataset. It might be helpful if the authors could further comment on the running costs of the method for larger datasets. \n\nOn the related work side, there have recently been many attempts to directly estimate the model performance with a (drifted) unlabelled test set, which is undoubtedly closely associated with this paper. (https://arxiv.org/abs/2202.05834) provides a nice list of some of these methods. In particular, (https://proceedings.mlr.press/v48/platanios16.html) also make use of multiple models' predictions to infer the actual error rate of the test set, which shares a strong link to the proposed method in this paper. It is therefore suggested to justify further the proposed method compared to the aforementioned related work.\n\nNovelty (3/5) The novelty is reasonable as the authors proposed an interesting statistical test on changing performance with the test set. As mentioned above, the problem has many existing attempts, and the idea of using model disagreements can be shared by existing work.\n\nReproducibility (5/5): This paper provides an excellent example of reproducible research. Both the methods and experiments are described in detail. The source code is also available for additional in-depth investigation.",
            "summary_of_the_review": "While the proposed framework provides interesting solutions for training different models and building the statistical tests, this paper might require further work on involving and discussing critical related work before getting published as high-impact research.\n\n\n=======After author feedback============\n\nThank the authors again for the extensive feedback. The added explanation and experiments regarding performance estimation are a nice addition to the paper. I have decided to change the score to 6 as a result, but I think some systematic refinement on the paper would make it much stronger (i.e. the disagreement loss, and connections to performance estimators). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2790/Reviewer_En7A"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2790/Reviewer_En7A"
        ]
    },
    {
        "id": "e_60FLLjhv",
        "original": null,
        "number": 3,
        "cdate": 1666933474371,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666933474371,
        "tmdate": 1670114857652,
        "tddate": null,
        "forum": "rdfgqiwz7lZ",
        "replyto": "rdfgqiwz7lZ",
        "invitation": "ICLR.cc/2023/Conference/Paper2790/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work develops a new hypothesis testing procedure for identifying harmful covariate shifts. To identify harmful covariate shifts, the authors propose to use disagreement cross entropy for multiple models and present Detectron (Disagreement) based on a permutation test. ",
            "strength_and_weaknesses": "- Strength\n  - The idea of using an ensemble of models that maximize out-of-domain disagreement makes sense. \n  - Related work section fairly covers literature in the field.\n  - Extensive experiments are performed.\n \n- Weakness\n  - The paper critically lacks clarity and is not mathematically rigorous. It really causes lots of confusion. Please see Clarity, Quality, Novelty, And Reproducibility.\n  - Implication of Theorem 1 is not clear. As far as I understand, it is almost impossible to know when the following condition holds: the rate which $g$ disagrees with $f$ on $n$ unseen samples from $\\mathcal{Q}$ is greater than that from $n$ unseen samples from $\\mathcal{P}$ w.p. greater than $p^{*} = (1-4^{-n} \\binom{2n}{n} )/2$. Given that the probability is derived from the expectation of such random experiments, we might need multiple sets of $n$ unseen samples, which is not realistic. \n\n- Question:\n  - Theorem 1: do you have an insight into why $p^*$ is independent of $N$? Does it imply that this theorem holds even $N=1$? It is not intuitive... \n ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: One of the main problems of this paper is clarity.\n - In Problem Setup of Section 3, $N$ is defined as the number of classes. In Theorem 1 and Algorithm 1, it becomes the sample size of $\\mathbf{P}$ and $\\mathbf{Q}$, respectively.\n - In Problem Setup of Section 3, $\\mathbf{P}$ is defined as a set of $n$ labeled samples, but in equation (3), it is a set of input data.\n - In the Constrained Disagreement Classifier, $\\mathcal{P}$ is not defined. \n - PQ learning part (page 4): $\\epsilon$ and $\\delta$ are not defined, and it is not clear the definition of $rej_h (x)$ and $err_h (\\tilde{x})$. How are they expressed as mathematical equations? What types of out-of-distributions are considered?\n\nReproducibility:\n - Code is submitted.\n\n",
            "summary_of_the_review": "The submitted paper considers an interesting problem, and the proposed approach makes sense at a high level. However, it lacks clarity which causes much confusion, so I think it is not ready for publication.\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2790/Reviewer_wkvQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2790/Reviewer_wkvQ"
        ]
    },
    {
        "id": "41ch1Dv9D0k",
        "original": null,
        "number": 4,
        "cdate": 1667390603287,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667390603287,
        "tmdate": 1670483644272,
        "tddate": null,
        "forum": "rdfgqiwz7lZ",
        "replyto": "rdfgqiwz7lZ",
        "invitation": "ICLR.cc/2023/Conference/Paper2790/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a learning-based testing method to identify distribution shifts (a new domain) with access to finite samples of the test domain. The authors define harmful covariate shift (HCS) that may weaken the generalization of a predictive model, and constrained disagreement classifiers (CDC) that performs consistently on ID data and inconsistently on OOD data. Then they detect HCS by the inconsistency of the outputs of the constrained disagreement classifiers. Theoretical analysis shows that the disagreement rate of CDCs can be used to detect covariate shifts. The proposed algorithm uses the empirical distribution of the CDCs on the training data to determine the reject region.",
            "strength_and_weaknesses": "**Strength:**\n1. The authors propose disagreement cross-entropy to improve the optimization procedure of the PQ-learning problem.\n2. This work develops a hypothesis testing method to detect covariate shifts and the detection principle is consistent with the common theoretical understanding that extrapolation increases the variance of outputs.\n3. Experiments show that the proposed method is sample efficient.\n\n**Weakness:**\n\n1. The motivation is unclear. In Section 3, the generalization set $\\mathcal{R}$ is a set of distributions. Is it possible that $\\mathcal{P}$'s support set overlaps with $\\mathcal{Q}$'s support set? If $\\mathcal{Q}$'s support set is a subset of $\\mathcal{P}$'s support set, how to understand the harmful covariate shift? Can you disentangle the effect of correlation shift ( the relation between the inputs and outputs $P(Y|X)$ ) and the effect of harmful covariate shift? \n2. Need more analysis on the utility of disagreement-cross-entropy. In Eq. (3), if we remove the disagreement-cross-entropy loss, how does the performance of Detectron change? Does the learning procedure of $g$ change the correlation between $Y$ and the features in $f(X)$? \n3. What are the requirements for the pre-trained model $f$? Is $f$ required to achieve good prediction accuracy? How does the proposed detection method perform if the ID data is ImageNet?\n4. If the pre-trained model $f$ is over-parameterized, can we force the disagreement rate on ID data to be 0%? Then how to implement Algorithm 1?\n5. Theorem 1: \"\u2026 If the rate \u2026 greater than \u2026 there must be covariate shift.\" This is already a hard threshold. Why do you still consider a hypothesis test approach?\n6. Can we consider the task in this paper as an OOD detection task with auxiliary data? If so, a comparison with recent OOD Detection methods is required, e.g. POEM.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty and Quality are fair. This work contributes some new ideas. It has minor technical flaws and some typos. The notations should be improved. For example, the notation $[ \\cdot ]$ in Eq. (2) should be an indicator function and there is no definition of $\\hat y_c$. \nThe clarity is poor. The content should be carefully reorganized.\n",
            "summary_of_the_review": "I recommend marginal rejection. Overall, this is a potentially promising work. But I do not think the current version is ready for publication. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2790/Reviewer_DJjN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2790/Reviewer_DJjN"
        ]
    }
]