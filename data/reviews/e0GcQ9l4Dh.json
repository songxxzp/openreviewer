[
    {
        "id": "AYk6vILx7d",
        "original": null,
        "number": 1,
        "cdate": 1666620257329,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620257329,
        "tmdate": 1666620257329,
        "tddate": null,
        "forum": "e0GcQ9l4Dh",
        "replyto": "e0GcQ9l4Dh",
        "invitation": "ICLR.cc/2023/Conference/Paper3076/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a privacy protection method against training data extraction attacks from language model based on R\u00e9nyi differential privacy. The authors point out the effect of sample length on privacy leakage and derive privacy bound based on R\u00e9nyi differential privacy, and then propose an improved method based on lazy sampling to identify training samples that may leak privacy. The authors construct corresponding experiments for each part of the proposed method by fine-tuning the language model and show that the privacy leakage under the proposed attack model is less than the privacy guarantee. ",
            "strength_and_weaknesses": "Strength\n1. This paper considers the sample length and derives the privacy leakage boundary of training samples based on the R\u00e9nyi differential privacy method, which seems to be reasonable and effective.\n2. The authors propose a lazy-sampling-based method to improve the existing method to perform training data extraction attacks, which improves the performing efficiency and effectively identifies potentially private samples. \n3. The organization and the results of the experiments in this paper appear to be efficient and reasonable. \n\nWeaknesses\n1. First, this article is difficult to read for me. I suggest that authors should be more careful with domain words and notations, such as \u201creconstruction attack\u201d. In my background knowledge, a \"reconstruction attack\" in federated learning or graph learning for privacy-preserving is to reconstruct the training data using information (gradient or aggregated information) from the model training in the training process. However, I think that the \"reconstruction attack\" in this paper is an attack method in the inference process just like the member inference attack. I am confused if the \u201creconstruction attack\u201d in this paper is the \u201ctraining data extraction attack\u201d. If it is a new attack, I would expect the author to redefine it.\n2. In my opinion, the core idea of this paper is to use differential privacy to achieve that the generated samples using the random attack model are satisfying the privacy guarantee. Although the proposed method is effective and reasonable, it is incremental work and lacks innovation. \n3. I think the organization of this paper needs improving. The framework of this paper looks fine, important sections are included and organized in a reasonable way. However, the content in each section is not easy to follow, and the organization of each section can be improved.",
            "clarity,_quality,_novelty_and_reproducibility": "This framework is of average quality and kind of lacking innovation. The description of this framework is clear and the theoretical analysis is sufficient. In the experimental part, the validity analysis of the results is relatively redundant. The originality of the work is acceptable.",
            "summary_of_the_review": "This paper proposes a privacy protection method against training data extraction attacks from language model based on R\u00e9nyi differential privacy. However, this work lacks some innovation for ICLR and the content in each section is not easy to follow and the organization can be improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3076/Reviewer_UcP3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3076/Reviewer_UcP3"
        ]
    },
    {
        "id": "ZiwxhW1X10n",
        "original": null,
        "number": 2,
        "cdate": 1666997760373,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666997760373,
        "tmdate": 1666997760373,
        "tddate": null,
        "forum": "e0GcQ9l4Dh",
        "replyto": "e0GcQ9l4Dh",
        "invitation": "ICLR.cc/2023/Conference/Paper3076/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work examines whether the secret information that may be contained in the training data set can be extracted from a model which satisfies RDP. This problem is positioned as a problem of intermediate difficulty between the reconstruction and membership attacks. The authors define leakage as the probability that an attacker identifies a secret in a model learned from data that contains the secret and in a model learned from data that does not. Through analysis, the authors obtain upper bounds on the log ratio of the two and compare them to empirical evaluations.",
            "strength_and_weaknesses": "Strength\n- Considered an interesting and novel problem\n- The considered problem well fits privacy that should be considered in the real world\n\nWeakness\n- Characterization of the obtained result seems to be a bit weak\n",
            "clarity,_quality,_novelty_and_reproducibility": "To me, the definition looks vague. Is secret defined as a full sample or a part of a sample?\nThe example discussed in the introduction looks like a  part of a sample, but from the definition in Section 3, it looks like a full sample. In experiments, the bit size of the secret is controlled. If the secret is a full sample, this cannot happen.\n\nI could not well understand the relationship between Section 3.1 and Section 3.2. \n\nIs it possible to characterize the absolute leakage concerning sample size or some other factors that we can control?\n",
            "summary_of_the_review": "Overall, the manuscript considers an interesting problem.\nExperimental results are interesting, while the characterization of theoretical results seems to e a bit weak.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3076/Reviewer_ruMF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3076/Reviewer_ruMF"
        ]
    },
    {
        "id": "BC0BBdnmcj",
        "original": null,
        "number": 3,
        "cdate": 1667100163407,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667100163407,
        "tmdate": 1669965163542,
        "tddate": null,
        "forum": "e0GcQ9l4Dh",
        "replyto": "e0GcQ9l4Dh",
        "invitation": "ICLR.cc/2023/Conference/Paper3076/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides an improved theoretical guarantee for the defense against reconstruction attacks using Renyi differential privacy. The correctness of the theoretical analysis is supported by the experimental analysis",
            "strength_and_weaknesses": "Strength: The theoretical bound in this paper is much better than the state of the art when \\sigma is small (e.g., 0.4). This makes the bound useful for application scenarios with moderate privacy requirements (but may with strong accuracy requirements)\n\nWeakness:\n1. The theoretical bound in this paper is just slightly better than the state of the art when \\sigma becomes larger (e.g., 0.5), which means the result is similar to the state of the art under usual privacy requirements.\n\n2. There is no comparison with Balle et al. when \\sigma > 0.6. I am not sure whether the proposed algorithm is better or worse than Bella et al. in the standard range of noise (1 < \\epsilon < 3).\n\n3. I think this paper can be improved by providing an experiment about the accuracy of different models under different privacy guarantees/noise levels (I am not an expert on NLP, please let me know if some experiments in the paper already analyzed some metrics similar to accuracy)\n\n4. I suggest the authors improve the readability by adding a statement that Renyi DP-SGD is implemented in the same way as DP-SGD. Otherwise, it's hard for the readers to know how Renyi DP is achieved for SGD.\n\n5. I would also suggest discussing other applications for Renyi DP. For example, improving robustness.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Okay, can be understood, but require some effort.\n\nQuality: Okay, there might be some technical issues as mentioned in the weakness.\n\nNovelty: Okay, but not extremely novel\n\nReproducibility: N.A., I am not an expert on NLP. Thus, I have no idea whether the experiments can be reproduced or not.",
            "summary_of_the_review": "I am not an expert on NLP, and all my judgments are based on the differential privacy part of this paper. \n\nAs mentioned in the Strength And Weaknesses, some details of this paper are not clear and there might be some technical issues. The presentation can also be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3076/Reviewer_2RkH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3076/Reviewer_2RkH"
        ]
    }
]