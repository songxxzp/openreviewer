[
    {
        "id": "VTD6ozOhHD",
        "original": null,
        "number": 1,
        "cdate": 1666191227448,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666191227448,
        "tmdate": 1668396766793,
        "tddate": null,
        "forum": "p6qlG1zXs9v",
        "replyto": "p6qlG1zXs9v",
        "invitation": "ICLR.cc/2023/Conference/Paper2691/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes the batch size for fastest convergence of optimization algorithms applied in deep learning. The metric is the total number of stochastic gradient estimations, which is the product of number of iterations and batch size. The optimal point should be in the middle, that both number of iterations and batch size are reasonably big. The proof is given in details and the experiments verify the conclusion.",
            "strength_and_weaknesses": "I think this paper treats the theory seriously and gives thorough proof in all details, on a fundamental problem in machine learning. The algorithm covers the most widely used optimization algorithms with tuning parameters such as momentum, adaptive gradient, etc. so is very practical. I have a few questions below.\n\nCritical: This paper proposes the upper bound and minimizes this upper bound wrt batch size. However, I\u2019m not sure if the upper bound of VI is tight, say, whether it\u2019s also a lower bound? Basically you want to minimize the exact rate (in big O) or at least a somehow tight upper bound, rather than a loose one. Does the experiment result exactly align with the upper bound? A figure with both experimental and theoretical \"iteration vs. error\" curves would justify the point. \n\nBut on the other hand, I think the bound makes some sense. The $1/K$ term is the bias and the $1/b$ term is the variance, but I'm not sure they are exact like both a plain inverse, in big O sense. With the same decomposition of bias/variance, does the bound, by any chance, match the bound for Table 1 of https://www.jmlr.org/papers/volume18/16-595/16-595.pdf? It should also be convincing if the bounds matches previous works' results. \n\nI would like to see more justification of SFO as a metric. For example, the gradient computation in batches can be parallel, thus intrinsically faster than running more GD iterations that cannot be concurrent. I think it also depends on how many machines you have, so that, in terms of experiments, I like the ones with SFO as y-axis rather than the tables using time as a metric (computation time really depends on the device, like how many computational \u201cunits\u201d it has). \n\nAre the step sizes among different batch sizes the same? And within the algorithm on a same batch size, is the step size fixed? For the latter question, I think ADAM can adapt the step size by itself but one can still vary batch size along the iterations to get a better result. Intuitively, when batch size is smaller, the noise is bigger and one needs smaller steps sizes for the same convergence error target $\\epsilon$. A diminishing step size can handle/mitigate the error caused by noise variance, i.e., caused by finite batch sizes, somehow.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is novel and written clearly. All the theories are reproducible. I would like to see the authors' response to my questions above to judge its quality/correctness.",
            "summary_of_the_review": "I would like to see the authors' response to my questions above to judge its quality/correctness. If they are addressed, I believe the paper is sound and solid. I think the score is 5 or 6 but I feel/expect the above questions can be addressed later and give a 6 here.\n\n=======================\n\nI still have some questions about the tightness but I think it matches the intuition and some other results. But given the proof technique for diminishing step sizes, I would like to tentatively rescore it as 8 (I would say 7 if the tightness issue still remains, and I'd suggest more reasoning about the SFO regarding the computational cost of \"parallelizable\" or \"unparallelizable\" steps). I believe diminishing step size is more important than constant step size if the bound is more optimal, since it can also achieve any small error for a non-smooth function and tweak the variance, etc., so I'd like to see it in the paper or appendix.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2691/Reviewer_vE9x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2691/Reviewer_vE9x"
        ]
    },
    {
        "id": "KvHQjjc6A4J",
        "original": null,
        "number": 2,
        "cdate": 1666533207239,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666533207239,
        "tmdate": 1666533207239,
        "tddate": null,
        "forum": "p6qlG1zXs9v",
        "replyto": "p6qlG1zXs9v",
        "invitation": "ICLR.cc/2023/Conference/Paper2691/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the relationship between the batch size and the required step number to achieve the local optimum in the conditions of small constant learning rate and hyperparameters close to 1. The authors also present a critical batch size. These theoretical results are also supported by numerical results.",
            "strength_and_weaknesses": "Overall, this paper presents novel results with solid foundations. My concerns mainly exist in the presentation. Some details are missing; some are not clear enough.\n\n1. The presented results on critical results are new to me, though the suggestion of using a small batch size is not new. The current draft needs a more detailed, comprehensive comparison with the related results. As described in Section 1.2, some similar results have been seen in the literature. The authors need to clarify the advances this paper has made more clearly.\n\n2. The proofs are given in the appendices in detail. However, I suggest the authors give a sketch in the main text. Also, the authors need to clarify the significance and difficulty of their proofs in the response and the revised draft.",
            "clarity,_quality,_novelty_and_reproducibility": "For clarity and novelty, please refer to point 1 above.\n\nThis paper is of fair quality in its current form.\n\n",
            "summary_of_the_review": "Overall, I recommend \"5: marginally below the acceptance threshold.\" I need the authors to give more details in their responses, and if this paper is accepted, the draft needs a revision.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2691/Reviewer_U2qx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2691/Reviewer_U2qx"
        ]
    },
    {
        "id": "WfoBWguSwQ",
        "original": null,
        "number": 3,
        "cdate": 1666684289575,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684289575,
        "tmdate": 1669395394309,
        "tddate": null,
        "forum": "p6qlG1zXs9v",
        "replyto": "p6qlG1zXs9v",
        "invitation": "ICLR.cc/2023/Conference/Paper2691/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an analysis of adaptive gradient methods with an emphasis on providing insights into the optimal setting of hyperparameters. The presented convergence results suggest settings that are in line with practical observations. From these results, the paper derives a \"critical batch size\", which minimizes the stochastic first-order oracle complexity.",
            "strength_and_weaknesses": "### Strengths\n\n1. I commend the effort to shed light on the influence of hyperparameter settings in adaptive gradient methods.\n\n2. I found it very interesting to see the experimental analysis regarding the critical batch size (Fig. 2), especially seeing that the SFO complexity for Adam is minimized at a relatively large batch size, in contrast to SGD.\n\n\n### Weaknesses\n\n1. I partially disagree with the premise of the paper that previous works suggest a setting of $\\beta_1\\approx \\beta_2 \\approx 0$. I checked a few of the provided references and, e.g., Zaheer et al (2019) analyze a setting with $\\beta_2$ close to 1 (they set $\\beta_1$ to 0 though). Moreover, some of the cited papers study a situation where the batch size is increased over time, so one should be careful to transfer conclusions to the setting of a fixed batch size. I would ask that the authors make the discussion of prior work more precise and specify exact results which they think constitute a gap between theory and practice. Overall, I don't think the present version of the paper does a great job of positioning itself in the context of the (admittedly numerous) existing papers analyzing adaptive gradient methods.\n\n2. The analysis is based on the \"variational inequality\" $K^{-1}\\sum_k \\nabla f(\\theta_k)^T(\\theta_k - \\theta) \\leq \\epsilon$ for all $\\theta$. I see why this is characterizing a stationary point for $\\epsilon=0$, but this seems not very meaningful for $\\epsilon>0$. I can just pick some $k$ and set $\\theta = c (\\theta_k - \\nabla f(\\theta_k))$ and if $\\nabla f(\\theta_k) \\neq 0$, I can always break the inequality by scaling up $c$. I would like the authors to clarify: To what extent does this inequality really characterize a stationary point? Has prior work used a similar formulation (if so, it should be cited)? Why is it meaningful to establish an upper bound on this quantity?\n\n3. The paper assumes a lot of absolute upper bounds on various gradient-related quantities, most strikingly $\\Vert \\nabla f(\\theta_k)\\Vert \\leq G$. I know that many previous works have used this assumption, but I still see this very critically since it is completely unrealistic. Usually, it is possible to relax such assumptions. For example, Eq. (7) could be replaced with a bound of the form $\\mathbf{E}[\\Vert g_k- \\nabla f(\\theta_k) \\Vert^2] \\leq \\sigma_0^2 + c \\Vert \\nabla f(\\theta_k)\\Vert^2$.\n\n4. As the paper rightly points out, Adam is not guaranteed to converge on a convex stochastic optimization problem. The \"breaking point\" of the convergence proof is that Eq. (8) is _not_ fulfilled by Adam. The present paper goes on to simply _assume_ that Eq. (8) holds for Adam. In my opinion, that is not acceptable. You can analyze variants of Adam that enforce Eq. (8) to hold (AMSGrad) but you can not just _assume_ an algorithm to have a property that it demonstrably does not have. I would like the authors to clarify whether the presented analysis would extend to AMSGrad? If so, the paper should be adapted to reflect that.\n\n\n### Update after Rebuttal\n\nThanks for clarifying point (2). This was mostly a misunderstanding on my part and I apologize for that! It seems quite clear in hindsight. Point (4) is still relevant and should be addressed in a possible camera-ready. I will increase my score.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: I have several concerns here, detailed in the weaknesses above. The meaning of the variational inequality formulation is not clear to me and is not discussed adequately in the paper (weakness 2). I also think related work needs to be discussed in more detail (weakness 1).\n\n- Quality: I think the theoretical analysis has some two flaws, detailed in weakness 3 and 4 above. The absolute upper bounds on gradient-related quantities should be relaxed where possible. Crucially, one can not simply assume an algorithm to have a property that is provably does not have (weakness 4).\n\n- Originality: To the best of my knowledge, the results are novel in the terms of the insights they provide w.r.t. the hyperparameter settings and the critical batch size.",
            "summary_of_the_review": "Given the caveats detailed above, I recommend rejection. I hope that the authors can clarify some of my questions/concerns in the rebuttal phase.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2691/Reviewer_hyhk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2691/Reviewer_hyhk"
        ]
    },
    {
        "id": "MAlxX6PQXDf",
        "original": null,
        "number": 4,
        "cdate": 1667017054771,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667017054771,
        "tmdate": 1667017054771,
        "tddate": null,
        "forum": "p6qlG1zXs9v",
        "replyto": "p6qlG1zXs9v",
        "invitation": "ICLR.cc/2023/Conference/Paper2691/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the gap between the theoretical and empirical optimal hyperparameters values for adaptive gradient methods such as Adam for stochastic non-convex optimization. It provides new convergence upper bounds for several of these methods under bounded gradients and variance assumptions. These upper bounds are shown to be the lowest for the aforementioned hyperparameters being close to one, which is close to practice. \n\n",
            "strength_and_weaknesses": "The first major problem with the paper is assuming both bounded variance and bounded gradients. It is well known that in this setting, mini-batching doesn't help because for usual problems $G\\approx \\sigma$ unless there is some stylized noise structure. Bounded variance assumptions are usually paired with smoothness assumptions, for instance, [this paper](https://arxiv.org/pdf/1912.02365.pdf). Under the paper's assumptions, if $\\sigma$ is replaced by $G$, then $C_2 \\leq C_3$ so there is no benefit of mini-batching! Similarly, it is not reasonable to assume the boundedness of parameters without explicitly making any projection steps in the algorithm. \n\nThe second major problem is that the paper makes no comparison to the existing upper bounds for these algorithms. As a result, it is unclear if Adam's analysis is an improvement over the existing work. And if the upper bounds are worse than the previous bounds, then it is a fallacy to make any conclusions about the optimal hyper-parameters. Ideally, one would never make such conclusions with just upper bounds without matching lower bounds. \n\nAnother issue is that the final guarantees in the paper are not comparable to usual guarantees upper bounding $\\frac{1}{K}\\sum_{k\\in[K]} \\mathbb{E}||\\nabla f(\\theta_k)||^2$, which guarantees that at least one iterate was an approximate stationary point. The paper instead uses a variational metric, but it is not reasonable to average it across $k$ because that doesn't imply that the best $\\theta_k$ satisfies the variational metric for all $\\theta$. \n\nOverall, the paper is not technically sound. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not very well written. \n\n1. I suggest using a different notation for oracle complexity, something like $N$ instead of $Kb$. It might be a bit confusing to think of $Kb$ as a function of the batch size. \n\n2. The paper mentions it studies stochastic first-order oracle complexity and does so in its theory because it considers a stochastic optimization problem. However, most of the previous empirical work mentioned in the paper uses SGD with multiple passes, i.e., it doesn't use sampling with replacement, which means different batches are not independent (for context on theory, consider [this paper](https://proceedings.neurips.cc/paper/2021/hash/e64c9ec33f19c7de745bd6b6d1a7a86e-Abstract.html)). This is confusing and can lead the readers to believe that there is a watertight connection between theory and experiments when there is not. Were the experiments in this paper performed using sampling with replacement? It is important to specify this explicitly in algorithm 1. \n\n3. Why do the authors mention Chen et al. reference for the convergence of SGD? There are several older papers analyzing SGD. In particular, in the convex setting, [this paper](https://arxiv.org/abs/1106.4574) discusses the linear scaling with batch size and when it fails to hold. In the non-convex setting, [this paper](https://arxiv.org/pdf/1912.02365.pdf) shows the optimality of several existing analyses for SGD. Finally, which results are people talking about in the following statement: \n> Accordingly, the practical results for large batch sizes match the theoretical ones\n\n4. It is unclear what is the metric in equation (5). The sentence below the equation as well doesn't explain the metric.\n\n5. It would be good to indicate predicted critical batch sizes for different algorithms in the plots in the experimental section. Furthermore, how was the learning rate tuned for SGD and SGD with momentum while changing the batch size? Ideally, the learning rate and momentum should be tuned separately for each batch size, but from the paper, it seems like a constant learning rate of $10^{-3}$ was used. ",
            "summary_of_the_review": "The paper tries to study an interesting gap between the theory and practice of adaptive methods. However, it lacks a clear comparison to existing theoretical results, and the provided guarantees seem weaker than usual stationarity guarantees. The experiments don't have any new insights, as similar experiments exist on much larger datasets in the references cited within the paper. I thus recommend rejecting the paper. \n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2691/Reviewer_vJ3a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2691/Reviewer_vJ3a"
        ]
    }
]