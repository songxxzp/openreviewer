[
    {
        "id": "E2sYqcaXk9p",
        "original": null,
        "number": 1,
        "cdate": 1666608569996,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666608569996,
        "tmdate": 1666608569996,
        "tddate": null,
        "forum": "TTSyyMBNUjd",
        "replyto": "TTSyyMBNUjd",
        "invitation": "ICLR.cc/2023/Conference/Paper2040/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This study starts from the view that neural collapse is a phenomenon only concerned with the explicit labels of the dataset. This study suggests that intrinsic structure of input distribution should also play a role in the last-layer representation structure. The authors construct two experimental settings where the input distribution is not consistent with the label, namely Coarse CIFAR-10 and Fine CIFAR-10, to verify the insights proposed in this paper.  ",
            "strength_and_weaknesses": "Strengths: \n\nThe motivation of this paper is interesting. The study on the representation structure is very important. The paper has a good presentation. \n\nWeaknesses:\n\n(1).\tThe core insight of this paper is that the input distribution should play a role in the neural network representation structure, which is ignored by neural collapse. At the first glimpse, it is interesting and brings something new over neural collapse. However, we should note that neural collapse is an ideal state. It is a target/goal of the optimization in a neural network. It is theoretically proved to be the global optimality (0/minimal loss) of unconstrained feature model or layer-peeled model. But in real implementations, we almost cannot realize this ideal state, and an optimized model almost cannot attain 0 training loss. Even in the neural collapse study (Papyan et al., PNAS), the within class variance approaches to 0, but is not 0. So, NC is a limit, instead of a real state. \n\nLet us assume that neural collapse indeed happened in the coarse label training. The within class variance would be 0, and all features in the same coarse class would be exactly the same. In this case, it is impossible to reconstruct the original classes by clustering. When the within class variance is not 0 (no matter how many coarse labels), it is not surprising that the representation structure is affected by the input distribution, and we can cluster the representation into the classes of input distribution.\n\nAs a conclusion, the main view of this paper that input distribution also affects the representation structure is correct. But is has no conflict with neural collapse. Neural collapse is just a limit. In a real neural network, we cannot infer that label is the only factor to decide the representation structure. So, this study does not bring something new or inspiring. The novelty and significance are limited. \n\n(2).\tI do not see any instruction from this study. The view that input distribution also affects the representation structure comes with no surprise. The paper verifies this view by coarse and fine label training. But how does this work instruct us for training a neural network? \n\n(3).\tThe main question claimed in Introduction that \u201chow can we reconcile the roles of the intrinsic structure of input distribution vs. the explicit structure of the labels in determining the last-layer representations in neural networks?\u201d is not proper. First, as given in (1), neural collapse has no conflict with the intrinsic structure effect. I do not think we need to reconcile the two roles. Second, the authors seem not to answer this question. As given in (2), this paper does not offer any guidance. I do not see any solution that can \u201creconcile the two roles\u201d. \n\n(4).\tReference is not sufficient. Many related neural collapse studies are not cited and discussed with. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity of this paper is good. ",
            "summary_of_the_review": "The study has a clear motivation. But its novelty and significance are very limited. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2040/Reviewer_VqKm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2040/Reviewer_VqKm"
        ]
    },
    {
        "id": "JulZFgvosSA",
        "original": null,
        "number": 2,
        "cdate": 1666668514736,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668514736,
        "tmdate": 1669246952919,
        "tddate": null,
        "forum": "TTSyyMBNUjd",
        "replyto": "TTSyyMBNUjd",
        "invitation": "ICLR.cc/2023/Conference/Paper2040/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper empirically studied the previously observed phenomenon of \"neural collapsing\", which suggests that the last layer representation collapse for each class. This paper investigated this problem with ResNet-18 on variants of dataset created from CIFAR-10/100 and found that even though the phenomenon does occur, finer grained information still presents in the representation. For example, when trained with 5 \"super-class\" labels created by combining two classes from CIFAR-10, the 10-class structures can still be found in the representations, and a simple kmeans clustering can separate the 10 classes with high accuracy even after 1000 epochs of training (into the neural collapse regime).",
            "strength_and_weaknesses": "**Strength**: This paper studies a simple question (does the representation depends on input structure in the neural collapse regime) and shows a clear answer (yes it does).\n\n**Weakness**:\n\n1. While the main conclusion in this paper is interesting and clear, I don't think this paper has enough material to make a full paper. One potential direction to enrich the results is to make the study more systematic. For example, currently there is an anecdotal observation on CIFAR-100 that when the sub-classes are \"semantically similar\" enough, the sub-class structures could be blurred when training with superclass labels. It could make the paper more solid if systematic and quantitative results are presented with studies over different network architectures, training algorithms and hyperparameters and different type of datasets. A related question would be are there other input structures other than the fine-grain labels recoverable from the representation in the neural collapse regime. Another potential direction is to formally analyze the reason behind such phenomenon. \n\n2. It is well known that a lot of information is retained in the final layer representations and sometimes even in the logit vectors after a model is trained with a classification loss. There is a whole line of research on recovering instance based information or even reconstructing the inputs that is missing from the related work section. e.g.\n\n    - Mahendran et al., Understanding deep image representations by inverting them.\n    - Dosovitskiy et al., Inverting visual rep- resentations with convolutional networks.\n    - Nash et al., Inverting supervised representations with autoregressive neural density models.\n    - Teterwak et al., Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers.\n    - Rombach et al., Making Sense of CNNs: Interpreting Deep Representations and Their Invariances with INNs.\n  \n3. The experiments in Section 5 show that when learning with super classes constructed from CIFAR-100 according to the semantic labels, some of the fine-grain class structures are missing in the learned representations. To what extent does this depends on the superclasses being semantically constructed? Because the original experiments was on cifar-10 forming a super class with 2 classes. It would be helpful to have a more controlled baseline with 5-class formed superclasses as well -- for example, on cifar-100 with randomly formed superclasses.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and easy to follow. ",
            "summary_of_the_review": "This paper studies a clear question and make a simple answer. My main concern is that this paper does not have enough materials to support a full conference paper. I've suggested a few directions to expand the studies that could potentially make this paper more solid in the section above. \n\n---------------------------------\nThanks to the authors for the reply and additional synthetic experiments. I'm willing to raise the score a bit, but since there is no rating between 3 and 5, I'm keeping my current rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2040/Reviewer_fK87"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2040/Reviewer_fK87"
        ]
    },
    {
        "id": "IKI8TPYvPR",
        "original": null,
        "number": 3,
        "cdate": 1666733562016,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666733562016,
        "tmdate": 1666733562016,
        "tddate": null,
        "forum": "TTSyyMBNUjd",
        "replyto": "TTSyyMBNUjd",
        "invitation": "ICLR.cc/2023/Conference/Paper2040/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the question of whether neural collapse is solely dependent on the output labels as suggested in prior work, or if it still contains information about the input distribution. To answer this question, the authors train models on Cifar10 and Cifar100 using coarse-grained and fine-grained labels, and measure if the original labels are still recoverable from the last layer activations even after neural collapse. They find that the original labels can be reconstructed through a Tsne and clustering mechanism, but less so when the class groupings are semantically similar. ",
            "strength_and_weaknesses": "### Strength:\n\n- The paper tackles an interesting question related to the phenomenon of neural collapse, which is whether information about the input distribution is still carried in the last layer activations after neural collapse. The experiments with coarse-grained and fine-grained class labels are well-designed to answer this question, and the visualizations and CLP accuracy provide compelling evidence for the role of the input distribution even with neural collapse. \n\n### Weakness:\n\n- I think a wider set of hyperparameter settings would be needed to fully complete the story. For example, using a learning rate decay schedule and a setting with 0 weight decay would have been good to see. This would help answer whether the input information would be erased when the network is able to converge to a lower loss, which would tell us if this phenomenon is inherent in neural network training / neural collapse, or if it is only an artifact of strong regularization in the terminal phase of training.\n\n- An explanation or intuition for why config #1 and #2 generate different behaviors in different experiments would be a valuable discussion to include.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and the experiments are clearly motivated and compelling. The work provides some new insights into the phenomenon of neural collapse.",
            "summary_of_the_review": "The work provides some new insights into the phenomenon of neural collapse, but some missing hyperparameter settings raises doubt over whether the observations are an artifact of the training procedure, thus it is marginally below the acceptance threshold in my opinion. Including those hyperparameter results will likely increase my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2040/Reviewer_MGwH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2040/Reviewer_MGwH"
        ]
    },
    {
        "id": "Jux9jSLgUX",
        "original": null,
        "number": 4,
        "cdate": 1667102406923,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667102406923,
        "tmdate": 1667102406923,
        "tddate": null,
        "forum": "TTSyyMBNUjd",
        "replyto": "TTSyyMBNUjd",
        "invitation": "ICLR.cc/2023/Conference/Paper2040/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The Neural Collapse phenomenon indicates that the last-layer representation of training samples with the same label would collapse into each other in well-trained networks. It means that the last-layer representation would only be determined by the labels, regardless of the input data distribution. This paper suggests that for practical networks with only approximate neural collapse, the small amount of remaining variation can still capture the intrinsic structure of input distribution. The contributions mainly involve the following two observations. First, the authors observe that the effect of input distribution appears earlier in training, while the effect of labels emerges at the terminal phase of training. Second, experimental results indicate that the collapsed representation corresponding to each label can still keep fine-grained structures determined by the input distribution. ",
            "strength_and_weaknesses": "## Strength:\n- This paper tackles an important and very interesting problem about whether and how the input distribution would affect the structure of the last-layer representation. \n- It reveals that for practical networks with only approximate neural collapse, the small amount of remaining variation can still capture the intrinsic structure of input distribution. In particular, the last-layer representations of different subclasses within the same class are separated into different clusters according to the input distribution.\n- This paper provides rich visualizations to evaluate the observed phenomena. For example, the authors use the heatmap of the class distance matrix to measure the clusters and capture the change during training. The Cluster-and-Linear-Probe(CLP) method is performed to validate the fine-grained structures determined by the input distribution.\n\n## Weaknesses:\n- As mentioned in Section 2, the phenomenon that last-layer representations of different subclasses within the same class are often separated into different clusters has already been discovered in the literature, such as by Sohoni et al. (2020). The only difference in this work seems to be that this phenomenon is still observed when the network is trained extremely long (such as 1000 epochs). As this is the main contribution of this paper, it seems the contribution or novelty is limited, though personally, I think the result is very interesting. \n- Following the above point, the level of neural collapse mainly depends on two factors: the training epochs and the network capacity (width and depth). The current experiments are conducted with ResNet-18 and include the setting with a sufficient number of training iterations. What if one uses a much larger network? Do we still observe similar additional structures within the neural collapse representations? The features tend to be more collapsed with a larger network.   \n- The two settings config #1 and config #2 only differ in the weight decay parameters, but learn different representations as shown in Figure 5 and Figure 6. Could the authors comment on why would the weight-decay rate be a critical hyper-parameter for the final fine-grained structure of last-layer representation?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: the methodology and findings are organized well.\n- Novelty: On one hand, this paper explores the effect of input distribution on last-layer representation within a neural collapse regime, which appears novel. But on the other hand,  the main observation that last-layer representations of different subclasses within the same class are often separated into different clusters has already been discovered in the literature, such as by Sohoni et al. (2020). Since the neural collapse regime only refers to the case where the network is sufficiently trained, the novelty seems limited. \n- Reproducibility: it provides hyper-parameter settings for each experiment, but no code files or links are provided. \n",
            "summary_of_the_review": "This paper studies the role of input data distribution on the last-layer representation of neural networks. I have mixed feelings about the recommendation. Given the recently observed neural collapse phenomena indicating the last-layer representation would only be determined by the labels, this paper provides a very interesting observation that the small amount of remaining variation can still capture the intrinsic structure of input distribution. This result could be of interest to the community. But on the other hand, as mentioned before, a similar phenomenon seems to have already been discovered in the literature. I look forward to the authors\u2019 response.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2040/Reviewer_jgRV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2040/Reviewer_jgRV"
        ]
    }
]