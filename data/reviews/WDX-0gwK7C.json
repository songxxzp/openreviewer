[
    {
        "id": "29bMgcdsM0",
        "original": null,
        "number": 1,
        "cdate": 1666320830244,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666320830244,
        "tmdate": 1666320888383,
        "tddate": null,
        "forum": "WDX-0gwK7C",
        "replyto": "WDX-0gwK7C",
        "invitation": "ICLR.cc/2023/Conference/Paper4801/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proves univeral approximation theorems for deep belief networks when the targeted distribution is continuous, with quantitative bounds for $L^p$ space with $p>1$. The proof idea is to make use of the efficient approximation of convex hull, each item is represented in the first layer, while the second layer is used to approximate the convex combination utilizing previous results for the discrete case.",
            "strength_and_weaknesses": "Strength:\n\nThe results are new and solid, generalizing previous results for the discrete case to the continuous case.\n\nThe proof idea is simple yet effective, through which this paper solves the targeted problem in a clean way.\n\nWeaknesses:\n\nThe result is well-expected due to numerous analogical universal approxiamtion results for neural networks, and similar techniques have appeared in that literature.\n\nWhat's the dependence of $M$ on $\\epsilon$ in theorem 3?\n\nThe analysis is based on some technical propositions from previous works without proof. Adding more explanation/proof idea/easy example to these introduced propositions would make this paper more readable. For instance, proposition 11 could benefit from explaining the special case of $L^2$, which admits a short proof because it's a Hilbert space.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. I believe the writing can be improved by moving some less important math to appendix and add more explanantion/discussion to the main-text.",
            "summary_of_the_review": "The results are new and solid, written in a mathematically clean way. I recommend accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4801/Reviewer_F3UE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4801/Reviewer_F3UE"
        ]
    },
    {
        "id": "rsjY0EkCn4-",
        "original": null,
        "number": 2,
        "cdate": 1666590080077,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666590080077,
        "tmdate": 1666590080077,
        "tddate": null,
        "forum": "WDX-0gwK7C",
        "replyto": "WDX-0gwK7C",
        "invitation": "ICLR.cc/2023/Conference/Paper4801/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This manuscript investigates the approximation error of classic deep belief networks (DBNs), in particular DBNs with two hidden layers of size m and m+1, respectively. It is demonstrated that, under both  L^q-norm and Kullback-Leibler divergence, DBNs are universal approximators. Moreover,  as claimed by the author, sharp bounds are obtained for the approximation error. ",
            "strength_and_weaknesses": "Strengths:\n\n1. This is a solid theoretical work on the analysis of approximation error of DBN, which has been an open problem. \n\n2. Two metrics, L^q-norm and KL deivergence, are considered. \n\nWeaknesses:\n\n1. It seems that the main results of this manuscript are restricted only to the case of DBNs with two hidden layers of size m and m+1 respectively. Can the results still hold for general size m and n for two layers? If not, why? \n\n2. The authors stated that, e.g., in the abstract,  \"we establish a sharp quantitative bounds on the approximation error in terms of the number of hidden units \". How to verify the sharpness of the obtained bounds? I did not see any supportive results to show that the obtained bounds are sharp. Can the authors add more details or explanations on this point? I would be very nice if some empirical evidence is provided. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written. The result of the theoretical analysis in this paper is novel and the obtained bounds of the approximation error of DBNs in the case of two hidden layers of size m and m+1, respectively.",
            "summary_of_the_review": "This manuscript is theoretically studied the approximation error of DBNs with   two binary hidden layers of sizes m and m + 1, respectively. In particular, under both L^q and Kullback-Leibler divergence metrics,  DBNs are proven to be universal approximators and the bounds of the approximation errors have been obtained.  It is a solid work with theoretical contributions. My main concern lies in the restriction of the sizes of the hidden layers to be m and m + 1, as well as a lack of (empirical) evidence of the sharpness of the obtained bounds. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4801/Reviewer_F7vv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4801/Reviewer_F7vv"
        ]
    },
    {
        "id": "l-c3Gp_ju0_",
        "original": null,
        "number": 3,
        "cdate": 1666678966332,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678966332,
        "tmdate": 1666678966332,
        "tddate": null,
        "forum": "WDX-0gwK7C",
        "replyto": "WDX-0gwK7C",
        "invitation": "ICLR.cc/2023/Conference/Paper4801/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors consider approximation of probability density functions by deep belief networks with binary hidden units. Convergence and rates of approximation are provided with respect to the L_q norm and K-L divergence. ",
            "strength_and_weaknesses": "The topic of approximation by deep belief networks is interesting. Rates of convergence are given by applying the Rademacher index of the L_q spaces. There have been many results in the literature of distribution regression, on learning of probability density functions. The authors do not provide any comparisons with the existing methods for approximating probability density functions. Using convolutions for approximations is fine, but regularity of the approximated function is not used. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper does not give a clear introduction for deep belief networks. The presentation can be improved. The mathematical results are not so deep, without using any regularity of the approximated function. ",
            "summary_of_the_review": "The topic of approximation by deep belief networks is interesting. Rates of convergence given by applying the Rademacher index of the L_q spaces might have applications in some other problems. But a large literature of distribution regression is missing in the paper. Using convolutions for approximations without involving regularity of the approximated function is rather shallow. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4801/Reviewer_V3vX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4801/Reviewer_V3vX"
        ]
    },
    {
        "id": "ywuh6qxKhZz",
        "original": null,
        "number": 4,
        "cdate": 1666747563309,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666747563309,
        "tmdate": 1666747563309,
        "tddate": null,
        "forum": "WDX-0gwK7C",
        "replyto": "WDX-0gwK7C",
        "invitation": "ICLR.cc/2023/Conference/Paper4801/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper looks at the approximation capabilities of a DBN with 2 layers in which the first layer is a has binary states (both hidden and visible) and the second layer has continuous visible state. Further, it is assumed that the conditional distributions for the last layer all come from the same parental distribution (that is, they are just shifts and rescalings of each other). Under such an assumption, we can think of the DBN as mixture model (such as a Gaussian Mixture model with $\\sigma I$ as the covariances when the parental distribution is Gaussian). \n\nIn this set up the paper shows that as the number of hidden nodes goes to infinity, the model can approximate arbitrarily well any measure that is absolutely continuous with respect to Lebesgue measure (i.e. measures that have distributions with respect to the Lebesgue measure.) That is, as the number of mixtures factors increases we can approximately any density. \n\nIn some cases the paper provides error rates. \n\n",
            "strength_and_weaknesses": "**Strengths**\n---\n\n1) I think the major strength of the paper are the error rates that it presents. In particular, I think even for Gaussian Mixture I dont the optimal error rate is known. \n\n2) I think writing is clear and proofs are easy to follow and use mostly standard analysis tools. \n\n**Weaknesses**\n---\n\n1) I think a discussion to prior theoretical work on approximations by mixture models is missing. Specifically in the case when we think of the special cases of mixtures of Gaussians and mixtures of exponential families. Significant prior work has been done. In this regard, I think the universality is known (please correct me if I am wrong). However, the strength of the this paper are the error rates. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe paper is well written and is easy to read and understand. \n\n**Novelty and Significance**\n\nI think the error rates presented are new and are of theoretical significance. \n",
            "summary_of_the_review": "In summary, I think this is a well written paper and with interesting and new results. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4801/Reviewer_heng"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4801/Reviewer_heng"
        ]
    },
    {
        "id": "WGEfL445shJ",
        "original": null,
        "number": 5,
        "cdate": 1667277329643,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667277329643,
        "tmdate": 1667277329643,
        "tddate": null,
        "forum": "WDX-0gwK7C",
        "replyto": "WDX-0gwK7C",
        "invitation": "ICLR.cc/2023/Conference/Paper4801/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "A purely theoretical contribution in which the authors present a series of approximation results describing which densities can be well-approximated by DBNs.  Additionally, they provide quantitative error bounds for both Lp based norms and the KL divergence.",
            "strength_and_weaknesses": "Strengths:  Fills gaps in the theoretical literature on what densities can be approximated by DBNs.\n\nWeaknesses:  Proofs are a bit terse (as is necessitated by space).  Doesn't contain any experimental evaluation, but I don't think that is necessary for papers of this type.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:  Overall, I found the motivation and the presentation of the theorems clear (though I had more difficulty following the proof details due to lack of expertise).  Still, I have a few comments:\n\n(1) L^q(R^d) is used in the intro before being defined.\n(2) At the beginning of section 1, RBMs are described as both planar and fully connected (these statements are antithetical).\n(3) The notation in Theorem 3 isn't consistent with Proposition 1 -- don't you need to sum over the hidden variables?\n\nQuality:  I'm not able to assess overall quality as I am not able to validate all of the proof details (and the proofs are a bit terse for a non-expert like me to follow completely).\n\nNovelty:  The work does appear to make novel contributions over existing works, and the authors are careful to point out exactly where those contributions are.\n\nReproducibility: n/a",
            "summary_of_the_review": "An interesting theoretical contribution that precisely describes classes of probability densities that can be well approximated by DBNs.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4801/Reviewer_atRJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4801/Reviewer_atRJ"
        ]
    }
]