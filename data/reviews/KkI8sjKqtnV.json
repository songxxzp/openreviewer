[
    {
        "id": "LeoSPxOL8b",
        "original": null,
        "number": 1,
        "cdate": 1666616472345,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616472345,
        "tmdate": 1666616472345,
        "tddate": null,
        "forum": "KkI8sjKqtnV",
        "replyto": "KkI8sjKqtnV",
        "invitation": "ICLR.cc/2023/Conference/Paper2066/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies fair federated learning (FL) where each group of clients are guaranteed to have the same loss function value upper bound. Theoretical guarantees for both model convergence (for convex loss functions) and fairness are provided, together with experimental results that support the claims.",
            "strength_and_weaknesses": "Strength:\n+ Given bounded group loss (BGL) as the fairness criterion, this paper develops a FL solution that optimizes empirical risks for different groups under absolute loss constraints. \n+ Theoretical analysis of convergence and fairness guarantee is provided.\n\nWeakness:\n- The biggest issue of this work is the fairness criterion that, in my opinion, violates the core principle of fairness. The BGL definition in Def. 1 uses an absolute average loss threshold for every (restricted) group $a \\in A$. The authors' argument to move away from the equity between different groups and move towards such absolute, one-size-for-all quality threshold is highly flawed. First, the authors argue that this makes the problem convex, but such consideration is based on whether the authors can technically solve the problem as opposed to what is the right metric for fairness. Second, the argument that this metric \"boosts the worst group's prediction quality\" is solely focused on one group's performance, as opposed to the fairness **among all groups**. Fundamentally, fairness must consider all stakeholders. Last but probably the most importantly, using a single threshold could essentially lead to one group (the so-called worst group) barely meeting this threshold, while other groups achieve significantly smaller loss values. The authors consider this case to be fair, but in reality, such prediction outcome discrepancy can still lead to unfair decision making. Additionally, choosing such threshold $\\zeta$ becomes non-trivial, as it significantly affects BGL. Having the algorithm fairness depend critically on hyperparameter tuning is dangerous and should be avoided when possible. As the whole work is built on such absolute threshold, I believe this is a critical issue that should be addressed.\n- As the authors stated in the Remark, $G$ is linear in $\\lambda$ and convex in $w_0$. I'm not sure why the optimal solution has to be defined w.r.t. a saddle point instead of the true optimum.\n- The novelty of this work is limited given the state of the art. The algorithmic approach described in Sec. 4.1 is a standard alternative optimization procedure. The technique used in the convergence analysis in Sec. 4.2 is very similar to the prior approach, e.g., in Li et al. 2019b. ",
            "clarity,_quality,_novelty_and_reproducibility": "- I'm not sure how Fig. 1 is supposed to be interpreted. I do not think it gives a clear motivation of the problem. I suggest the authors to either modify this figure (and caption) to make it more illustrative, or remove it all together. Instead, I suggest the authors to add Alg. 1 to the main paper, as opposed to leaving it entirely in the appendix.",
            "summary_of_the_review": "This work is technically sound given the premise of the fairness formulation. However, this premise does not seem to satisfy the core principle of fairness -- treating all groups fairly.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "As stated in the main review, I worry about the formulation of fairness and how it affects the direction of fairness ML. This formulation is exactly the type of (un)fairness that we as a society are moving away, i.e., setting the **SAME** threshold for **EVERY** group to satisfy, i.e., the same credit score for all groups. We should avoid this!",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2066/Reviewer_KBz5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2066/Reviewer_KBz5"
        ]
    },
    {
        "id": "qCWD74A5gA",
        "original": null,
        "number": 2,
        "cdate": 1666690493151,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690493151,
        "tmdate": 1668661315440,
        "tddate": null,
        "forum": "KkI8sjKqtnV",
        "replyto": "KkI8sjKqtnV",
        "invitation": "ICLR.cc/2023/Conference/Paper2066/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a fair learning objective for federated settings via Bounded Group Loss. The authors propose a scalable federated solver to find an approximate saddle point for the objective. Theoretically, they provide convergence and fairness guarantees for the method. Empirically, they show that their method can provide high accuracy and fairness simultaneously across tasks from fair ML and\nfederated learning.\n\n",
            "strength_and_weaknesses": "Strength:\n- The topic of fair federated learning is relevant.\n- The technical idea of reducing to federated saddle point optimization is natural.\n\nWeaknesses:\n- The selection of Bounded Group Loss is not very well motivated. To my understanding, this selection is due to its convexity instead of its practical interest. The advantage of the proposed algorithm in Bounded Group Loss is not surprising since the baselines are not defined for this fairness measure.\n- Lack of discussion on techniques. The reduction to federated saddle point optimization is natural, and hence I expect to see a comparison with federated saddle point optimization algorithms in the literature. For example, [Hou et al., Efficient Algorithms for Federated Saddle Point Optimization, 2021] and [Shen et al., FedMM: Saddle Point Optimization for Federated Adversarial Domain Adaptation, 2021].",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The writing is ok, but lacks some literature search for federated saddle point optimization.\nQuality: The optimization approach is standard.\nNovelty: The provable guarantee for fair federated learning is novel.",
            "summary_of_the_review": "The paper proposes a fair learning objective for federated settings via Bounded Group Loss. The selection of Bounded Group Loss is not very well motivated. The writing is ok but lacks some literature search for federated saddle point optimization. Overall, I do not recommend acceptance.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2066/Reviewer_ScJT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2066/Reviewer_ScJT"
        ]
    },
    {
        "id": "_Tri2qBuy9",
        "original": null,
        "number": 3,
        "cdate": 1666844132974,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666844132974,
        "tmdate": 1666845201647,
        "tddate": null,
        "forum": "KkI8sjKqtnV",
        "replyto": "KkI8sjKqtnV",
        "invitation": "ICLR.cc/2023/Conference/Paper2066/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a group federated learning algorithm with formal convergence/BGL guarantees.",
            "strength_and_weaknesses": "-Privacy leakage/secrecy of the proposed algorithm is not discussed. It is unclear how much privacy leakage occurs. The proposed algorithm requires the exchange of additional information \"r\" per round, so one *must* discuss the implication of this. Federated learning algorithms with frequent synchronization/additional data exchange may be subject to an unacceptable level of information leakage, making it no different from sharing all the local datasets with every client. To avoid this kind of pitfall, one must clearly discuss the privacy aspect of new algorithms.\n\n-BGL seems to be ok if it's used for regression or used for classification without any preferred labels. ACS Emp./COMPAS are the tasks with preferred outputs, so I am not sure if using BGL makes sense in these settings.\n\n-Some of the experimental results look weird. In most of the experiments, FedAvg, which should minimize the error rate better than any other constrained counterparts, does not seem to achieve the lowest error rate. Sometimes it performs the worst among all tested methods. Why?\n\n-How are the hyperparameters chosen for the baseline methods?\n\n-Some missing baselines. Also, please add the performance of the standard reduction method with central data (upper bound). \n\n[1] FairFed: Enabling Group Fairness in Federated Learning https://arxiv.org/abs/2110.00857\n[2] GIFAIR-FL: A Framework for Group and Individual Fairness in Federated Learning https://arxiv.org/abs/2108.02741",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written.",
            "summary_of_the_review": "See my comments above.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2066/Reviewer_SyJQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2066/Reviewer_SyJQ"
        ]
    },
    {
        "id": "vrz4ALzf6v",
        "original": null,
        "number": 4,
        "cdate": 1667060114465,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667060114465,
        "tmdate": 1667060114465,
        "tddate": null,
        "forum": "KkI8sjKqtnV",
        "replyto": "KkI8sjKqtnV",
        "invitation": "ICLR.cc/2023/Conference/Paper2066/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper discusses the use of bounded group loss (BGL) in the context of fair federated learning. The core objective is to learn a classifier that satisfies some predefined constraints on the expected loss function of each group on average across all clients. It is expected that the client-conditional data distribution may differ between clients.\n\nThe paper further assumes that the bounded loss function is convex w.r.t. the model parameters, and proposes a distributed saddle point optimization algorithm to recover a model that minimizes empirical risk subject to these group constraints. A convergence proof for the algorithm is also provided\n",
            "strength_and_weaknesses": "Strengths:\n\nThe paper allows the direct specification of target group-specific loss minimums, which are a natural way of specifying group constraints. The resulting algorithm is simple and well grounded, and the resulting fairness-utility tradeoffs seem comparable to existing approaches.\n\nWeaknesses:\n\nI think the novelty of BGL is limited, the proposed objective is equivalent to FedMinMax, which is discussed on the paper,  with an additional static bias added to the group-conditional loss terms. The formulation as a saddle point presented in this paper is not fully compatible with the bounds on the Lagrangian weights presented in Eq 2, and Algorithm 1 makes it seem like the weights asymptotically reach the norm bound B, which begs the question on why is that a separate optimization parameter from \\beta in Eq 2.\n\nThe theoretical comparisons with existing methods, in particular group DRO, which uses a similar exponential weight update rule for lambda and thresholds on the group losses (albeit with an additional relu), and FedMinMax, which in its paper formulation has a lower bound constraint on lambda  which makes both objectives theoretically equivalent, is sorely lacking.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easily reproducible",
            "summary_of_the_review": "The proposed method is sensible, but there is limited novelty, and uncertainty if the method is not equivalent to existing approaches as it stands.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2066/Reviewer_juWF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2066/Reviewer_juWF"
        ]
    }
]