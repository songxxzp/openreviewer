[
    {
        "id": "j7P69hj_Zo-",
        "original": null,
        "number": 1,
        "cdate": 1666665548004,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665548004,
        "tmdate": 1666665548004,
        "tddate": null,
        "forum": "FDlfFbnI7AR",
        "replyto": "FDlfFbnI7AR",
        "invitation": "ICLR.cc/2023/Conference/Paper3153/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "To improve the attack-defense gap for robust classifiers, the paper suggests using a sample-specific attack-based flag, Counter-Attack(CA),  that tells the user if there are adversarial samples within a $\\epsilon$-ball of the given sample. In the case of a perfect attack, the authors show that Counter-Attack also provides a computational robustness guarantee that it is hard to find unflagged adversarial examples beyond the threat models $\\epsilon$ radius. \n\nHowever, the paper also formally proves that, under broad assumptions, attacking a polynomial-time classifier is $NP$-complete, while training a polynomial-time model that is robust on even a single input is $\\Sigma^P_2$-complete. They also extend these results to the case of general non-polynomial time classifiers.\n\nThe authors suggest using a heuristic attack to flag examples for practical purposes. To improve the validity of this approach, the authors recommend using a higher radius for the heuristic attacks to make up for their relative weakness to a perfect attack. To this end, the authors study the consistency between the average distance of an adversarial example found by the heuristic attacks vs. the perfect attack or MNISt and CIFAR10 datasets. The empirical results show that for a well-chosen suit of attacks, the gap is pretty consistent in the evaluated examples.",
            "strength_and_weaknesses": "Strengths\n- The complexity results for attacking polynomial models and training robust polynomial models provide a reasonable explanation for the hardness of doing these tasks on practical datasets.\n- The empirical results showing that the attack radius achieved by a suit of weaker attack strongly correlates with the actual radius of the decision boundary around an example is very interesting and provides a good justification for the CA approach.\n\nWeakness\n- The underlying ideas presented in the paper are not very novel. Similar hardness results for ReLU networks do exist in previous papers. The flagging of specific examples provides a heuristic guarantee in contrast to the solid theoretical guarantees provided by other certification methods. The use case of Counter-Attack is not very clear to me.\n-  Due to the computational limitations of the MIP solvers, the empirical justification for the method is only shown on small neural networks. It is not clear whether these results extend to general neural networks with more complicated architectures.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper provides an interesting approach to bridging the attack-defense gap for robust classifiers. Some of the ideas and empirical results in the paper are new and interesting. The paper is also very clearly written and provides detailed proof of the theorems listed. The authors also provide all relevant details to reproduce the results. They also created a dataset UG100, recording some of the results for optimal bounds provided in the paper.",
            "summary_of_the_review": "The idea of using the success of a suit of heuristic attacks to flag vulnerable examples is interesting, but it is not clear if there is a specific use case where it would help. Although the paper establishes some interesting theoretical results for the power of this method when used with a perfect attack, none of the guarantees extend to the setting of heuristic attacks. The empirical evidence provided to show the effectiveness is only limited to a very small set of architectures for shallow neural networks. It is not clear if these observations hold in general. As for the setting of shallow neural networks, some of the formal verification methods already provide good bounds. \n\nI feel the paper provides some interesting results that might serve as an interesting starting point for further investigation. However, it does not provide enough justification to use the proposed Counter-Attack method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3153/Reviewer_wxLx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3153/Reviewer_wxLx"
        ]
    },
    {
        "id": "kOEFqIcCDn",
        "original": null,
        "number": 2,
        "cdate": 1666673970792,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673970792,
        "tmdate": 1666673970792,
        "tddate": null,
        "forum": "FDlfFbnI7AR",
        "replyto": "FDlfFbnI7AR",
        "invitation": "ICLR.cc/2023/Conference/Paper3153/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proves that attacking a polynomial-time classifier is NP-complete, and training a polynomial-time model that is robust on a\nsingle input is \u03a3_2^P-complete. The authors proposed a method that evaluates on the fly if a model is robust on a specific point by running\nan adversarial attack on the input. Based on this, the authors proposed a dataset of provably optimal adversarial examples found on\nimages of MNIST and CIFAR10 for 6 small-scale Neural Networks.",
            "strength_and_weaknesses": "Strength:\nThe authors provided a thorough theoretical analysis of the complexity of neural network adversarial robustness. The proposed counter-attack method is very simple but is able to take advantage of existing defenses and provide robustness guarantees. The authors also showed detailed information about their experiments.\nWeakness:\n1. In the main text, it would be great if the authors can present a table for comparison between CA and existing defense methods on their certified robustness accuracy, as well as adversarial accuracy under different attacks. \n2. Please refrain from only using color to distinguish curves in Figure 2, as it may not be friendly to readers with color blindness.",
            "clarity,_quality,_novelty_and_reproducibility": "In general, I think this paper is well-written and organized. The motivation and rationale of the proposed method are introduced clearly.",
            "summary_of_the_review": "In general, I think the proposed method is well-motivated and the studies are solid. So I recommend a weak accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3153/Reviewer_mBWL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3153/Reviewer_mBWL"
        ]
    },
    {
        "id": "L31jw2016_Y",
        "original": null,
        "number": 3,
        "cdate": 1666883661135,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666883661135,
        "tmdate": 1669514105512,
        "tddate": null,
        "forum": "FDlfFbnI7AR",
        "replyto": "FDlfFbnI7AR",
        "invitation": "ICLR.cc/2023/Conference/Paper3153/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper aims to study the \u201ccomputational complexity\u201d of attacking versus that of defending machine learning models against adversarial attacks, which are test-time attacks that find so-called adversarial examples.\n\nAn adversarial example for an input x is a bounded perturbation of x that is misclassified.\n\nThe paper studies the worst-case computational complexity of finding adversarial examples (for a given perturbation threshold) vs. training a robust model. It is shown that in the *worst case* (even for basic ReLU models):\nFinding adversarial examples is NP complete, while\nFinding robust models is Sigma-2 complete. Sigma-2 is a bigger than (or equal to) complexity class than NP (and is conjectured to be strictly bigger).\n\nThe paper then defines a way of perhaps defending adversarial examples by trying to test robustness of the points one by one, and abstaining from answering when there is a non-robustness evidence. In particular, the paper applies a class S of attacks on each point, using a specific perturbation eps. Then, the thesis is that if we don\u2019t find an example with eps perturbations, then perhaps it is relatively safe to assume that there is no such example for smaller eps\u2019 < eps. The paper verifies this experimentally for small models for which the exact robustness can be computed.\n\nEvaluation: I think the formulation of the problem this way has serious limitations that could prevent us from using these results for any real world insights. So I cannot support acceptance. See my comments below.\n\nConsidering NP completeness to reflect \u201ceasier task\u201d than being Sigma-2 complete is both correct (theoretically) and wrong (practically). Theoretically, Sigma-2 is a \u201charder class\u201d as it contains NP, but when it comes to running algorithms efficiently, there is no difference between them, as they both contain problems that we don\u2019t know how to solve in polynomial time. So, there is a fundamental difference between the \u201ccomputational approach\u201d of this paper and that of e.g., Garg et al (cited work) where the distinction there is drawn between poly-time vs. non-poly-time algorithms. This makes the theoretical insight of this paper quite irrelevant to practice.\n\nThe paper does not pay attention that their notion of adversarial examples might *not* lead to misclassified inputs. For example, suppose we perturb x into x\u2019 and the model changes the label for x\u2019 while the true label is the changed label. (e.g., changing a cat image to a dog image while the model predicts the dog as dog). This should not constitute an adversarial example. E.g., see the work of \u201crevisiting adversarial examples\u201d by Suggala et al. Without this subtle point being fixed, it is hard to interpret the results of the paper as attacks. This might seem like a minor thing, but it becomes clear that it is a major issue when one notices that hardness of robust or non-robust learning of a task crucially depends on *what class of concept functions* we want to learn. E.g., one can show that simple function classes *can* be learned robustly and the literature is full of such results. This shows another flaw with the formulation of the problem in this paper.\n\nThe computational complexity of adversarially robust learning is an *average-case* phenomenon. Namely, we would like to know, what is adversary\u2019s chance of making x misclassified, when we (say i.i.d) sample the training set, then train (perhaps in a randomized way) a model, and then pick x at random, and finally let adversary perturb x. So, even though the perturbation itself is studied in the worst-case, the whole problem is average-case. As an example, breaking crypto-systems is an average case task, even though one can find specific ciphertex whose complexity *in the worst case* is NP complete, yet finding NP complete encryption schemes is a major open question in cryptography.\n\nBy now, there is a rich literature on \u201ccertifying\u201d robustness of decisions per instance. The paper seems to invent the wheel here and give a *weaker* certification than what those works offer, in terms of its guarantee. While other works on certification are provably right, here we only get a heuristic claim. This looks like a step back.\n\nIn addition, there is also a (by now rich) line of work on \u201cabstaining\u201d from answering queries for the sake of robustness. This paper\u2019s approach is different, but still quite relevant, to that line of work. So it is good to do a discussion and compare. (As I understand those works try to abstain from answering the perturbed inputs, while this paper\u2019s approach is to not answer even when x is not perturbed, but is just close to a dangerous perturbation). My understanding is that this approach hurts the correctness/accuracy much more, as it might abstain from answering normal instances as well.\n",
            "strength_and_weaknesses": "Strength: aiming for a computational understanding of the adversarial example phenomenon and explaining why defending is hard.\n\nMild weakness: the NP completeness results, as formulated, are rather trivial. For example, if we allow the model to compute SAT instances, is is clear that finding a \"close point that is misclassified\" could be NP complete, because one can choose the model and the input in such a way that doing so is the same as solving an arbitrary SAT instance.\n\nWeakness: there are several severe limitations on how much one can interpret the results of this paper in relation with practice. The experimental part is extensive, but its guarantee is only heuristic while there is a line of work on provably certifying robustness of decisions.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written quite clearly. The approach is novel, but also flawed in some sense.\n",
            "summary_of_the_review": "Interesting direction for studying the complexity of adversarial examples vs defending against test time evasion attacks. The weakness is that the formulation of the problem does not give an actual insight on how much is the running time of those tasks in practice, as the formulation is in the worst case and there is no major difference between NP completeness and sigma-2 completeness in practice; they are both not feasible. See more details in the detailed review.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3153/Reviewer_2UCv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3153/Reviewer_2UCv"
        ]
    }
]