[
    {
        "id": "8qB_mkGw652",
        "original": null,
        "number": 1,
        "cdate": 1666318567841,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666318567841,
        "tmdate": 1668721427437,
        "tddate": null,
        "forum": "iPWiwWHc1V",
        "replyto": "iPWiwWHc1V",
        "invitation": "ICLR.cc/2023/Conference/Paper3643/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a network dissection method without using any labelled image data. This is achieved by utilizing a pretrained CLIP model to compute an image-concept similarity matrix. With the help of the matrix, the concept label for a neuron unit will be the one that maximizes the similarity between the neuron query and concept vector.",
            "strength_and_weaknesses": "Strength\n- Since the introduction of CLIP model, its powerful representation ability has been proved by a variety of applications. This paper showcases an interesting application of CLIP model.\n- The experimental results are impressive and shows the effectiveness of the proposed method.\n\nWeaknesses\n- The contribution of the paper may be exaggerated. The superior results over existing methods may be largely attributed to the powerful representation ability of CLIP that is pretrained on hundreds of millions of image-text pairs.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well written and easy to follow.\nReproducibility: I believe the paper is reproducible although I didn't try that.\nNovelty: Novel but limited.",
            "summary_of_the_review": "Overall, this paper is an interesting application of CLIP model, but the main reason it works well is the pretrained CLIP's powerful representation ability.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3643/Reviewer_Ujcx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3643/Reviewer_Ujcx"
        ]
    },
    {
        "id": "hFtDXmRTiV",
        "original": null,
        "number": 2,
        "cdate": 1666588011751,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588011751,
        "tmdate": 1666588011751,
        "tddate": null,
        "forum": "iPWiwWHc1V",
        "replyto": "iPWiwWHc1V",
        "invitation": "ICLR.cc/2023/Conference/Paper3643/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new method for describing the representations obtained in each neuron of neural networks. Unlike previous methods that require a dedicated dataset with annotations and thus can only detect concepts that appear in the dataset, the proposed method CLIP Dissect is able to describe the representation of neurons on the basis of the implicit knowledge stored in the pretrained large-scale vision-language model (CLIP). The experiment reveals that the proposed method can not only achieve the aforementioned goal, but also provide better descriptions than previous methods even on the predefined datasets.",
            "strength_and_weaknesses": "## Strength\n1. The paper provides sufficient background so that the position of the paper is clear. The difference with relevant works such as Network Dissection is well explained. Obtaining the capability of describing role of neurons without limiting the vocabulary to a predefined set is a good contribution to the community.\n1. The proposed approach is reasonable and technically sound. The method is so simple that readers can easily integrate the method into further studies in this domain.\n1. The paper is clearly written and easy to follow.\n\n## Weakness\n1. The technical novelty of the proposed method is limited since the method is rather straight-forward adaptation of CLIP for network dissection problems. The paper claims that it addresses the limitations stated in the last line of the 2nd paragraph in the introduction \u201cAlthough these\u2026 new concept.\u201d, but it is achieved mainly by the capability of CLIP pretrained model. In addition, the results presented in the experiment section is somewhat \u201cas expected\u201d given the CLIP\u2019s capability. I acknowledge the paper\u2019s empirical contribution, but the contribution in terms of the knowledge advancement in the community might be a little lower compared to standard ICLR papers. I would appreciate if the authors could discuss the novelty of the paper and new findings brought by the paper to the community.\n\nMinor points.\n1. The name \u201cCLIP-Dissect\u201d is emphasized with the bold font everywhere it appears, which I think is unnecessarily distracting.\n1. p3 \u201cSTOA\u201d -> \u201cSOTA\u201d\n1. Please provide the citation for WPMI.\n1. p5 \u201camong the three\u201d -> \u201camong the four\u201d\n1. Section 4.3: \u201cSoftPMI\u201d -> \u201cSoftWMPI\u201d\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is very clearly written and easy to follow.\n- Quality: The quality of the presentation is good and the method is technically sound.\n- Novelty: The paper has marginal technical novelty as it owes main capability of describing the activation to CLIP, while it has slightly more empirical novelty since this is the first attempt to utilize large vision-language model for explaining the behaviors of neurons without limiting the vocabulary to a predefined set.\n- Reproducibility: I believe the sufficient information for reproducing the result is provided.\n",
            "summary_of_the_review": "In my view, although the technical contribution of the paper is not that significant, the paper provides important extension in understanding the representations of neurons in neural networks. In addition, the paper clearly states its position and writing quality is good. Overall, I am leaning to positive on this paper, but I am open to the discussion with the authors and the other reviewers to make the value of this paper clearer.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3643/Reviewer_xZV4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3643/Reviewer_xZV4"
        ]
    },
    {
        "id": "DAoEncqovm",
        "original": null,
        "number": 3,
        "cdate": 1666606552780,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666606552780,
        "tmdate": 1668776049985,
        "tddate": null,
        "forum": "iPWiwWHc1V",
        "replyto": "iPWiwWHc1V",
        "invitation": "ICLR.cc/2023/Conference/Paper3643/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new way to label internal neurons of convolutional neural networks. The proposed method relies on the CLIP's vision and text embedding to generate the labels. This is an improvement over related work that relied on manually labeled datasets. For the evaluation, two networks are used: a ResNet-50 trained on ImageNet and a ResNet-18 for the Places-365 dataset. Their method is compared against two baseline methods on different datasets and tasks. ",
            "strength_and_weaknesses": "**Strengths:**\n\n- The paper cleverly uses CLIP embeddings to generate labeled annotations for neurons.\n- Labeling internal neurons could be very useful for many downstream applications. For example, it could be used to monitor networks in production (e.g., does new data change the internal activations in unintended ways?). \n- The evaluation contains a good collection of qualitative and quantitative experiments.\n- Even though this new approach does not require any labels, it outperforms previous label-based approaches.\n\n**Weakness:**\n\n- My main concern is that this work did not measure the gap between the assigned meaning (as textual description) and actual meaning (as neuron activations). Previous works in that direction also did not analyze this gap in detail. For example, the following experiments could be used to measure this gap:\n\n  - How well can the final classes be predicted using the textual description of the internal neurons? This could also be used to evaluate different labeling strategies: the better the description, the higher the final score should be. \n  - Show that the most activating neurons correspond to the assigned concepts. Just providing the 5-most activating examples is not enough. I would like to see a uniform selection of most-activating samples from the top 0.1%,1%, and 5%-most activating examples per neuron. I would suspect that already the top 1% are quite diverse. \n\n- Similar to the previous comment: In Appendix A.3, it is stated: *\"Unintrepretable neurons: Some neurons seem to not be interpretable\"*. Could you provide a qualitative estimate on how many neurons are \"uninterpretable\" and how you measure this? \n\n- Limitations are only discussed in Appendix A.2. Please move them to the main paper. There is enough whitespace on page 9 to include them there. Furthermore, it should be discussed how well the meaning of a neuron matches the textual description. \n\n## ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** I found the work to be written well and clearly. \n\n**Quality:** The experiments are well-designed, and the paper is also well-presented.\n\n**Reproducibility:** Their approach is well described. However, the authors did not comment on whether they plan to release their source code. \n\n**Novelty:** As far as I know, CLIP embeddings were not previously used to label internal activations. However, the idea is also not revolutionary.\n\n**Minor issues:** The tables' captions should be above the table.",
            "summary_of_the_review": "I think this paper makes interesting contributions and could be a good fit for ICLR. However, I doubt a few words can fully describe a neuron's functionality, and I think this mismatch must be analyzed or at least discussed. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3643/Reviewer_XsCd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3643/Reviewer_XsCd"
        ]
    },
    {
        "id": "Ns-wGz4FqeR",
        "original": null,
        "number": 4,
        "cdate": 1666816176229,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666816176229,
        "tmdate": 1669146555652,
        "tddate": null,
        "forum": "iPWiwWHc1V",
        "replyto": "iPWiwWHc1V",
        "invitation": "ICLR.cc/2023/Conference/Paper3643/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper addresses deep neural network explainability and specifically proposes a method for generating textual description of neurons in a pre-trained network. The method operates as follows: Given a pretrained network f, a \"probe set\" D (a set of images), a \"concept set\" S (a set of words/phrases) and a neuron in f, the method finds the concept in S that best describe the neuron, in terms of explaining what it \"does\"/responds to\" based on its activation map on the probe set D. The method works as follows:\n\n1. First, we use CLIP to compute image embeddings for D and text embeddings for S. \n2. Next, we compute the concept activation matrix P where the i,j entry in P is the the result of the inner product of the i'th image embedding with the j'th concept embedding. \n3.  For a given neuron k, we compute the mean of its activation on map on all the images in D and denote the resulting vector as q_k.\n4. Finally, we find the \"most similar\" concept t_l (in S), where the similarity function is computed using q_k and the matrix P. The paper explores several alternatives for a similarity function and provides detailed experiments comparing the different alternatives. \n\nThe paper provides quantitative as well as qualitative evaluations that demonstrate the improve performance, flexibility and runtime of the proposed method. ",
            "strength_and_weaknesses": "Strengths:\n1. The paper demonstrate a new and non trivial use case of strong pre-trained Vision and Language models which is, in my opinion, a contribution by itself to the field of joint multi-modal embedding. \n\n2. The proposed method is far more flexible than previous methods - leveraging CLIP's latent space, the method can work on practically any set of natural images and concepts without requiring any training or ground truth labels. The method is also dramatically faster than previous methods. \n\n3. The experiments demonstrate the methods flexibility and improved performance (compared to existing methods). The paper also demonstrates two non-trivial use cases of the method: detecting missing concepts from a given image set and investigating the connection between pairs different neurons in a pre-trained network.\n\n\nWeaknesses:\nEDIT: the rebuttal and the revised text (additional experiments and analysis) completely addressed the drawbacks I mentioned below. \n\n1. In my opinion, the quantitative analysis basically measures how well the method assigns the correct class labels for the neurons in the final layer, which is a very limited aspect of explainability. The authors discuss the limitations of different evaluation methods in the paper, but the fact is that there is no quantitative experiment evaluating the ability of the network to describe intermediate neurons in the network. I understand the limitations of having no ground truth, but I do believe designing a small user study could benefit the paper.\n\n2. minor comment: under \"Rank reorder\" similarity function, it says \"This is done by replacing the i-th largest element of P:,m by the i-th largest element of qk for all i.\". The resulting vector is a reordered version of q_k, wouldn't that mean that we replace the i'th largest element of q_k by the i'th largest element of P:,m? If I understand correctly, \"Rank reorder means re-ordering q_k using P:,m order\", I think the phrasing of this section can be made a bit more clear. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The method is well presented and the paper is well written and easy to follow.  The method is easy to reproduce and very original in its use of strong pre-trained vision and language models to a completely new research area. ",
            "summary_of_the_review": "In my opinion, the paper presents a useful, general, flexible method fast method for addressing an important aspect of model explainability. The technical novelty of the paper is limited in the sense that it proposes a simple method which relies heavily on the strength of CLIP, but I do think the paper is good fit for ICLR.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3643/Reviewer_SGoY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3643/Reviewer_SGoY"
        ]
    }
]