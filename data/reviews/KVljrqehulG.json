[
    {
        "id": "nEbi656Qhdq",
        "original": null,
        "number": 1,
        "cdate": 1666591729437,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591729437,
        "tmdate": 1667198435663,
        "tddate": null,
        "forum": "KVljrqehulG",
        "replyto": "KVljrqehulG",
        "invitation": "ICLR.cc/2023/Conference/Paper169/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper describes a new method for AutoML (FALCON) that comprises a online search in the space of designs that is guided by a GNN model (\"meta-GNN\") that is trained along the way. The GNN comprises a task-agnostic component that tries to capture similarity between designs using various features and relations, as well as a task-specific component that performs label propagation based on a set of evaluated designs, and is trained using a pairwise rank loss. Experiments are presented on a variety of graph-based AutoML tasks, and results on an image classification task are included in the Appendix. ",
            "strength_and_weaknesses": "Strengths:\n- The proposed approach is novel to the best of my knowledge. \n- The experimental results show that FALCON out-performs simple baselines like random search, simulated annealing and Bayesian optimization, as well as a variety of generic AutoML approaches (DARTS, ENAS) as well as AutoML methods designed specifically for graph tasks (AutoAttend, GraphNAS, GASSO).\n- The ablation studies show that (a) the search is significantly improved by using the meta-GNN and (b) the meta-GNN is significantly improved by using the task-specific component. \n\nWeaknesses:\n- There is some confusing terminology in this paper. It is stated that FALCON \"learns the inductive biases\" that are present in the problem using a \"meta-model\". The examples given for the inductive biases seem to be simply relations that exist between various hyper-parameters and design choices in the optimization domain. For instance, the example given \"using a large number of layers without batch normalization degrades the performance\u201d seems like a simple relation that can be learned by any model-based AutoML framework (even BO can in principle learn this). Why is the meta-GNN a \"meta\" model - does it learn anything across different tasks or datasets? The task agnostic component is not really learned across different tasks as far as I can tell. It is more like a component that captures some domain-specific knowledge about the design space. The meta-learning terminology is confusing to the reader. \n- It is stated that other sample-based AutoML methods require \"thousands of models from scratch\". This may be somewhat true, but there also exist efficient methods like Successive Halving and Hyperband which train many models but using a small fraction of the training data in order to eliminate designs that are not promising very quickly. Why were these methods not discussed or used as baselines?\n- The main algorithm is not described in sufficient detail to allow the reader to follow what is going on. Algorithm 1 is presented with little-to-no discussion of any of the subroutines that are present. Figure 1(b) is not sufficiently explained in the text. Important details like the design distance are offloaded to the appendix with little discussion. \n\nOther questions:\n- Section 4.2: what are the \"one-shot methods\"?\n- Table 1 and Table 2: what statistical test was used to compute p-values + why are the p-values missing from some columns?\n- Section 3.2: can you please elaborate on how the \"instance-wise performances\" are measured? In Figure 2 they seem to be binary vectors. What exactly do these vectors represent?\n- Section 2: what is a \"computational cell\"?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work in this paper seems novel and code has been provided to enable the reader to reproduce the results (although I have not tried to run it). However, the paper is not written to a high standard and many details are unclear. ",
            "summary_of_the_review": "This paper contains some interesting work and ideas and the results are promising, but needs a rewrite to clean up some of the terminology used and present the details of the algorithm in a clearer way. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper169/Reviewer_ZhyY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper169/Reviewer_ZhyY"
        ]
    },
    {
        "id": "arjvmDty4s",
        "original": null,
        "number": 2,
        "cdate": 1666852754840,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666852754840,
        "tmdate": 1666852754840,
        "tddate": null,
        "forum": "KVljrqehulG",
        "replyto": "KVljrqehulG",
        "invitation": "ICLR.cc/2023/Conference/Paper169/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed an efficient automated model design algorithm called FALCON. FALCON views the overall design space as a design graph. Each node in the design graph represents an individual modeling choice, which includes the model architecture and the optimization hyper-parameters such as learning rate and weight decay. The nodes are connected following the predefined rules. The key intuition of FALCON is that the connected nodes tend to have the same performance. Thus, FALCON trains a performance model by applying GCN on the design graph, and use the performance model to explore new modeling choices and predict their performances. Results show that FALCON achieves better performance than previous auto-search algorithms, and the overall GPU-cost is not very huge. The author also conducted ablation analysis to show that modeling design relations is important for performance improvement.\n",
            "strength_and_weaknesses": "Strength\n\n- The performance of FALCON on GNN tasks is strong.\n- It is reasonable to view the design space as a graph. However, the final performance might be sensitive to the rules that FALCON pick for constructing the graphs.\n\n\nWeaknesses\n\n- The author only conducted experiments on GNN tasks. Experiments on Image tasks are just conducted on a CIFAR-10 while previous papers usually report the performance on NAS-Bench. Actually, the author claimed in the end of Introduction (Page 2) that \"Without loss of generality, we mainly focus on AutoML for graph representation learning in this paper.\". I do not think that the author can claim \"Without loss of generality\" by just conducting experiments on GNNs. The method is very general and the author claimed it to be an AutoML framework. Experiments on a more diverse collection of tasks (such as more NAS-Bench, or NLP tasks) is necessary.\n- The method is largely based on inductive bias of smoothness. However, I think smoothness won't hold in the general scenario (e.g., certain learning rates are preferred for certain network architectures). Thus, the author may need to elaborate more on the smoothness hypothesis.\n- I think the method is also related to Population-based Training (PBT), and the author needs to mention the connection / difference between FALCON and PBT.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is clearly written.\n\n- Quality: The technical quality is good and the author also attached the source code.\n\n- Novelty: The novelty of the ranking loss (equation 2) is marginal because using paired data for training a ranking model has been studied in \"[NeurIPS2020] BRP-NAS: Prediction-based NAS using GCNs\". However, I think using a design graph to accelerate the model choice search for GNN is novel.\n\n- Reproducibility: The author has attached the source code in https://anonymous.4open.science/r/Falcon . The reviewer hasn't run the source code but believe it should be largely reproducible.\n",
            "summary_of_the_review": "I voted for weak reject due to concerns that the author is overselling their method. In addition, the smoothness inductive bias has not been discussed in depth.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper169/Reviewer_Wvmw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper169/Reviewer_Wvmw"
        ]
    },
    {
        "id": "1gp5PboKxJ7",
        "original": null,
        "number": 3,
        "cdate": 1667099303389,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667099303389,
        "tmdate": 1667099303389,
        "tddate": null,
        "forum": "KVljrqehulG",
        "replyto": "KVljrqehulG",
        "invitation": "ICLR.cc/2023/Conference/Paper169/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a sample-based method named FALCON, to search for the optimal model design. The key idea is to build a design graph over the design space of the architectures and hyperparameter choices. A meta-model is built to capture the relation between the design graph and model performance. The method uses GNN to learn and predict the performance of a specific design giving the corresponding nodes in the design graph. The GNN model includes both a task-specific module and a task-agnostic module.",
            "strength_and_weaknesses": "**Strength**\n1. The idea of design of modeling the search space as a design graph is quite novel. The method is also carefully designed with many thoughtful insights. For example, the construction of the design graph, the design distance, and the design of the subgraph. The task-agnostic and task-specific modules are also designed with caution.\n\n2. The empirical evaluation is quite comprehensive and solid, showing improved performance compared to state-of-the-art baselines.\n\n**Concerns and questions** \n\nThe scope of applicability of the proposed method is not clearly stated. It seems to me this method only works for cases where the choices are all discrete. What if there are hyperparameters with continuous domains? For example, the dropout ratio, momentum, and weight decay are all hyperparameters with continuous values. Is FALCON directly applicable to handling a search space with those continuous hyperparameters? The current experiment only takes several discrete values. Does it mean one has to discretize the continuous hyperparameters? If so, how to discrete them? In Table 12, the candidate values for the momentum of SGD are 0.5, 0.9, 0.99; the LR decay factor is 0.1, 0.2, 0.5, 0.8. I do not see any pattern in this discretization step. This specialized design space raises my doubts about how general this method can be. If there is a newly developed model needs to be tuned, how should one correctly configure those carefully designed components?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well-written. The overall quality of this work is very high.  ",
            "summary_of_the_review": "This work proposes a sample-based method named FALCON, to search for the optimal model design. The key idea is to build a design graph over the design space of the architectures and hyperparameter choices. A meta-model is built to capture the relation between the design graph and model performance. The method uses GNN to learn and predict the performance of a specific design giving the corresponding nodes in the design graph. The GNN model includes both a task-specific module and a task-agnostic module.\n\nThe idea of design of modeling the search space as a design graph is quite novel. The method is also carefully designed with many thoughtful insights. For example, the construction of the design graph, the design distance, and the design of the subgraph. The task-agnostic and task-specific modules are also designed with caution. The empirical evaluation is quite comprehensive and solid, showing improved performance compared to state-of-the-art baselines.\n\nBut I do have concerns about the applicability of the proposed method to more general search spaces (see concerns and questions in the Strength And Weaknesses section)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper169/Reviewer_xdfg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper169/Reviewer_xdfg"
        ]
    },
    {
        "id": "y6mvm-tZHXT",
        "original": null,
        "number": 4,
        "cdate": 1667574502814,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667574502814,
        "tmdate": 1667574502814,
        "tddate": null,
        "forum": "KVljrqehulG",
        "replyto": "KVljrqehulG",
        "invitation": "ICLR.cc/2023/Conference/Paper169/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, a new method of sample-based NAS is proposed, Falcon. The key idea is to represent the model design space as a graph, where nodes correspond to separate designs and an edge corresponds to a minimal change in one of the design parameters. The problem of architecture search is thus translated to a black-box optimization task over the design graph. Falcon builds a GNN meta-model, which approximates the values in the nodes and use it in a search strategy.  ",
            "strength_and_weaknesses": "+ The idea of representing the design space as a graph looks novel for NAS\n\n- Related work seems incomplete. In particular, the claim \u201cPrevious works generally consider each design choice as isolated from other designs\u201d seems too strong. I think, there should be an overview of previous papers related to meta-models over design spaces. Apart from NER, there should be also literature on design spaces in the context of classical ML. In this context, I am not convinced that the scientific problem is well-formulated. When we look at the empirical setting of the work, we see that the design parameters enable a simpler approach without any design graphs: each parameter can be quantified as a separate dimension of a joint design space, and the problem becomes a black-box optimization on the obtained grid.\n- The motivation behind the proposed Falcon is questionable. Why not using Bayesian approaches? Gaussian processes?\n- Some parts of the paper lacks of details. In particular, GNN meta-model and its training pipeline is not fully described\n- The pipeline of the empirical part is not reproducible based solely on the text of the paper. The whole system is complex, it has many hyperparameters and itself design choices. This choice is not transparent, and I cannot be sure that the obtained empirical improvements are not an attribute of these choices.",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of the main idea seems questionable. When we look at the empirical setting of the work, we see that the design parameters enable a simple approach without any design graphs: each parameter can be quantified as a separate dimension of a joint design space, and the problem becomes a black-box optimization on the obtained grid. Why this scenario is not addressed in the paper?\n\nSome parts of the paper lacks of details. In particular, GNN meta-model and its training pipeline is not fully described The pipeline of the empirical part is not reproducible based solely on the text of the paper. The whole system is complex, it has many hyperparameters and itself design choices. This choice is not transparent, and I cannot be sure that the obtained empirical improvements are not an attribute of these choices.",
            "summary_of_the_review": "I think, this paper does not appropriately place itself in the context of state-of-the-art and classical algorithms relevant to the problem, and needs a major revision.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper169/Reviewer_1F3M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper169/Reviewer_1F3M"
        ]
    }
]