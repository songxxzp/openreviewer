[
    {
        "id": "piubJs2wSpx",
        "original": null,
        "number": 1,
        "cdate": 1666517406866,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666517406866,
        "tmdate": 1666564304165,
        "tddate": null,
        "forum": "3_NvTLGjDKy",
        "replyto": "3_NvTLGjDKy",
        "invitation": "ICLR.cc/2023/Conference/Paper2489/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a model of hippocampal / entorhinal representation that handles both physical and conceptual space with a common mathematical formulation, drawing on successor representations from RL, and building on dimensionality reduction accounts of grid cells such as the work of Dordek et al. The authors propose a novel dimensionality reduction technique that generates a disentangled pair of vectors. In the case of physical space, the authors show that grid cell-like representations emerge with their method. The authors show that their representations support spatial navigation, where one of the two vectors represents the starting position and the other represents the goal position. The most striking finding of the paper is that the very same method can be used in a language setting to generate word embeddings. The resulting word embeddings exhibit desirable properties familiar from the literature such as conceptual clustering and the ability to support analogical inference through vector algebra.",
            "strength_and_weaknesses": "Strengths and weaknesses\n\nStrengths: The paper suggests a profound connection between two superficially different domains of representation - physical and conceptual space. As well as shedding light on the neuroscientific question of how the hippocampal formation functions, the authors\u2019 method potentially provides a powerful method for use in ML. The method is well motivated, both scientifically and mathematically.\n\nWeaknesses: I don\u2019t see any important weaknesses. But I have some questions.\n\nQ1. Section 4.2: Emergence of grid-like representations. You present results for x(s), but I was wondering whether grid-like representations emerge in w(s) too? In the objective function (Eq.6), x(s) and w(s) are treated almost symmetrically, but the correlation term only applies to x(s) not to w(s) (Eq.8). Is this constraint responsible for the grid-like representations?\n\nQ2. Is there an intuitive meaning to w(s) in the word embedding case? It seems that all the results presented for conceptual space use x(s) only. (Is that correct?)\n\nQ3. I\u2019m a little unclear on how the optimisation process works. If x and w are functions that map state vectors to vectors with reduced dimensionality then how are they represented. My reading is they are represented in tabular fashion, i.e. x(s) and w(s\u2019) are represented independently for all s and s\u2019. Is that right? I assume x and s could be approximated by a neural net, but that isn\u2019t the method used here. Is that correct?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written, and novel. I'm fairly confident I could reproduce the results (but see my Q3 above).",
            "summary_of_the_review": "The paper is excellent, in my opinion. The proposed method is novel and makes some quite deep connections that could have important implications. The paper easily merits acceptance at ICLR, unless one of the other reviewers uncovers a serious shortcoming.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2489/Reviewer_pkWT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2489/Reviewer_pkWT"
        ]
    },
    {
        "id": "vKCha22N1KV",
        "original": null,
        "number": 2,
        "cdate": 1666553313534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666553313534,
        "tmdate": 1666553313534,
        "tddate": null,
        "forum": "3_NvTLGjDKy",
        "replyto": "3_NvTLGjDKy",
        "invitation": "ICLR.cc/2023/Conference/Paper2489/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a technique called disentangled successor information that extends successor representations (SR) in RL and shows the representations learned by the proposed method resembles grid cells in the hippocampus. The authors apply their technique to spatial navigation and language modeling and show that 1) representations for spatial navigation look like grid cells 2) representations for words resemble concept cells in the brain with each feature more or less corresponding to a single concept. They take these results to argue that perceptual and conceptual procesing in the brain could be using a similar mechanism.\n\nIn more detail, given a set of discrete states, the proposed method first forms the successor matrix S (where Sij is the discounted probability of observing state sj given state si). Then it calculates an information metric called positive successor information from this matrix. Finally, a dimensionality reduction technique based on non-negative matrix factorization is used to extract lower dimensional state representations. This dimensionality reduction technique has an additional objective term to minimize the correlation between different state vectors. This helps to create a more disentangled state representation.\n\nThe authors show that this proposed technique is related to value functions in linear RL and also to word vectors obtained by skip-gram models.\n",
            "strength_and_weaknesses": "Overall I think this is an interesting paper but I'm not sure if ICLR is the right venue for this work. This work mainly tries to establish a connection between their approach (disentangled successor information) and perceptual and conceptual processing in the brain. This is an interesting and valuable endeavor but it's unclear how relevant this would be to the ICLR community. To me it seems like a cogsci/neuro venue or even NeurIPS would be a better choice for this work.\n\nIn general, I think the paper is well written and easy to follow.\n\nIn terms of contributions, the additional step of using decorrelative NMF to get state representations seems novel. However, the connection of successor representations to value functions is well-known and the connection between DSI and value functions seem to follow from this, so the connection is not quite novel. Perhaps the connection to word representations in skip-gram models is novel but given that successor representations are essentially bi-gram models, this is not very surprising.\n\nIn terms of results, there is a nice evaluation of the technique on spatial navigation and on word processing but these seem rather limited. Finding that the learned representations resemble grid cells is interesting but not a very strong evidence since many other techniques also learn representations with grid-like receptive fields.\n\nSimilary for word embedding results, I didn't really find them very strong. The authors show that features in their learned representations are on average more concept specific than some alternative techniqes. Since the brain also seems to have concept specific cells, they take this as evidence for brain using a similar mechanism. However, as far as I know it is still not very clear to what extent the concept representations in the brain are like concept cells. There seems to be many cells that do not cleanly respond to a specific concept for example. Even if this was not an issue (and we knew brain used concept cells), it is still not very strong evidence because other techniques also learn representations with similar properties. In fact, one could also directly use one-hot vectors as word representations and these would be 100% concept specific.\n\nOverall, I think it'd be great to have more evidence supporting the use of representations like DSI in the brain, by perhaps testing out other properties of these representations (like word similarity etc.).\n\nOne major issue with the paper for me is that it conflates conceptual processing with language processing. Language (and words) are only a part of conceptual processing. In fact, one can argue that concepts are more primary than words and language. Then I don't think you can take results with words and use these to make statements about conceptual processing in general. In a couple of places, the authors argue that because their technique can be applied to both spatial navigation and words, it suggests that perceptual processing and conceptual processing use the same mechanisms. However, for the reason I mentioned, this does not follow. Humans certainly have a conceptual understanding of object shape as well. Can the proposed technique also capture key properties of this conceptual space?\n\nRelated to this point, not all conceptual spaces have a 2D (grid-like structure), so it is unclear why this method should work for such conceptual spaces as well.\n\nAlso, the hypothesis that perceptual and conceptual processing might be using the same mechanisms is not novel. There is a long line of research in CogSci that makes this argument, and it'd be good to mention these (for example Lakoff's work).\n\nOther points:\n- One limitation of the method is that is assumes discrete states. Can the method be extended to continuous states? It might be good to mention some ideas along these lines.\n- In 4.4., in what cases would we expect addition to work? If it is only the case where the room is an exact superposition of two previous rooms, then this seems too limited.\n- How about comparing against embedding from next word prediction models like GPT-3? Would these show low concept specificity?\n- In 5.3, many methods can do the same vector based computation in conceptual space (even ones not based on successor representations).\n- In related work, the authors say \"spatial and conceptual processing can be theoretically unified into a single vector-based computational principle\". This is a very strong statement not supported by the results in the paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I think there are some novel ideas in the paper (like de-correlative NMF), and the writing is quite clear. However, the results don't really support the claims made by the authors.\n",
            "summary_of_the_review": "I think this is overall an interesting paper but it seems more suited for a venue with more neuroscience/cogsci focus. I don't think the results will be very interesting to the ICLR community (for that it needs much stronger empirical results on some relevant task/problem) and as it stands, the empirical results are quite limited.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2489/Reviewer_33UN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2489/Reviewer_33UN"
        ]
    },
    {
        "id": "UzuK0h_eLD",
        "original": null,
        "number": 3,
        "cdate": 1666625367444,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625367444,
        "tmdate": 1670173368132,
        "tddate": null,
        "forum": "3_NvTLGjDKy",
        "replyto": "3_NvTLGjDKy",
        "invitation": "ICLR.cc/2023/Conference/Paper2489/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors develop a \u201cdisentangled successor information\u201d framework for understanding the neural representation for physical and conceptual spaces. The paper then applied this framework to study two problems: grid cells and word embedding.\nOverall, this paper contains some interesting and intriguing ideas. But the technical content is not entirely rigorous.\n\n\n",
            "strength_and_weaknesses": "Strength:\n* The paper studies both grid cells and word embedding problems.\n* The attempt to link grid cells and word embedding is ambitious.\n* Using successor information to study the grid cells is an interesting and novel idea.\n\n\n\nMain concerns: \n* Why decorrelative NMF is a good objective function for the grid cell system is unclear. Eq. (6) needs better motivations. \n\n* It is unclear what determines the structure of the emerging representation. Does the word embed similarly lead to a grid representation? If not, why?\n\n* The improvement of this model (in the case of word embedding ) compared to prior work is in fact very subtle (e.g., Table 2).\n\n* How is the present work different from prior work, e.g., Stachenfeld et al 2017, and Dordek et al 2016 is unclear. This paper performs matrix factorization, while previous work did PCA. Are they really different? This needs to be better explained.\n\n* The similarity between Eq 13 and Eq 15 seems to be superficial. If they are indeed equivalent, the implication seems to be that it should be possible to use an objective function based on Eq. 13 to derive the grid cells.  But this is not discussed or shown.\n\n* Interpreting an information-theoretical quantity as the neural activity seems to be risky. (I am actually not sure if it makes any sense.) This crucial assumption needs to be better justified. \n\n\nOther comments:\nIn Dordek et al., 2016; Sorscher et al., 2019, in addition to the non-negativity, the inhibition surround is also critical for the grid firing fields to emerge. This point should be discussed in more detail.\nSeveral relevant papers on understanding grid cells with machine learning approaches are missing, e.g.,  Cueva & Wei, ICLR, 2018;  Gan, Xie, Zhu, Wu, ICLR, 2019.\nIn Section 4.2, this explanation is not the first that attempts to theoretically explain the grid responses and the scale ratio. Prior work on this should be discussed.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarify, quality and rigor needs substantial further improvement. ",
            "summary_of_the_review": "Overall, I enjoyed reading this paper, but there are many question marks upon reading it. I\u2019d be more comfortable endorsing this paper if the work is done in a more rigorous way, and the model assumptions are better justified. Right now, there is uncertainty about the novelty of this paper makes and whether the results are indeed solid.\n\n\n%%%%%%\nI would like to thank the authors for their detailed responses to my comments. Although the responses and the revision clarified some issues, unfortunately, they did not lead to major improvements in the paper in my opinion. The paper has nice potential to be further improved, but at the moment the contribution still seems rather incremental, and the work would benefit by making the arguments more rigorous.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2489/Reviewer_Eei5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2489/Reviewer_Eei5"
        ]
    },
    {
        "id": "2oOeNytI6S",
        "original": null,
        "number": 4,
        "cdate": 1666674170816,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674170816,
        "tmdate": 1670033635475,
        "tddate": null,
        "forum": "3_NvTLGjDKy",
        "replyto": "3_NvTLGjDKy",
        "invitation": "ICLR.cc/2023/Conference/Paper2489/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper defines positive successor information in the framework of reinforcement learning, and proposes positive decorrelative nonnegative matrix factorization for dimension reduction of successor information. The paper applies the proposed method to spatial navigation and word embedding, and obtained meaningful results. The paper also analyzes theoretical relationship to linear reinforcement learning. ",
            "strength_and_weaknesses": "Strengths:\n\n(1) The theoretical foundation of the paper is solid, especially relationship with linear reinforcement learning. \n\n(2) The empirical results on grid cells are strong. \n\n(3) The experiments on word embedding are solid. \n\nWeaknesses: \n\n(1) Non-negative matrix factorization is widely used, such as in the recommender system, to factorize the value function or the affinity matrix. The successor representation has also be used to model place cells, which are connected to grid cells via matrix factorization or eigen decomposition. Thus this paper is not entirely novel, although the specific form of decorrelative NMF is new. \n\n(2) Given the large number of grid cells in EC, dimension reduction may not be the most convincing point of view for embedding. It is more about population coding, where generalization does not need to rely on dimension reduction. Other forms of regularizations may also work. \n\n(3) Can your method on grid cells explain path integration? \n\n(4) The sequence of words in NLP is not a trajectory. The contexualized embedding such as BERT seems more reasonable. ",
            "clarity,_quality,_novelty_and_reproducibility": "(1) The paper is very clearly written. \n\n(2) The theoretical foundation of the paper is solid. \n\n(3) The proposed method is new, but is not entirely original, given the past work on successor representation for place cells and past work on word embedding. Matrix factorization is an element in both areas. Word embedding in NLP has gone far beyond matrix factorization. Dimension reduction may not explain grid cells either given the large number of grid cells in EC. ",
            "summary_of_the_review": "The paper makes a solid and useful contribution, but it is not extremely novel. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2489/Reviewer_K3nY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2489/Reviewer_K3nY"
        ]
    }
]