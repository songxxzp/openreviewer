[
    {
        "id": "sEA7SXdZTZH",
        "original": null,
        "number": 1,
        "cdate": 1666669940677,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669940677,
        "tmdate": 1666669940677,
        "tddate": null,
        "forum": "9X3UZJSGIg9",
        "replyto": "9X3UZJSGIg9",
        "invitation": "ICLR.cc/2023/Conference/Paper4263/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes INR-GAN-based text-to-image generation method, where model weights are modulated with the additional module called hypernetwork and RGB value is predicted given pixel coordinate as input.\nTo make a model to reflect the textual condition better, the authors propose a new method of generating modulating weights by incorporating word-level predictions.\nSpecifically, they first compute word-level modulating weights and then merge these values based on their similarities between the weights of the model.\nFurther, to reduce the computational burden, the authors propose to use the canonical polyadic decomposition to decompose the tensor to predict.",
            "strength_and_weaknesses": "Strengths\n- The proposed method successfully extend INR-GAN for the text-to-image generation task.\n- Improved scores on some quantitative evaluation metrics compared to prior GAN-based works.\n\n\nWeaknesses\n- The reason of choosing INR-based generator would be to generate images free from the constraint on the image resolution. But the results that promote this aspect is insufficient, and in fact, it does not seem to be helpful for generating realistic images. For example, the two images in the first column in Fig 1. show that they are merely mirroring the edges of the original images, which hinder their realism. It is not also analyzed in quantitative manner. \n- The content in Related Work part is poor. I understand that this work is based on GAN, and might not be fair to compare with current diffusion models, but it would be better to mention those works and differentiate this work describing what factors make this work more beneficial compared to them.\n- I have an impression that the DAMSM regularizer has a larger influence than the proposed method in improving the image quality from Table 2. Maybe the order should be reconfigured if the authors want readers to focus on the efficacy of using word-level modulating mechanism.\n- The authors did not show how much computational burden can be reduced by using the canonical polyadic decomposition method.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method proved that INR-GAN-based works can also be extended for the text-to-image generation task.\nHowever, the traits that make INR-GAN-based works tempting such as outpainting an original image is not good enough, especially considering current diffusion-based models' ability to generate quite impressive results not only in outpainting but also in super-resolution images.\nMoreover, the proposed method of using word-level attention mechanism is somewhat incremental, and its efficacy is not really clear as the remark I made regrading the ablation study.",
            "summary_of_the_review": "Extending INR-GAN in the text-to-image generation task is an interesting approach, and the proposed method show improved results compared to GAN-based prior methods.\nHowever, the demonstrated improvements are somewhat limiting both considering its lack of ability to extend images retaining the realism of the original images and \nresearch on prior text-to-image generation works.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4263/Reviewer_xx7K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4263/Reviewer_xx7K"
        ]
    },
    {
        "id": "vL5EZnq_u_2",
        "original": null,
        "number": 2,
        "cdate": 1666692623535,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692623535,
        "tmdate": 1666692623535,
        "tddate": null,
        "forum": "9X3UZJSGIg9",
        "replyto": "9X3UZJSGIg9",
        "invitation": "ICLR.cc/2023/Conference/Paper4263/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose a framework for text to image generation using an implicit image generation model. The text conditioning is achieved through means of a novel hypernetwork inspired architecture that modulates the weights of the generator and discriminator network based on the text prompts. State of the art results are demonstrated on 3 datasets. \n",
            "strength_and_weaknesses": "## Strength \n\n1. **Writing:** The paper is well written with all components explained in adequate detail.\n\n2. **User study:** The user study performed demonstrates the effectiveness of the approach. \n\n3. **Comparison:** Adequate comparisons are made both with respect to baseline approaches and various dataset highlighting the usefulness of the approach. \n\n4. **Parameter efficiency:** The proposed approach provides a high degree of parameter efficiency compared to multistage baseline approaches, while also giving the ability for extrapolation and superresolution for free. This is particularly useful in not being tied to a specific spatial resolution while generating an image. \n\n## Weakness\n\n1. **Related work:** Any manuscript on text to image generation would be incomplete without the inclusion of the recent large text to image diffusion models.  The related work only tackles GAN based text to image generation without referencing any of the advances in diffusion models. Although each method has distinct advantages and disadvantages, the related work section would benefit from a more in depth treatment of this line of work.\n2. **Approach:** The section regarding how the individual word level embeddings are used to modulate the network weights is unclear. Particularly, a more detailed description of the WhAtt component would be instructive.\n2. **Experimental analysis:** There are certain sections of the experiment section which is unclear, Particularly,  \n>a) It is unclear what the DAMSM-R score represents. Particularly, how do we interpret the percentage score here?  \n>b) The ablation study demonstrates the need of some of the losses but it does not make the need for a conditional discriminator clear. In particular, why does the discriminator also need to be modulated with a hypernetwork ? Any isights regarding this would be helpful.\n4. **Reproducibility:** Some aspects of the WhAtt component are unclear, making it hard to reimplement and reproduce the framework.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** The approach section can be made clearer to better convey the information flow of the WhAtt component of the framework. In particular, the way the word embeddings are used to generate the final modulation vector at each layer is unclear.  \n\n**Quality:** The quality of writing and results is adequate. Efforts have been taken to report both qualitative and quantitative results. The related work section could use some discussion about diffusion based pipelines for text driven image generation. \n\n**Novelty:** The approach is sufficiently novel. The authors propose a hypernetwork based design for text driven image synthesis using an implicit representation . Although similar ideas have been explore in Scene Representation Networks (Sitzmann et al.) this work provides important extension of the framework to the text guided case. \n\n**Reproducibility:** Due to lack of code and some unclear technical details of the WhAtt component, the method maybe hard to reproduce. ",
            "summary_of_the_review": "\nAlthough the authors present a reasonably novel approach for text to image synthesis, this work would greatly benefit from an adequate treatment of the diffusion models literature and some more details regarding the word level hyper network design. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4263/Reviewer_3nfM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4263/Reviewer_3nfM"
        ]
    },
    {
        "id": "hbOCn6u02Sp",
        "original": null,
        "number": 3,
        "cdate": 1666751236189,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666751236189,
        "tmdate": 1666790257552,
        "tddate": null,
        "forum": "9X3UZJSGIg9",
        "replyto": "9X3UZJSGIg9",
        "invitation": "ICLR.cc/2023/Conference/Paper4263/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present a method for Implicit Neural Representations conditional continuous image generation. Basically the authors augment  INR-GAN, which is a hypernetwork based generative adversarial network for image generation, to include text embeddings. This is doen either by concatenation or by using an additional hypernetwork for the text embeddings. ",
            "strength_and_weaknesses": "The approach is interesting as it achives similar or better performance  as other SOTA methods with much less parameters. \nHave you done any running time experiments to see how the training and generation time compares with SOTA. The experiments are extensive and done on multiple datasets. \nThe only point of concern is the limited novelty in terms of the actual approach it self. \nI dont see any error bars (runs with multiple seeds). This is important for GAN based generative models as they can have a lot of variability from one seed to another. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well explained and easy to read. \nIn terms of reproducibility I dont see any error bars (runs with multiple seeds). This is important for GAN based generative models as they can have a lot of variability from one seed to another. \n\nThe novelty of the paper is rather very limited and a straight forward extension of a previous work INR-GAN which is an unconditional INR based GAN. ",
            "summary_of_the_review": "Text to image generation has gained a lot of popularity in recent months and it would be interesting to benchmark the current approach \n (which can run with little amount of parameters) with ViT based and diffusion based approaches.  I do believe the paper has merits in application but the theoretical novelty is very limited. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4263/Reviewer_opUk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4263/Reviewer_opUk"
        ]
    },
    {
        "id": "qdEeIJXGTd",
        "original": null,
        "number": 4,
        "cdate": 1666764607061,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666764607061,
        "tmdate": 1666764607061,
        "tddate": null,
        "forum": "9X3UZJSGIg9",
        "replyto": "9X3UZJSGIg9",
        "invitation": "ICLR.cc/2023/Conference/Paper4263/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work tackles to solve the text-to-continuous image generation task (T2CI) by introducing a hypernetwork that modulates the weights of discriminator and generator based on the text condition. The proposed architecture can be considered as an extension of INR-GAN, making the modulator take the text condition. In specific, the authors have proposed two types of modulator to handle sentence-level or world-level information. Experiments on small-scale datasets have shown that the proposed approach is better than existing INR-based generative models in the T2CI task. ",
            "strength_and_weaknesses": "Strengths:\n* The motivation and details of HyperCGAN are reasonable. To minimize the cost of INR, using CP decomposition is also appealing. \n* Experiments show that the proposed approach is able to extrapolate the outside of the input boundary. \n\nWeaknesses: \n* In my opinion, the novelty of HyperCGAN is quite limited, since this could be interpreted as a fairly straightforward extension to INR-GAN for processing the text condition. Obviously, injecting the text condition to INR-GAN brings many practical advantages, but more technical contribution would be required. \n* The out-painting quality of this method is quite worse than SOTA methods, including infinity GAN and NUWA-infinity: \n  * https://hubert0527.github.io/infinityGAN/\n  * https://arxiv.org/abs/2207.09814 (NUWA-infinity)\n* Clarity needs to be improved. For instance, I failed to see why introducing another T2I benchmark improves the contribution of this work. \n\n**Detailed comments**\n\nI\u2019m not sure whether the proposed T2I benchmark adds the value of this work or not.  In most cases, establishing a new benchmark contributes to the research community significantly. However, in this work, I failed to see the connection of the new benchmark and proposed method. Could you elaborate why the existing datasets are not good enough to evaluate the proposed method? \n\nIn my understanding, WhAtt does not take any sequence-level information. In CLIP, the sequence-level and sub-word level information are both useful representations, so it might be better to make use of both information in modulating the weights. \n\nTable 2 shows DAMSM regularizer performs better than CLIP. It seems to be quite unusual, since the representation power of DAMSM would be weaker than CLIP for most cases. Could you elaborate more why DAMSM is better than CLIP? \n\nI\u2019ve found that the comparison to SOTA in the T2I task is quite misleading, since the proposed method is not compared to real SOTA methods. In addition, the authors argue that the proposed method outperforms DALL-E 1, but DALL-E 1 reports the performance of zero-shot evaluation, which is not the case of this work.",
            "clarity,_quality,_novelty_and_reproducibility": "In terms of clarity and novelty, I've found some issues on the contribution of this work. Please refer to my detailed comments above. \n\nFor reproducibility, I haven't found major issues to implement the proposed method. I expect all the number in this manuscript would be reproducible. ",
            "summary_of_the_review": "I\u2019m leaning towards rejection, since the novelty of the proposed method is somewhat limited, and insufficient empirical justification makes it difficult to evaluate the potential of the method. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4263/Reviewer_cHep"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4263/Reviewer_cHep"
        ]
    }
]