[
    {
        "id": "_U2CgZm3QD",
        "original": null,
        "number": 1,
        "cdate": 1666355440696,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666355440696,
        "tmdate": 1666355440696,
        "tddate": null,
        "forum": "mmFtinp4wQ_",
        "replyto": "mmFtinp4wQ_",
        "invitation": "ICLR.cc/2023/Conference/Paper6443/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors investigate multi-objective RL with thresholded lexicographic ordered objectives. The authors start by investigating the shortcomings of the existing TLQ algorithm. While some of these shortcomings are already known (for example Vamplew et. al.) the authors also show that TLQ does not work under certain circumstances (terminating reachability on the constrained objective and the unconstrained is non-terminating).\nAfter that, the authors propose a lexicographic projection algorithm that projects the gradients onto hypercones and shows how to use the projection algorithm in RL.\nIn the end, the authors evaluate the lexicographic projection on a simple analytic function. For RL they evaluate their approach on a simple Maze setting only with no comparison to baselines.",
            "strength_and_weaknesses": "Strong points:\n* A dive into the shortcomings of TLQ is interesting\n* The method could lead to improvements over existing methods. Unfortunately, due to a lack of evaluation, it is unclear how well this method performs.\n\nWeak points:\n* The approach and the reasoning why certain things are that way are extremely hard to follow without the appendix. The description of the approach just takes 1 1/2 pages. The authors should write the paper in a more self-contained way.\n* The influence and sensitivity wrt. several hyperparameters and how to choose them is unclear (what is a good value for b, \u2206,...?)\n* The approach of the paper (projecting into hypercones instead of positive half-spaces) is not very novel and the improvements of that in practice are unclear.\n* The evaluation of the proposed method has serious flaws:\n** The comparison to relevant baselines like TLQ or Lexicographic Multi-Objective Reinforcement Learning (Joar Skalse et al.) is missing. ** All experiments are only done with two objectives. This is problematic because with more objectives finding a good solution is likely much more challenging, and it is unclear how well the method performs with more objectives.\n** Comparison to different (existing projections) like into positive half-spaces is missing. Therefore, it is unclear whether the proposed projection into hypercones has benefits in practice.\n** It is unclear how the proposed method works in settings where TLQ does not fail.\n** Important and commonly used metrics in the evaluation are missing. In RL, the average reward (for each objective) gathered by the policy is one of the most commonly used metrics. However, the authors do not report such metrics.\n** The comparison has been made on a single RL setting only; for more benchmarks see for example A Survey on Discrete Multi-Objective Reinforcement Learning Benchmarks, Thomas Cassimon et. al.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very hard to follow without the appendix. The authors should focus on creating a version of the paper that is more self-contained. The approach of the paper (projecting into hypercones instead of positive half-spaces) is not very novel, the improvements of that in practice are unclear, and there have been investigations into the shortcomings of TLQ. However, the authors present new insights into where these approaches can fail. The experimental evaluation is flawed. Reproducibility is good.",
            "summary_of_the_review": "Because of the evaluation, the benefits and limitations of the proposed approach in practice are unclear. The experiments done in the paper are insufficient. They are on very few settings (e.g., only two objectives), and relevant baselines, and metrics are not reported. Additionally, the paper should be more self-contained and is very hard to follow without reading the appendix.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6443/Reviewer_Ekhh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6443/Reviewer_Ekhh"
        ]
    },
    {
        "id": "0kRkN0Ikba",
        "original": null,
        "number": 2,
        "cdate": 1666559522283,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666559522283,
        "tmdate": 1666559522283,
        "tddate": null,
        "forum": "mmFtinp4wQ_",
        "replyto": "mmFtinp4wQ_",
        "invitation": "ICLR.cc/2023/Conference/Paper6443/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Multiobjective (MO) RL  has many applications with the thresholding algorithm often used. The authors point out a deficiency of such algorithms and propose a PG based approach. They also conduct a numerical study comparing their algorithm with benchmark algorithms. ",
            "strength_and_weaknesses": "Strengths:\nThe idea of using PG for MO-RL. \nThe example showing the drawbacks for the thresholding algorithms. \n\nWeaknesses:\nSome references and algorithms are missing in the background work (and experiments). See M. Fleischer. The measure of Pareto optima applications to multi-objective metaheuristics. EMO, Springer Verlag, pages 519-533, 2003.\nThe computational experiments are not strong. It seems the experiments only deal with the proposed algorithm and don't compare the algorithm vs existing algorithms. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to understand and it is well written. \nReproducibility is questionable but in view of the weakness of the experiments this is a no issue. ",
            "summary_of_the_review": "While adapting PG to MO is new the underlying ideas are not significantly innovative. \nI don't find the identified problems with reachability of significant importance. \n\nLack of sound experiments is a significant drawback of the work. There are no benchmark algorithms and the datasets are very easy. \n\nThere is also abundant work in MO-MAB that might be applicable here. While MAB doesn't have the notion of PG, they can serve as benchmarks. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6443/Reviewer_oJxc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6443/Reviewer_oJxc"
        ]
    },
    {
        "id": "OmNY7Lfiq36",
        "original": null,
        "number": 3,
        "cdate": 1666707685826,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666707685826,
        "tmdate": 1666707685826,
        "tddate": null,
        "forum": "mmFtinp4wQ_",
        "replyto": "mmFtinp4wQ_",
        "invitation": "ICLR.cc/2023/Conference/Paper6443/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on lexicographic multi-objective problems. Firstly, the shortcomings of the existing algorithm Lexicographic Q-Learning (TLQ) are analyzed, and the scenarios in which it is not applicable are pointed out. Secondly, this paper proposes the lexicographic projection algorithm (LPA), which performs multi-objective optimization by a heuristic projection of gradients, so that the current optimization targets the highest priority objective that does not reach the threshold while preventing the degradation of higher priority objectives as much as possible. Finally, this paper combines LPA with a policy-based reinforcement learning algorithm and validates it in the MAZE environment.",
            "strength_and_weaknesses": "**Strengths**:\n\n1. The problem this paper tries to solve is greatly concerned in the RL community.\n2. The idea of solving lexicographic MORL problems with policy-based algorithms is a good attempt. The LPA algorithm is very versatile since it can be combined with any gradient-optimization algorithms and any policy-gradient (and actor-critic) algorithms.\n3. This paper is well organized.\n\n**Weaknesses**:\n\n1. The motivation is not well-supported. This paper focus on the lexicographic multi-objective problems that can not be solved by Lexicographic Q-Learning (TLQ), and two types of maze example are analyzed. Each of them will be discussed below.\n   1) Problems with Reachability Constraint (Fig.1). The problem can be solved by TQL, but the problem itself needs to be redefined. The problem should contain a third objective time cost. The primary objective (Reach G) does not consider the time spent, i.e. no discounting situation, so the algorithm converges to a random policy (described in the paper as \"all actions would have the value $\\tau_1$\"). If the time factor is not taken into account, the random policy is right, but it's not an ideal policy. The discount factor is a mathematical trick to make an infinite sum finite, and it also chooses a suitable planning horizon. So discount factor can be viewed as a trade-off of task rewards and time costs, which is also necessary in reality, because no one can do something without considering the time. Then, the secondary objective (avoid bad tiles) will lead to another stochastic policy that does not step on tiles but may move back and forth. The third objective (time cost) can be formalized as a time penalty or discount factor, leading to the deterministic ideal policy.\n   2) Problems with Non-reachability Constraint (Fig.3). This problem (or just the example in Fig.3.) can be solved by TQL. The primary objective (minimizing the cost of tiles) can converge to the unique path (including *(0-3,0), (3,1), (0-3,2), (0,3), (0-1,3)* ), but there may be backtracking (like *Left* in *(1,0)* with previous step *Right* in *(0,0)*). And the secondary objective (minimizing the time) then allows the agent to reach the goal without backtracking, i.e., in 11 steps.\n2. If the number of objectives is greater than or equal to 3, which means that \"return None\" of Algorithm 2 may occur, no theoretical proof of the convergence or experimental verification is given in this paper.\n3. Here are some minor typographical errors and suggestions.\n   1) Spelling error on page 4, line 9. \"iff\" $\\rightarrow$ \"if\"\n   2) The full name of LMDP should be given before using LMDP in Sec. 1.\n   3) The description of Fig. 1 uses $H$s and $h$s indicate tiles, while $HH$s and $hh$s are in the description of Fig. 3.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** This paper is well organized and can be easily understood.\n\n**Quality:** This work is not complete, since it needs more theoretical proof and experimental verification.\n\n**Novelty:** This paper introduces a new algorithm, but the motivation of this work is not well-supported.\n\n**Reproducibility:** This work can be reproduced.",
            "summary_of_the_review": "I think this paper introduces a new algorithm combined with policy gradient, which approaches the lexicographic multi-objective problem from a new perspective. However, this paper has some fatal flaws. (1) The motivation is not well-supported. (2) The experiment needs to be more fully validated.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6443/Reviewer_Jg7J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6443/Reviewer_Jg7J"
        ]
    },
    {
        "id": "W2YHAnaOGd",
        "original": null,
        "number": 4,
        "cdate": 1667430789734,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667430789734,
        "tmdate": 1667430789734,
        "tddate": null,
        "forum": "mmFtinp4wQ_",
        "replyto": "mmFtinp4wQ_",
        "invitation": "ICLR.cc/2023/Conference/Paper6443/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors discuss shortcomings of existing approaches for thresholded lexicographic ordered multiobijective problems in reinforcement learning.  Additionally, the authors provide a policy gradient algorithm that performs well on this class of problems.",
            "strength_and_weaknesses": "Strengths: The paper did an excellent job formalizing the problems existing with current methods (especially failure cases), as well as how the author's approach directly addresses these problems.\n\nWeaknesses: It's a tired objection, but it's somewhat difficult to situate the author's results with respect to the rest of the literature.  While this is certainly true of any novel work, for which there isn't much else to compare to, then the onus is exceptionally upon the authors to provide compelling results on difficult problems.  While the problems that the authors chose to present were great as educational devices demonstrating the utility of their methods, it's unclear how _practically_ useful the author's algorithm is.  It would be great if the authors could further discuss how their algorithm might fare in a more difficult, less synthetic setting.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality: Both extremely high\nNovelty: Methods in this space are sparse, as the authors discuss, and the authors approach is fairly novel (as a synthesis of several existing ideas in the literature).\nReproducibility: The authors provide some code, and promise to release more.",
            "summary_of_the_review": "A solid submission tackling a fairly new space.  The manuscript could use more extensive, and more practically relevant experiments, though.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6443/Reviewer_NWsv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6443/Reviewer_NWsv"
        ]
    }
]