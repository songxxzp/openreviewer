[
    {
        "id": "xTBe2pHzBM",
        "original": null,
        "number": 1,
        "cdate": 1666696716196,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666696716196,
        "tmdate": 1666696716196,
        "tddate": null,
        "forum": "F_EhNDSamN",
        "replyto": "F_EhNDSamN",
        "invitation": "ICLR.cc/2023/Conference/Paper3193/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper explores a learning-based approach for computing exponential maps for shape spaces modeled as Riemannian manifolds. Previous works have shown that statistical shape spaces represented as meshes can be modeled effectively using the NRIC  (Non-Rigid Invariant Coordinates)  system, allowing for operations like Principal Geodesic Analysis that encapsulate the dominant factors of variation in the corpus of shapes. However, the more meaningful tasks like shape interpolation require a computation of the exponential map which is arguably costly. \n\nThe authors propose a framework for computing the exponential map of specific shape manifolds - namely those that can be factored and written as product manifolds of much smaller dimensions (as required by conditions 0,1,2). The architecture is decomposed to look structurally identical to an affine sum of exponential maps of the submanifolds. By training the composite network to follow the idealistic exponential map in the training dataset, the authors demonstrate that there is merit in their approach, and show R2 confidence scores compared with a straightforward network and the direct axiomatic competitor (Sassen2020b). Overall the perception is that the network generates impressive models of shape variation at a (possibly?) much lower memory and compute cost. \n",
            "strength_and_weaknesses": "Strengths \n\n- I find the problem setup very original, novel, and interesting. The arguments that motivate the use of neural networks to model computationally expensive exponential maps are very refreshing \n- Overall - the writing of this paper is fantastic despite some missing necessary details (see weaknesses) and occasional lack of background\n\nWeaknesses, and Questions \n\n- It is unclear what the role of the sparsity is in the process? Can it be said that the proposed approach cannot work for shape spaces having factors of variation that are not necessarily spatially localized? If so this seems restrictive. To ask a more direct question - does the proposed approach only work for some conceptually similar version of the freaky torus? \n- I find the background on NRIC coordinates to be very short. To that aid, I think it is wise to provide a little more background on the time-discretization by Rumpf&Wirth (2015) for the generation of the exponential and logarithmic maps and hence to have at least some insight as to why that is a computationally expensive affair. \n- On the practical side, it would be much more convincing to see interpolation/smoothly varying factors like in Fig 6 for more practical manifolds shown earlier. \n- Figures 1,3 and 4 merit a better explanation. How does a more uniform density of points be indicative of a better model fit? And it is also important I think to also see the output of Sassen2020b for these figures (probably mention runtimes as well). In Table 1 what is the methodical difference between column 5 (Affine) and column 8 (Sassen 2020b) ?\n- There is very little evaluation reported on the computational gains of the proposed method \n- (suggestion) It would be interesting to explore some way to measure the generalization of the proposed approach, especially as a function of the size of training data. How does the quality of the fit change as I progressively increase the training samples from say 100 to 1000? ",
            "clarity,_quality,_novelty_and_reproducibility": "See strengths and weaknesses. Very good clarity and novelty. Unsure about reproducibility especially for very generic shape manifolds.",
            "summary_of_the_review": "Overall I find this to be a very interesting work, mainly for the novelty of the idea and clarity of discussion. However, the results seem to give an impression of \u201cproof of concept\u201d rather than convincing practical use cases. Put together I am weighing for a weak accept.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3193/Reviewer_SjVS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3193/Reviewer_SjVS"
        ]
    },
    {
        "id": "snD_AdZbvt",
        "original": null,
        "number": 2,
        "cdate": 1666706874376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666706874376,
        "tmdate": 1666706874376,
        "tddate": null,
        "forum": "F_EhNDSamN",
        "replyto": "F_EhNDSamN",
        "invitation": "ICLR.cc/2023/Conference/Paper3193/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper seeks to approximate a given high-dimensional manifold with a product of some smaller, simpler manifolds. It learns this approximate mapping using neural network based optimization. The paper motivates this approach by discussing the concept of PGA (which is essentially Tangent PCA as stated here) and a sparse Tangent PCA. The idea is to approximate the range space of exponential map from the dominant tangent subspace by a composition of some smaller maps \\psi_l. Both the smaller maps and their composition \\Psi are learnt from the data using fully connected CNNs. The paper provides some experimental validation using shape space of surfaces. It generates training data using space PGA or TPCA and exponential maps of the individual components. It then compares the quality of approximation with some other methods. ",
            "strength_and_weaknesses": "Strengths\n\nIf the idea and approximation hold, then there is an efficient way to parameterize elements of a complex manifold. \n\nWeakness: \n\nThere are a number of questions that arise from the paper. Perhaps it is my lack of understanding of the paper. \n\n--I am not quite sure when the approximation is feasible and meaningful? Especially given how the training data is generated. I get the impression that the paper is trying to approximate: \nExp_z(v1 + v2 + \u2026+ vn) \nBy the quantity\nPsi(Exp_z(v1) \\times Exp_z(v2) \\times \u2026. \\times Exp_z(vn)). \nExcept that the latter factors are learnt from neural networks. \nIf this is correct, perhaps there is a simpler way to state the problem. \n\n--- Can the authors provide an analytical nontrivial example where the factorization along principal directions is feasible? \n\n-- I assume that in most interesting manifolds this factorization is not valid but perhaps one can make a good approximation. Can one always do that? When should we expect the approximation to be good? This brings us to the three assumptions. \n\n-- The paper states several assumptions which seem plausible but not verified in the experimental setup. In the set up \u2013 \u201cWe assume that the manifold can be smoothly approximated by a product of much lower dimensional manifolds\u2026.\u201d When does this work and not work?\n\n-- For the shape manifolds studied here, do these assumptions hold? How can we verify them? For instance, the statement that \u201cThe different factor manifolds correspond to different spatial regions\u201d. Is this forced or is it a fortuitous outcome of SPGA? Also, the local shape variations are claimed to be \u201cindependent\u201d of global variations. Again, why? \n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe paper is mostly clear in the geometric tools that it uses. However, I feel that the overall goal can be better described. It can state the objective function upfront and use that to motivate the problem they are solving. The discussion about PGA, SPGA, etc, are useful but not necessary to understand the main goal. ",
            "summary_of_the_review": "\nTo the extent I understand the presentation, the paper is trying to find efficient ways to explore complex manifolds. The approach is ambitious (learn the overall exponential map using a CNN). However, it is not so clear when the approach will be work and when it will not. It would have been useful to take some simpler intuitive examples where the approach succeeds and fails, and discuss those cases. Then, move on to more complicated shape manifolds. The validity assumptions need to be discussed further. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3193/Reviewer_ADhs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3193/Reviewer_ADhs"
        ]
    },
    {
        "id": "Ib5w8hjr6jJ",
        "original": null,
        "number": 3,
        "cdate": 1666709894214,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666709894214,
        "tmdate": 1666709894214,
        "tddate": null,
        "forum": "F_EhNDSamN",
        "replyto": "F_EhNDSamN",
        "invitation": "ICLR.cc/2023/Conference/Paper3193/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper works on learning the exponential map of a shape space using deep neural networks.  The proposed method adopts a product structure to approximate shape manifolds using a sum of multiple low-dimensional submanifolds. The method is evaluated on synthetic data and manifolds extracted from data using sparse principal geodesic analysis. ",
            "strength_and_weaknesses": "Strength:\n\n+) This paper targets an interesting and challenging problem and the proposed method seems to make sense. \n\n+) The qualitative and quantitative results demonstrate the effectiveness of the proposed method. \n\nWeaknesses:\n\n-) Missing a good literature review.  The related work should provide a summary of existing works that tackle the same or similar problem; while the related work in this paper looks like a background. A missing discussion on existing methods makes it unclear where this paper stands in the literature. \n\n-) The equations in this paper are not clearly explained, which brings difficulty in understanding the details of this paper.  For example, in Eq. 1, what is \\bar{z}? Is it the mean of a set of shapes? If so, how to calculate it? The same question for the defintion of R2 score. Constructing an atlas for shapes is also non-trivial. In Eq. 3, what is l_e^b, l_e^a, etc.? In Eq.5, what is Z_e^0? It is unclear what is the input and output of the neural networks. The readers have to guess, perhaps explicitly pointing this out would make the paper more understandable and readable. \n\n-) In the loss function J_l(\\theta), what is exp operator? Is \"a\" a vector? then what is S^l? a set? but it says \"a random sample\"? In J(\\theta), the \\sum_\\omega_i(a^i) is the sum of tangent vector. Should it be the composition of several exp operators? When a point moves on manifolds, it goes step by step and each is a small step to make sure it stays on manifolds. How to understand the definition of J(\\theta)? ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of this paper needs further improvement. Because of missing details, the soundness of the proposed method is doubtful and the reproducibility of this paper is low. The originality of the work is hard to evaluate due to the missing appropriate discussion on related work. ",
            "summary_of_the_review": "The experimental results look good; however, the current shape of this paper is not ready for publishing. More efforts are desired to make this paper easy to understand and use by other researchers. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3193/Reviewer_VcAe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3193/Reviewer_VcAe"
        ]
    }
]