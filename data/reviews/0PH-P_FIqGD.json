[
    {
        "id": "p3Tymla7Zwt",
        "original": null,
        "number": 1,
        "cdate": 1666415633158,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666415633158,
        "tmdate": 1666415633158,
        "tddate": null,
        "forum": "0PH-P_FIqGD",
        "replyto": "0PH-P_FIqGD",
        "invitation": "ICLR.cc/2023/Conference/Paper1432/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work aims to study how to effectively achieve compact bilinear representations. To this end, the authors proposed a low-rank factorized bilinear pooling method. Specifically, as shown in Eq. (10), $k$ parallel 1x1 convolutions are used to reduce feature dimension before bilinear pooling, and then $k$ bilinear representations are summed following by a fully-connected layer. As such, size of bilinear representations could be controlled by 1x1 convolution and fully-connected layer. The experiments are conducted on several small-scale fine-grained image benchmarks.",
            "strength_and_weaknesses": "Strength:\n\n+: The idea on parallel 1x1 convolutions for reducing feature dimension before bilinear pooling seems bring clear improvement (Fig.5 and Table 1), which may be a feasible solution to balance performance and model complexity for bilinear pooling methods.\n\n+: On several small-scale fine-grained image benchmarks, the proposed method achieves competitive results with a relatively low representation dimension.\n\n+: The proposed method is clearly written and is easy to implement.\n\nWeaknesses:\n\n-: Could $\\mathbf{U}$ and $\\mathbf{V}$ in Eq. (10) be shared?\n\n-: The idea on combination of 1x1 convolutions and fully-connected layer for obtaining compact covariance representations appeared in [r1] (Section 3.5 and Table 4), where sequential 1x1 convolutions (not parallel) are used to reduce feature dimension before covariance pooling, and then a fully-connected layer is adopted after covariance pooling. It would better discuss relationship between the proposed method and [r1].\n\n[r1] Deep CNNs Meet Global Covariance Pooling: Better Representation and Generalization. IEEE T-PAMI, 2021.\n\n-: The concerns on experiments.\n\n(1)\tAre hyper-parameters $k$ and $h$ sensitive to various datasets? Particularly, could the proposed method be flexibly adopted to large-species classification (e.g., iNat2017 [r2]) with the same hyper-parameters?\n\n[r2] The iNaturalist species classification and detection dataset. CVPR, 2018.\n\n(2)\tI wonder what meaning of \u2018Param\u2019 in Table 2. Particularly, why Param of iSQRT-COV with backbone of ResNet-50 is 312M? To our best knowledge, iSQRT-COV is parameter-free itself. For dimension reduction, a 1x1 convolution (2048, 256) is used and contain about 0.5M parameters. If \u2018Param\u2019 in Table 2 contains one of classifier, it clearly varies for different datasets, while RK-FBP (2+) and RK-FBP (2+,3+) should have different parameters.\n\n(3)\tBesides parameters, FLOPs and running time are important metric to evaluate model complexity.\n\n(4)\tI would like to know how about performance of the proposed method under training-from-scratch setting (e.g., ImageNet-1K).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally very written. The idea on parallel 1x1 convolutions for reducing feature dimension before bilinear pooling may be a feasible solution. The proposed method is easy to implement. More evaluation and discussion seem be necessary.  ",
            "summary_of_the_review": "I have reviewed this work in previous conference, and the current version achieves clear improvement on writing and experiments. However, discussion on previous works and experimental evaluation could be further strengthened. Particularly, the authors should clarify meaning of \u2018Param\u2019 in Table 2. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1432/Reviewer_MKth"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1432/Reviewer_MKth"
        ]
    },
    {
        "id": "IEver_lN4a",
        "original": null,
        "number": 2,
        "cdate": 1666630973376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630973376,
        "tmdate": 1666630973376,
        "tddate": null,
        "forum": "0PH-P_FIqGD",
        "replyto": "0PH-P_FIqGD",
        "invitation": "ICLR.cc/2023/Conference/Paper1432/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on bilinear pooling in fine-grained image classification tasks. Bilinear pooling (that generates second-order features) is an effective method to capture details of fine-grained classes and achieves better performance than first-order features. Because bilinear features are high in dimension, many bilinear pooling methods adopt Hadamard product-based bilinear projection to reduce dimensionality. However, the paper argues that this misses \"a lot of possible projecting directions which will significantly harm the performance\". The paper then proposes low-rank factorized bilinear pooling (RK-FBP). The paper argues that RK-FBP \"does not miss any projecting directions\". The paper further extends RK-FBP to pooling features of multiple layers and forms multi-linear features. Experiments show the proposed methods achieves the state-of-the-art.",
            "strength_and_weaknesses": "Strength\n- Improving bilinear features is important as standard bilinear features (via outer product) is very high-dimensional.\n- Datasets used in the experiments are good.\n- The way to produce compact multi-linear features is interesting.\n\nBelow are weaknesses.\n\n- The paper does not discuss how to address the burstiness of bilinear/multi-linear features. To note, for bilinear features and low-rank bilinear features (Lin, 2015; Kong & Fowlkes, 2017), it is crucial to apply signed square root transform and L2 normalization. \n\n- It is confusing to state \"(RK-FBP) does not miss any projecting directions\" in the abstract. Authors should clarify, e.g., how to define \"any directions\".\n\n- In Introduction, it is unclear how existing factorized bilinear pooling (FBiP) methods address the burstiness problem. It seems the paper only mentions that these methods aim to reduce dimensionality of bilinear features. Can authors discuss?\n\n- Figure 1 is confusing. In this example, is it just a Fisher Discriminant Analysis? There is not much insight from this figure.\n\n- Figure 2: For methods that use Hadamard product, they learn U and V to optimize classification accuracy, which is the goal of the classification problem. Then, what does it mean by \"missing values\" in Hadamard project.\n\n- Section 4: The proposed Rank-K Factorized Bilinear Pooling (RK-FBP) seems like an extension of low-rank bilinear pooling (Kong & Fowlkes, 2017). This makes the novelty incremental.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality of the paper should be improved. Section 2 and 3 are hard to follow. The presentation should be organized in a more structured way. The novelty is incremental as it seems like an extension of low-rank bilinear pooling (Kong & Fowlkes, 2017). It is unclear how to reproduce the results as the paper does not discuss how to address the burstiness of bilinear features and does not provide open-source code.\n\n",
            "summary_of_the_review": "The paper improves over low-rank bilinear pooling (Kong & Fowlkes, 2017) so the novelty is limited. It does not discuss how to address burstiness of bilinear features, particularly multi-linear features. The literature reports that normalizing bilinear features (e.g., using signed square root and L2 normalization) is crucial. Presentation of the paper should be improved further. Therefore, the paper is rated as Reject.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics issues as I am aware.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1432/Reviewer_xdH1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1432/Reviewer_xdH1"
        ]
    },
    {
        "id": "DiXLUF64Fpy",
        "original": null,
        "number": 3,
        "cdate": 1666671722848,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671722848,
        "tmdate": 1666671722848,
        "tddate": null,
        "forum": "0PH-P_FIqGD",
        "replyto": "0PH-P_FIqGD",
        "invitation": "ICLR.cc/2023/Conference/Paper1432/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Many Factorized Bilinear Pooling (FBiP) uses Hadamard product-based bilinear projection. This paper reveals that the Hadamard product misses a lot of possible projection directions. This paper proposes a general matrix-based bilinear projection based on rank-k matrix base decomposition. This paper uses the proposed bilinear projection to design RK-FBP. To leverage high-order information in local features, the authors nest several RK-FBP modules. ",
            "strength_and_weaknesses": "Strengths\n------------\n- The rank-K projection, which performs dimension reduction with a sum of $k$ dimension-reduced vectors, is novel and effective.  \n- The mathematical discussion seems to be correct. \n- This paper shows the connection of the proposed method to existing BiP methods.  \n- This paper shows the effects of parameters and combination with several improved BP (SMSO, MPN, iSQRT-COV). \n- The performance of RK-FBP is higher than state-of-the-art with lower embedding dimensions. \n\nWeakness\n-------------- \n-  Reading this paper is heavy due to many equations. \n-  Some discussions are based on the speculation of authors. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n--------\nThis paper should improve readability. \n\nP3 above Eq.(4):  \u201creplace 1 in Eq.(2) by a learnable vector $p_r$\u201d, but Eq.(4) is $P$ and $_r$ is not used for $U$ and $V$. \n\nP5, 6 In the general bilinear projection in Eq.(9), the dimensions of $P, V, U$ differ from the previous sections. To clarify this fact, it would be better to use different notations. \n\nSec.10 \nNormalization: The discussion of 2DLDA seems to be irrelevant to the results of the normalization strategy.  \n\nThere are typos, eg.,  \n\nP3 Theorem 1. $R^{m\\times n}$ seems to be $R^{mn \\times 1}$\n\nP7 Rk-FBP \n\nP8 Figure 6: RK-RBP\n\nP13 $rank(U\\otimes V) = l_1 + l_2$ seems to be $l_1 l_2$\n\nQuality\n----------\nSec.2.2. argues that the $U \\otimes V$ does not cover the entire feature space of bilinear features. I agree that if $U$ and $V$ are fixed, it is correct. However, in deep learning, the dimension reduction and classifier are learned jointly. For Eq.(5), all dimensions in $vec(x_s y_t^T)$ are involved in learning dimension reduction followed by the classifier. \n\nP4. \u201cBecause those auxiliary parameters are implicit, we can not train them as other parameters of our model.\u201d, \u201cHowever, those free auxiliary parameters probably make the learned projection unsuitable.\u201d are based on the author\u2019s speculation.  \n\nNovelty\n----------\nThe rank-K projections seem to be novel and effective. In the rank-k projection,  each dimension of the output vector is given by $y_p = {\\rm Tr}(U_p^TXV_p )$ and the vector is concatenation of $p=1,..h$ values. In contrast to the conventional rank-1 projection $y = U^T X V$, each dimension of rank-K projection is a summation of k projected values, where $K$ is the rank of $U_p, V_p$. Thus, the rank-K projection is unlikely to miss the discriminative information of the original high-dimensional space when the output dimensions are small.\n\nReproducibility\n--------------------\nP7: The number of epochs, batch size, and learning rate for SGD are not shown. \n\n",
            "summary_of_the_review": "The Rank-k dimension reduction for bilinear pooling seems novel and effective compared with existing bilinear pooling. It appears to be technically correct. Parameter analysis and comparison with state-of-the-art seem to be sufficient. \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1432/Reviewer_yyVh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1432/Reviewer_yyVh"
        ]
    }
]