[
    {
        "id": "PMo93vYKyqa",
        "original": null,
        "number": 1,
        "cdate": 1666540518793,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666540518793,
        "tmdate": 1666540518793,
        "tddate": null,
        "forum": "oztkQizr3kk",
        "replyto": "oztkQizr3kk",
        "invitation": "ICLR.cc/2023/Conference/Paper3952/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a differentiable NAS approach based on DARTS. The main discovery is that \"DARTS suffers from a specific structural flaw\" so that it \"gives an unfair advantage to layers closer to the output\". To solve this issue, a new regularization method is proposed by \"harmonizing operation selection via aligning gradients of layers\". Experiments look good on a few benchmarks.",
            "strength_and_weaknesses": "Strengths\n1. The paper is well written.\n2. The studied problem (why DARTS often collapses) is important for the NAS community.\n3. The provided explanation is somewhat new.\n\nWeaknesses\n1. One of the most significant issues for all recent research in DARTS (not only for this paper) is the limited ability to justify the statements by experiments. Let me explain. The search space of DARTS is relatively small and well studied. The community knows a few tricks (e.g. increasing the depth, or constraining the number of skip-connections) that can easily generate strong architectures in the space. On the opposite, the gap among recognition accuracy of different methods is very small, e.g. 0.1% on CIFAR10 and 0.2-0.3% on ImageNet-1K. This makes it quite difficult to judge whether the improvement indeed comes from the proposed method (or simply an implementation trick).\n\nGoing deeper, we know that DARTS underwent a few approximations, e.g. the 1st-order method discards higher-order gradients, and the computation of the inverse Hessian matrix is **largely** approximated -- see the analysis in [Zela et al., ICLR20] and [Bi et al., arXiv19]. The question is that, the proposed method used another strong approximation to alleviate the issue caused by an existing approximation. Integrating it with the above concerns, I cannot believe that the marginal improvement (the authors may argue that the improvement on CIFAR100 is relatively large, but this is because the CIFAR100 is not well tuned by other methods) indeed comes from the proposed method.\n\nTo further validate the effectiveness, the authors may try to (i) use a more complex search space (e.g. the spaces constructed in [Bi et al., arXiv19] -- although the authors investigated the spaces offered by [Zela et al., ICLR20], but these spaces are actually even simpler than the original DARTS space) so that the design tricks have weaker impacts, or (ii) report search results with more epochs (e.g. 200 or more), since the collapse issue happens more frequently.\n\nIt is also good to discuss the relationship to other works that discussed the stability of DARTS, e.g. DARTS+ [Liang et al., arXiv], XNAS [Nayman et al., NeurIPS19], Fair DARTS [Chu et al., ECCV20], GOLD-NAS [Bi et al., arXiv20].",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good.\n\nQuality: the weakness mainly lies in the justification of the proposed explanation.\n\nNovelty: moderate -- the proposed explanation for collapse looks new, but it was not well validated.\n\nReproducibility: code was not provided, and the formulation looks complex, but I think it is reproducible given the code.",
            "summary_of_the_review": "I think the efforts in solving the collapse problem are good and the explanation is interesting, but the empirical results are insufficient to fully justify the statements.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3952/Reviewer_iL42"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3952/Reviewer_iL42"
        ]
    },
    {
        "id": "xn5t2n1PVld",
        "original": null,
        "number": 2,
        "cdate": 1666758819126,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666758819126,
        "tmdate": 1670170718737,
        "tddate": null,
        "forum": "oztkQizr3kk",
        "replyto": "oztkQizr3kk",
        "invitation": "ICLR.cc/2023/Conference/Paper3952/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes two new regularization terms to prevent performance collapse by harmonizing operation selection via aligning gradients of layers. The extensive experiments demonstrate its superiority. ",
            "strength_and_weaknesses": "Strength:\nThe observation and analysis of performance collapse in darts is from a novel perspective.  \nAnd the regularization terms used in bi-level darts are solid and useful. \n\nWeaknesses:\n1. In table 3, CIFAR10 and SVHN dataset on S1 search space, the performance of the proposed method is worse than previous methods.  Seem there are no detailed analyses of this phenomenon.  It is better to show the visual comparisons of searched architectures between previous methods and the proposed method (if possible), and compare the values of layer alignment metric that these previous methods can obtain, on S1 search space (if possible). \n2. What's the effect when putting the proposed regularization term for upper-level optimization on  $\\alpha$  in bi-level Darts?  The proposed metric is currently only used in lower-level optimization for $w$.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear and the derivation of the optimization steps on the modified bi-level darts is solid.\n",
            "summary_of_the_review": "I give 6 due to several concerns in the weakness part. I may change the score according to the author's response. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3952/Reviewer_iyeQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3952/Reviewer_iyeQ"
        ]
    },
    {
        "id": "lfYcaiGt4w",
        "original": null,
        "number": 3,
        "cdate": 1666764807080,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666764807080,
        "tmdate": 1666764860123,
        "tddate": null,
        "forum": "oztkQizr3kk",
        "replyto": "oztkQizr3kk",
        "invitation": "ICLR.cc/2023/Conference/Paper3952/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new differentiable architecture search (DARTS) variant to alleviate the performance collapse problem in DARTS. The authors first give theoretical and empirical analysis of the convergence of DARTS, which shows that the weight-sharing framework causes the undesirable skip-connection dominance problem in posterior layers. Based on the analysis, the paper introduces two regularization terms to alleviate the performance collapse problem via aligning gradients of layers.\n",
            "strength_and_weaknesses": "Strengths:\n1. This paper introduces a function named layer alignment to measure the correlation between the gradient of layers in DARTS, theoretical and empirical results show that this function might be a good indicator of the performance collapse situation. This could be a benefit for future works to better understand DARTS.\n\n2. The proposed regularization terms are simple to adopt, and they can alleviate the performance collapse problem according to the experimental results.\n\nWeaknesses:\n1. The performance collapse problem in this paper is not novel, which has been widely discussed and solved by many works since 2019.\n\n2. The performance improvements are marginal. On NAS-Bench-201, the performance of the proposed method is the same as previous work $\\beta$-DARTS. In Table 2, the improvements on CIFAR-10 and CIFAR-100 are marginal, the results on ImageNet are not state-of-the-art. \n\nQuestions:\n1. Can the authors provide the search cost of the proposed method? Besides, extensions of the method to other DARTS variants (e.g., some variants that can directly search on large-scale datasets) are preferred to better validate the method.\n\n2. In Figure 4 (c), why the performance drops after 100 epochs? Does the proposed method just delay the performance collapse process?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is presented well. ",
            "summary_of_the_review": "Overall, I think this paper is a borderline paper. My major concern of this paper is the limited contribution of novelty as well as the performance to the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3952/Reviewer_2qFj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3952/Reviewer_2qFj"
        ]
    },
    {
        "id": "Oi5JxECTCE",
        "original": null,
        "number": 4,
        "cdate": 1666791313329,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666791313329,
        "tmdate": 1666791313329,
        "tddate": null,
        "forum": "oztkQizr3kk",
        "replyto": "oztkQizr3kk",
        "invitation": "ICLR.cc/2023/Conference/Paper3952/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper finds the main reason behind performance collapse in DARTS. A new algorithm named  lambda-DARTS is proposed, which is able to generate better performance compared with other methods. Specifically, two new regularization terms are proposed to prevent performance collapse by harmonizing operation selection via aligning gradients of layers. The lambda denotes the dubbed layer alignment, which is a measure to show the correlation between the gradient of each layer corresponding to the architecture parameters.",
            "strength_and_weaknesses": "Strength: \n1. It has a good starting point and the results of proposed methods can solve the mentioned issues to some extent. \n\n2. The writing is good and easy to understand. A good analysis of lambda w.r.t. the DARTS performance collapse is provided. Low value for lambda means the optimal architecture corresponding to each layer varies wildly. This work seems to be the first to investigate DARTS from the weight-sharing framework and convergence conditions.\n\nWeakness: \n\nFor DARTS tasks, the search cost (GPU days) is a crucial evaluation. But the authors did not mention it in the experimental results.\n\nAlthough the test accuracy is better than other methods, the number of parameters seems to be larger than most of the previous methods. Since the parameters are one of the factors in Table 2, I wonder how to explain this factor.\n\nTables 1 and 2 reports test accuracy, while Table 3 reports test error rate. I wonder if there is any cherry pick?\n\nAlso the performance of ImageNet is not reported in these tables. Is is known that NAS runs slow and how best to scale NAS to run on large datasets such as ImageNet is crucial for real-world applications. And as I know many DARTS methods can only search on toy datasets such as CIFAR-10. Can you report if your NAS method can be trained on large-scale dataset?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good.\n\nQuality: good.\n\nNovelty: good.\n\nReproducibility: No experimental setup and implementation in the paper.\n\n",
            "summary_of_the_review": "Overall, I think it is a good paper with a good starting point and clear writing. I mentioned 2 doubts in the weakness part.\n\nAlso, there are many existing works on the performance collapse of DARTS. For example, the MS-DARTS which leverages the mean shift to prevent performance collapse at the final discretization step of DARTS.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3952/Reviewer_hXod"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3952/Reviewer_hXod"
        ]
    }
]