[
    {
        "id": "f0ZHaZ6VZjn",
        "original": null,
        "number": 1,
        "cdate": 1666641004506,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641004506,
        "tmdate": 1666641004506,
        "tddate": null,
        "forum": "PolHquob8M7",
        "replyto": "PolHquob8M7",
        "invitation": "ICLR.cc/2023/Conference/Paper1076/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, they propose a new formulation of scaled dot-product attention for token by token inference on continual stream. That reduces the time complexity from O(n^2 d) to O(n d) while keeping the outputs and weights identical to the original Transformer outputs and weights.",
            "strength_and_weaknesses": "Strengths:\n- The proposed approach is novel and provides an efficient and simple solution for continual data streams.\n- It is a significant contribution to reduce the time and memory complexity per prediction while keeping the performance on par.\n- To the best of my knowledge, the statements are correct.\n\nWeaknesses:\n- I had difficulties to follow the paper. I think a re-organization or restructuring of the sections is needed. At least in the beginning of the Section 3, there could be an outline of the section, so we can follow the story in the section.\n- Besides, the experiment results could be analysed in more detail. It is difficult to understand whether the proposed method's accuracy is sufficiently close to the baselines or what are the outcomes of the experiments.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper proposes a novel approach for the transformer efficiency on continual data streams. But the writing is not so clear. I think with a revision of the text, it can be a good paper.",
            "summary_of_the_review": "I think the paper's contribution is important for community, it only requires a more clear presentation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1076/Reviewer_RfrZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1076/Reviewer_RfrZ"
        ]
    },
    {
        "id": "JWISmm1xMCv",
        "original": null,
        "number": 2,
        "cdate": 1666643050018,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643050018,
        "tmdate": 1669675925329,
        "tddate": null,
        "forum": "PolHquob8M7",
        "replyto": "PolHquob8M7",
        "invitation": "ICLR.cc/2023/Conference/Paper1076/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a novel transformer model of the Scaled-Dot-Product Attention. The authors claim that the proposed continual transformers will accelerate the stream processing of transformer architectures on time-series data. The continual transformer architectures were experimentally validated in Online Action Detection and Online Audio Classification.",
            "strength_and_weaknesses": "Strength:\n- The paper is well presented and the proposed architecture is explained in detail.\n- The paper is well positioned with respect to prior work\n- The experiments show that the proposed model reduces the time and memory complexity per predictions.\n\t\nWeaknesses:\n- The motivation in some aspects is not clear. For example, the authors suggest the use of Recycling Positional Encoding to accommodate progressive caching of partial attention results for continual data streams. In section 3.7 they said that \u201cThere have been multiple prior works (Shaw et al., 2018; Huang et al., 2019; Dai et al., 2019) which create relative encodings by augmenting the SDA with positional offsets between query and keys. While such a modification to the continual attentions is possible, it hinders compatibility with the regular SDA.`` Why is Recycling Positional Encoding more helpful here and how does the previous model hinder the compatibility? \n- The authors said that the proposed model works better with shallow architecture; is it still applicable to deeper architecture?\n- How is the information accumulated from other tokens prior to classification?\n- Uni-directional Recurrent Neural Networks are an example of Continual Inference Networks. How is the proposed model improved upon the Uni-directional Recurrent Network? For example, in SDA prior step features must be cached, re-loaded, and re-processed by the transformer in each step in correspondence with a predefined window-size of prior steps. Don't the authors think this is very complicated compared with RNN?\n- The experiments show that the proposed model performs well in time and memory reduction but it does not outperform SOTA. Can the authors explain more about that?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and is clear to follow. However, I found that there is a lack of motivation behind some points, such as the motivation for using Recycling Positional Encoding, and architecture modification. This makes it hard to draw conclusions from experiments.",
            "summary_of_the_review": "Overall, I think this is an interesting paper even though the reported results are slightly lower than SOTA. The proposed model is still able to reduce the time complexity. I have some concerns related to motivation and the clarity of presenting or reasoning around some points that I explained in the weaknesses section.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1076/Reviewer_DN9Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1076/Reviewer_DN9Y"
        ]
    },
    {
        "id": "mtedilp6e9",
        "original": null,
        "number": 3,
        "cdate": 1667162517987,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667162517987,
        "tmdate": 1667162517987,
        "tddate": null,
        "forum": "PolHquob8M7",
        "replyto": "PolHquob8M7",
        "invitation": "ICLR.cc/2023/Conference/Paper1076/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an architectural modification of the transformer module, suitable for online predictions. The modification is meant to prevent heavy re-computations when new tokens arrive.\n\nAuthors propose two flavors of the continual transformer: one in which previous outputs are updated (revised), and one focused only on the prediction corresponding to the new token.\n\nThe paper clearly explains the computational gains of the continual transformer and tests it on a battery of online prediction datasets. The results show the benefits of the continual transformers on tasks that require shallow architectures.",
            "strength_and_weaknesses": "Strenghts:\n - novelty: to my knowledge there are no previous works trying to optimize computation inside the transformer architecture for online predictions\n - well written paper\n - fair discussion of the empriical results and of the limitations (the new architecture benefits more shallow models)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written in a clear manner.\nThe idea is novel.\nFormulas and explanations in section 3 should be enough to reproduce the implementation and the results.",
            "summary_of_the_review": "Online prediction is crucial for a large set of problems, therefore the work is novel and of interest, and I suggest for the paper to be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1076/Reviewer_bhNe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1076/Reviewer_bhNe"
        ]
    }
]