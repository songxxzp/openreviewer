[
    {
        "id": "vExOSmtEcyl",
        "original": null,
        "number": 1,
        "cdate": 1666343493054,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666343493054,
        "tmdate": 1666343493054,
        "tddate": null,
        "forum": "PldynS56bN",
        "replyto": "PldynS56bN",
        "invitation": "ICLR.cc/2023/Conference/Paper627/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the problem of computer vision with convolutional neural networks (CNNs). The authors propose to contextualize the CNN by incorporating category priors. This is achieved by adding two new features to the CNN architecture. \n\nSome convolutional layers are contextualized by deforming the convolution grid using predicted spatial offsets, and by dynamically changing the kernel weights using predicted weight offsets. The offsets are predicted from the visual features extracted at the previous layer and a context prior. \n\nThe introduced context prior is computed by a new context layer which performs two tasks. First, from the visual features are projected to the space of classes embeddings, and a cosine similarity is measured to retain the top-n classes, which will be used in the subsequent contextualized convolutions. Then, the embeddings for the retained classes are merged to compute the context prior. \n\nThrough experiments on ImageNet and two adaptation tasks, the author show that the proposed model outperforms existing methods with a limited increase of computational complexity on GPU. An ablation study shows the importance of the various features proposed.",
            "strength_and_weaknesses": "Strenghs:\n  - The idea is simple and well-explained, and extends and combines nicely the notions of deformable convolutions and dynamic convolutions with context\n  - The experiments report better results with a limited impact on computational efficiency\n  - The method is simple enough and looks easy to integrate\n\nThe main weakness of the idea, compared to the conventional CNN backbones lies in the adaptation of such a model to different tasks. It might be useful to only adapt the top layer(s) for a new task. In the presented experiments, it looks like the whole network needs to be fine-tuned. One experiments is reported in which class embeddings are frozen. It would be interesting to also measure the accuracy after only fine-tuning the top layer.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and easy to read and to follow. \nThe analysis are useful and provide the right level of illustration.\n\nFinally a few things were not entirely clear to me\n  - is the merging network the same for each stage?\n  - are the class embedding constant in the whole network or are there different class embeddings after each stage?\n\nThe idea seems closely related to the topics of deformable and dynamic convolutions and, to some extent, of attention, which is clearly detailed in section 2. The main novelty lies in the conditioning of these techniques on category priors which looks interesting and novel enough.\n\nThe text looks clear enough for reproducibility, besides a few details. Reproducibility should be ensured with the availability of the code, as mentioned by the authors.",
            "summary_of_the_review": "The idea of using some context to compute the offsets of a deformable convolution is interesting and could be seen as a form of attention. For some application, it might even be more interesting to be able to select the list of classes to give to the network.\n\nCombining this idea with the conditioning of a dynamic convolution is equally interesting.\n\nAdditionally to the GPU results, the efficiency on CPU would be nice to have, compared to an efficient convolution operation, as well as memory considerations, although this potential drawback is shared with deformable and dynamic convolutions.\n\nOverall, I found the paper easy to read, the analysis convincing and the idea simple and interesting, worth sharing with the community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper627/Reviewer_2om9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper627/Reviewer_2om9"
        ]
    },
    {
        "id": "g4gzlRoHDT",
        "original": null,
        "number": 2,
        "cdate": 1666560355168,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666560355168,
        "tmdate": 1668619125209,
        "tddate": null,
        "forum": "PldynS56bN",
        "replyto": "PldynS56bN",
        "invitation": "ICLR.cc/2023/Conference/Paper627/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a neural network called contextual convolutional network, which adds classification layers at multiple stages and uses the top (most probable) class embeddings in following stages. The proposed layers / blocks are added to ConvNeXt models and tested on several mainstream / large-scale datasets (ImageNet, MS COCO, Kinetics-400).",
            "strength_and_weaknesses": "Strength:\n- The paper is fairly easy to follow.\n- The experiments are conducted on representative datasets.\n\nWeaknesses:\n- The authors seemed to have missed some competing works bearing almost identical names [A, B] based on the idea of leveraging the context during convolution. The authors should point the differences / novelty with respect to these existing works. To me, it seems that the proposed convolution can be seen as a generalization of the convolution proposed in [A]. I also believe a head-to-head comparison with [A, B] is in order. At the moment, I have doubts about the novelty of the method.\n- An analysis of the failure cases is missing. The authors should also point out when the proposed method is expected to fail.\n- Are the performance improvements statistically significant? Are the improvements consistent over multiple runs? Such questions need to be answered to validate the shown improvements.\n\nMinor issues / language corrections:\n- The citation for SEnet is missing.\n- \u201chave a output\u201d => \u201chave an output\u201d.\n\n[A] Duta et al., \"Contextual Convolutional Neural Networks\". In: ICCVW, 2021.\n[B] Marwood et al., \"Contextual Convolution Blocks\". In: BMVC, 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear, but I am not sure about the novelty of the method.",
            "summary_of_the_review": "Currently, I believe the weaknesses slightly outweigh the strengths.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper627/Reviewer_CrqV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper627/Reviewer_CrqV"
        ]
    },
    {
        "id": "3nse2oL94rA",
        "original": null,
        "number": 3,
        "cdate": 1666579311799,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666579311799,
        "tmdate": 1666579311799,
        "tddate": null,
        "forum": "PldynS56bN",
        "replyto": "PldynS56bN",
        "invitation": "ICLR.cc/2023/Conference/Paper627/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to increasingly refine at each CNN stage the knowledge about the class of a given image and use this information to build a more discriminative model. The more discriminative model is build with two contributions: i) add variations to the static CNN filters that are classes specific (based on the set of classes that are selected so far) and on the sampling location similar to deformable CNN. Both contributions seem important to boost classification results on imageNet and also other tasks,",
            "strength_and_weaknesses": "\\+ The paper is well written and clear to understand\n\\+ Related work is complete\n\\+ Experimental results are thorough with the important ablation studies. \n\n\\- The convolution offsets is the same as deformable convolutions (Dai et al. 2017). How does the proposed method compares with Deformable convolutions?\n\\- The proposed approach has as hyperparameters the number of classes to consider on each layer. Did the authors think about considering it as a single scaling factor instead of an independent value for each stage?\n\\- In my understanding, in the contribution and title I would put more emphasis on the fact that the following stage of convolution depend on the results of the previous, thus rendering convolutions dynamic in a smart way.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity:\nThe paper is simple and clear. I do not see any issues here.\n\n- Quality:\nThe quality of the results is self-explanatory based on the excellent results on ImageNet.\n\n- Novelty:\nIn my opinion the contribution of this paper is very good as in my knowledge it is the first work that shows that conditioning on class knowledge on the different stages of a CNN has a positive effect on results. This shows that there is a need of more specialized layers towards the end of a CNN.\n\n- Reproducibility:\nI am happy with the level of reproducibility presented in the paper.\n",
            "summary_of_the_review": "I consider this paper a solid and balanced contribution in the field of convolutional neural networks.\nIt has:\n- A promising contribution\n- Goof and clear presentation\n- Complete related work (in my knowledge)\n- Excellent results & ablation studies",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper627/Reviewer_oBzi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper627/Reviewer_oBzi"
        ]
    },
    {
        "id": "kYXrRib5yS",
        "original": null,
        "number": 4,
        "cdate": 1667392621138,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667392621138,
        "tmdate": 1667392621138,
        "tddate": null,
        "forum": "PldynS56bN",
        "replyto": "PldynS56bN",
        "invitation": "ICLR.cc/2023/Conference/Paper627/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new Convolutional Neural Network, Contextual Convolutional Network, for visual recognition. Most existing convolutional backbones follow the representation-toclassification paradigm, where input representations are first generated by category-agnostic convolutional operations and then fed into perceptual classifiers (e.g., classification and segmentation).\nTop-k likely classes are encoded as a contextual prior vector. Based on this vector and the previous features, convolution offsets and kernel weights are generated. The new convolutions can easily replace their plain counterparts in existing CNNs and be trained end-to-end without supervision.\n",
            "strength_and_weaknesses": "Strength \nmodified Convolutional Neural Network, named Contextual Convolutional Network\ndeviate from this classic paradigm and propose to augment potential category memberships as contextual priors in the convolution for contextualized representation learning\ntrained end-to-end by standard back-propagation without additional supervision\n\nWeaknesses\nThe method is not very novel and can not be regarded as a new CNN, \nIt is a modification of a current method that provides better results in same cases\nthe method is not well presented and it can be included more details about architecture\nMore explanations on the experiments and the discussion on results\nNo discussion about computation and the architecture performance\nNo comparison with other CNN(classic and dynamic as their definition in paper) for performance\n",
            "clarity,_quality,_novelty_and_reproducibility": "This manuscript contains some intriguing ideas, one of which is this. On the other hand, the paper is difficult to follow, and the primary concept that underpins the approach that is being suggested is not simple to grasp. ",
            "summary_of_the_review": "The paper presented a modified architecture for CNN and it can be improved by doing some changes on presentation and adding more explanation on architecture and experiments to make the work more mature!",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper627/Reviewer_y6tk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper627/Reviewer_y6tk"
        ]
    }
]