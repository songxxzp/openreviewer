[
    {
        "id": "b2hUlubfPY",
        "original": null,
        "number": 1,
        "cdate": 1666531928906,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666531928906,
        "tmdate": 1669111081918,
        "tddate": null,
        "forum": "ZIkHSXzd9O7",
        "replyto": "ZIkHSXzd9O7",
        "invitation": "ICLR.cc/2023/Conference/Paper2602/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper takles the problem of overfitting in the RL setting, where early stopping is not directly applicable due to the evolution of the dataset. The proposed method adjusts the update to data (UTD) ratio during training which trades off between underfitting and overfitting. The experiments are conducted using a model-based RL algorithm (DreamerV2) on the DeepMind Control Suite and Atari 100k benchmark, which showed competitiave resutls compared with extensive hyperparameter search.  \n\n",
            "strength_and_weaknesses": "Strength: \n* The paper is well written and easy to follow. \n* The proposed method is easy to use in practice. \n* The experiments showed promising results with the DreamerV2 algorithm. \n\nWeakness: \n\nthe novelty of the paper is limited. The proposed method is a heuristic method which does not add much theoretical contribution. The form of the UITD ratio update is intuitive but it is not clear why this particular multiplicative form is chosen among other possibilities.  \n\nThe choice of c is not well explained or examined, I am also curious to know whether having an adaptive form of c that changes over time helps. \n\nAll the results are based on one model-based RL algorithm (DreamerV2).\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the paper is well presented and well written. But the novelty is somewhat limited. ",
            "summary_of_the_review": "The paper is easy to follow and the proposed method leads to easy applicability of RL algorithms in real life, which will bring benefits especially to practitioners. However, the novelty of the paper is limited and it does not provide much insights into a principled way of trading off under- versus overfitting in RL problems. \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2602/Reviewer_V4T4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2602/Reviewer_V4T4"
        ]
    },
    {
        "id": "FOPIwCUs3wi",
        "original": null,
        "number": 2,
        "cdate": 1666626206731,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626206731,
        "tmdate": 1666626206731,
        "tddate": null,
        "forum": "ZIkHSXzd9O7",
        "replyto": "ZIkHSXzd9O7",
        "invitation": "ICLR.cc/2023/Conference/Paper2602/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper looks at overfitting in world model learning. It proposes a mechanism to dynamically update the update to data ratio during training. This can be seen as an analogue of early stopping in supervised learning. The paper introduces a small validation replay buffer used for adjusting the UTD ratio and adapts it based on the target validation loss. The paper evaluates the approach building on the dreamerv2 method. Empirically this approach is competitive with dreamerv2 with a tuned UTD ratio.",
            "strength_and_weaknesses": "Strengths:\n* simple idea that is relatively easy to implement and could potentially be extended to other settings as well.\n* clear, well-written paper with extensive empirical evaluation.\n* promising empirical results. The fact that the approach is competitive with a UTD ratio tuned by grid search is particularly promising because tuning the UTD is often very costly and time-consuming in practice.\n\nWeaknesses:\n* in appendix B there seems to be some special casing for the control suite. This detracts somewhat from the simplicity of the methods. Can you show empirically what happens when this is removed?",
            "clarity,_quality,_novelty_and_reproducibility": "* clearly written paper\n* no concerns about reproducibility\n* thorough empirical evaluation\n* to the best of my knowledge this is the first paper introducing a variable UTD ratio.",
            "summary_of_the_review": "Nice paper that thoroughly evaluates a simple idea to dynamically update the UTD in world model based algorithms and shows strong empirical results. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2602/Reviewer_uze4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2602/Reviewer_uze4"
        ]
    },
    {
        "id": "jc6MJz-lzL",
        "original": null,
        "number": 3,
        "cdate": 1666668592481,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668592481,
        "tmdate": 1669044640007,
        "tddate": null,
        "forum": "ZIkHSXzd9O7",
        "replyto": "ZIkHSXzd9O7",
        "invitation": "ICLR.cc/2023/Conference/Paper2602/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to improve the training of Model Based Reinforcement Learning methods by introducing an automatic method of setting/dynamically tuning the update-to-data (UTD) ratio, a key hyper parameter which controls how much the world model over- or under-fits the training data.  They propose using a held out set of validation samples, updated throughout training, to estimate whether the world model is over- or under-fitting and to update the UTD ratio accordingly.  This method both enables the parameter to be set automatically, avoiding the need for hyper parameter tuning, and also allows the parameter to be dynamically updated as the training stage evolves.  They present results using their approach with DreamerV2 on both the DeepMind Control Suite and the Atari100k benchmark, demonstrating comparable performance on Atari100k and a slight improvement on DeepMindControl Suite when compared to training with the best fixed value of UTD when it is carefully tuned.  Additionally they show that the final performance on both environment is highly dependent on carefully tuning UTD when using a fixed value, but the performance of their method is robust to a wide range of values for its primary hyper-parameter.",
            "strength_and_weaknesses": "Strengths:\n- The method, though intuitive, is not standard practice and they make strong case for why DUTD could benefit all MBRL practitioners by improving their training performance while reducing the amount of manual tuning necessary.  Such improvements also enable fairer comparisons between future methods by removing one of many possible confounding factors in their evaluation.\n- Thorough experimentation and presentation of findings, including their many plots in the Appendix supports both drawing additional conclusions and reproducibility. Reporting results using methods described in Agarwal et al 2021 supports fair comparisons and a stronger ability to draw meaningful conclusions.\n- Not mentioned in the work, but the method also allows for the UTD parameter to be adaptively set for each individual task automatically, with different tasks using substantially different average values (as seen in Figure 12).\n\nWeaknesses:\n- These results are only evaluated on DreamerV2, therefore it is unclear if they extend to other MBRL methods or are Dreamer specific.\n- The claim that DUTD is more robust to hyper parameters than a fixed value of UTD is supported by comparing DUTD to the default fixed value of UTD which was already demonstrated to be sub-optimal.  Since it was already demonstrated that DUTD outperforms the default UTD with the default hpms then it is unsurprising that this continues to be the case when changing the hpms.  To isolate whether the adaptive choice of UTD as done by DUTD is more robust to varying hpms (the goal of this ablation), it should be compared to the best fixed UTD value as chosen by careful tuning.\n- Similarly, it seems a possible takeaway from the longer runs is the value of adaptive UTD setting as we see in Figure 13 that the value chosen by DUTD varies throughout training.  However, because the comparison in Figure 8 is to the default value instead of the best tuned value of UTD, it is unclear whether there is benefit from adaptively setting UTD throughout training or just from choosing a good value of UTD.",
            "clarity,_quality,_novelty_and_reproducibility": "- The method, primary contributions, and demonstration of effectiveness is presented clearly.  \n- The experiments demonstrate the high quality of their method for automatically tuning the UTD parameter and the importance of this choice.\n- The idea of using a validation set to identify overfitting and perform early stopping is not novel, but their method for applying this concept to MBRL is new as far as I know.\n- Their thorough description of their method, hyperparameters and results, and their use of multiple seed so provide error bars makes this work easily reproducible, as will their code release. \n\nSmall Note:\n- In the final paragraph in Section 2, missing word: \u201cIn contrast to these approaches we [present] our method\u201d",
            "summary_of_the_review": "This paper (1) provides strong evidence that tuning the UTD parameter substantially impacts the outcomes of DreamerV2 training (2) provides a method for automatically tuning the UTD parameter that consistently matches or improves the performance of the best fixed value of UTD when chosen with careful tuning.  \n\nThis method is likely broadly applicable to MBRL practitioners and can be used as a standard tool for effectively training world models, saving time/resources in hyper parameter tuning and making results more directly comparable by removing the confounding factor of how well tuned the UTD parameter is.  However, it is not clear that this is the case since the method has only been evaluated on one such method, DreamerV2 (my main concern when making my acceptance recommendation).  Additionally, there isn\u2019t clear evidence for the claim that *adaptively* setting UTD throughout training is more beneficial than choosing the optimal fixed UTD (my main concern when evaluating correctness at 3 out of 4).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2602/Reviewer_U8TF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2602/Reviewer_U8TF"
        ]
    }
]