[
    {
        "id": "v_mRwziFZr1",
        "original": null,
        "number": 1,
        "cdate": 1665840931399,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665840931399,
        "tmdate": 1669710786435,
        "tddate": null,
        "forum": "xeg2fW5E2l3",
        "replyto": "xeg2fW5E2l3",
        "invitation": "ICLR.cc/2023/Conference/Paper2268/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new set-level self-supervised learning (SLSSL) method for combating noisy labels. By corrupting the labels of each training mini-batch, the proposed SLSSL enables the model to exhibit sufficient robustness. The comparison with SOTA methods demonstrates the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Pros:\n\n1. The idea of conducting set-level self-supervised learning for tackling label noise is novel and interesting.\n2. The proposed algorithm is rigorously explained.\n3. The experiments are convincing.\n\nCons:\n\n1. I\u2019m not fully understand the sentence \u201cAs for instance-level pretext tasks (e.g., rotation prediction or contrastive-based instance discrimination), they are expected to produce proper representations from unlabeled data; it is not clear whether such techniques would result in robust representations when handling input data with noisy labels\u201d in the introduction. In fact, the pretext tasks usually do not depend on labels, so I\u2019m not clear why the data with noisy labels might affect the model output aided by this technique. I cannot see the connections between these two things.\n2. It would be better if pseudo code of the proposed SLSSL can be provided.\n3. I\u2019m not fully understand why the designed set-level augmentation and label corruption way (e.g. Eq. 1) can help resist label noise. It would be more helpful if some explanations can be provided.\n4. In fact, for label noise learning, the estimation of label transition matrix is a quite challenging task. I want to see the estimation accuracy when the data dimension is high. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written. It would be better if the advantages of set-level augmentation over instance-level augmentation can be explained in a clearer way. The code is not provided.",
            "summary_of_the_review": "I do not have major concerns on this paper. It seems that the proposed algorithm works well.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_szQw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_szQw"
        ]
    },
    {
        "id": "6Cr1e_FB1FB",
        "original": null,
        "number": 2,
        "cdate": 1666862666526,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666862666526,
        "tmdate": 1666862666526,
        "tddate": null,
        "forum": "xeg2fW5E2l3",
        "replyto": "xeg2fW5E2l3",
        "invitation": "ICLR.cc/2023/Conference/Paper2268/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the task of noisy label learning (NLL), which proposes set-level self-supervised learning (SLSSL) to model noisy label data.\n\nSLSSL performs self-supervised learning at mini-batch levels with observed noisy labels. \n\nThe proposed method relabel the samples from label i using label j, which is called label corrupt, an augmentation method.\n",
            "strength_and_weaknesses": "\nStrength:\n\nThis paper tries to use set-level self-supervised learning (SLSSL) to address noisy label learning (NLL) tasks.\n\nThe proposed SLSSL can be utilized for sample reweighting technique based on a novel high-impact principle.\n\n\n\nWeakness:\n\nThe paper writing is a bit confusing, for example, \u201cadditional supervisory signal can be derived to improve robustness of the model against label noise.\u201d and \u201cit is not clear whether such techniques would result in robust representations when handling input data with noisy labels.\u201d These two points contradict each other.\n\nThis paper lacks sufficient theoretical support. Can you provide a theoretical analysis of this method regarding stability and generalizability?\n\nExperiments should be conducted on more and larger data sets.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper lacks reasonable motivation. \n\nWhy EMA and labels corrupt are used without a clear explanation?\n\nThe connection between corrupting the labels and model robustness remains to be explained. In my opinion, label corrupt may introduce unnecessary noise and confuse the model even more.\nHow is label corrupt carried out in the experiment?\n \nNo code is provided in this paper, and innovation is limited.\n",
            "summary_of_the_review": "\nThe contribution of this paper is limited and the paper lacks a reasonable explanation of the proposed modules.\n\nIt is suggested to explain the motivation of this paper in detail.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_yAaK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_yAaK"
        ]
    },
    {
        "id": "wD3BtVvLys",
        "original": null,
        "number": 3,
        "cdate": 1666887661318,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666887661318,
        "tmdate": 1666888038192,
        "tddate": null,
        "forum": "xeg2fW5E2l3",
        "replyto": "xeg2fW5E2l3",
        "invitation": "ICLR.cc/2023/Conference/Paper2268/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tackles the problem of learning with noisy-labels. By combining different existing approaches, such as noise transition matrix estimation and sample reweighing, with a new set-level self-supervised learning pipeline, the proposed approach achieve strong performance on NNL (noisy label learning) vision benchmarks. The method can also be seen as an expectation-maximization (EM) algorithm.",
            "strength_and_weaknesses": "\nStrengths:\n\n1) The paper is clearly written and does a great job at explaining the proposed method in details.\n\n2) The proposed method is interesting and introduces the notion of set-level self-supervised learning, and invariance to label corruption, which is a new concept in the field of learning with noisy labels.\n\n\nWeaknesses:\n\n\n1) Calling the method self-supervised feels odd given that the task to solve is (noisily) supervised in itself, and that the gradients from the $\\mathcal{L}_{CE}$ loss are retro-propagated into $\\theta$.\n\n2) How does the model perform without $\\mathcal{L}_{CE}$ ? In particular, it would be interesting to train the model with only with the self-supervised loss and evaluate the model on a linear frozen classification task, similar to what is done in standard SSL [1].\n\n3) How do you balance $\\mathcal{L}_{CE}$ and the self-supervised loss ? More generally, it would be great to have an ablation on the different criterion of the method, in order to better understand the role that each one plays.\n\n4) The experimental results are a bit weak. Comparison with several recent methods cited in the paper are missing. In particular none of the methods mentioned in the \"Self-Supervised Learning for NLL\" paragraph of the related work section are compared in the experiences. For exemple, Jo-SRC [2] achieves 75.93\\% test accuracy on the clothing-1M dataset against 74.51\\% reported by SLSSL in the paper.\n\n5) It would be great to have results on the Cifar-100 dataset which has significantly more classes than Cifar-10 and Clothing-1M.\n\nRemarks and questions:\n\n1) Do you apply different augmentations (Random cropping, color jittering) in the batches S and S' ?.\n\n2) \"most SSL strategies are performed at the instance level, regardless of the correctness of its label\". I don't understand this sentence, as there is no label in SSL.\n\n3) The parallel made with the EM algorithm feels a bit far-fetched, would it be possible to provide a more mathematical comparison ?\n\n[1] T. Chen et al., A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020\n[2] Y. Yao et al., Jo-SRC: A Contrastive Approach for Combating Noisy Labels, CVPR 2021",
            "clarity,_quality,_novelty_and_reproducibility": "\n1) Other than the set-level SSL approach, the rest is just existing approaches put together, which makes the method convoluted for not so much gains in practice compared to prior work.\n \n\n2) Some critics of prior work in the related work section are reproduced in the method:\n- \"MLNT corrupts labels randomly\" -> This is also the case in the procedure described in Section 3.2 \n- \"accurately estimate the noise transition matrix remains a challenging problem\" -> This is still a challenge for the proposed method.\nMore generally, could you explain how your method alleviate the short-comings of related works ?\n\n3) Figure 1 is unclear. It is hard to understand the intuition of the method by watching at the Figure, as it feels like a single representation and class probability vector are computed for the entire set. This might also be due to the naming \"set-level SSL\" which sounds like an SSL loss is applied at the same level and not at the instance level.\n\n4) The difference between \"single model\" and \"co-training model\" needs to be clarified, the term \"single model\" only appears in Section 4.2 for the first time. What is the difference between Table 1 and Table 2 ? And between \"w/o co-training\" and \"w/ co-training\" in Table 2 ?",
            "summary_of_the_review": "The proposed method is conceptually interesting and valuable for the community. However it is hard to understand the contribution of all the components of the proposed method and the results are not significantly better compared to the current state-of-the-art. The paper would be greatly improved if the benefit of using a self-supervised component for learning with noisy-labels was clearer.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_rqJk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_rqJk"
        ]
    },
    {
        "id": "dBQ3hFD67AI",
        "original": null,
        "number": 4,
        "cdate": 1667377063745,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667377063745,
        "tmdate": 1667377063745,
        "tddate": null,
        "forum": "xeg2fW5E2l3",
        "replyto": "xeg2fW5E2l3",
        "invitation": "ICLR.cc/2023/Conference/Paper2268/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on set-level self-supervised learning (SLSSL) and proposes a new method for this problem with the same name SLSSL. In order to eliminate the bad effects of corrupted labels, SLSSL augments a minibatch by corrupting labels and obtains a augmented model. Then SLSSL enforces the consistency between outputs from the augmented model and the weight-averaged model. The paper then shows how to use SLSSL in model training: the consistency statistics are further translated into noise transition matrix, which refines classification prediction and helps learn the parameters of the model.",
            "strength_and_weaknesses": "Strength:\n\nThe paper proposes a novel method. The idea of putting SLSSL into a EM-like algorithm is interesting.\n\nWeakness:\n\nThe name of the problem is somewhat confusing. It is suggested to explain the difference between set-level self-supervised learning and supervised contrastive learning.\n\nSLSSL acts as part of the whole learning strategy. It is advised to give a new name to the whole strategy, which may help readers understand the relation between the whole training process and SLSSL better. \n\nWhy should Section 3.2 and Section 3.4 be separated into two parts? Also, the information shown in Fig 2 is all included by Fig 4 and the two figures are very similar. It is suggested to think about the structure of Section 3.\n\nIt is advised to explain why SLSSL is only compared with some weak baselines.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of the paper is ok. The authors give implementation details, but source codes are not attached.",
            "summary_of_the_review": "The idea is interesting, but the paper still has some problems. It is not qualified right now, but has potential to become better after some revision.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_fPbP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_fPbP"
        ]
    },
    {
        "id": "tBxV9g1ihT",
        "original": null,
        "number": 5,
        "cdate": 1667474857705,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667474857705,
        "tmdate": 1669102425751,
        "tddate": null,
        "forum": "xeg2fW5E2l3",
        "replyto": "xeg2fW5E2l3",
        "invitation": "ICLR.cc/2023/Conference/Paper2268/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an SSL method to learn from noisy labeled data.  Instead of the previous method, which is based on instance-level SSL, this method is a set level. Authors proposed to learn M models (with parameters $\\teta^'_i=(1:M))$ ) wherein in each of the models, the labels of two classes are corrupted. Finally, all models, and also the primary model, are forced to have the same behavior concerning the same inputs.  The final is robust aginst the models and explicitly learning the nosie transition matrix.",
            "strength_and_weaknesses": "+The idea is very interesting. It has been shown that the model efficiently works on noisily labeled data. As far as I know, there has been no similar work previously. \n+Results are promising and confirm the feasibility of the idea. ",
            "clarity,_quality,_novelty_and_reproducibility": "Generally, the paper is well written. However, I think Fig 2 and 3 confuse the readers. \n\nAs the paper mentioned, several methods try to use the SSL task as an auxiliary task to make the main model robust. Can we compare their performance against this paper? If so, please do that. \n\nFig 5 is not readable in the printed version. ",
            "summary_of_the_review": "The idea is novel, and the paper is well-written. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_xdaD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_xdaD"
        ]
    },
    {
        "id": "LPwJdiSztd1",
        "original": null,
        "number": 6,
        "cdate": 1667482344610,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667482344610,
        "tmdate": 1667482344610,
        "tddate": null,
        "forum": "xeg2fW5E2l3",
        "replyto": "xeg2fW5E2l3",
        "invitation": "ICLR.cc/2023/Conference/Paper2268/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studied how to address the negative impact of noisy labels during training. To tackle this challenge, the authors propose a set-level self-supervised learning loss, which augments a set of images  instead of one instance with label corruption and then maximizes the agreement between two augmented sets.  In addition, the authors show that the proposed learning strategy can also be used for estimating the noise transition matrix and sample selection following typical ways of noisy-label learning.  Experiments and ablation studies demonstrate that the proposed method has better performance than several baselines.\n\n",
            "strength_and_weaknesses": "Strengths:\n1. The paper targets an interesting and relevant problem .\u2028\n2. The idea of using set-level self-supervised learning  for noisy-label learning is simple and reasonable. The presentation of the proposed method is clear and readable.\u2028\n3. Experimental results show that the proposed schemes offer gains over the baselines.\u2028\n\n\nWeakness:\n1.  Baselines are not enough and the contribution is not clear. There are some existing works which studied the same problems[1, 2, 3]. It would be nice if the authors can discuss the relationships between these existing works and the approach in this paper.   Experimental comparisons with some of these works would be desirable.\u2028\n2. Experimental results on more datasets and configurations should be included.  Only two datasets are considered for evaluation, which make it hard to estimate the generalisability  and robustness of the proposed approach. The effect of different data augmentation techniques on the proposed approach can also be discussed in the paper or in the  Appendix.\u2028\n3. Although the authors provide preliminary visualization of the noise transition matrix and the analysis of  sample weights. I think the paper's quality will improve  if the authors can investigate an in-depth  analysis and theoretical justifications on why the use of set-level self-supervised learning improves noisy-label learning \u2028\n4. The authors do not address the limitations of their work.\u2028\n\n\n[1] Robust Training under Label Noise by Over-parameterization, ICML 2022\n[2] ProMix: Combating Label Noise via Maximizing Clean Sample Utility\n[3] Beyond Images:Label Noise Transition Matrix Estimation for Tasks with Lower-Quality Features, ICML 2022",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow and clear. The novelty could be improved. ",
            "summary_of_the_review": "The paper investigated an interesting problem with an easy-to-follow method. But the experiments and more analysis should be provided to demonstrate the idea. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_XcmF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_XcmF"
        ]
    },
    {
        "id": "3r4ahm5TBl",
        "original": null,
        "number": 7,
        "cdate": 1667553505325,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667553505325,
        "tmdate": 1669192211415,
        "tddate": null,
        "forum": "xeg2fW5E2l3",
        "replyto": "xeg2fW5E2l3",
        "invitation": "ICLR.cc/2023/Conference/Paper2268/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a set-level self-supervised learning method for noisy label learning. The authors propose to enforce model robustness against label corruption by making model predictions (other than the corrupted classes) the same between the original model and augmented models. The noise transition matrix is estimated based on the observation that mislabeled class pairs would lead to fewer differences between the models in the SSL process. The noise matrix is then utilized for sample reweighting. Experiments on two synthetic noisy datasets and one real-world noisy one show the efficacy of the proposed method.\n",
            "strength_and_weaknesses": "Strength\n1. The writing is clear and the proposed method seems straightforward and easy to implement.\n2. The idea of combining self-supervised learning for noisy label learning based on simple intuition is novel.\n\nWeaknesses\n1. The improvement over DivideMix seems marginal (Table 2 & 3 & 4). More analysis is needed (statistical test, comparison of training computational cost)\n2. The method is tested with one kind of model (ResNet). Can the method generalize to other architectures like Vision Transformers?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written. The method part contains multiple steps so an algorithm table would be helpful to understand the overall training process. I think the method is novel. The method seems straightforward and can be reproduced.",
            "summary_of_the_review": "I think the proposed method is clear and novel. Experimental results are a bit weak. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_HztF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2268/Reviewer_HztF"
        ]
    }
]