[
    {
        "id": "HSv1JJb1UP",
        "original": null,
        "number": 1,
        "cdate": 1666486140688,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666486140688,
        "tmdate": 1666545911263,
        "tddate": null,
        "forum": "6dlC7E1H_9",
        "replyto": "6dlC7E1H_9",
        "invitation": "ICLR.cc/2023/Conference/Paper5654/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new prompting method called algorithmic prompting that aims to teach algorithms to LLMs via in-context learning. Specifically, the method focuses on using very details execution traces and explanations as part of the in-context demonstration to remove any ambiguity. Using this prompting method, in-context learning performance on simple arithmetic operations is significantly improved, especially under length extrapolation setting. Furthermore, the authors also demonstrate that models can acquire more/composed skills by having algorithmic prompting demonstrations of two different/composed algorithms in the context. The authors also show that it is possible to use the learned algorithm as a tool for solving math word problems.\n",
            "strength_and_weaknesses": "Strength:\n- The method makes sense conceptually and demonstrates significant performance gain upon standard scratch pad/CoT in multiple aspects.\n- The performance improvement for length extrapolations is especially impressive.\n- The paper is clearly written with concrete examples.\n\nWeaknesses:\n- The demonstrated examples are all for very basic arithmetic operations, and it is unclear whether this technique can scale to more complicated cases.\n- The algorithmic prompting method conceptually requires that we know how to break down the target operation (like parity) into recurrent steps and provide traces. With this level of specificity required, it is no longer necessary to use prompting to solve the tasks (we already solved it with our brain when providing the prompt and can totally write a program for it). Some generalizations (other than length) from the demonstration to the actual task would make the method much more useful. Some examples:\n    - Can algorithmic prompting be extended to operations we only know the structure of? Like solving dynamic programming problems with demonstrations for a DP problem with different state space or update functions? \n    - The skill composition could also be very useful if the demonstration of combining skills A and B can be generalized to combine A and C or even B and C. \n- The length extrapolation intuitively only works because the recurrent step is the same for all lengths, so LM can follow the demonstration in a straightforward and local way. Although this is arguably possible for all operations, it could lead to a very large state to carry over in each recurrent step. In fact, even for just addition, the copying of a large number of digits in context repetitively already seems very expensive given the limited context window. The second-pass strategy seems a promising start to handle this. Maybe the demonstration can just only demonstrate the recurrent step from f(n + 1) to f(n), so that each recurrent step can be generated in a separate context like a recursive call? \n- For the final tool use demonstration, what is the advantage of <call_add_algo> over calling the injected calculator as designed in the original dataset?",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method is built upon Chain of Thoughts and scratchpad line of ideas but the specific method is novel to the best of my knowledge. The claims are supported by thorough experiments and results under each setting. And the paper is clearly written.",
            "summary_of_the_review": "Overall, the proposed method demonstrates significant improvement in some simple settings and I think this is an interesting direction. But so far it is unclear whether this technique can be scaled to more complicated problems.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5654/Reviewer_Ke6m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5654/Reviewer_Ke6m"
        ]
    },
    {
        "id": "Xi11xX2P-D",
        "original": null,
        "number": 2,
        "cdate": 1666555522230,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666555522230,
        "tmdate": 1669304949639,
        "tddate": null,
        "forum": "6dlC7E1H_9",
        "replyto": "6dlC7E1H_9",
        "invitation": "ICLR.cc/2023/Conference/Paper5654/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors study whether degrees of algorithmic reasoning emerge from pre-trained large language models (LLMs) by only intervening on the input prompt. Basically, the authors show that by describing all mathematical passages underlying a certain mathematical operation, e.g. addition, directly in the prompt, LLMs improve at solving these mathematical questions and they exhibit a problem solving process which is akin to an algorithm.",
            "strength_and_weaknesses": "The underlying idea of showing a language model how to break down \u201ccomplex\u201d data manipulation is quite interesting. From the results, the model seems to benefit from this prompting technique. \n\nI found the paper somewhat hard to follow and the writing style rather confusing. For instance, I did not understand what \u201cdefining algorithms as skills\u201d means exactly. This is mentioned several times in the manuscript but it looks to just boil down to using the aforementioned prompting technique. The paper should make a clear statement somewhere that the language model is not re-optimized and no weight updates actually happen, since the name \u201cin-context learning\u201d might be misleading.\n\nThe main drawback of the manuscript is that technical details of the experimentation are largely missing. The authors do not mention explicitly how many examples they provide in the prompt for each task, whether it is only one or multiple. Details of the baseline are not provided either, such as which prompts are used, making the comparison between different ideas harder.\n\nSection 3, evaluation metric paragraph -- \u201cWe measure [..] OOD performance [..] where the model sees shorter examples at training time and generalize\u201d -- missing s, typo -- \u201dto longer problems at test time\u201d. This should be rephrased, there is no training/test time as long as a learning problem is not formulated and executed.\n\nUsage of space is not well-balanced, the authors dedicated a full subsection for explaining \u201ctwo-number addition\u201d (section 3.1) and only a few paragraphs for all the others. In particular, the algorithm for parity is not even mentioned but its results appear in table 2.\n\nSection 4 shows that we can \u201cteach\u201d multiple problems at a time, such as subtraction and addition. This looks to perform worse than showing each problem separately, therefore I am not exactly sure what conclusions can be drawn from there.\n\nWhile it is undoubtedly interesting that \u201chard\u201d reasoning capabilities emerge from LLMs with the right prompt, the problems faced in the paper seem only to be \u201ctoy problems\u201d. The fact that this prompting fails on logical tasks (section 6) poses serious concerns on its applicability on harder problems other than simple mathematical operations and, therefore, its real-world utility.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity can certainly be improved (see above). The technical quality and novelty is scarce, there is not actual learning but just interventions on the input prompt. \n\nI would like to raise a concern about reproducibility of the experiments. I wanted to reproduce some of the experiments but found out that OpenAI\u2019s Codex code-davinci-002 model is not publicly available yet, which is somewhat awkward. The best I could do was to try reproducing OOD results with the text-davinci-002 model, which exhibited similar reasoning processes but I got results (for 14 digits) that are much lower, i.e. 30%, than what is shown in Figure 3, i.e. approximately 97-98%. I tried with 10 samples where each addend has 14 digits, while providing the model with a single example of summation with 3-digit numbers.\n\nPerhaps the authors can explain why this happened? Is that only due to a model difference or too few examples in the prompt? Please find here a txt file showing the prompt I used (the same as in section G.2) and the response I got from the text-davinci-002 model:\nhttps://file.io/vqUWwuH8uZpq\n",
            "summary_of_the_review": "While I find the prompting scheme interesting, I don\u2019t find the overall contribution to be relevant for ICLR. The technical contribution is poor, i.e. no learning whatsoever, and the manuscript is not precise and does not present experimental details very clearly. Lastly, a final concern about real-world utility as the prompting does not perform well on harder problems (logical tasks).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5654/Reviewer_9LM1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5654/Reviewer_9LM1"
        ]
    },
    {
        "id": "u4sD1H1iu0",
        "original": null,
        "number": 3,
        "cdate": 1666680647328,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680647328,
        "tmdate": 1666680647328,
        "tddate": null,
        "forum": "6dlC7E1H_9",
        "replyto": "6dlC7E1H_9",
        "invitation": "ICLR.cc/2023/Conference/Paper5654/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduced a more detailed prompt for LLM math reasoning, and study their impact on task composition and assisting in solving math problems in a recent dataset. The proposed method shows very high accuracy in simple tasks and starts to fall short when the tasks are more complex. One of the challenges is the max number of tokens that the model accepts, so they study multiple alternatives.\nIn summary, the paper shows that precision on the prompt can help to improve LLM reasoning, while the method requires longer prompts so it becomes harder to realize its gains. ",
            "strength_and_weaknesses": "Strengths\n- The systematic study of the structure of the prompt, including influential related work.\n- Focus on the number of digits to study the generalization\n- Study of kind of errors in the proposed prompt strength the proposal\n\nWeaknesses\n- The interference found is a negative result in an area poorly understood. It's entirely possible that a slight variation of the prompt managed to fix the issue. I'm not ready to accept strong interpretations of \"X prompt didn't work\". I suggest lowering the tone of such findings. \n- The jump from addition to combination with subtraction is relatively small at an abstract level: just four combinations. This could become a strength if presented as a simple change that becomes challenging for the LM.\n\nPotential weakness\n- The study of the composition of skills gets confusing around multiplication. The setting of using a shorter explanation becomes out of the general hypothesis. This might become a strength if it were presented as a limitation and not as a solution.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, the method is clear, and the analysis is well-organized. The work is as original as it can be given this is such a popular topic. The paper didn't mention if the code would be distributed, but I assume so. End of the day, it's a set of templates.\n\nMy main concern is presenting some blurry methods as contributions. I think the paper would be more honest by discussing harder the number of tokens and how the method cannot be generalized properly. I'm not convinced that the experiments in GSM8k add clear insights beyond addition. The solution of calling another model contradicts slightly a comment about \"external tools\" right before the subsection \"Contributions\" on page 2. Perhaps there is a missing lesson on how the limitation of the number of tokens can be alleviated by composition. \n\nAdditional comments:\n- Given the workarounds for dealing with the max amount of tokens, I'd like to see a reflection on the potential limitations of LLM reasoning. There are models that accept more tokens, and some that claim to accept an unbound number (I expect them to deteriorate). Given how much space it took to increase the accuracy, we have to wonder if the max token is a solid ceiling for more precise reasoning. \n- Page 6. \"Length 8\"  in the caption of table 2 probably refers to multiplication.",
            "summary_of_the_review": "The paper proposes a prompting method that focuses on providing a more precise text description of the reasoning. It performs very well in addition, parity and others, but space limitation of the context of LLM precludes performing the real experiments when doing composition and the MATHS problems.\n\nI recommend the acceptance of the paper based on the positive findings, while I recommend interpreting the results without assuming that LLMs would be able to achieve high performance.  \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5654/Reviewer_UYXX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5654/Reviewer_UYXX"
        ]
    }
]