[
    {
        "id": "nWi9IqXTXL4",
        "original": null,
        "number": 1,
        "cdate": 1666018206045,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666018206045,
        "tmdate": 1666018206045,
        "tddate": null,
        "forum": "TXPN6MtdSE4",
        "replyto": "TXPN6MtdSE4",
        "invitation": "ICLR.cc/2023/Conference/Paper1104/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new image processing method based on combination of several technics. First the authors introduce a generalization of the iterative reweigthed least square (IRLS) optimization scheme. Based on the majorization-minimization scheme, it solved a classical optimization problem build from a least square loss function with regularizers. Second, they proposed to learn the filters used in the regularization using deep learning method, for that they proposed to use the previous method as a layer of a recurrent network. The results show the effectiveness of the framework on different inverse problems in image processing.  ",
            "strength_and_weaknesses": "# Strength\n\nThe proposed method is a complete framework to solve inverse problem for image processing. It relies on regularizers that are well known to be effective for such problems and leads to pretty good results. The biggest asset of the framework is the construction of a layer which relies on the IRLS with learned filters for regularizations. Using in some way the best of two worlds it leads to a fairly strong neural network that is able to solve inverse problems.\n\n# Weakness\n\nThey are several weakness in the proposed framework. \n1. The image formation model (with a Gaussian additive noise) is very general, but fails on some non-additive noise like Poisson noise in low-luminosity setting. As changing the noise change the loss function, the construction of a surrogate may become delicate.\n2. The IRLS layer asks for solving a optimization problem. Even if conjugate gradient is a very powerful method it takes times to apply, thus with deep network the learning process may become untrackable...\n3. While the results are among the best, both quantitative and qualitative results show little improvements.\n4. The results are computed for the specific level of noise. It would have been interesting to try a blind version, where the noise level is random during the training.\n5. The reproduction of the methods could be difficult (see below).",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity\n\nThe paper is rather clear. Only part where the neural network is introduced are a little bit confused. For example, I cannot see the number of layers used for the experiment in the text.\n\n# Quality\n\nThis is good quality work.\n\n# Novelty\n\nThe framework by itself is new and compares favorably against unfolding methods that have been used for such tasks (see https://arxiv.org/abs/2108.06637).\n\n# Reproducibility\n\nAs it is some part remains unclear as the number of layers. However the authors put most of the parameters in the paper and the data sets are public, so most of the framework could be reproduced.\n\n",
            "summary_of_the_review": "To sum up the paper, it is good paper that introduce a  image processing method. The two main contribution are the generalization of the IRLS  with a extended set of regularizers and a new neural network layer based on the optimization framework. While some part still need to be clarified, the full method is exposed. The results are among the best, but shows minor improvement compare to state-of-art. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1104/Reviewer_vB8U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1104/Reviewer_vB8U"
        ]
    },
    {
        "id": "kLGJ5yKlPt9",
        "original": null,
        "number": 2,
        "cdate": 1666633018870,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633018870,
        "tmdate": 1668268556795,
        "tddate": null,
        "forum": "TXPN6MtdSE4",
        "replyto": "TXPN6MtdSE4",
        "invitation": "ICLR.cc/2023/Conference/Paper1104/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "# Summary of The Paper\nThe paper considers the problem of image recovery with sparse or\nlow-rank regularizers. The method considered to solve this problem is\niteratively reweighted least-squares (IRLS), as well as its learned\nvariant. The methods are implemented and applied to several image\nprocessing problems, e.g., super-resolution, demosaicking, and MRI reconstruction.",
            "strength_and_weaknesses": "# Weakness\nThe present manuscript has several drawbacks.\n\n## 1. Insufficient Review of Related Works\nThere are many papers on IRLS with $\\ell_p$\nregularization/minimization, which are overlooked by the paper (except\nDaubechies et al. 2010, Mohan & Fazel 2012). This leads to several\ninaccurate claims and developments, e.g.,\n1. the paper claims that the proposed algorithm is a generalization\n   of IRLS.\n2. the paper claims that the bounds in Lemma 1 and Lemma 2 are novel.\n3. The paper attempts to learn the weights for IRLS in the experiments.\n\nPlease allow me to elaborate on why the two claims are inaccurate.\n1. First it should be noted that the proposed algorithm with $\\ell_p$\n   regularization has been studied in\n   - [1] \"Improved Iteratively Reweighted Least Squares for Unconstrained\n     Smoothed $\\ell_q$ Minimization\" (SIAM Journal on Numerical\n     Analysis, 2013)\n\n   There, Lemma 1 has been proved. In fact, Lemmas 1 and 2 are already\n   known by experts who have read Daubechies et al. (2010), and Mohan &\n   Fazel (2012). And the reweighting strategy of the paper follows\n   directly from Mohan & Fazel (2012).\n2. The reweighting strategy derived by Mohan & Fazel (2012) could be\n   sub-optimal, but the provably optimal weights have been found\n   in a recent paper:\n   - [2] \"A Scalable Second Order Method for Ill-Conditioned Matrix\n     Completion from Few Samples\" (ICML 2021)\n\n   Thus it is a bit unclear to me why the paper proposes to learn\n   the weights.\n3. The paper believes the proposed IRLS variant is a generalization, perhaps\n   because Eq. (11) has an additional augmented Lagrangian term, which\n   is to make a matrix in (11) invertible. However, this modification\n   is problematic for several reasons. First, note that (11) requires\n   knowing the variance of the noise, which is typically unavailable.\n   Anyway, let us suppose the variance is known. The second issue is\n   that, the matrix would be invertible in general. Finally, even if\n   it is not invertible, the least-squares problem has infinitely many\n   solutions, a natural way to handle this is to take the minimum-norm\n   solution, or to project $x^k$ onto the space defined by normal\n   equations. With this mysterious regularization term\n   $\\|x-x^k\\|_2^2$, it is theoretically unclear whether the algorithm\n   will be convergent (even to stationary points), as the\n   least-squares problem is not solved exactly in each iteration.\n   Later we will see this creates a lot of issues.\n\nI would recommend reading more on the recent developments of IRLS\nsince Daubechies et al. (2010), appreciate prior works more, and write\nmore humbly regarding actual contributions.\n\n## 2. Technical Clarity\nThe paper has great potential of confusing the readers. Details are\ngiven below:\n1. In Section 2.1, the reader would get confused at the very\n   beginning, as there is no motivation of having the regularization\n   operator $G$ in (3). Is $G$ given? Where does it come from? Why\n   should we have it there? Can there be any example of $G$?\n\n2. Section 2.2 should undergo a complete revision, and should be\n   turned into a review of IRLS. Note that the\n   majorization-minimization interpretation of IRLS has been known in\n   the literature, and the two lemmas are not new in my opinion. There\n   seems to be no need to emphasize them so much. Instead, this\n   section should have an algorithmic listing for IRLS (which is very\n   easy to understand but now missing) with an explanation of each\n   step. Then write down clearly the objective\n   function that IRLS is minimizing; only after this it makes sense to talk\n   about stationary points of that objective, and cite some papers\n   which prove stationary convergence. (Note that the paper\n   keeps talking about stationary points and local minima, but it is not\n   clear which optimization problem is referred to.)\n3. To my knowledge, Section 3 is the main novelty of the paper. But it\n   is poorly written.\n   - The first question to which the paper should answer intuitively\n     is **why would it be a good idea of learning/unrolling IRLS**? Is it\n     because others are doing this kind of thing for different\n     optimization algorithms and getting cited? Note that IRLS is a well-defined\n     algorithm that converges to some stationary point of the objective\n     that it minimizes (which is not the case for the proposed IRLS\n     variant, as explained above). It could be the case that learning IRLS\n     destroys this guarantee, as the convolution matrix $G$ is\n     changing over time (it remains the same only if the gradient is\n     zero), and as the weight prediction module does not compute the\n     weights in an IRLS way.\n   - In Section 3.1, the way that the paper addresses the challenge of\n     training is not well justified. It has not been proven that the\n     proposed IRLS variant converges to stationary points or some\n     fixed points (which are two different concepts but the paper\n     confused them at Eq 12), hence (12) is mainly a heuristic choice.\n     And it has not been proved that $x^*=f_{\\theta}(x^*;y)$.\n## 3. Experiments\nFirst of all, my impression is that there are too many manually chosen\nfactors that impact the performance, and different experiments have\ndifferent settings. Hence I will not take this section into serious account.\n\nHowever, I do want to point out the importance of ablation study,\nwhich seems missing in the paper.\n\n1. The first thing that the paper should convince me is that, **the learned IRLS\n   performs significantly much better than IRLS**. Without such an experiment, I would\n   not be able to agree that the proposed method is a good idea.\n   Please note that the learned IRLS is significantly more complicated than\n   IRLS when it comes to tuning parameters. And also note that\n   nowadays many of our researchers are using a lot of electricity,\n   money, and trees to train deep networks. We have to avoid training\n   deep networks for simply this reason, unless absolutely necessary.\n\n2. The second experiment the paper should conduct is to **show that, at\n   least empirically, the learned IRLS is actually convergent**, e.g.,\n   the optimality condition holds approximately. This is easy to\n   detect and plot. Otherwise, no such claim should be made in the paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "See the above comment.",
            "summary_of_the_review": "The paper has an interesting idea of unrolling IRLS with several applications to image processing. \n\nHowever, it also has several problems when it comes to related works and technical clarity. In its current form, the paper appears suboptimal for an ICLR publication and a significant revision is needed in my opinion. This is why I intend to give a score of reject.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1104/Reviewer_gm4f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1104/Reviewer_gm4f"
        ]
    },
    {
        "id": "HoVWunIw_wm",
        "original": null,
        "number": 3,
        "cdate": 1666727883689,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727883689,
        "tmdate": 1666727883689,
        "tddate": null,
        "forum": "TXPN6MtdSE4",
        "replyto": "TXPN6MtdSE4",
        "invitation": "ICLR.cc/2023/Conference/Paper1104/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work introduces an algorithm for incorporating a learned sparse and low-rank penalty for image recovery tasks. A majorization bound of the penality functions (as quadratic functions) are derived for efficient optimization by Iteratively Reweighted Least\nSquares (IRLS). Due to the recurrent nature of the algorithm, the authors propose to learn the prior function $G_i$, the weights and $p$ by \na recurrent network, while the other parts follow the IRLS framework. The proposed algorithm is shown to achieve competitive performance on several image recovery tasks.",
            "strength_and_weaknesses": "Strength:\n\n1. The idea of the paper is interesting and very natural. Many existing unrolling networks require a large number of parameters and data, whereas the proposed network only learns a few number of functions as components of IRLS framework. This is a nice and natural way to combine neural networks with a regularized least squares, where the regularizers lie in some restricted spaces that can be efficiently represented by neural networks. Moreover, the majorizors that the authors developed enables incorporating sparse or low rank prior for deep image recovery by combining IRLS with an \"unrolling\" strategy. \n\n2. The paper is overall well-written. The idea is clearly motivated and easy to follow. I really enjoyed reading this paper.\n\n3. The experiments look impressive. The proposed method achieves top performance on a variety of datasets for different image recovery tasks.\n\nWeaknesses and questions:\n\n1. There seems to be no regularizer that consistently outperforms others, which makes sense. However, it raises another question that how to choose the \"searching space\" of regularizers for different datasets. Could author(s) add some discussions on whether fixing $p$ or not, and whether promote sparsity or low-rankness? The authors should also explain the reason why sometimes fixing $p=1$ (instead of learning $p$) can be more advantageous, which is somewhat counterintuitive (is that a case of overfitting?).\n\n2. In some image recovery and matrix recovery problems, the ground truth could be both low rank and sparse. Is there anyway to jointly emphasize the sparsity and low-rankness?\n\n3. In the literature review, the authors missed some related works in unrolling networks, and I name a few:\n\n[1] Algorithm Unrolling: Interpretable, Efficient Deep Learning for Signal and Image Processing\n\n[2] Deep Algorithm Unrolling for Blind Image Deblurring\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty: The general idea of majorization minimization and unrolling are not new, but they are combined in a novel way.\n\nClarity: The paper is clearly written. The proofs look correct, although I did not check every details.\n\nQuality: The overall quality of the paper is satisfying.",
            "summary_of_the_review": "Overall, this paper has made a solid contribution to the area of deep image recovery. It uses a novel combination of unrolling networks and the majorization minimization framework to efficiently incorporate sparsity and low-rank prior. In the meantime, I encourage the authors to include more discussions on the choices of regularizers and $p$. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1104/Reviewer_FciL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1104/Reviewer_FciL"
        ]
    }
]