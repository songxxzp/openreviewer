[
    {
        "id": "B-cqYsf33LK",
        "original": null,
        "number": 1,
        "cdate": 1666611237796,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611237796,
        "tmdate": 1666611237796,
        "tddate": null,
        "forum": "d8VrVfNARSy",
        "replyto": "d8VrVfNARSy",
        "invitation": "ICLR.cc/2023/Conference/Paper3763/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores the nullspace ( adding an element on this space does not affect the output) for Vision Transformer. The authors first demonstrate that a non-trivial nullspace exists for the patch embedding matrices. This idea is extended to the non-linear layers of the vision transformer, which is learned via simple optimizations. \nThe applications to image watermarks and unconstrained targeted noise are presented. ",
            "strength_and_weaknesses": "Strength\n\n- The introduction of nullspace for analyzing ViT has originality. This paper investigates if the nullspace exists for the linear input layer (patch embedding layer) and non-linear layer (self-attention layer). The patch embedding layer's linear structure produces a larger nullspace than CNNs, as shown in Sec.5.2. \n\n- The watermark application and the targeted nullspace noise are interesting. They are largely different from conventional adversarial attacks. \n\nWeakness\n- The results of watermark supervision and targeted nullspace noise are not visually attractive.\n\n- Writing quality is not high. \n",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity \n- In the introduction section, the explanations of background and  ViT are long, and the number of references (6 pages) seems too large. The authors can reduce the number of papers for the introduction sections. \n\n- Figure 1 left bottom equation: What means $\\eta$, $\\beta_1$, $\\beta_2$ is unclear. \n\n- Figure 1 caption. \"right)\" seems to be a typo. \n\nQuality\n\nThe application is evaluated only with the visualization of a few images. \nThese applications might be evaluated quantitatively, e.g. recognition accuracy for the unconstraint targeted noise. \n\nNovelty\n\nThe introduction of nullspace for analyzing ViT is novel. The two applications are also novel and interesting.  \n",
            "summary_of_the_review": "This paper presents a nullspace analysis of ViT, which have originality. The two applications of nullspace are also interesting. Though it is understandable, the writing and evaluations are not high quality. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3763/Reviewer_kZts"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3763/Reviewer_kZts"
        ]
    },
    {
        "id": "TsGlVSFOAn",
        "original": null,
        "number": 2,
        "cdate": 1666660997093,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660997093,
        "tmdate": 1666660997093,
        "tddate": null,
        "forum": "d8VrVfNARSy",
        "replyto": "d8VrVfNARSy",
        "invitation": "ICLR.cc/2023/Conference/Paper3763/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper explores the concept of nullspace for vision transformers (ViTs). The authors find that there usually exists non-trivial nullspace for vision transformers by finding the exact nullspace of their patch-embedding stages. Extending to the case of non-linear transformation where the nullspace is ill-defined, the authors look for an alternative set that adds any element in that set to the input, and the output of the transformation remains unchanged. The authors also study the robustness of ViTs with the nullspace noise perspective. In addition, they show that ViTs are more robust than CNN models under the noise generated via the nullspace. The authors also illustrate that nullspace noise can be used to impose perceptible watermarks on images.\n",
            "strength_and_weaknesses": "**Strong points:**\n\n1. The paper addresses an interesting problem as finding the nullspace of ViT will tell us certain types of input perturbations to which the network is inherently robust.\n\n2. The paper is well-written with illustrative figures.\n\n**Weak points:**\n\n1. In the nonlinear case where the nullspace is ill-defined, the authors \u201cattempt to preserve the axiom of most interest to us, closeness under vector addition\u201d and look for an alternative set that adds any element in that set to the input, and the output of the transformation remains unchanged. This approach lacks theoretical justification.\n\n2. The relaxation to study an individual element of the nullspace in the nonlinear case (e.g. when deriving the nullspace for the self-attention stage) removes all interesting insights from studying the whole nullspace of ViT and makes the contribution of the paper mediocre.\n\n3. For noise robustness application, I have a concern about the significance of the idea. Finding the nullspace noise only allows us to know a certain set of noise to which the ViT is robust. How does it help improve the generalization of ViT models? How to constrain other input permutations/corruptions to be in the approximated nullspace of the network?\n\n4. For the applications relating to watermarking, the nullspace-noise-added images significantly decrease the quality of the original images, making them not useful in reality. For the applications on output-preserved image watermarking, the quality of the corrupted images is bad and much worse than the adversarial examples generated by adding adversarial noise into the original images.\n\n**Additional Concerns and Questions for the Authors:**\n\n1. $\\tilde{v}\\_{\\phi}$ is learned by minimizing Eqn. 6. I am confused whether the authors learned one $\\tilde{v}\\_{\\phi}$ or a set of $\\tilde{v}\\_{\\phi}$ for each self-attention stage. In the first case, $\\tilde{v}\\_{\\phi}$ will be input-independent, then it will no longer be a noise added to the input. \n\n2. The Eqn. 2 in the paper is not correct. \\Delta should be a subspace, not vectors.\n\n3. There is no discussion of related work on the nullspace of transformers, such as the work in [1].\n\n**References:**\n\n[1] Brunner, Gino, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer. \"On identifiability in transformers.\" arXiv preprint arXiv:[1] Brunner, Gino, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer. \"On identifiability in transformers.\" arXiv preprint arXiv:1908.04211 (2019). (2019).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. The quality and novelty are not high. There is no submitted code to reproduce the results.\n",
            "summary_of_the_review": "Overall, I vote for rejecting. The idea of approximating the nullspace noise of the Vision Transformer (ViT) model is interesting. However, my main concerns about the paper are: 1) the low significance of the proposed method to study the nullspace of ViTs in the paper, 2) the lack of theoretical justification of the proposed method in the nonlinear setting, and 3) the unconvincing applications of the proposed method.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no ethics concerns for this paper.\n",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3763/Reviewer_upQH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3763/Reviewer_upQH"
        ]
    },
    {
        "id": "HxeTTI4xjB",
        "original": null,
        "number": 3,
        "cdate": 1666688487847,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688487847,
        "tmdate": 1666688487847,
        "tddate": null,
        "forum": "d8VrVfNARSy",
        "replyto": "d8VrVfNARSy",
        "invitation": "ICLR.cc/2023/Conference/Paper3763/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents an exploration of the concept of nullspaces for vision transformers. It presents an approach towards determining the nullspace vectors of the vision transformer: a deterministic non-trivial nullspace for linear projection components and an approximate nullspace for non-linear attention layers. Further, it explores its implications in 3 areas: robustness, watermarking and adversarial sample generation to fool models and interpretability methods. Finally, the paper shows that ViTs are more robust to higher amounts of nullspace-generated noise than state-of-the-art CNNs, and demonstrates the application of nullspace noises in watermark imposition and adversarial sample generation.",
            "strength_and_weaknesses": "Pros\n\t- The exploration of nullspace noise for analyzing the robustness of models is a novel application of the concept. I would love to see more exploration into the implications of nullspace noise on robustness.\n\t- The paper was very well written, and easy to follow.\n\nCons\n\t- The paper mainly discusses 3 areas: robustness, watermarking and adversarial generation.\n\t\t\u00a7 Robustness:\n\t\t\t\u00a7 I think this area requires more analysis in the paper. The introduction and conclusion sections imply that the existence of nullspace vectors, and the high amount of noise generated using this nullspace basis, has significant implications on the robustness of the vision transformer.\n\t\t\t\u00a7 Section 5 presents the question: \"How robust are different architectures to nullspace noises.\" However, this seems a bit confusing. My understanding is, all architectures are, by definition, robust to their nullspace noises. Further, each network has its own nullspace noise distribution, depending on its architecture. The main question, I think, is more about the properties of the nullspace noises of a given model. A model is more robust if:\n\t\t\t\t\u25a1 Its nullspace has a high sampling limit (with a high threshold on accuracy or %match)\n\t\t\t\t\u25a1 The nullspace samples, as described by equation (4), have high accuracy too.\n\t\t\t\u00a7 Figure 3 (a) and (b) show that ViT have higher sampling limits than state-of-the-art CNNs. I think these results give evidence for the first property of robustness w.r.t nullspace noise as described above. However, I think it will be really helpful to analyse these results on more datasets, instead of just a single one.\n\t\t\t\u00a7 Figure 3(c) shows low transferability of noise between models, however, I don't think this particular graph makes any implications on which model is better. Since the noise vectors are obtained using ViT-S model, they are strongly biased against the CNNs. It's very likely that the ViT models will perform equally poorly on the noise vectors obtained using the CNNs. I think it would be helpful if this point is highlighted.\n\t\t\t\u00a7 Figure 3 doesn\u2019t include any quantitative results for the 2nd property described above, regarding the accuracy of nullspace samples from equation (4). This can probably be done by presenting the deviation of accuracy at different nullspace noise vectors, and increasing the number of sampling limits considered.\n\t\t\t\u00a7 It would be helpful if the point about \"lack of effective nullspace noise\" in CNNs is elaborated a bit more. Does the difficulty in finding effective noise due to non-linearity necessarily mean lack of effective nullspace noise?\n\t\t\u00a7 Watermarking:\n\t\t\t\u00a7 In this application, a slightly different watermark will be applied on each image. It would be helpful if the implications of this are discussed. Would this significantly comprise the tasks for which the watermarks are being used for? I am curious to see if this has any practical implications.\n\t\t\u00a7 Adversarial Sample Generation:\n\t\t\t\u00a7 I think more results are required for this application. It would be really helpful if the following results are included:\n\t\t\t\t\u25a1 An accuracy score on a few datasets on the misclassifications, along with the qualitative examples, shown in Figure 5 (a).\n\t\t\t\t\u25a1 It would be helpful if the explainability attribution map mistakes are quantified in some manner, probably IoU/DICE etc, to get some quantitative results to support the qualitative results shown in Figure 5 (b).\n\nMinor Edits\n\t- In section 2, paragraph starting with \"A trivial nullspace, \u2026\", shouldn't \"axiom 1\" be used instead of \"linear mapping as described in equation 1\", and \"equation 2\" instead of \"linear equations as described by equation 1\"?\n\t- In section 3.1 Classification Stage, shouldn't it end with \"project it through a linear classification layer, followed by a softmax layer\" instead?\n\t- In equation (5), it would be more clear to add f_phi instead of just f.\n\t- Section 6 method details: typo at delta x_j -> \"changed\" to \"change\"\nIn section 7 line 3, helpful to write equation (7) instead of just 7.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was very well written and was very easy to follow. The authors did not mention that the code will be released upon publication. The exploration of nullspace noise for analyzing the robustness of models is a novel application of the concept. ",
            "summary_of_the_review": "Borderline reject. The paper definitely proposes a novel idea, however, I think a bit more analysis and results are required. If the author(s) are able to address the above comments, the decision will be reconsidered.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3763/Reviewer_ZgDX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3763/Reviewer_ZgDX"
        ]
    },
    {
        "id": "SGf9QQxpSO",
        "original": null,
        "number": 4,
        "cdate": 1667195294755,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667195294755,
        "tmdate": 1667195294755,
        "tddate": null,
        "forum": "d8VrVfNARSy",
        "replyto": "d8VrVfNARSy",
        "invitation": "ICLR.cc/2023/Conference/Paper3763/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an approach to identify the null space of vision transformers (ViT). They divide this into two parts \u2013 (a) null space of patch embeddings, and, (b) null space for the self-attention module(s). The paper uses the above to study the robustness of ViT to null space noise, compare the impact on different architectures, and study its transfer properties. Finally, the null-space is used to apply perceptible watermarks to images without altering the test-time performance (of image classification).\n",
            "strength_and_weaknesses": "## Strengths\n- Investigation of null spaces of vision transformers is a good direction of research and of relevance to the ICLR and wider AI/ ML community. \n- There are some interesting ideas regarding finding perturbations in the input space to which the output of ViT is invariant and to use it for perceptible watermarking.\n\n## Weaknesses\n\n__(W.1) Importance__: The motivation and importance is not quite clear. The authors make an argument about inherent robustness (invariance) to certain perturbation in the input. It is quite obvious that by design, classification networks are sought to be invariant to tp intra-class differences and geometric and photometric transformations that are deemed not to result in changes in the class identity, leading to all kinds of data augmentations. That set of subspaces of inputs may lie in the nullspace is the key idea here but it is not clear what nontrivial value the authors provide in the paper. Please elaborate.\n\n __(W.2) Technical Approach__: I have several questions regarding the technical approach proposed in Section 4  (if I\u2019m understanding it correctly):\n- __(W.2.1)__: Equation (5) describes a set which is invariant to the input $\\mathbf{u}$. However, the loss functional defined in equation (6) doesn\u2019t reflect his invariance to the input $\\mathbf{u}$. Is there an empirical expectation over the entire training dataset?\n - __(W.2.2)__: The loss in Equation (6) admits a trivial solution ($\\tilde{\\mathbf{v}}_\\phi = 0$. There is no regularization which keeps the solution away from the trivial one. Even if empirically the minimization (perhaps through SGD) yields a solution that\u2019s nontrivial, it can still be arbitrarily close, lying in a small non-isotropic neighborhood around $\\mathbf{u}$ with the same output value. The non-isotropic nature (think elongated tubes, for example) will allow a random permutation of the perturbation to change the output. The existence of this small neighborhood is just a statement about local regularity (of data spaces) and smoothness (of functions) on it. How is this useful in the sense that you seek to exploit in the paper? \n- __(W.2.3)__: The solution of Eqn. (6) resulting in a  'nullspace' noise vector which is not arbitrarily close to __any input u__ while maintaining the same prediction as __u__ for every __u__ (i.e., it is independent of __u__) is not demonstrated or validated. At least on the training data, the % match predictions should be 100% for the definition to make sense. Is this the case?\n- __(W.2.4)__: It seems like a single noise vector is learnt over the entire training dataset per training episode (for each random initialization of the noise vector $\\tilde{v}_\\phi$). This is rather inefficient for generating \u2018infinite such perturbations\u2019 as claimed in the motivation. Secondly, it\u2019s unclear what the diversity of the set of noise vectors is.\n- __(W.2.5)__:  Magnitude(v) should be shared and the significance of MSE(logit) should be explained. It is not clear what to make of the numbers on the y-axis (0-7000.0). How large are those numbers with respect to the magnitudes of the logits?\n- __(W.2.6)__:  For classification networks, simple geometric transformations like translations, rotations, small scalings etc. should lead to the same classification output. How are the random perturbations found through equation (6) useful beyond the above?\n- __(W.2.7)__:  The whole discussion in 5.2 about strided convolutions and intractability of determining a null space seems mathematically flawed (if we ignore the pointwise nonlinearity of the ReLU etc. which the authors don\u2019t seem to be considering) since tensor multiplication is a linear operation. Kindly explain. [1\u2019] below may be useful to take a look at. \n\n[1\u2019] Zou, Dongmian, Radu Balan, and Maneesh Singh. \"On lipschitz bounds of general convolutional neural networks.\" IEEE Transactions on Information Theory 66.3 (2019): 1738-1759. \n\n __(W.3) Experimental Evaluation__: \n- __(W.3.1) Robustness__: In Figure 3(a), why should the scale of the features with respect to which the sampling limit is meaningful, be the same? \n- __(W.3.2) Robustness__: Comparison of robustness to noise across different networks makes sense when the performance degradation under the same perturbation of the input is analyzed. In the experiment in Figure (3), the learnt perturbation also varies and is from an optimization algorithm (which may fail to do what\u2019s expected of it). In this \u2018approximate\u2019 setting, everything is so amorphous and the different factors are so entangled that it\u2019s hard to have any takeaways from this experiment. Kindly explain. \n- __(W.3.3) Watermarking__: Equation (7) suggests that the transformed image should look like the target (watermark) and yet have the same classification semantics as the source. However, the transformed images in Figure (4) look more like the source images while images in Figure (5) look more like the target images. It is not clear what the goal of visible watermarking is and what kind of visual perturbations are considered to be of acceptable quality and for what purposes. Kindly clarify. \n- __(W.3.4) Watermarking__: Qualitative results provided in A.2 should be in the main paper. \n- __(W.3.5) Watermarking__: Only twenty images are used for evaluation. This is unsatisfactory. Since only the null space of the patch embeddings is considered ($v_\\theta$), why should this be prohibitively expensive?\n- __(W.3.6) Watermarking__: Finally, since by design, $v_\\theta$ should not alter the representations by the network, should the classification accuracy be higher than 85%, and 100% if training images are used?\n- __(W.3.7) Unconstrained Targeted Noise__: No quantitative results are provided.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The paper is mostly clear. There are some easily fixable typos.\n\n**Quality**: Should be improved. I feel that the technical portion of the paper is not rigorous enough. The parallel between the concept of linear null spaces and that of a single additive perturbation to the input is stretching the concept too much. The technical framework appears artificial. Also, experimental evaluation needs to be improved. \n\n**Novelty**: Though I\u2019m not aware of any work that analyzes the null space of patch embeddings in ViT, it is straightforward and well-known in classical literature. The extension to nonlinear parts of the network would be of interest but is ad hoc and does not add much value to the field.\n\n**Reproducibility**:  Should be reproducible. \n",
            "summary_of_the_review": "I\u2019m not convinced of the merits of the paper. There are some interesting ideas. However, the technical framework appears artificial, somewhat ad hoc, and not rigorous enough. More extensive experimental validation is required. As a result, I don\u2019t assess the current submission to be of publishable quality. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3763/Reviewer_ZuY5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3763/Reviewer_ZuY5"
        ]
    }
]