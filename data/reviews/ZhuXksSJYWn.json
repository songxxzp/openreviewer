[
    {
        "id": "DbXrhrLrMw",
        "original": null,
        "number": 1,
        "cdate": 1665945149429,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665945149429,
        "tmdate": 1668985811754,
        "tddate": null,
        "forum": "ZhuXksSJYWn",
        "replyto": "ZhuXksSJYWn",
        "invitation": "ICLR.cc/2023/Conference/Paper2015/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers training vision-language models by masked reconstruction. Specifically, the model is trained to reconstruct a masked image conditioned on the paired text, and vice versa. In addition to this task, the standard contrastive and ITM tasks are added during pretraining. The authors use public small-scale VL datasets for pretraining and beat baseline methods on retrieval and VQA/NLVR-type tasks. The authors also show that the proposed method works well in low-data regimes and that all tasks contribute to the final performance. \n\n",
            "strength_and_weaknesses": "Strengths:\n\n* The proposed method is simple and intuitive.\n* The paper relies on open-source datasets which significantly improves reproducibility.\n* The paper shows strong improvements over baseline methods.\n\nWeaknesses:\n\n* There are multiple missing references. The novelty of the proposed method should be discussed in light of these.\n* Given that there are missing references, the baselines in the tables might need to be revised. Thus, it is not clear how well the model performs compared to SOTA methods.\n* Error bars are missing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear. The method is simple which I think is a strength, and this additionally makes the exposition clear. \n\nThe paper relies on public datasets which aid in reproducibility \u2013 I\u2019d like to ask if the authors plan to open-source the code. One potential issue with reproducibility is the lack of error bars in the comparisons. Could error bars be added at least for the fine-tuning tasks? This would ensure that the improvements are not statistical outliers.\n\nA significant issue with the paper is that several closely related references are missing. Here are a few:\n\n* https://arxiv.org/abs/2208.10442\n* https://arxiv.org/abs/2205.14204\n* https://arxiv.org/abs/2204.01678\n\nThe novelty of the proposed method should be discussed in light of these previous papers which also focus on masked training for VL tasks. Some of these methods should also be added to the tables to ensure that the method is compared to the latest work.\n\n",
            "summary_of_the_review": "The paper is well-written and presents a clear and intuitive idea that works well empirically. The authors use public datasets which improve reproducibility but there seem to be no error bars. The biggest issue is that multiple closely related papers which also focus on masked training for VL models are missing. When accounting for these papers, the novelty of the paper is decreased and the numbers in the tables might also need to be revised,",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2015/Reviewer_cmqf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2015/Reviewer_cmqf"
        ]
    },
    {
        "id": "kOu6X6hg_b",
        "original": null,
        "number": 2,
        "cdate": 1666426367680,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666426367680,
        "tmdate": 1669271349229,
        "tddate": null,
        "forum": "ZhuXksSJYWn",
        "replyto": "ZhuXksSJYWn",
        "invitation": "ICLR.cc/2023/Conference/Paper2015/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a simple multimodal vision and language mask model for multimodal training, called MaskedVLM. The main idea is to leverage both masked image patches with full text description and masked language tokens with full image patches for semantic align. \nAdditionally, the method employs two pretraining losses: image-text matching and CLIP-style image-text contrastive losses. \nAfter pretraining with 4M data, they evaluated the proposed MaskVLM on 4 standard multimodal downstream tasks: I2T, T2I, VQA, andMultimodal NLI. Experimental results look promising compared to other baseline methods and include ablation studies. ",
            "strength_and_weaknesses": "### Strength\n* Both modality masking idea is simple and seems effective even if the idea is not very novel. \n* Experimental results seem promising with significant margins compared to the baseline methods. \n* Under limited pretraining data, this method show promising and competitive performances. This is meaninful for academic or small-scale industry research groups. \n* Overall, the paper is clear and easy to read. \n\n### Weakness\n* [Major] Even if the main idea is clear and simple, most of them are from other previous work such as MLM, MAE, and MIM. Of course, the novelty is not all. However, for technical contribution, the combination or integration of the previous ideas needs to be not trivial and challenging, considering the nature of ICLR. Also, the authors need to describe how to effectively integrate in details. Unfortunately, I could not find the details of their proposed method. For example, there is no detailed implementation on how to make joint integration for image decoder. Figure 2 did not show the details. Figure 3 has no architecture of image cross-modal decoder. If the page limitation is an issue, the author can use the appendix. \n* [Major] Some important related previous works are missed such as CoCa [Yu et al. 2022], SimVLM [Wang et al. 2022a], Florence [Yuan et al. 2021], and BEiT-3 [Wang et al. 2022b]. Among them, the author need to compare their method to SimVLM with discussion even if the pretraining dataset is not same. For the rest, the author need to introduce them in related work at least.\n* [Major] For limited dataset experiments, how is the pattern on more data? I wonder the performance of MaskVLM on larger data including CC12M. For example, ALBEF presented two versions such as 4M and 14M. I think the contribution of MaskVLM can be enhanced with the same setting of ALBEF-14M. That is, it will be meaninful under 14M setup for Figure 4. \n* [Minor] How is the performance on image recognition of this method, such as ImageNet-1k fine-tuning? Of course, image recongnition performance is out of scope of this paper but the comparable performance on image recongnition can improve the contribution of this paper. \n*  [Minor] If I correctly understand, the training data are doubled due to the proposed mask approach. How is the performance on the same computing cost?\n\n### References\n* [Yu et al. 2022] [CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/abs/2205.01917). arXiv:2205.01917\n* [Wang et al. 2022a] [SimVLM: Simple Visual Language Model Pretraining with Weak Supervision](https://arxiv.org/abs/2108.10904). ICLR 2022.\n* [Yuan et al. 2021] [Florence: A New Foundation Model for Computer Vision](https://arxiv.org/abs/2111.11432). arXiv:2111.11432.\n* [Wang et al. 2022b] [Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks](https://arxiv.org/abs/2208.10442). arXiv:2208.10442.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this paper is well-written and easy to follow. However, there are some points to be further clarified.\n* Figure 1 and its corresponding desciption in Introduction might lead to mislead. The proposed method relies on the random masking approach not considering explicit sematic prior. However, the examples look strongly-aligned masking. Therefore, the author need to clarify the description in Introduction.\n* The author argued that \"This will potentially lead to biased performance in cross-modal retrieval tasks such as image-to-text or text-to-image retrieval.\" This argument need reference or prelimnary empirical analysis. \n* The author need to describe what ViT model is used for reproducibility. Also, I recommend writing reproducibilty section. \n* For novelty, please see the weakness.\n",
            "summary_of_the_review": "Overall, this paper proposes a simple but effective method and is easy to read. On the other hands, this paper has limited novelty and missed some important related work. If the author can address my concerns, I am willing to raise my score. I look forward to the author response. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2015/Reviewer_RfUs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2015/Reviewer_RfUs"
        ]
    },
    {
        "id": "lNWQCGQ4UK",
        "original": null,
        "number": 3,
        "cdate": 1666574891459,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666574891459,
        "tmdate": 1669776904605,
        "tddate": null,
        "forum": "ZhuXksSJYWn",
        "replyto": "ZhuXksSJYWn",
        "invitation": "ICLR.cc/2023/Conference/Paper2015/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose masked language and image modeling to pre-train image and text representations for downstream tasks such as image-text retrieval and various image-language reasoning tasks (VQA, NLVR, visual entailment). The proposed model is trained on paired image-text data, where one modality is masked while the other is kept unmasked. Given these two inputs the goal is to reconstruct the masked modality. In addition, a contrastive text-image loss (like CLIP) and an image-text matching loss are used.\n\nThe authors compare against a multitude of baselines on the tasks, which for the wide majority they beat. In addition, they show that their method works better in the low-data regime, probably because of the additional masked image modeling objective. The losses in the model are then ablated on the image-text retrieval text and it is shown that adding MIM and MLM improves the model. Finally, qualitative results are presented showing how additional cross-modal information helps prediction. Ablations on masking strategy and ratios are given in the appendix.",
            "strength_and_weaknesses": "Strengths:\n\nThe authors compare against a large number of baselines, and are able to show better performance in almost all instances.\n\nQualitative results as well as ablation of the objectives for pre-training provide compelling evidence that cross-modal masked modeling can help to produce better representations.\n\nWell designed experiment shows that in the low-data regime, adding a masked image modeling loss can improve performance by making more efficient use of the data.\n\nGood ablation studies of masking ratios and strategies in the appendix.\n\nWeaknesses:\n\nOne significant problem with the paper is that many recent and concurrent methods and references are missing. As a result, many of the tables that claim to compare MaskVLM to SOTA results do not actually:\n\n- Table 1 lists SOTA methods on MSCOCO and Flickr30k image-text retrieval with finetuning. It is however missing Florence, which beats all methods in the table.\n- Table 2 lists SOTA methods on zero-shot Flickr30k image-text retrieval. It is also missing Florence, and CoCA.\n- Table 3 lists SOTA methods on VQA but is missing the above methods and SimVLM and Flamingo.\n\nIf the claim is that MaskVLM achieves SOTA with specific qualifications (such as pre-training on a smaller dataset) this should be stated and shown explicitly. If the claim is that MaskVLM achieves SOTA outright, then I don't think this is true.\n\nSome papers that should be cited:\n- Florence\n- CoCA and SimVLM\n- Flamingo\n- MultiMAE\n- M3AE (could be optional I think)\n- BEiT-3 (this came out after ICLR submission deadline, but is relevant)\n\nIn addition, claims such as \"In the domain of V+L learning, there exist limited works on end-to-end joint modeling of V+L signals\" are untrue, in light of the above references.\n\nThe method and its presentation are also a tad complicated. It could help significantly if the explanation of the method was cleaned up a bit. Here are some things that I found confusing:\n\n- The first term in Eq. 1 appears to show the MLM loss is applied to unmasked tokens as well as masked tokens, as opposed to just masked tokens as in BERT. Is this a typo, or actually the case? And if so, why was this decision made?\n- In Eq. 1, there is an expectation over D on the second term but not the first. I'm assuming this is a typo? \n- One more nitpick for eq 1: I think $ g_{im}^{de} $ in the definition of $ \\phi_{im} $ should also take $ f_{txt}(T) $ as an argument, as the image decoder is also allowed to cross-attend to vision text information.\n- Why use an L1 loss for MIM rather than L2 like in MAE?\n- In order to fuse representations, element-wise multiplication is used (in the downstream task heads, and the ITM loss). This seems non-standard to me, so perhaps a reference to previous work that does this could be helpful.\n\nOne small nitpick: MAE is cited as a preprint, but it was published at CVPR 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: For the most part clear. The method exposition and the math could be tidied up a bit.\n\nQuality: Decent quality.\n\nNovelty: Somewhat novel. MaskVLM is a rearrangement of existing basic building blocks (cross-attention, ITM loss, contrastive loss, MLM/MIM) from existing papers in a novel way.\n\nReproducibility: Could probably be roughly reimplemented from the details in the paper and the appendix, although a few details are missing (pre or post norm in the transformers, layer decay during fine tuning, weighting on the three losses, etc).",
            "summary_of_the_review": "This paper presents a somewhat novel method for image-language pre-training and shows that it beats a large number of baselines. The problem is that it omits quite a few recent or concurrent baselines in the experimental and related work section. The claims of SOTA in the paper then seem somewhat dubious to me. See the weaknesses section for more details.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2015/Reviewer_kwqv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2015/Reviewer_kwqv"
        ]
    },
    {
        "id": "HAyHudBxRb",
        "original": null,
        "number": 4,
        "cdate": 1666647982309,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647982309,
        "tmdate": 1666647982309,
        "tddate": null,
        "forum": "ZhuXksSJYWn",
        "replyto": "ZhuXksSJYWn",
        "invitation": "ICLR.cc/2023/Conference/Paper2015/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the use of joint masking for vision and language for learning representations from text and images. The proposed approach is tested on multiple tasks and achieves state-of-the-art results especially in the case of limited training data.",
            "strength_and_weaknesses": "Strengths\nThe paper is well written and easy to follow. \n\nAn interesting approach based on jointly masking images and text for learning visual and text representations is presented.\n\nExperiments on multiple tasks and datasets are presented. A detailed ablation study is also conducted.\n\nWeaknesses.\n\nIn several occasions the authors claim that other methods have been trained on much larger datasets. As a consequence this makes comparison with these works problematic. Is it possible that these methods are trained on the same amount of data as the one used in this study? Alternatively, would it be possible that the proposed method is trained on much larger datasets?\n\nIn order to investigate the impact of the each loss term it would be desirable that only one of them is removed at a time. In other words, it would it would be very informative to also include results with the following losses (in Table 5): MLM + MIM + ITC and MLM + MIM + ITM.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the proposed method is novel enough. The authors do provide a lot of details about the training process, so it might be possible for the results to be reproduced. ",
            "summary_of_the_review": "A very interesting contribution with several experiments and a detailed ablation study.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2015/Reviewer_UZ4d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2015/Reviewer_UZ4d"
        ]
    }
]