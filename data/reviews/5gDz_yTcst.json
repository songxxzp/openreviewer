[
    {
        "id": "AUdSnu_imK",
        "original": null,
        "number": 1,
        "cdate": 1666342641074,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666342641074,
        "tmdate": 1670529774060,
        "tddate": null,
        "forum": "5gDz_yTcst",
        "replyto": "5gDz_yTcst",
        "invitation": "ICLR.cc/2023/Conference/Paper3510/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose to revise the conclusions of two different selective classification methods. They argue this methods obtain betters results by just providing a more robust training. Thus, at inference step, the selective solution can be discarded. With this variation, they are able to increase the results obtained by the original paper. Furthermore, they provide an entropy regularization term that can be used in semi-supervised learning. ",
            "strength_and_weaknesses": "Strength\n* The idea is simple and easy to implement\n* The results are interesting\n\nWeaknesses\n* The semi-supervised entropy regularization is not new [1].\n* Only one network was tested.\n* The paper organization is poor.\n\n[1] Grandvalet, Y., & Bengio, Y. (2004). Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n* The idea is simple and easy to implement.\n* Both bold and underlined values presented in all Tables are not properly explained. They should be explained in the regular paper, not only at the appendix.\n* It is not clear how the results of Table 5 are obtained. The authors suggest the entropy-regularized loss function for semi-supervised purposes, but I did not find any mention about how they remove some labels of the supervised dataset.\n* I am not sure how the authors select the coverage from the state-of-the-art methods. In the case of SelectiveNet, did the authors train the model choosing different $c_{target}$ values? In the case of the self-adaptive training, did the authors use the last Eq. of section 3.1? It seems the self-adaptive training is made with the purpose of discarding a solution only if the $C+1$ class has the highest accuracy.\n* All equations should be numbered\n\nQuality\n* The provided results are interesting, but only one network was tested. I suggest the authors to introduce more networks with different architectures, in order to ensure the results provided are consistent.\n\nNovelty\n* Although the idea is simple, it could be very effective. However, I am not sure if the experimental results are fair, given the concern I had explain regarding to the clarity of the paper.\n* Besides that, the entropy-regularization term for semi-supervised learning is not novel [1]. Although, as the entropy regularization is zero whenever all the values are o or 1, the idea is pretty similar to the active learning approach, where the non-labelled examples are labelled after each training epoch.\n\nReproducibility\n* The authors provide enough information to successfully reproduce the networks, but they should include more information regarding how the state-of-the-art methods were trained (see the clarity section for details). \n\n[1] Grandvalet, Y., & Bengio, Y. (2004). Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17.",
            "summary_of_the_review": "Although the idea is interesting, the proposed solutions are either not new (entropy-regularization term) or not properly explained. Thus, I cannot recommend to publish this contribution in its actual form.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3510/Reviewer_6JTV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3510/Reviewer_6JTV"
        ]
    },
    {
        "id": "JJMgS8E800c",
        "original": null,
        "number": 2,
        "cdate": 1667072232150,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667072232150,
        "tmdate": 1669244804638,
        "tddate": null,
        "forum": "5gDz_yTcst",
        "replyto": "5gDz_yTcst",
        "invitation": "ICLR.cc/2023/Conference/Paper3510/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors point out that predictive distribution base classifier within in selection classification captures\nbetter signal for selection rather than a separate selection/abstention logit present in various approaches of selective classification within deep learning. \n",
            "strength_and_weaknesses": "Strengths:\n\nPaper is well written and easy to understand, and authors include various techniques of selective classification.\n\nWeakness:\n\nThough the point made is effective in experiments its only marginally better, including entropy regularized loss.\nOn CIFAR, we see the difference as we decrease coverage to 70 but even at 80% coverage selection mechanism and softmax response are at par and already <1% error. In case of imagenet we see 1.5% improvement at 80% coverage.\n\nAlso, authors do not provide further insight into why the observation exists, inspired by OOD detection work which demonstrate that max-logit is best performing OOD metric, and [3] makes an argument that max-logit is better because within Deep learning we check for familiarity of an input rather than absence of key features. So, based on OOD/anomaly detection signals & familiarity hypothesis we know it's always easy to detect presence than reject/ do good uncertainty quantification within our current deep learning practices.\n\nIs there any reason for authors to not consider \n\t1. H. Mozannar et al. Consistent Estimators for Learning to Defer to an Expert (Learning to Defer) (ICML 2020)\n\t2. N. Charoenphakdee et al. Classification with Rejection Based on Cost-sensitive Classification (ICML 2021)\n        3. T.G. Dietterich et al. The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set Methods (arXiv: 2203.02486)",
            "clarity,_quality,_novelty_and_reproducibility": "Authors make a useful observation for using selective classification, but whole idea of selective classification is to obtain a cost-sensitive prediction at training and inference time. As the current paper does not utilize the observed insight to propose a an improved selective classification training paradigm or provide more insights/practices, nor significant performance improvement.\nAlso, w.r.t entropy minimization regularization it might not be fruitful to minimize entropy on all instances?\nInspired by works which leverage uncertainty in case of pseudo labels[1] or even selective classification. In case of [2] authors discard hard samples to improve overall coverage vs accuracy tradeoff within selective classification. \n\nReferences:\n\t1. M.N. Rizve et al . In Defence of Pseudo-Labeling: An Uncertainty aware pseudo label selection framewrok for semi-supervised learning (ICLR 2021)\n\t2. Y. Ding et al. Uncertainty-Aware Training of Neural Networks for Selective Medical Image Segmentation (MIDL 2020)\n",
            "summary_of_the_review": "Though authors make an interesting point and demonstrate effectiveness of their observations. For lack of novelty, or lack of additional insights, currently in my humble opinion not convinced if the work warrants an ICLR acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3510/Reviewer_wKA4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3510/Reviewer_wKA4"
        ]
    },
    {
        "id": "zY8S20T07n1",
        "original": null,
        "number": 3,
        "cdate": 1667281659292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667281659292,
        "tmdate": 1667281659292,
        "tddate": null,
        "forum": "5gDz_yTcst",
        "replyto": "5gDz_yTcst",
        "invitation": "ICLR.cc/2023/Conference/Paper3510/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The problem of Selective Classification considers how to train the model\u2019s ability to abstain from a decision when the credibility is low. In this paper, the authors analyzed the state-of-the-art methods and found that learning a more generalizable classifier is more important than using some selection mechanisms in the problem of Selective classification. Besides, they found that the Softmax Response performs better than the entropy-based selection mechanism. Through researching the relationship between Selective Classification and Semi-supervised learning, the authors proposed an entropy-regularized loss function to improve the performance. The experimental results show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "**Strengths**\n\n1. The authors introduced the state-of-the-art methods (SelectiveNet, Self-Adaptive Training, and Deep Gamblers) and the proposed method through mathematical expressions. \n2. This paper analyzes the actual effective part of the state-of-the-art methods and the correlation between the Selective Classification problem and Semi-supervised learning. The proposed method has a better research premise and innovation.\n3. The experiment compares the state-of-the-art methods with the proposed method and illustrates the point of view of the paper.\n\n**Weakness**\n\n1. The proposed method is a bit hard to follow.  \n2. The technique of the proposed method seems to be simple (i.e., mainly from the  entropy-regularized loss function).\n3. The experiment is not well executed (e.g., $\\beta$ is chosen as 0.01 for the proposed method, and not results under different settings).  The experimental results do not seem to work well on the Cifar10 dataset, while performing well on the two synthetic datasets generated by the authors. This kind of experimental results is unconvincing, as they may look artificial.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The overall presentation is okay, but the description of the proposed method can be improved (cf. Summary of The Review)\n\nQuality and Novelty: This aspect is average, as the proposed method is mainly based on entropy-regularized loss. The authors may need to clarify or highlight the novelty of the work.\n\nReproducibility: The results look reproducible.",
            "summary_of_the_review": "* The authors may consider adding some visual expressions to help readers better understand the proposed method.\n* The proposed method is mainly based on the  entropy-regularized loss function, which has a small amount of space in the article. The authors may point out more properites of the proposed method, so that readers can better appreciate the method. Besides, the authors may consider demonstrating the advantages of the proposed method compared with the state-of-the-art methods from the theoretical level.\n* In the experiment, $\\beta$ is chosen as 0.01 for the proposed method, the authors may consider giving some specific experiments to illustrate the effectiveness of the method under different settings.  The experiments do not seem to work well on the Cifar10 dataset, while the good results are on the two datasets constructed by the authors themselves. The authors need to add some datasets which also used by other researchers to improve the credibility of the experiment.\n\n* Typo: the the -> the (cf. the last paragraph line 4 on page 4)",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3510/Reviewer_F9DQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3510/Reviewer_F9DQ"
        ]
    },
    {
        "id": "CXmePsLj8g",
        "original": null,
        "number": 4,
        "cdate": 1667334046086,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667334046086,
        "tmdate": 1667334046086,
        "tddate": null,
        "forum": "5gDz_yTcst",
        "replyto": "5gDz_yTcst",
        "invitation": "ICLR.cc/2023/Conference/Paper3510/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper tackles an interesting question regarding model decision making. Specifially, the authors are looking into the ability of a model to abstain from making a decision, when it has low confidence scores: The \u201cSelective Classification problem\u201d. \n\nThe authors demonstrate that: \n1) the current SOA models\u2019s performance can be attributed to their ability to train a more classifier than is more generalizable (than the logit selection mechanism).\n2) semi-supervised learning-based models have better selective classification ability. \n3) the new selective classifier (SR: SoftMax response) could improve current SOA models\n",
            "strength_and_weaknesses": "Pros:\nI found the paper to be generally very well written and the experiments well motivated. The most interesting part of the paper to me was the connection with self-supervised models.\nI also appreciated the authors reporting the results with errorbar statistics. I wish more CS/AI researchers emulate this\nI found the results to be quite thorough. I liked seeing the results generalize to different tests and reported across the number of classes (where selective decision becomes more of a problem). \n\nCons:\nIn some instances, I felt the results were slightly overstated. The difference between the entropy loss and the SR loss is relatively subtle across the board. \nThe relation with semi-supervised learning-based models could be better explored. At the moment it is an observation. Readers would like to get an intuition for why this happens. \n",
            "clarity,_quality,_novelty_and_reproducibility": "I found the paper well written. The experiments were thorough and the relation with semi-supervised learning is indeed novel, albeit less well fleshed out. ",
            "summary_of_the_review": "Overall the paper addresses an important question about Selective Classification. I was wondering if there are practical applications of this specific decision making approach. It would be a good addition to articulate the utility of this approach, especially in CV related tasks. The relation with the semi-supervised learning approaches is also interesting but less well understood. I encourage the authors to spend more time fleshing that section out in the paper (or in future work). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3510/Reviewer_hDSo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3510/Reviewer_hDSo"
        ]
    }
]