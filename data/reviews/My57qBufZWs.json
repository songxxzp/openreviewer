[
    {
        "id": "CImyyUemevx",
        "original": null,
        "number": 1,
        "cdate": 1666118604653,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666118604653,
        "tmdate": 1666119383793,
        "tddate": null,
        "forum": "My57qBufZWs",
        "replyto": "My57qBufZWs",
        "invitation": "ICLR.cc/2023/Conference/Paper807/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new optimizer called Bort to improve model explainability (comprehensibility and transparency) with Boundedness and orthogonality constraints on model parameters.",
            "strength_and_weaknesses": "Strengths\n+ Explainability is a critical and challenging field in ML, and the mathematical attempt proposed in this paper is encouraging.\n\nWeaknesses\n- Boundedness constraint seems similar to cosine similarity. Any difference?\n- The discussion to treat CNN and Transformer as MLP is quite sketchy, e.g. \"can be unified under one meta-structure, a multi-layer perceptron (MLP for short) with optional nonparametric operations. For example, the convolutional layer and the transformer layer additionally use folding/unfolding and the self-attention operation, respectively\". Will parameter sharing and attention introduce extra complexity and inconsistency to the definition and analysis of comprehensibility and transparency?\n-\"the row vector wij in Wi\". wij is a weight, right? How can a weight \"look similar to a specific data pattern\"? What does \"look similar\" mean? How can weights look similar to an input data pattern after many layers of non-linear transformation?\n- The definition of comprehensibility is confusing. Is a semantic data pattern z a vector or scalar value? How will z be proportional to a weight wij? especially in later layers?\n- The definitions of comprehensibility and transparency are debatable, and it is also a question whether comprehensibility and transparency are sufficient for complete explainability.\n- Both boundedness and orthogonality constraints seem to limit the expressivibility of FNNs, any insight why they actually improve accuracy? some regularization effects?\n- Although  a two-layer network is a universal approximator, proposition 1 is on  a two-layer linear network.",
            "clarity,_quality,_novelty_and_reproducibility": "Quality\n\nThis paper is technically solid. However, it is over-ambitious as over-simplifying explainability to two measures: comprehensibility and transparency, whose definitions are debatable. \n\nExperiments are not adequate as it only compares with CAM, a 2016 paper.\n\nClarity\n\nThe paper is written well in general. Please fix notations as pointed out above.\n\nOriginality \n\nThe attempt of mathematical definition and analysis of explainability is novel and plausible.",
            "summary_of_the_review": "The approach of mathematical definition and analysis of explainability proposed in this paper is novel and plausible. However, the paper is over-ambitious and experiment is not adequate.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper807/Reviewer_dhB8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper807/Reviewer_dhB8"
        ]
    },
    {
        "id": "ACjmDzq3Vv",
        "original": null,
        "number": 2,
        "cdate": 1666641110753,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641110753,
        "tmdate": 1668877269212,
        "tddate": null,
        "forum": "My57qBufZWs",
        "replyto": "My57qBufZWs",
        "invitation": "ICLR.cc/2023/Conference/Paper807/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces two methodological components: (1) BORT, a regularization for feed-forward neural networks term that encourages the various layers to implement a bounded orthogonal projection, (2) SAT, a saliency algorithm specifically designed for BORT-regularized networks.  The empirical evaluation highlights how BORT can improve accuracy of several models on MNIST and CIFAR10, and that BORT+SAT produces \"clear\" saliency maps with good deletion and insertion scores (which measure sensitivity of model predictions to pixels identified as relevant by the saliency maps).  In addition, the BORT+SAT combo facilitates acquiring features with input reconstruction capabilities and generation of adversarial examples.",
            "strength_and_weaknesses": "PROS\n- Proposed idea is is based on a solid intuition.\n- Formalization is clean and looks correct.\n- No performance drop upon applying BORT - actually, quite the contrary.\n- Text is mostly well written.\n\nCONS\n- Key element (SAT) is not well described, complicating evaluation of the results.\n- Other aspects of the experiments are not well explained.\n- Some terminological choices are a bit confusing.\n- Relationship to existing approaches is not entirely clear.\n- A subset of the results rely on guided backpropagation, which is known to be flawed.\n- Only two smaller image classification data sets (not a huge deal).",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**:  The text is generally well written and well structured.  English is good, with only few grammatical issues here and there.  Pictures are also quite clear.\n\nThe formalization is relatively clean and convincing.\n\nI think that some key terminological choices could be improved.  The terms \"explainability\", \"comprehensibility\", and \"transparency\" have a far more general (and fuzzily defined) meaning - in English - than the mathematical properties defined and used here.  Specifically:\n\n - \"Explainability\" has as much to do with algorithmic aspects of the explanation process as it has to do with human-centric, psychological factors such as knowledge and cognitive biases.  As such, attempting to solve it formally is (currently!) not possible.  The authors should tone down some of their claims regarding the scope of their formalization (e.g., my honest opinion is that being able to map latent features back to input space is not quite enough:  a latent point might map back to an example that has very little semantic meaning for human observer, say, a garbled image resulting from a measurement mistake or an uninformative region of a valid image.  The BORT formalization does little - and *can* do little - to prevent this from happening, see also below.)\n\n - What the authors called \"comprehensibility\" is essentially the idea that weights should recover concrete examples, i.e., prototypes.  \"Prototype-based\" is a better and more well-known term compared to \"comprehensibility\".\n\n - What the authors called \"transparency\" means that latent variables can be mapped back to input space.  This sounds more like an approximate form of \"invertibility\" to me.\n\nWhat is confusing is that all three terms (explainability, comprehensibility, transparency) are used with different meanings in the rest of the literature.  I do encourage the authors to rename them to something more unique and easily recognizable.\n\nAnother element I did not understand is why BORT is being marketed as an optimizer.  To me it really looks like a regularization term.  Not a huge deal, just slightly confusing.\n\n\n**Quality**:  The idea underlying BORT is to turn a neural network into a stack of orthogonal projections, with individual weight vectors ending up resembling a set of diverse (i.e., orthogonal) examples.  The intuition is quite clear and reasonable, and it is very much in line with prototype-based models (see below).  BORT is overall quite straightforward, but this to me is actually a plus.\n\nOne aspect I would mention in the paper is that stacks of orthogonal (linear) projections are incapable of learning, because they do preserve information whereas generalization is by its own nature lossy.  BORT works regardless because, thankfully, architectural limits (e.g., non-square weight matrices) prevent BORT-optimized neural networks from acquiring perfect orthogonal projections.\n\nI found the empirical evaluation not entirely convincing, for several reasons:\n\n- The experiments in Sec. 4.1 and 4.2.1 are quite clear and are positive.\n\n- Those in Sec. 4.2.2 are very compressed and hard to parse.  It is not clear to me how guided backpropagation (a saliency technique) is used to \"reconstruct the input\" (Fig 4) and what the output is meant to represent: an input or a saliency map?\n\n- Another issue with that guided backpropagation is known to be insensitive to the weights learned by the model, see:\n\n  Adebayo J, Gilmer J, Muelly M, Goodfellow I, Hardt M, Kim B. Sanity checks for saliency maps. NeurIPS 2018.\n\nHow does this impact Fig 4?\n\n- The **worst** issue in the paper is the description of the SAT algorithm, which is very compressed and hidden in the experiments.  SAT, despite not having been mentioned at any point in the paper, turns out to be **integral** to BORT.  In fact, using more well-known saliency techniques with BORT-trained model is not ideal (see Table 3).  Alas, I could hardly understand what SAT does.  For instance, why the result needs to be averaged over multiple images:  the way it is described, SAT sounds like a deterministic procedure so I don't see the need for averaging.  I am very confused as to why the \"backprojection\" SAT procedure should result in a saliency map, and what the semantics of this map are meant to be, i.e., why it can be used as an explanation.  This makes it very hard to evaluate the saliency map experiments, which are the core of the paper.\n\n- The decomposition and synthesis experiments are also overly compressed and hard to follow.\n\nI think SAT should be introduce much earlier in the paper, as early as the abstract, and the reason why it is needed should be clarified.  Right now it is not clear why BORT - whose main aim is to improve interpretability -  does *not* really improve interpretability unless used together with SAT.  It is very hard to evaluate BORT in isolation.\n\n\n**Novelty**:  The idea of learning orthogonal transformation is not new in ML, but to the best of my knowledge this it has not been used in the context of learning deep feed-forward neural nets for the purpose of explainability.\n\nThe authors do a good job at discussing related work from the white- and black-box models, but they completely neglect the recent literature on \"gray-box\" (or \"concept-based\") models, including:\n\n  Prototype Classification Networks (PCNs):  Li, Oscar, Hao Liu, Chaofan Chen, and Cynthia Rudin. \"Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions.\" AAAI 2018.\n\n  Part-prototype Networks (PPNets):  Chen C, Li O, Tao D, Barnett A, Rudin C, Su JK. This looks like that: deep learning for interpretable image recognition. NeurIPS 2019.\n\n  Concept Whitening (CW):  Chen, Z., Bei, Y. and Rudin, C.. Concept whitening for interpretable image recognition. Nature Machine Intelligence, 2020.\n\n  GlanceNets:  Marconato E, Passerini A, Teso S. GlanceNets: Interpretabile, Leak-proof Concept-based Models. arXiv 2022.\n\nMost of these works are somehow related to BORT:  (i) PCNs make use of autoencoders to map latent variables to input space, adding extra constraints to ensure that the reconstruction resembles a concrete example (in a vein reminiscent of BORT), (ii) CW anticipates the idea of leveraging orthogonality to enhance interpretability, although from a slightly different angle than BORT,  (iii) PCNs and PPNets work by learning prototypes, which seems very much the objective of the \"comprehensibility\" criterion introduced here;  (iv) GlanceNets do come with a sound formal definition of explainability, as does the work by Hazan et al (not a gray-box model, though):\n\n  Wolf L, Galanti T, Hazan T. A formal approach to explainability. AIES 2019.\n\nThere are several other attempts at formalizing explainability out there.\n\nI think that the authors should at the bare minimum mention the existence of gray-box models (especially those based on prototypes and orthogonalization, which are *very* close in spirit to BORT) and their relationship to the BORT desiderata, as well as highlight that there *are* alternative attempts at formalizing explainability.\n\n\n**Reproducibility**:  The code is provided in the supplement.",
            "summary_of_the_review": "Explainability-enhancing regularizer based on per-layer orthonormalization + ad-hoc saliency map algorithm.  Alas, the latter is poorly described, complicating evaluation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper807/Reviewer_t9ra"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper807/Reviewer_t9ra"
        ]
    },
    {
        "id": "CislBJN4_8",
        "original": null,
        "number": 3,
        "cdate": 1666675621159,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675621159,
        "tmdate": 1666675621159,
        "tddate": null,
        "forum": "My57qBufZWs",
        "replyto": "My57qBufZWs",
        "invitation": "ICLR.cc/2023/Conference/Paper807/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors develop notions of comprehensibility and transparency as indicator of explainability, and develop formal notions to introduce these as constraints during model training. The authors also propose a new technique for generating saliency maps. Subsequently, they show that inclusion of these constraints improves accuracy as well as the insertion/deletion metric for quantifying the quality of saliency maps.",
            "strength_and_weaknesses": "Baking explainability into models by somehow making them inherently interpretable is a relevant problem. The approach proposed in the paper seems sensible and the results look convincing. Some more thoughts:\n\n1. I am not sure if the notions of comprehensibility and transparency are well-established. With that in mind, the presentation of these appears somewhat ad-hoc.\n\n2. While the development appears logical, the end result is quite simple \u2014 instead of penalizing the L_p norm of the parameters in the loss function, penalize deviation from orthogonality instead. I wouldn\u2019t be very surprised if a little more algebra reveals some relationship to L_p norm. However, simplicity itself is not something to criticize.\n\n3. While I wasn\u2019t very convinced of the value of the proposal during the development, the results look quite good. Specifically, the improvement in accuracy, improvement in explainability metrics, feature composability, and generation of adversarial examples are all well documented. The only sub-section that seems unsurprising is the verification of properties \u2014 orthogonal constraints lead to orthogonality, and the inherent reconstruction constraint leads to good reconstruction.\n\n4. I am not quite sure if I can clearly argue why the technique produces good results. The constraints were developed for one set of objectives yet they somehow improve on another (better established) set as well. A discussion of that and authors\u2019 thoughts on the possible explanation would improve the paper.\n\n5. Minor errors: \u201cpossesses comprehensible and transparent simultaneously\u201d, wrong double quotes in \u201crelation\u201d, \u201conly and if only\u201d, \u201cequivalent whether to exert\u201d, \u201cbenefits of the two constraints benefit of transparency\u201d, add x-axis label to Fig 3a, \u201cassumption f enough\u201d.",
            "clarity,_quality,_novelty_and_reproducibility": "Fairly clear, good quality, novel without being ground-breaking, and appears reproducible especially once the code is released.",
            "summary_of_the_review": "Overall, while I think the formalization does reduce to straightforward constraints, the results are (surprisingly) good, and the community could benefit from wider dissemination. I would recommend the paper to be accepted at the venue based on the performance but I would have much appreciated any deeper insights.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper807/Reviewer_eZA8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper807/Reviewer_eZA8"
        ]
    }
]