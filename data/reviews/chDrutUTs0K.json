[
    {
        "id": "81vlaT2DyI",
        "original": null,
        "number": 1,
        "cdate": 1666125008705,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666125008705,
        "tmdate": 1666240034274,
        "tddate": null,
        "forum": "chDrutUTs0K",
        "replyto": "chDrutUTs0K",
        "invitation": "ICLR.cc/2023/Conference/Paper1223/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submission introduces a library containing 14 POMDPs, each with three levels of difficulty, and 13 memory model baselines. The submission benchmarks 12 of these memory baselines.",
            "strength_and_weaknesses": "### Strengths\n\n- I agree with the submission's characterization that there is a lack of suitable partially observable testbeds.\n- Because the submission compares existing algorithms, rather than introducing new ones, the experiments are less likely to suffer from experimenter bias.\n- From a brief perusal, the code appears easy to read and well organized.\n\n### Weaknesses\n\n- While (again) I agree that there is a dearth of good POMDP benchmarks, I am not sure the submission does a great job addressing the problem:\n    - Six of the 14 games are small modifications of fully observable settings\n    - Four are described as \"diagnostic\", meaning useful as \"memory introspection and debugging tools\"\n    - Two are relatively simple card games,\n    - Two are navigation based, which, as the submission points out, are one of the few types of POMDPs for which there are suitable existing benchmarks.\n\n    What I am getting at is that the partial observability in POPGym largely doesn't feel *strategically interesting*.\n- The other gripe I have is with algorithm benchmarking:\n    - First, given that the submission is a benchmarking paper, it seems disappointing that it implements but does not benchmark differentiable neural computers. The submission states that this is because computation cost, despite also indicating that POPGym environments induce relatively quick convergence.\n    - Second, the metrics that were reported are not up to date with modern deep RL benchmarking -- see *Deep RL at the Edge of the Statistical Precipice* (NeurIPS 2021). \n        - The most obvious criticism here is that each algorithm is over only three seeds. It is well documented that three seeds may be insufficient to accurately reflect performance. Given that it is a benchmark paper, I'd hope for at least 5 or 10. \n        - Additionally, it would be good to include the metrics endorsed by the statistical precipice paper, such as interquartile mean, optimality gap, performance profiles, and stratified bootstrap confidence intervals.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the work is good.\n\nThe quality is fair; however, as articulated in the weaknesses section, there is substantial room for improvement.\n\nThe originality is good in the respect that there has not been much work organizing POMDP benchmarks for deep RL.\n\nThe reproducibility is high in the sense that code is available and appears to be well organized but less high in the sense that outlier-sensitive results were reported over a small number of runs.",
            "summary_of_the_review": "While the idea motivating the submission (providing good POMDP benchmarks for deep RL) is one that is very valuable to the community, I think the submission falls short of its potential both in terms of the proposed environments and the benchmarking of existing algorithms.\n\nStill, I think the submission ought to be accepted, as the value it provides to the community is strictly positive. \n\nI hope the authors will consider addressing the weaknesses listed above in future revisions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1223/Reviewer_HVVV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1223/Reviewer_HVVV"
        ]
    },
    {
        "id": "_nZouDzuIL",
        "original": null,
        "number": 2,
        "cdate": 1666664256214,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664256214,
        "tmdate": 1666664256214,
        "tddate": null,
        "forum": "chDrutUTs0K",
        "replyto": "chDrutUTs0K",
        "invitation": "ICLR.cc/2023/Conference/Paper1223/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduced a set of new environments to test RL algorithms on partially observable environments. The authors introduced a diverse set of environments in which different aspects of the hardness of partially observed system was tested. Further, the authors conduced quite extensive study and tested the performance of previously suggested algorithms for the partially observed domain. \n",
            "strength_and_weaknesses": "Pros:\nPartially observed system are a crucial challenge to tackle for applying RL in real-world applications. I tend to agree with the authors claim that currently there are not enough benchmarks to test different RL algorithms for the partially observable settings. I hope that this work will encourage further research into the partially observable setting, and lead to the design of new and practical algorithms for this domain. \n\nCons:\nAlthough the authors introduced several new environments there are several aspects of partially observed systems that are not being investigated. First, the problems (in my opinion) are quite basic and still seem quite far from practical applications. Second, the theoretical RL community recently studied several POMDP models that would be interesting to explore from practical perspective (in my opinion):\n1. Weakly revealing POMDPs (see https://arxiv.org/pdf/2204.08967.pdf).\n2. MDPs with latent context (see https://arxiv.org/abs/2102.04939).\n3. \\gamma observable POMDP (see https://arxiv.org/abs/2206.03446)\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is quite clear and in good quality in my opinion. Below are some minor comments.\n\n1. This it would be worthwhile to add that current benchmarks test the high dimensional observations.\n2. Figure 3. Can you normalize the reward such that 1 would indicate the optimal value? if I understand correctly it seems that the normalisation might make it seems the performance is high although it is extremely sub optimal (this can happen due to the normalization).\n3. End of section 1.1. should put period \".\" at the end of each line.\n4. page 3, \"wall following behavior\", what does that mean?\n5. page 5 \"Like Repeat Fist\" -> \"Like Repeat First\"\n6. Page 5. Reference to additional bandit book: https://tor-lattimore.com/downloads/book/book.pdf.",
            "summary_of_the_review": "Overall, I think this paper has a clear and nice contribution: it introduced new benchmarks for study partially observed RL problems, This, in my opinion, is very much worthwhile and will hopefully encourage the community to study these type of problems in more depth. Further, I hope that the authors/additional contributers will expand the set of task in the future.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1223/Reviewer_X3ta"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1223/Reviewer_X3ta"
        ]
    },
    {
        "id": "jE5ouCakJI",
        "original": null,
        "number": 3,
        "cdate": 1666683781742,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683781742,
        "tmdate": 1666683781742,
        "tddate": null,
        "forum": "chDrutUTs0K",
        "replyto": "chDrutUTs0K",
        "invitation": "ICLR.cc/2023/Conference/Paper1223/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper performs benchmarks on partially observable RL environments and a bunch of network architectures on memory. The authors make sure the learning of the POPGym environments is affordable with consumer-grade GPU to ensure the experiments are feasible for researchers. They conclude that GRU is the best general-purpose memory model. ",
            "strength_and_weaknesses": "Strength\n+ A very good and in-depth summary of the current RL benchmarks, especially on the partially observable environments. After analysis of the shortcomings of the current benchmarks, they motivate the work;\n+ The author conducted an extensive set of experiments with 14 partially observable environments and with 13 memory model baselines;\n+ Considerations have been made for wide adoption of the proposed benchmark, including low compute cost for a wide community, compatible APIs with the popular training library, etc;\n\nWeaknesses\n- I would expect the analysis part of a benchmark paper would take the largest portion of the paper and will have a very detailed analysis of the benchmarks and experiments. However, the major parts of the paper are about the task description and baseline description; \n- Though the authors have run very extensive experiments, the results do not seem dramatically different than what people already understood;\n- I  found limited insights from the benchmark results;\n- The authors should point out potential directions for tackling the partially observable tasks after the benchmarking. ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is high, the quality is decent, novelty is on a borderline level. The reproducibility is good.",
            "summary_of_the_review": "I found unsatisfactory that a benchmark paper does not provide much content on the in-depth analysis of experiment results, even though the authors conduct an extensive set of experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1223/Reviewer_eVQA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1223/Reviewer_eVQA"
        ]
    }
]