[
    {
        "id": "agzw6IuMCap",
        "original": null,
        "number": 1,
        "cdate": 1666517212891,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666517212891,
        "tmdate": 1668798671383,
        "tddate": null,
        "forum": "kjkdzBW3b8p",
        "replyto": "kjkdzBW3b8p",
        "invitation": "ICLR.cc/2023/Conference/Paper2673/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the problem of learning a set of diverse policies that are simultaneously nearly-optimal for a given reward function. In their formulation, the diverse near-optimal policies are learned by maximizing an objective function related to diversity, while near-optimality is ensured through a linear constraint on the value of each policy. Crucially, the diversity between the policies is measured through a distance (either L2 or Van Der Waals force) of the expected state-action features they induce. The paper proposes to optimize the introduced problem through a Lagrangian relaxation that can be seen as a three-player game. Finally, the paper provides an experimental analysis in the DeepMind Control Suite to show that\n- the proposed approach leads to the discovery of diverse nearly-optimal behaviors, where near-optimality and diversity can be traded-off in various ways;\n- the proposed Lagrangian approach dominates a simple scalarization of the diversity and near-optimality objectives;\n- the set of diverse nearly-optimal policies allows for efficient k-shot adaptation to reward perturbations.",
            "strength_and_weaknesses": "*After discussion*\n\nHaving read the authors' response and other reviews, I am changing my score from 5 to 6.\n\n----\n\n*Strengths*\n- (Novelty and relevance) The paper introduces an interesting objective that measures the diversity between the policies through a direct distance in the space of the state-action distributions. While previous works have tackled diverse skills discovery, this direct formulation is, to the best of my knowledge, novel;\n- (Experimental analysis) The proposed approach is evaluated in challenging domains, and the experiments' section includes nice visualizations and interesting findings.\n\n*Weaknesses*\n- (Theoretical ground) The theoretical ground for the proposed framework is weak. Especially, the paper proposes to maximize non-concave objectives, and convergence guarantees for the approach are questionable;\n- (Algorithm) The paper dedicates little space to the presentation of the algorithm, which should be a relevant contribution per se given the novelty of the problem, and the corresponding technical challenges (e.g., how the objective function can be estimated from samples); \n- (Comparison with previous works) Whereas the authors made clear that the experimental analysis is not intended to demonstrate the superiority against any baseline, previous works addressed both reward robustness as well as diverse skills discovery, and even an indirect comparison could have been telling.\n\n*Detailed Comments*\n\nOptimization Problem\n\n(C1, Major) The discovery of near-optimal policies is formulated through a maximization problem of a non-concave function, e.g., the L2-distance between the expected state-action features. While the objective is at least convex, and thus the gradient ascent procedure may work fine within the space, it is unclear what happens at the boundaries. This problem has been pointed out before, such as in (Eysenbach et al., 2021), which is mentioned in the paper. Especially, I am wondering whether the optimization problem proposed in the paper is tractable even when the underlying MDP is fully known. Can the authors assess the computational complexity of the problem?\n\n(C2) Also related to the previous comment: The optimal set of policies is attained by deterministic Markovian policies in this setting? To my intuition, if I am just maximizing the L2-distance between the occupancies of two policies, I should converge to deterministic policies. However, it is not obvious to me what happens with more than two policies and the value constraint.\n\n(C3, Major) Even ignoring the potential intractability of the problem, I am wondering what kind of game-theoretic guarantees do we have if all the policies are optimized simultaneously via gradient ascent. Have the authors some ground to say that the optimization would converge to some notion of equilibrium?\n\n(C4) The paper proposes two different objective formulation, either the L2-distance or Van Der Waals force. Can the authors better motivate why these two alternative have been chosen over other distances/divergences? Do we necessarily need a proper distance for the objective function? KL-divergence, which is not symmetric, could work as well?\n\n(C5, Minor) The problem is formulated through a maximization of the diversity objective under the value constraint. Do the authors considered also an alternative formulation though a set of problems in which the objective is the value of a policy under a diversity constraint? Would this formulation change the nature of the problem to make it (possibly) tractable?\n\nMethodology\n\n(C6) The methodology is not clearly explained in the main text, as it is mostly relegated to the Figure 1a, in which the architecture is presented, and a few lines at the end of Sec. 3.3 and the Agent paragraph in Sec. 4. Can the authors describe the methodology in details, and what kind of peculiar challenges it has to address?\n\n(C7) Especially, I am wondering how problematic it can be to estimate the objective function in practice. Estimating a distance/divergence through density estimations is known to be cumbersome in continuous/high-dimensional domains. Do the authors validated their estimators empirically?\n\n(C8, Minor) Previous works in imitation learning (e.g., Schroeker and Isbell, State aware imitation learning, 2017; Schroeker et al., Generative predecessor models for sample-efficient imitation learning, 2018) have considered the gradient of the L2 distance between state-action occupancies. Clearly, they follow a gradient descent procedure instead of the gradient ascent proposed in this paper to maximize diversity, but the gradient formula could still have some common traits.\n\nExperiments\n\n(C9, Major) Whereas the authors explained that they do not intend to compare their approach to any baseline, it is quite hard to actually evaluate the results, as we do not know how far the obtained policies are from the global optimum. Especially, \n- Can the authors show the value of the objective during training, to understand whether the algorithm is actually improving the value of the objective with a monotonic trend?\n- Can the authors show that the algorithm is converging, or do they just stop the training at some point while the policies would continuously change?  \n\nRelated Works\n\n(C10) While there is not a clear baseline for the proposed approach, as the problem formulation is novel to my knowledge, there exist previous methods that addresses similar problems. Especially,\n- Maximum entropy regularization for RL is known to provide policies that are reward-robust (see Husain et al., Regularized policies are reward robust, 2021);\n- The proposed objective with a void value constraint is similar in nature to the problem of (unsupervised) diverse skills discovery (e.g., Eysenbach et al., 2019).\n\n(C11, Minor) Another recent work (Mutti et al., Reward-free policy space compression for reinforcement learning, 2022) comes up with an optimization problem that have some similarities with the one proposed here, although the underlying motivation is different.",
            "clarity,_quality,_novelty_and_reproducibility": "To the best of my knowledge, the work proposes a novel formulation of diverse skills discovery through a direct distance between state-action occupancies induced by the policy, which is interesting. Thus, I think originality is a strength of this paper. The motivations for this work are well-reported, but the underlying theoretical ground and the proposed methodology could be described with more details and clarity.",
            "summary_of_the_review": "The idea proposed in this paper is interesting and original, and it has interesting connections with several previous works in the literature. On the one hand, it can be seen as a formalization of skills discovery methods when the reward constraint is set to void, whereas it is related to constrained convex RL otherwise, and to multi-objective RL if we scalarize the combination of the objective and constraint. It has also technical similarities with apprenticeship learning and state-based imitation learning.\n\nHowever, the provided theoretical ground for the approach is somewhat weak: It is not clear if the proposed optimization problem is tractable, and what kind of guarantees can we have optimizing it. The algorithmic contribution is also relegated to tiny bits of the paper, while it should be a main contribution to my understanding.\n\nOverall, my current evaluation for this work is borderline, but I am open to raise my score if the authors could address my comments above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2673/Reviewer_vof2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2673/Reviewer_vof2"
        ]
    },
    {
        "id": "E2S0y4gj_0i",
        "original": null,
        "number": 2,
        "cdate": 1666540144727,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666540144727,
        "tmdate": 1666595779671,
        "tddate": null,
        "forum": "kjkdzBW3b8p",
        "replyto": "kjkdzBW3b8p",
        "invitation": "ICLR.cc/2023/Conference/Paper2673/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the problem of jointly learning a diverse set of (nearly) optimal policies within a single RL environment. The paper follows the SMERL paper and adopts a novel mathematical framework based on convex MDP and Fenchel duality. The derived algorithm shows that we can simply solve the constrained optimization by designing an intrinsic reward based on the successor feature of the learning policy and the gradient of the diversity measure.",
            "strength_and_weaknesses": "## Strength\nI do enjoy reading the paper from a technical perspective and feel really interested to see the conclusion that the derived algorithm can be simply built upon the gradient of diversity measure, which is novel in my opinion. \n\nThe experiments are sufficient and solid from my perspective. The section contains sufficient ablation studies, emergent behaviors as well as the full suites of experiments in the original SMERL paper. \n\n## Weakness\nMy major concerns are about the presentation of this paper. Some of the details are listed below.\n\n1. The title is definitely a bad one. I do understand the space is pretty limited, but I still think the authors could have done a better job to come up with a more informative title, i.e., at least including the terms \"diverse\" and \"nearly optimal\". It is really a misleading title, although I do know this will be addressed in the camera-ready version. \n\n2. I have to express my disagreement with the statement \"we do not explicitly compare it with other work nor argue that one works better than the other.\" This is really an awkward statement in a paper, particularly knowing that it is even presented twice! I could understand that there could be some reason, but I do believe there are better ways to position this work in the literature. Note that you do compare with SMERL and outperform it! It is completely okay to say this work is better than SMERL, isn't it? Also, wouldn't it be natural to include a few SMERL follow-ups as additional baselines for improved soundness? The only tricky part of baselines that I could imagine would be those diversity methods on a swapped objective, i.e., maximizing the extrinsic reward with a diversity constraint. This line of research is related but, in fact, parallel to this work and will result in a completely different solution set. More interestingly, the algorithmic framework here will no longer hold if the diversity and reward objectives are swapped. So, I would suggest the authors spend some texts discussing the difference between the two lines of work (i.e., SMERL v.s. diversity MARL) in the main paper for better paper positioning rather than repeatedly leaving such an awkward execuse. \n\n3. **A technical flaw in VDW objective**. I want to point out the derivation holds under the assumption of convex MDP. However, in equation (7), the diversity metric induced by the Van der Waals force isn't really convex over the entire domain --- it is only convex in a sub-space. Although in practice, it seems that this doesn't really matter much, I do think this part should be fixed or at least be discussed further. \n\n4. **novelty and positioning**: A large body of the content, particularly the algorithmic framework, directly follows [1]. In the current draft, the whole related work section is deferred to the appendix without a careful discussion of the relationship between this work and [1]. To me, it looks like the authors adopt the framework from [1] and apply it to the SMERL setting. I do think this should be explicitly presented in the introduction section. \n\n[1] Reward is enough for convex MDPs, Tom Zahavy, Brendan O'Donoghue, Guillaume Desjardins, Satinder Singh, NeurIPS 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "Regarding clarity, although I enjoy reading the technical content to learn all the detailed derivations, I do feel the entire paper looks unnecessarily complicated, particularly for the general audience. The paper reads like a technical tutorial, which makes a clear attempt to explain every single step to a reader but doesn't really look like a conference paper. Unfortunately, I don't really have very concise constructive feedback, so I just express my feelings here. \n\nThe technical quality and novelty are good. I also believe the results are reproducible. ",
            "summary_of_the_review": "This is a technically interesting paper with solid experiments, although there still exist some flaws. Assuming the paper will be improved by the authors, my current perspective is leaning towards acceptance since I did learn something from this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2673/Reviewer_oggL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2673/Reviewer_oggL"
        ]
    },
    {
        "id": "h-ux17qig1",
        "original": null,
        "number": 3,
        "cdate": 1666673568150,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673568150,
        "tmdate": 1666673568150,
        "tddate": null,
        "forum": "kjkdzBW3b8p",
        "replyto": "kjkdzBW3b8p",
        "invitation": "ICLR.cc/2023/Conference/Paper2673/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a method for training diverse populations fo agents, using any RL algorithm, through a carefully constructed adaptive reward function.",
            "strength_and_weaknesses": "Strengths: \n* Reformulating the problem of promoting diversity through Fenchel duality is novel and interesting\n* Using this reformulation to recast the problem as a multi-agent RL problem allows flexibility of the method as algorithms change\n\nWeaknesses:\n* It is difficult to understand the quality of the diversity metrics without more extensive qualitative evaluation.  One could expect a state-occupancy based diversity measure could be degenerate in the same way the noisy-TV problem effects surprise based exploration.  If put into a best buy full of TVs, it seems like each policy could learn to look at a different TV, ignoring many non-TV looking behaviors.\n* Sometimes multi-agent and single-agent problem formulations are confused, for instance in Section 3.3 the reduction is to a 3-player game, so it is not a reduction to an MDP, but to a Markov Game.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "As far as I know this approach to generating diverse policies is novel.",
            "summary_of_the_review": "I'm recommending a weak accept for this paper due to the novel framework for optimizing diversity based methods, which appears to be practical for optimizing the objectives it was tested on. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2673/Reviewer_9fff"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2673/Reviewer_9fff"
        ]
    },
    {
        "id": "dhFHKSfW275",
        "original": null,
        "number": 4,
        "cdate": 1667530018052,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667530018052,
        "tmdate": 1668811120990,
        "tddate": null,
        "forum": "kjkdzBW3b8p",
        "replyto": "kjkdzBW3b8p",
        "invitation": "ICLR.cc/2023/Conference/Paper2673/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates how to learn near optimal policies with diverse gaits by augmenting a simple reward function in the domain with a diversity objective, which can be implemented as either constrained optimization, or as a multi-objective reward. The contributions of the paper include a diversity objective that repurposes the reward function from Abbeel & Ng (2004) to repel policies based on their expected feature similarity. Additionally, a second diversity objective is presented inspired by the Van Der Waals force, which extends the repulsive force to have a repulsive and attractive term---this is appealing because once learned policies are diverse enough, the Van Der Waals force is zero, and policies can purely maximize rewards.\n\nThe authors propose a framework for optimizing these objectives using Constrained MDPs, and provide ablations for a corresponding multi-objective variation of the objectives.The resulting method is called DOMiNO, and solves a constrained optimization problem, where the aim is to find a near optimal set of policies (where near optimal is defined in terms of recovering a fraction $\\alpha$ of the performance of the optimal policy), while maximizing the diversity objective.\n",
            "strength_and_weaknesses": "Strengths Of The Paper:\n\n1)\n\nThe diversity objective inspired by the Van Der Waals force is appealing because it encourages the set of policies to be diverse enough (as specified by a new hyperparameter referred to as the Van Der Waals contact distance), but not too diverse as to hinder reward maximization.\n\n2)\n\nProvided videos of the resulting policies show that diverse behaviors are indeed learned in all domains, which is particularly impressive in the DM-Control dog.walk domain.\n\n3)\n\nAblations show that the method is not particularly sensitive to the size of the set of policies over which optimization is performed, which is encouraging since tuning this hyperparameter could otherwise require knowledge about the number of different gaits the agent can learn in advance.\n\nWeaknesses Of The Paper:\n\n1)\n\nResults on K-shot adaptation when baselines are tuned for performance (see Figure E10 in the appendix), appear to show that SMERL with $\\alpha=1.0$ performs as well as DOMiNO in all environments, and occasionally performs better such as in \u201cWalk+Torso length\u201d. The authors note that SMERL has the additional hyperparameter of $c_d$, while DOMiNO does not since the method uses constrained optimization, which can be seen as adaptively turning $c_d$.\n\n2)\n\nK-shot adaptation is a misleading title since in practice, for a set of 50 policies, if 10 trajectories are collected for each policy, this results in the agent collecting 500 trajectories. In the spirit of k-shot evaluation, this experiment could be made stronger if results were reported as K varies. Additionally, the authors are encouraged to be clear about how many environment transitions are collected in all K-shot adaptation experiments in the paper for reproducibility.\n\n3)\n\nWhile the ablations presented are sound, and the proposed framework is novel, the empirical results in the paper are weak: the proposed method, DOMiNO, does not perform distinguishably better than its alternatives, especially the SMERL implementation. The additional work of using constrained optimization, including a \u201ccost player us[ing] the Follow the Leader (FTL) algorithm\u201d (Page 3) does not provide a benefit over the simpler SMERL, which uses standard RL. Evaluation on more tasks could provide evidence to counter this interpretation, and I encourage the authors to provide additional results and clarifications on the comparison to SMERL.\n\n4)\n\nThe proposed diversity objectives are not compared to other diversity objectives in the literature, such as DIAYN (Eysenbach et al., 2019), and MaxEnt RL (Haarnoja et al., 2018), which makes it hard to judge the utility this paper provides over existing methods for diversity maximization. It could be the case that the proposed method better controls the tradeoff between maximizing rewards and maximizing diversity (which may hinder reward maximization) via the Van Der Waals force. However, the paper is missing such a comparison, and it should be added.\n\nSuggestions For The Paper:\n\n1)\n\nOn page 5, the authors write \u201cone could consider other combinations of powers and coefficients\u201d for the diversity objective defined in Equation 7. While not necessary, and not currently negatively affecting my review (see the listed weaknesses above), the paper could be made stronger if an ablation of different powers and coefficients was given in the Appendix.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally written appropriately with sections organized logically. The inner workings of the Constrained MDP optimizer are described without much technical detail in the main paper, but information needed for implementation (and sample code) is in the Appendix. In addition to the sample code, hyperparameters for all methods, including the proposed DOMiNO are given in the appendix. Some technical details are not described in the paper, including the implementation details of the policy gradient optimizer used for the policy, but a link to source code via the RLAX library is given, which appears to be sufficient for reproducibility.\n\nThe main components of the paper (1) two diversity objectives based on expected features, and (2) a framework for optimizing diversity while satisfying an expected reward constraint, appear to be novel, but it is possible I am not familiar with all necessary related works. In terms of quality, several weaknesses outweigh the paper\u2019s strengths, including but not limited to the lack of baselines (see Weakness (4)). Comparisons to other diversity methods not proposed by the authors are lacking, and would significantly improve the quality of the paper if present.\n",
            "summary_of_the_review": "Overall, this paper studies an important problem in reinforcement learning: finding policies with diverse gaits that effectively balance diversity with task performance. The proposed method has appealing qualities: higher diversity, insensitivity to hyperparameters, and good task performance once diversity is attained. However, four significant weaknesses outweigh the paper\u2019s strengths and consequently I cannot recommend the paper for acceptance in its current form. I am willing to reconsider my evaluation in light of new information and encourage the authors to address my concerns or provide clarifications where necessary.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2673/Reviewer_PkBm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2673/Reviewer_PkBm"
        ]
    }
]