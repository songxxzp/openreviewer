[
    {
        "id": "Y6gn6yFLsQ",
        "original": null,
        "number": 1,
        "cdate": 1666606991745,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666606991745,
        "tmdate": 1670835667728,
        "tddate": null,
        "forum": "EN8YE5dkOO",
        "replyto": "EN8YE5dkOO",
        "invitation": "ICLR.cc/2023/Conference/Paper4034/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes Zeroed Gumbel-Rao (ZGR) stochastic gradient estimators for the discrete random variables. The authors prove that the limiting the temperature to zero in the Gumbel-Rao results in closed-from solution. They also show that the proposed ZGR estimator is in the middle of two well-known stochastic gradient estimators, namely Straight-Through estimator and DARN estimator. They argue that the proposed ZGR has merits both the low bias & variance perspective (when the temperature is less than 0.1), and the low computational complexity, based on their experimental results.\n",
            "strength_and_weaknesses": "The paper is theoretically grounded and the proposed ZGR surprisingly filling the gap between ST and DARN.\n\nError bars are missing in some tables and figures. Also, the authors did not explore various temperatures (and temperature scheduling) for the baselines, which makes unfair comparison. Finally, the experimental result in Table 1 is not very promising.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is neatly written and theoretically qualitative, and therefore has originality.\n\nHowever, the source code is not available at the moment. \n\nHere are some questions to the authors.\n\n- What do you think complemented ST and DARN, each other?\n\n- In Figure 1, does temperature annealing utilized in the baselines? Or, is it just a figure from bunch of fixed temperatures?\n\n- Same question on Figure 2. Does GS variants utilized the temperature annealing? If not, I think is unfair not to utilize the temperature annealing since it is crucial hyper-parameter is the GS variants.\n\n- Perhaps, there is an inconsistency of temperature variable.\n",
            "summary_of_the_review": "I am very neutral on this paper. The work is theoretic and seems to be correct. However, the concern on the fair comparison in the experiment section needs to be resolved, since the GS variants are sensitive to the temperature tuning. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4034/Reviewer_GHDv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4034/Reviewer_GHDv"
        ]
    },
    {
        "id": "iVdf9USDmx2",
        "original": null,
        "number": 2,
        "cdate": 1666662301024,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662301024,
        "tmdate": 1668906895979,
        "tddate": null,
        "forum": "EN8YE5dkOO",
        "replyto": "EN8YE5dkOO",
        "invitation": "ICLR.cc/2023/Conference/Paper4034/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a gradient estimator for distributions over discrete variables, following in the tradition of the Gumbel-softmax estimator. In this paper, the proposed estimator is the zero-temperature limiting value of the Gumbel-Rao estimator. Interestingly, this is shown to be the same as an average of the DARN and classic Straight-Through (ST) estimator. The authors show that their proposed estimator has zero bias on quadratic objectives, and compare its performance on several downstream tasks.",
            "strength_and_weaknesses": "Strengths\n+ The theoretical results are nice. It is very interesting to see that the zero-temperature limit of the GR estimator is given by such a simple combination of the DARN and ST estimator.\n+ The experimental results are fairly comprehensive, and presented quite fairly, as far as it is possible to judge.\n+ The bias and variance properties of the new estimator are clearly described, and illustrated in the experiments. \n\nWeaknesses\n+ The experimental results ultimately do not clearly show superiority of the proposed ZGR estimator. For instance, across table 1 and Figure 2, the Gumbel-Rao estimator performs similarly or better than the ZGR. Similarly in figure 2, the GS-ST(t=2) strictly dominates the ZGR. The experiments do show that the ZGR is a decent enough estimator in practice, but there is still enough variation between all the different estimators that the best bet for the practitioner is to just do a hyperparameter sweep including all the available estimators.\n+ Similarly to the previous point, I think there needs to be more discussion about the connection between the bias and variance of the estimator itself, and the error in the optimization process, particularly when combined in with the particular problem of training a neural network with an adaptive optimizer. As the authors point out, simply considering the MSE in the gradient is not necessarily that relevant for the actual question of how well an NN will convergence. For instance, an interesting experiment could be to train the VAE with all the different estimators, using both an adaptive optimizer and regular SGD, to see if the bias might be worse than variance as the authors hypothesise.\n+ Minor points: \n  + In page 3, shouldn't the CDF of the Gumbel be $e^{-e^{-u}}$?\n  + 'unique amongst single-sample categorical estimators' isn't REINFORCE unbiased? Or you need to define what you mean by single-sample estimator\n  + Can you not just get in touch with people to get the MNIST dataset you need?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n+ The paper is very clear, contextualizing the previous work in the area while accurately differentiating their own work.\n\nQuality\n+ The work is relatively high quality. There is a substantial derivation to get the low-$\\tau$ behavior of the GR estimator, as well as a thorough experimental evaluation of all of the competing gradient estimators. It is pretty polished.\n\nNovelty\n+ The novelty is somewhat limited for me. At this point we have so many estimators for discrete random variables it's unclear what utility is gained by introducing another one. Without a clear argument as to why this estimator's place on the bias-variance curve is uniquely novel, this does not seem very novel overall.\n\nReproducibiltiy\n+ I don't see any provided code, but since the estimator is so simple I can't imagine there would be concerns about reproducibility",
            "summary_of_the_review": "This paper is a nice piece of work, containing a derivation of the zero-temperature limit of the Gumbel-Rao estimator. However, the experimental comparisons don't currently justify the novelty of the work for me. As it stands, the experiments demonstrate that some gradient estimators are better than others at different tasks, and different combinations of bias and variance are useful for different tasks. This paper provides a new option on the bias-variance curve, but it's not obvious to me that this actually translates to real gains on any downstream task. \n\nUpdate after the rebuttal: \nI'm impressed by the authors' additional experiments and will raise my recommendation to 6. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4034/Reviewer_Waws"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4034/Reviewer_Waws"
        ]
    },
    {
        "id": "h-d4M-UaCq",
        "original": null,
        "number": 3,
        "cdate": 1667045887831,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667045887831,
        "tmdate": 1667045887831,
        "tddate": null,
        "forum": "EN8YE5dkOO",
        "replyto": "EN8YE5dkOO",
        "invitation": "ICLR.cc/2023/Conference/Paper4034/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a new estimator called ZGR aimed for the stable and accurate use of discrete random variables as part of approximate inference pipelines.",
            "strength_and_weaknesses": "Strengths:\n  i) The studied problem is important for the probabilistic machine learning community\n  ii) The paper reports comprehensive experiments that span many critical cases where such an estimator could be useful.\n\nWeaknesses:\n i) The technical novelty is limited.\n ii) Experiment results are weak. ZGR does not appear to show an improvement over for instance ST. I understand that it outperforms the GR family, but what that family can do ST cannot is not clear from the experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written in a rather obscure and indirect language. The state of the hard is comprehensively provided but their key weaknesses and the solution strategy proposed in the paper are not clarified. As a minor point, the acronym of the main contribution of the paper ZGR is defined nowhere in the paper.",
            "summary_of_the_review": "This is a promising paper, but gives the impression of unfinished work. The proposed method is a very close variant of what already exists. The results are also not strong, although the experiments are comprehensive.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4034/Reviewer_PG9N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4034/Reviewer_PG9N"
        ]
    },
    {
        "id": "pMMjojKThX",
        "original": null,
        "number": 4,
        "cdate": 1667368925169,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667368925169,
        "tmdate": 1669206515369,
        "tddate": null,
        "forum": "EN8YE5dkOO",
        "replyto": "EN8YE5dkOO",
        "invitation": "ICLR.cc/2023/Conference/Paper4034/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a variance reduction technique that improves the Straight-Through version of the Gumbel-Softmax estimator by taking the zero-temperature limit. The paper further shows that the proposed estimator could be decomposed into a sum of straight-through estimator and DARN. The superior performance of the proposed estimator using 1 or 2 sample(s) is demonstrated through experiments on MNIST, by comparing against an unbiased categorical estimator ARSM, and Gumbel-Rao with large number of Monte-Carlo samples. \n\n",
            "strength_and_weaknesses": "Strength:\nThe paper idea is simple but sharp: inspired by the analysis of Gumbel-Rao (GR) estimator (Paulus et al., 2021) that the estimator performs better at lower temperature, they push the estimator to the zero temperature limit, and found superior performance. Another major contribution is that the paper identifies the connection between GR with Straight-Through (ST) and DARN at the zero temperature limits, for both binary and categorical cases. The insights and analysis are incremental but very important, in terms of both theoretical understanding and improving applicability of the estimator. \n\nWeakness:\nThere are some minor problems / questions:\n\n1. Questions:\n  - In Figure 1, the author compares the computational time across different estimators, is this comparison based on the numbers from different implementations, or are all the estimators implemented in the same code base? Instead of shown the comparison of time, it would be great to include a discussion about complexity analysis on each algorithms, which is not subject to the difference in the implementation and helps the readers / reviewers to better understand how the estimator scales with increasing number of categories. E.g. ARSM requires $\\mathcal{O}(C^2)$ evaluations, where $C$ is number of categories.  \n  - The author(s) discussed the multi-sample version, ZGR s = 2, in experiment section. How does it formulated? Taking average of two estimations from each samples?\n  - Is there any analysis for scaling the number of categories? Would the relative performances of different estimators hold for larger / smaller number of categories?\n  - MNIST is a standard yet simple task. It would be good to extend the analysis on other benchmarks as well, e.g. FashionMNIST and Omniglot.\n  - The author(s) claimed that the bias is very small comparing to the variance, based on the bias and variance estimations at \"an evaluation point\" \"after 100 epochs of training with ARSM\". It would be nice if this analysis could be done at an early stage of the training. \n  - A side question: instead of setting the temperature to zero or a finite number, how performant is the model if one makes the temperature learnable?\n\n2. Missing baselines:\n  - Binary case: as the paper considers the binary case in 3.1. In this case, it's worth to compare against the latest unbiased gradient estimators for binary variables, ARM (Yin et al. 2019) and DisARM (Dong et al. 2020). Both are using antithetical sampling to improve REINFORCE, and DisARM is Rao-Blackwellized ARM by analytically integrating out the randomness. \n\n  - Categorical case: ARSM is a categorical version of ARM. However, previous research (Dong et al. 2021) found ARSM underperform simpler REINFORCE Leave-one-out (RLOO, Kool et al. 2019) baselines by significant amount with comparable number of samples. Also, Dong et al. 2021 proposed Rao-Blackwellized ARSM with binary reparameterization, further improves RLOO. \n\n3. Missing References and things to be fixed:\n  - line below Eq (1) \"It has proven efficient in practice to ...\" Please add reference for this. Also the sentence should be \"It has been proven ...\"\n \n  - Eq (2), in general, especially for VAE, as the paper considered in experiments, \"x\" the latent variable, therefore it is usually sampled from posterior distribution. In this case, $\\mathcal{L}(x)$ will also depends on $\\eta$. Correct derivation should include two terms, e.g. Dong el at. 2021 Eq (2) and relevant discussions, but the term with gradients of $\\mathcal{L}(x)$ can typically be estimated with a single Monte Carlo sample and neglected from discussion. \n\n  - Broken sentence: line below Theorem 2: \"By quadratic functions we understand quadratic functions .. with arbitraty coefficients\"\n\n  - paragraph above 4.2, \"GR-MC with K = 10 or K=100 MC samples\", it's better to use \"M\" instead of \"K\", as the paper used \"K\" for number of categories. \n\n  - Please try to make the main text self-contained. E.g. Eq (10) mentioned $p_Z(\\eta)$, and it is the logistic density. Writing an expression for it, would help the reader to easily reproduce the results.\n\n\nReference:\n  - Yin, M. and Zhou, M. (2019). ARM: Augment-REINFORCE-merge gradient for stochastic binary networks. ICLR 2029\n  - Dong, Z., Mnih, A., and Tucker, G. (2020). DisARM: An antithetic gradient estimator for binary latent variables. NeurIPS 2020\n  - Kool, W., van Hoof, H., and Welling, M. (2019). Buy 4 reinforce samples, get a baseline for free! Deep RL Meets Structured Prediction ICLR Workshop.\n  - Dong, Z., Mnih, A., and Tucker, G. (2021). Coupled gradient estimators for discrete latent variables. NeurIPS 2021\n\n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with minor things that are fixable. The work is solid. Even though the work is incremental on top of ST-GR, but the insights and idea are sharp and useful for the community. ",
            "summary_of_the_review": "The paper did thorough theoretical analysis on the proposed gradient estimator, with good explanation. The author(s) did good evaluations in experiment section. The reviewer thinks that the experiment section could be improved with better baselines and more analysis on different benchmarks and ablations as mentioned in \"Weaknesses\" section.  \nI would raise the score, if the author(s) could help address some of the concerns in Weaknesses section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4034/Reviewer_wSJz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4034/Reviewer_wSJz"
        ]
    }
]