[
    {
        "id": "YcbBaHLi_Z",
        "original": null,
        "number": 1,
        "cdate": 1666658417715,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658417715,
        "tmdate": 1669503301584,
        "tddate": null,
        "forum": "mXPoBtnpMnuy",
        "replyto": "mXPoBtnpMnuy",
        "invitation": "ICLR.cc/2023/Conference/Paper3864/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an unsupervised method to train deep neural networks using Hebbian/Anti-hebbian learning on 3D object classification from point clouds. The authors highlight (empirically and theoreticalaly) key disadvantages of using just Hebbian or Anti-hebbian learning; they demonstrate the improved expressivity and representational characteristics of using a neural activity based combination of both Hebbian and Anti-hebbian learning. They show that the latter outperforms other Hebbian learning rules on ModelNet10 and ModelNet40 3D object classification tasks.",
            "strength_and_weaknesses": "Strengths:\n1. The authors present a neat combination of a biologically plausible local learning method (Hebbian/Anti-Hebbian networks are backed both by prior computational neuroscience modeling and experimental data on cortico-cortical connections) with deep neural networks for 3D object classification. The proposed NeAW Hebbian learning is clearly more expressive in the low-training data regime as shown in Fig. 7 even in comparison with supervised learning, this is very promising. \n\n2. NeAW quite significantly improves the performance of other Hebbian learning based methods. It is very interesting that this simple yet effective twist on Grossberg's rule results in ~15% and ~20% gains on ModelNet10 and ModelNet40 respectively.\n\n3. This paper is written really well with adequate graphical explanations to distinguish the different Hebbian learning rules from NeAW (Fig. 3 clearly shows how they are different from Hebbian and Anti-hebbian learning). Section 3.3 which is an important part of the paper that introduces NeAW is written particularly well.\n\nWeaknesses:\n1. Performance differences between NeAW and supervised methods on the full training set is quite big. Part of this can be explained by the difference in number of parameters between these models (model size). I think it would be fairer to compare NeAW when its model size is increased to match that of PointNet, PointNet++, DGCNN. The paper's contribution would seem even stronger if they match or improve performance of their unsupervised local learning rule with above mentioned supervised counterparts.\n2. There seem to be multiple supervised approaches I find at https://paperswithcode.com/sota/3d-point-cloud-classification-on-modelnet40 that achieve >90% on ModelNet40 with as few as 1.1M parameters (~4.4 MB if using fp32). If the authors would like to emphasize the strength of their model with few parameters, they must compare to these methods using very few parameters and yet achieving high performance on ModelNet.\n3. This is not necessarily a weakness per se, but I find it odd that the performance is insensitive to choice of a/b. Is it possible that the authors didn't cover a wide enough range to start seeing differences? Given that purely Hebbian or Anti-Hebbian methods work much poorly, setting very low learning rates for either (a or b) might produce significant performance differences. It would be great if the authors could test a wider range of a/b ratio and see if performance differences arise.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is written very clearly with adequate figures as required. The contribution made by the paper to combine biologically plausible unsupervised local learning with deep neural networks -- that typically work well only with gradient-based training -- is novel and presents an interesting direction for future exploration.\n- I don't see code uploaded with the submission, I request the authors to please consider uploading the implementation for reproducibility of presented results.\n",
            "summary_of_the_review": "The paper presents an interesting combination of local learning with Hebbian/Anti-hebbian rules with deep neural networks and demonstrates its utility on 3D object classification on ModelNet10 and ModelNet40. The paper makes novel contributions that overcome key limitations of using Hebbian and Anti-hebbian learning rules in the context of deep learning -- prior computational modeling work has however experimented the combined use of Hebbian and Anti-hebbian learning rules (authors cite relevant work in this direction in Sec 3.3 Theorem 2). \n\nI have two main concerns: 1) Comparison to the current supervised techniques in the paper don't match model size, 2) there are other supervised methods that use very few parameters and achieve high performance on ModelNet40. That said, NeAW makes a significant improvement in using $\\textbf{local learning}$ to train deep neural networks. I recommend accept (6/10) at this stage due to the above two evaluation concerns. If the authors could show how their model performs when they increase model size, I may update my score as the current evaluation only uses a small number of parameters with model size 3.1 MB.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3864/Reviewer_rCep"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3864/Reviewer_rCep"
        ]
    },
    {
        "id": "WaRk8EmyJbe",
        "original": null,
        "number": 2,
        "cdate": 1667259071582,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667259071582,
        "tmdate": 1669836657929,
        "tddate": null,
        "forum": "mXPoBtnpMnuy",
        "replyto": "mXPoBtnpMnuy",
        "invitation": "ICLR.cc/2023/Conference/Paper3864/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an unsupervised learning rule, NeAW, combining Hebbian and anti-Hebbian learning and applies it to 3D point cloud classification. The learning rule is motivated by the observation that Hebbian and anti-Hebbian learning don't produce diverse representations across neurons (there is low variance in neuron activity). The authors find that NeAW outperforms Hebbian and anti-Hebbian learning, achieves high performance with fewer parameters than comparable models, and outperforms backprop in low-data settings.",
            "strength_and_weaknesses": "**Strengths**\nThe learning rule proposed by the authors is interesting, and some theoretical properties about it are proven. In my view, the most compelling point of this paper is the design of a learning rule that can produce strong representations of the data in an unsupervised way (as illustrated by Figures 4 and 5). Moreover, the theoretical analysis appears solid. A number of experiments are presented showing the properties of NeAW. The paper is also generally well written and easy to understand.  \n\n**Weaknesses**\nThe motivation for the paper is unfortunately a little unclear to me. What is the purpose of the NeAW rule applied to the 3D point cloud task? Do the authors wish to show good performance on the 3D point cloud task under model size constraints (as suggested by Table 2)? Do the authors wish to show good performance in low-data settings (as suggested by Figure 7)? Alternatively, is the main motivation to demonstrate an unsupervised learning rule that can balance neuron activity (i.e. Figure 2), with the 3D point cloud task as a case study?\n\nIf the motivation is to solve the 3D point cloud task more effectively, I would suggest emphasizing the relative advantages of NeAW more clearly throughout the paper. If the motivation is to propose a better Hebbian learning rule, I would suggest focusing the discussion and experiments on other similar Hebbian rules to emphasize that NeAW is superior. I would also suggest commenting on the biological significance of the rule since Hebbian learning rules have traditionally been proposed as models of biological neuron learning (e.g. to what extent can NeAW be considered biologically plausible?).\n\nIn terms of the specific motivation of NeAW rule, section 3.2 suggests that the poor distribution of neuron activity in Hebbian and anti-Hebbian learning rules leads to poor performance, and that NeAW achieves better performance by creating a more uniform activity distribution. While it is clear that NeAW achieves a more uniform activity distribution and higher performance, the causal link between these two is not clear to me unfortunately. Can the authors demonstrate empirically that a more uniform activity distribution *causes* higher performance (right now only a correlation is demonstrated)?\n\nI also have some concerns regarding the experiments. From Table 2, the paper argues that NeAW achieves good performance despite a lower model size. However, the model size seems linked to the model architecture used in the paper and not NeAW. If the authors wish to show that NeAW can be used to train small sized models more effectively than comparable learning rules, they may want to apply all the learning rules in Table 2 to a single (small) model architecture and evaluate performance. \n",
            "clarity,_quality,_novelty_and_reproducibility": "**Quality**\nThe theoretical contributions in the paper are solid. Empirically, there are a few concerns as highlighted above. One of the main concerns with the paper is how the contributions are framed; if the paper's contributions are appropriately framed, its significance will become more clear.\n\n**Clarity**\nThe paper is generally well-written and the figures are particularly well illustrated. Some minor comments:\n\n- A parenthesis appears to be missing from Eqn 2\n- The paper has a few typos\n\n**Originality**\nIt is unclear to what extent NeAW is different from other Hebbian-like learning rules proposed in the past. Is this the first Hebbian-like rule to avoid the activity concentration problem illustrated in Figure 2? If so this would be a quite significant contribution, but it would also require the authors to perform experiments with more learning rules to demonstrate this.",
            "summary_of_the_review": "The paper proposes an interesting Hebbian learning rule with some solid theoretical analysis. However, there are some concerns with the experiments and especially with the motivation of the paper as noted above. It would also be great if the authors could further clarify the novelty of the proposed rule.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3864/Reviewer_oMed"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3864/Reviewer_oMed"
        ]
    },
    {
        "id": "fPC_ZOLim",
        "original": null,
        "number": 3,
        "cdate": 1667463048061,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667463048061,
        "tmdate": 1669191468177,
        "tddate": null,
        "forum": "mXPoBtnpMnuy",
        "replyto": "mXPoBtnpMnuy",
        "invitation": "ICLR.cc/2023/Conference/Paper3864/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors proposed an unsupervised learning method NeAW Hebbian learning that dynamically switches the learning rule of synapses between Hebbian (H) and anti-Hebbian (aH) depending on the activity of the output neuron. The proposed method is claimed to have reduced neuron activity bias, which leads to the activation of a different subset of neurons for different objects, thus, facilitate better learning. The authors tested the learning algorithm on 3D objects / point cloud recognition tasks and found that compared to Hebbian and anti-Hebbian learning, NeAW training reduces the activity bias of output neurons. On the 3D classification tasks, NeAW Hebbian learning rule also outperforms the existing Hebbian-based rules and end-to-end supervised learning when the data is limited. ",
            "strength_and_weaknesses": "**Strength**:\n\nIn general, this paper proposed an elegant extension of the Grossberg\u2019s rule to address the biased activity problem for both Hebbian and anti-Hebbian learning rules. Some of the strengths of this work are listed below:\n\n- To facilitate the study of different Hebbian learning rules for 3D object classification, the author proposed a model with an encoder containing WTA modules at each layer. \n\n- The author identified the problem of biased activity (a few neurons are activated most of the time regardless of the object) with both Hebbian and anti-Hebbian learning in 3D object classification with their proposed model and linked this observation to the loss of local features in complex 3D objection recognition tasks. \n\n- To remove this bias, NeAW uses Hebbian rule when the output neuron activity is smaller than the average neuron activity p*=1/#output-neurons and uses anti-Hebbian when the activity is larger than p*. Intuitively, this seems like an implicit regularizor to reduce the activities of more activated neurons and increase the activity of less activated ones, and the activities are more evenly distributed across output neurons of a specific layer. \n\n- To illustrate this idea, the authors provided intuitive illustration to understand the geometry of the problem that NeAW is trying to address (Figure 3). And they also claimed to have proved NeAW relieves the biased activity under a given geometric condition. \n\n- The authors also empirically showed that the proposed NeAW results in a better feature representation which leads to better performance compared to other Hebbian-based rules and can even outperform end-to-end supervised learning when data is limited. \n\n**Weaknesses**\n\n- In the motivation study, the authors identified the biased activity problem with Hebbian-based learning rules and argued that a relaxation of this skewed activity is the key to improve learning. In addition to observing the activities resulting from Hebbian learning, I would also suggest studying if end-to-end learning with better classification performance would result in similar biased activities. \n\n- In this work, the proposed NeAW method is only studied with a specific task (3D learning) and a specific model. To make this learning rule more general, the authors are suggested to also test it on other tasks and model architectures. For example, would it also improve models without the WTA module?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the paper is well written. The motivations are clearly explained. The experimental design is also sound. One novelty of this paper is that it added a reference output value to decide if Hebbian and anti-Hebbian learning rule should be applied such that the resulting activities are more evenly distributed across output neurons, which facilitate better learning. The source code is not provided; thus the implementation is not considered for this review. \n\nSome minor typos and unclear phrasing. \n- Page 2, Equation (1). The $\\Delta$ before w(t+1) should be removed.\n- Page 4, Definition 1. The phrasing is not very clear. \n",
            "summary_of_the_review": "This paper proposes a variant of the Hebbian learning rule which uses the output activity to decide if Hebbian or anti-Hebbian learning should be used. The proposed method claimed to have relieved the biased activity problem from existing Hebbian and anti-Hebbian learning rules, thus resulting in better local feature representation for downstream tasks such as 3D object recognition. It needs further validation to be a general unsupervised learning rule.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3864/Reviewer_Sia3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3864/Reviewer_Sia3"
        ]
    }
]