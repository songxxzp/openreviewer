[
    {
        "id": "gPVqfLNcinw",
        "original": null,
        "number": 1,
        "cdate": 1666656686024,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656686024,
        "tmdate": 1666656686024,
        "tddate": null,
        "forum": "GUSf17i8RMZ",
        "replyto": "GUSf17i8RMZ",
        "invitation": "ICLR.cc/2023/Conference/Paper2181/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a locally dense, globally sparse alternative to the feedforward neural network architecture. The proposed architecture consists of interconnected blocks where the neurons within each block are connected via feedforward, attention, and product-between-neurons mechanisms. The authors compare the performance of their model to that of the feedforward networks in function approximation, reinforcement learning, computer vision, and time series forecasting tasks, where they report an increased performance compared to the baseline models of their choice.",
            "strength_and_weaknesses": "The goal of this paper is important, as there is growing evidence that the sparsity/modularity of neural network models may lead to improved performance with less computation. The authors have performed a rigorous evaluation of their model\u2019s performance on various datasets where they compared it to the performances of multiple baseline models, published previously.\n\nAmong the others, the strengths of this submission are:\n\n-The paper contains a comprehensive Methods appendix which includes thorough descriptions of datasets and models. The baseline models are thoroughly referenced.\n\n-In the synthetic dataset for the function approximation, the Authors use a large enough class of the functions to be approximated, which goes beyond simple linear or quadratic functions.\n\n-CircuitNet is compared to milestone models in vision (e.g. ResNet, ViT) and time series forecasting (e.g. ARIMA, etc.) tasks. Those models include other sparse, modular approaches (e.g. ViT) that naturally have a similarly small number of parameters.\n\nWhile the strength of the proposed approach is in using a single architecture to outperform multiple baselines, meticulously optimized by previous Authors, it should be noted in the text that the current SOTA is higher than the reported performance on the used datasets. For example, on ImageNet it\u2019s 91% and not the ballpark of 78% (https://paperswithcode.com/sota/image-classification-on-imagenet); on CIFAR-10 it\u2019s above 99% (https://paperswithcode.com/sota/image-classification-on-cifar-10) etc. Understandably, these results required bells and whistles specific to each of the said datasets and do not devalue the results in this paper \u2013 though they still need to be mentioned and discussed not to mislead the paper readers.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The text is well-written including a comprehensive methods section. The experiments are well-designed including the usage of standard datasets/tasks and milestone baselines; however, the more recent models with higher scores need to be at least mentioned.",
            "summary_of_the_review": "The paper addresses the important task of improving the deep learning models\u2019 performance and resource usage by introducing sparse and modular architecture. The Authors have compared their model to a variety of classic baseline approaches on multiple standard datasets, showing competitive performance. At the same time, the model\u2019s scores fall below current leading scores for the used datasets (though, those likely required finetuning specific to each dataset) \u2013 this needs to be mentioned in the text. For these reasons, I recommend borderline acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2181/Reviewer_zeXY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2181/Reviewer_zeXY"
        ]
    },
    {
        "id": "SD5y28_uXQ",
        "original": null,
        "number": 2,
        "cdate": 1666662090398,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662090398,
        "tmdate": 1666662090398,
        "tddate": null,
        "forum": "GUSf17i8RMZ",
        "replyto": "GUSf17i8RMZ",
        "invitation": "ICLR.cc/2023/Conference/Paper2181/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes CMU, a motif unit formed by a group of densely connected neocons, which is inspired by the structure of human brain. Experiment results suggests better performance. ",
            "strength_and_weaknesses": "Strength:\n* The paper technically sounds correct and claims well supported by theoretical analysis and experimental results.\n* Related works and background knowledge related are covered and discussed. \n* Experiments are conducted extensively in comparing the proposed CMU net against other networks in a variety of tasks.\n\nWeekness:\n* The clarity can be further improved. For instance, the experiment section can add a table or explain in the comments on the difference between CMU, CMUL+A and +P. \n* Experiment section can benefit from a small summary of why CMU can outperform in many different tasks while maintain very small size.\n\nQuestions:\n* While the learning curve compares CMU against MLP in training speed, how much more or less training computing cost per iteration does CMU need compare to traditional ANNs?\n* For eqn 3 (attention), what doe value, key and query represents?\n* For the clipsin activation function, the appendix suggests it is somewhat inspired by leakyRELU. Is it possible to have the [-3,3] linearly for less computation cost?\n* Can the authors clarify how does CMU handle conv layers, I did not quite follow the part in 3.2.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is well written and organised.  Experiment section may benefit from more implicit label explanation in +a and +p, and a more in depth centralised result discussion.\n\nQuality:\nThe paper technically sounds correct and claims well supported by theoretical analysis and experimental results. Multiple experiments are conducted to support the claims.\n\nNovelty:\nTo my knowledge, the idea proposed is novel.\n",
            "summary_of_the_review": "Overall, I really like the idea of CMU and its potential of providing a more robust NN unit to build more capable networks. The experiment conducted in varies types of applications backs the idea that CMU net can be more accurate and maintain a reasonable size given it follows the design rule of locally dense and globally sparse. The paper feels most robust and novel. Clarity in explaining some CMU operation and experimental discussion can be slightly improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2181/Reviewer_CjTa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2181/Reviewer_CjTa"
        ]
    },
    {
        "id": "gV_JeF_kQSo",
        "original": null,
        "number": 3,
        "cdate": 1666688560127,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688560127,
        "tmdate": 1666688560127,
        "tddate": null,
        "forum": "GUSf17i8RMZ",
        "replyto": "GUSf17i8RMZ",
        "invitation": "ICLR.cc/2023/Conference/Paper2181/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work explores how circuit motifs observed in biology can be built into artificial neural networks and finds they improve performance vs standard ANNs.\n",
            "strength_and_weaknesses": "Strengths\n- Viewing circuit motifs as computational building blocks is an interesting and important research direction with relevance for both AI and neuroscience.\n- The model presented is well tested on a number of benchmarks and performs well.  \n\nWeaknesses\n- The biological inspirations of this paper (feedforward excitation/inhibition, lateral inhibition and mutual inhibition Luo, 2021) rely on the presence of Dale's law in their definition. However Dale's law is absent in these networks, making it unclear how this work connects to biology. \n- It is unclear which circuit motifs underlie the performance of CMUs. There are no ablation studies, nor analysis of the CMUs that have been learned. For example it may be that neuron-wise attention is required for all of the results, in which case the \"generic\" nature of CMUs is a bit misleading. \n- How neuron-wise attention maps on to biology is unclear. ",
            "clarity,_quality,_novelty_and_reproducibility": "This work is of high quality and seems novel. \n",
            "summary_of_the_review": "An interesting and novel study with good empirical results. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2181/Reviewer_oKif"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2181/Reviewer_oKif"
        ]
    }
]