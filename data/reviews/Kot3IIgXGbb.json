[
    {
        "id": "pDGQQOo8vMe",
        "original": null,
        "number": 1,
        "cdate": 1666556987807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556987807,
        "tmdate": 1666556987807,
        "tddate": null,
        "forum": "Kot3IIgXGbb",
        "replyto": "Kot3IIgXGbb",
        "invitation": "ICLR.cc/2023/Conference/Paper5880/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a technique to learn a globally Lipschitz function when the data lie near a manifold. They derive a suitable constrained optimization problem together with the associated optimization scheme. Theoretical results are provided which show that under some conditions it is sensible to solve the empirical optimization problem as it approximates the true problem. The effectiveness of the approach is demonstrated in the experiments. ",
            "strength_and_weaknesses": "There are many parts of the paper that I like e.g. explaining some steps in good detail, but there are few parts which are a bit unclear to me. The introduction of the problem is ok, relying on the example in Fig. 1 and on the difference between favour vs guarantee that a function is globally Lipschitz. Maybe you could elaborate a bit more why using the manifold information is better. The technical content seems reasonable, but I have not checked all the derivations in details. I think that additional experiments should be conducted.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed idea seems to be an extension of the method from the Euclidean space on manifolds. This constitutes a potentially significant contribution, bit I think that the empirical evaluation should be extended. The paper is well written and there are only few spots where it is unclear.",
            "summary_of_the_review": "Questions:\n1. In Euclidean spaces it is known that if the norm of the gradient is bounded then the function is Lipschitz. Does the same result hold when $f$ is defined on Riemannian manifolds?\n\n2. I think that the efficiency of your method highly depends on the construction of the graph Laplacian, and for this reason some discussion and demonstrations should be included.\n\n3. Similarly, I think that the experimental setting is a little bit short compared to the rest of the paper. In my opinion some additional (synthetic) experiments as the one in Fig. 1 can help, as well as classic semi-supervised learning problems with real data.\n\n4. Minor: \n\t- In the sentence after Eq. 6 there is a typo in the integral.\n\t- In Eq. 9 should it be $P^*$ or $D^*$? \n   \n   \nI think that the paper has a lot of merits and the technical content seems solid. Of course, this is theoretical contribution, but I since it focuses on a particular problem that has potential benefits in applications, I would like to see some additional (synthetic/real-world) experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5880/Reviewer_2RoL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5880/Reviewer_2RoL"
        ]
    },
    {
        "id": "k1plm_lkaU",
        "original": null,
        "number": 2,
        "cdate": 1666597583844,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597583844,
        "tmdate": 1669059643468,
        "tddate": null,
        "forum": "Kot3IIgXGbb",
        "replyto": "Kot3IIgXGbb",
        "invitation": "ICLR.cc/2023/Conference/Paper5880/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper learns smooth functions (as measured by the Lipschitz constant) on Riemannian manifolds embedded in Euclidean space. Notably, these methods assume the manifold is unknown, presenting a significant technical challenge. The paper then develops several optimization problems and shows to make these tractable through various analyses and perspectives. Empirical results are presented for two tasks.",
            "strength_and_weaknesses": "Strengths\n-----------\n* The method is quite nicely developed. In particular, I found the construction to be well-reasoned and nicely created, from the initial optimization problem formulation to the final algorithmic implementation.\n* The proposed method seems to be technically correct. In particular, each component is justified nicely.\n* The method is quite a novel departure from previous methods, as it handles what feels like a significantly more complex problem formulation. In particular, it merges both lipschitz and manifold constraints.\n\nWeaknesses\n---------------\n* The experimental section is rather weak/toy.\n* The method may not be very applicable. In particular, outside of the standard manifold learning justifications, I question how well this will scale to high dimensional data/large number of data points.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n--------\nI found the paper to be very clear and concise. In particular, the overall structure of paper (going from problem to solution to implementation) is quite nice and makes the paper quite enjoyable to read.\n\nQuality\n--------\nI found the paper to overall be of pretty high quality. In particular, the contributions are quite nicely developed, and the overall contributions are generally well-formulated.\n\nNovelty\n---------\nThe paper is reasonably novel. In particular, it approaches and established problem and develops a novel methodology to sharpen the constraints.\n\nReproducibility\n-----------------\nThere was code to reproduce the experiments.",
            "summary_of_the_review": "I lean to accept the paper, mostly due to the technical novelty and very clear exposition. However, I do think that the experimental setup is quite weak, and the applications may be rather limited. However, since this a general trend with manifold learning papers in general, I am willing to somewhat overlook them given the technical novelty.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5880/Reviewer_HRYo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5880/Reviewer_HRYo"
        ]
    },
    {
        "id": "Q4MdvzsQahT",
        "original": null,
        "number": 3,
        "cdate": 1666847812654,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666847812654,
        "tmdate": 1666847812654,
        "tddate": null,
        "forum": "Kot3IIgXGbb",
        "replyto": "Kot3IIgXGbb",
        "invitation": "ICLR.cc/2023/Conference/Paper5880/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studied the problem of learning a Lipschitz continuous function on a manifold. The authors used the Lagrangian dual problem and showed that its empirical version can be an approximation of the primal with statistical consistency. The authors further showed that, by using a weighted point cloud Laplacian, the evaluation of manifold Lipschitz constants can be recast in a more amenable form. The numerical results show the effectiveness of the proposed method.\n\n\n",
            "strength_and_weaknesses": "Strength:\n\nThis paper presents a computationally efficient method to learn a Lipschitz continuous function on a manifold, which is a combination of techniques from semi-infinite constrained learning and manifold regularization. Theoretical analysis is also provided.\n\nWeakness:\n\n1. The authors should provide more background and motivation about the problem formulation, i.e., Problem (3). Otherwise, it is difficult for readers to follow.\n\n2. The authors should provide some insights for Proposition 3 about why the integral of the manifold gradient term can be approximated by the point cloud Laplacian term.\n\n3. It's confusing about which problem do the updates (17)-(19) aim to solve.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors should provide more background and motivation about the problem considered, such that readers could understand it better. The main contributions of this paper should be clarified clearly. The novelty of the paper is good.",
            "summary_of_the_review": "It is not easy for the reviewer to follow this paper, because of the insufficient background and motivation for the problem formulation considered. The authors should make clearer the main contributions of this paper compared to the previous work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5880/Reviewer_FBLh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5880/Reviewer_FBLh"
        ]
    }
]