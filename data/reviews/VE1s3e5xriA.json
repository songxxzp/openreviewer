[
    {
        "id": "uC1TBlGKJ7",
        "original": null,
        "number": 1,
        "cdate": 1666368515265,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666368515265,
        "tmdate": 1666368515265,
        "tddate": null,
        "forum": "VE1s3e5xriA",
        "replyto": "VE1s3e5xriA",
        "invitation": "ICLR.cc/2023/Conference/Paper5538/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper adresses data-free model stealing, an ambitious framework where one attempts to build a model that imitates a target one without accessing the target parameter or any training data point (true or substitute ones). In this field, SOTA approaches use a student network whose outputs are meant to minimize a distance between its outputs and the outputs of the target (which is the only information unveiled by the target model). The student competes with a an adversary as part of a min-max game in order to efficiently explore the input space. \n \nCompared to previous approaches, the authors use two students and exploit disagreement between the two to better explore the input space in regions where harder inputs live. Disagreement is exploited by an adversary input generator which will try to maximize conflict. The generated inputs are the one on which student and teacher prediction should align. ",
            "strength_and_weaknesses": "Pros : \n- the paper is well written and pleasant to read\n- the proposed contribution requires fewer queries to the to-be-stolen teacher model and is also applicable in situations where the number of classes is not known in advance. \n- the proposed approach is fully differentiable which is a nice increment over SOTA.\n\nCons : \n- the paper does not mention computation time/complexity overhead incurred by using an additional student",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear. The contributions are well motivated and fairly new. \nThe authors provide a reproducibility guidelines in the paper.",
            "summary_of_the_review": "Major remarks: \n\nThe only missing aspect in the paper is an analysis of increased time/memory requirements of the method  compared to prior arts.\n\nLet alone that, the paper is interesting and I do not have compelling remarks to address but rather a few questions that came to my mind :\n\nIs there any benefits in training more than 2 students ? Perhaps more precise finer-grained disagreement regions could be obtained in this way.\n\nDo the author have an opinion on possible counter-measures that a defender could implement to misguide the students ?\n\nMinor :\n\nIn some parts of the paper, the authors use the word \u00ab\u00a0image\u00a0\u00bb as if it was obvious that inputs are images. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5538/Reviewer_aGyH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5538/Reviewer_aGyH"
        ]
    },
    {
        "id": "qUgbfE5hP4Y",
        "original": null,
        "number": 2,
        "cdate": 1666438015768,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666438015768,
        "tmdate": 1666438015768,
        "tddate": null,
        "forum": "VE1s3e5xriA",
        "replyto": "VE1s3e5xriA",
        "invitation": "ICLR.cc/2023/Conference/Paper5538/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper has addressed the problem of data free model stealing, where a dual student based framework is proposed for better estimating gradients of the target model without access to its parameters, and generating a diverse set of images that thoroughly explores the input space. While the proposed framework looks interesting, its design is not well justified and not well ablated.",
            "strength_and_weaknesses": "**Strengths**\n\n(1) The paper addresses an interesting problem of data-free learning or data-free model stealing, where they proposed a dual-student based framework.\n\n(2) The paper has reported encouraging results on the available benchmarks for both standard classification accuracy and transfer-based attack effectiveness.\n\n**Weaknesses**\n\n(1) It is not clear why a dual student model is needed for this. How are they creating more effective criteria? In my understanding it can also be done with the target model and only one student. In that case, the only student will be trained to match the target model, while the generator can be tasked to generate samples on which the student and the target model disagree for creating hard samples. More details with writing is needed together with appropriate ablation study.\n\n(2) There are some clarifications on the reasons at the very last paragraph of section 3.1, however those reasoning aren't well grounded. So please cite the papers that utilize the forward differences method to estimate the gradient through the target model. Additionally, ablative studies should be performed or indicated to justify the design choice.\n\n(3) Results on CIFAR 100 dataset are missing in table 1. As the model is motivated based on its effectiveness of generating more diverse samples to be effective for datasets with more number of classes, results on CIFAR 100 is important. In table 1 of the supplementary material, both the rows show results on CIFAR 10, is it correct? If so, why are the numbers in the DFMS-HL/SL rows different?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is reasonably clear. It is well written in good English and the reported results are encouraging. The authors haven't provided the code, however I expect it will be available at some point facilitating the reproducibility of the method. While the model is interesting, certain design choices are not clear or well ablated and some important experiments (CIFAR 100) are missing.",
            "summary_of_the_review": "The paper is interesting with a new architecture, but the design choices are not well grounded. Some important experiments are missing which makes it difficult to judge the effectiveness of the proposals.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5538/Reviewer_yxtQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5538/Reviewer_yxtQ"
        ]
    },
    {
        "id": "G3q_CEUKr8_",
        "original": null,
        "number": 3,
        "cdate": 1666802974737,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666802974737,
        "tmdate": 1670849590982,
        "tddate": null,
        "forum": "VE1s3e5xriA",
        "replyto": "VE1s3e5xriA",
        "invitation": "ICLR.cc/2023/Conference/Paper5538/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies how to perform model stealing in the data-free setting.\nThe authors first identify two weaknesses of existing data-free model stealing methods: (1) the generated query data are not informative enough, and (2) the estimation of black-box gradients is not stable for optimization.\n\nTo address these two issues, the authors then design a novel dual-student model stealing method.\nThe dual-student method, which involves two student networks and a query data generator, aims to solve a minimax problem. The two student networks aim to imitate the behavior of the victim model on generated query data distribution, while the generator aims to generate query data that maximize the prediction differences between student networks and the victim.\nThrough a simple substitution, the solution of the maximization problem can be approximated as maximizing the disagreement between the two student networks, which therefore does not require black-box optimization anymore.",
            "strength_and_weaknesses": "The proposed dual-student method is interesting and empirical results show that it surpasses existing data-free approaches in some situations. However, I think the current version of the paper can be improved in many aspects, which are listed below:\n\n1. The writing of the paper is not well, making it a little confusing to follow the ideas behind the paper. Specifically, in Section 1, the authors put most of the details of dual-student in a single paragraph, which makes it difficult for reading. Besides, Section 2 uses too much content in reviewing previous works (more than 1 page), which however is not so necessary for introducing the proposed dual-student methods.\n\n2. The insight of the proposed dual-student method is not clear enough.\n    - Why replacing the objective problem in Eq.(1) with Eq.(2) is appropriate? I suggest the author give at least an intuitive explanation about it.\n    - An ablation study on the relationship between the step numbers for updating the generator and student networks would help understand why the dual-student method performs well in practice.\n\n3. Some experiment details are missing.\n    - What is the model architecture of the query data generator?\n    - Please explain how to combine the two methods, DFMS-HL/SL and DS in detail.\n\n4. The authors claim that they conduct experiments on the CIFAR-100 dataset. However, I could not find corresponding experiment results in tables or figures from the paper.\n\n5. Some suggestions about improving the empirical studies.\n    - Study the relationship between query budget and stealing performance of the dual-student method.\n    - Involve more black-box attacks for evaluation, e.g., membership inference attacks, and model-inversion attacks.\n\n6. Typos: In Eq.(3): $G(x)$ -> $G(z)$.",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed dual-student method is interesting, but the authors fail in conducting a comprehensive empirical study.",
            "summary_of_the_review": "In general, the proposed method is novel and shows some performance advantages. However, the authors did not fully justify its effectiveness. Besides, the writing of this paper needs to be improved. Therefore, I tend to reject this paper for now. I suggest the authors to improve the writing, add more experiments, and submit the paper to another top conference.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5538/Reviewer_xQYc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5538/Reviewer_xQYc"
        ]
    },
    {
        "id": "KAfv2P6EDZ",
        "original": null,
        "number": 4,
        "cdate": 1666837856420,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666837856420,
        "tmdate": 1670475310108,
        "tddate": null,
        "forum": "VE1s3e5xriA",
        "replyto": "VE1s3e5xriA",
        "invitation": "ICLR.cc/2023/Conference/Paper5538/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a Dual Student (DS) to mimic the behavior of a black-box model without any prior about input data. The main advantage of DS over existing work lies in that it allows a naturally differentiable process for the training of generator. ",
            "strength_and_weaknesses": "Pros:\n1. The introduced additive student model provides an alternative way to update generator which looks reasonable and convincing.\n2. The derived gradients are experimentally demonstrated closer to the true gradient (computed by target model) than DFME which only uses a single student.\n3. The experiments showed that DS can approximate the target model more accurately than the baselines across various tasks.\n\nCons:\n1. Is it possible that two student behaves like two experts in MoE. Let\u2019s say two students work in a complementary way. Will this be risky for simply using the average output during inference? Any experimental observation about this concern?\n2. The authors mentioned that Hong el al. maximized the confidence of the student output. According to my experience, this insight might be helpful to learn a data distribution that is close to the real training data. Has this method been included as a baseline?\n3. Following 2, I felt that data-free model stealing has some overlap with the topic of black-box inverse attack which aims to steal the training samples. The authors should discuss their relations. Particularly, according to the presented gradient error (fig 2), I suspected that the generated data by DS or DFME does NOT looks similar to the true training samples.\n4. I agreed that the proposed formulation may benefit the training of generator. However, the claim of exploring diverse training data has not been well supported. Any further evidence about it?\n5. It would be perfect if some theoretical analyses are provided to support the distance of Eq. 7 is minimized by DS.\n6. Please check the equation number, some of which are not properly referred.",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is novel. The writing needs improving as many sentences are too long to read, and some grammar mistakes should be corrected as well.",
            "summary_of_the_review": "This paper is overall good. The writing needs improving as many sentences are too long to read, and some grammar mistakes should be corrected as well. I expect the authors\u2019 response to my above comments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5538/Reviewer_vQa1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5538/Reviewer_vQa1"
        ]
    }
]