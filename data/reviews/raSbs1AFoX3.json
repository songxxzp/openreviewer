[
    {
        "id": "jAdtNWfhhS8",
        "original": null,
        "number": 1,
        "cdate": 1666211978382,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666211978382,
        "tmdate": 1666211978382,
        "tddate": null,
        "forum": "raSbs1AFoX3",
        "replyto": "raSbs1AFoX3",
        "invitation": "ICLR.cc/2023/Conference/Paper6137/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper claims that SGD converges to global minimum almost surely, under some assumptions.",
            "strength_and_weaknesses": "I have a few technical concerns on different aspects of the theoretical results in this paper.\n\nProblematic assumptions:\n\nAssumption 1: $g(\\theta)$ is already non-negative by definition, and there is no need for repitition. The actual meaning of smoothness needs to be made clear here ($C^1$ or $C^{\\infty}$ ?)\n\nAssumption 3: the set of roots itself is already a closed set, as long as $g(\\theta)$ is continuous which is implicit: one only needs to prove any limit point $x$ of roots $\\{x_i\\}$ is still a root. Suppose for contradiction $g(x)>0$, then by its continuity there exists an open neighborhood $S$ containing $x$, such that for any $x'\\in S$ we have that $g(x')>g(x)/2>0$. However, some root $x_N$ will fall into $S$ which finishes the proof.\n\nAssumption 4: this is a rather strong assumption in my opinion. The simple example $f(\\theta,x)=(\\theta-x)^2$ fails to satisfy the assumption, let alone neural networks. Even if the domain is restricted to be a compact set of $\\mathbb{R}^d$, the constant $c$ is likely to be a large poly factor in the dimension $d$. \n\nAssumption 2.3: can you give any example included in the large family of functions as claimed? \n\n$\\tilde{\\nabla}g(\\theta)$ is not defined. Is it an unbiased estimator of $\\nabla g(\\theta)$ or what? The model of SGD isn't explained either.\n\nMain results:\n\nI will only take Theorem 3.1 as an example. The argument in step 3 is doubtful: $\\theta_n$ might not converge at all with positive probability. Suppose I am the adversary that generates the stochastic estimator $\\tilde{\\nabla}g(\\theta)$, then at any root I'm allowed to generate $\\pm a v$ by Assumption 2.2 where $v$ is randomly drawn from the unit sphere. Then such sequence of $\\{\\theta_n\\}$ doesn't converge, and one can only talk about limit points. Unfortunately, even a non-root point can be a limit point if $\\tilde{\\nabla}g(\\theta)$ is an unbiased estimator.\n\nI suspect that the proof goes through when $\\tilde{\\nabla}g(\\theta)$ is exact and the randomness of SGD decreases with function value, as in equation (5). However, this is a very strong and non-standard assumption and I feel it's not what the authors mean.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing can be improved, some sentences are hard to parse.",
            "summary_of_the_review": "Based on the mathematical issues I found, I doubt the correctness of this theoretical paper.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6137/Reviewer_fVFB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6137/Reviewer_fVFB"
        ]
    },
    {
        "id": "r1jeWKI37b",
        "original": null,
        "number": 2,
        "cdate": 1666604347779,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604347779,
        "tmdate": 1666604411514,
        "tddate": null,
        "forum": "raSbs1AFoX3",
        "replyto": "raSbs1AFoX3",
        "invitation": "ICLR.cc/2023/Conference/Paper6137/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper studies the convergence property of SGD under the over-parameter setting. Under some regular assumptions of the loss and subdifferential gradient noise, this paper gives some novel results: (1) SGD must converge to a global optimum with probability 1. (2) SGD converges to a sharper global optimum not as easy as a flat one. (3) SGD achieves an arbitrary accuracy in polynomial time.",
            "strength_and_weaknesses": "Strength:\nThe paper is technically sound. \nThe paper studies an interesting question related to the over-parameterized problem and provides more understanding of the optimization landscape.\nThe theoretical results are important to the machine learning community.\n\nWeakness\uff1b\nMore discussion should be added after each Theorem.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is written well, well-organized, and has nice reproducibility.",
            "summary_of_the_review": "The paper provides some novel insights on SGD\u2019s optimization landscape. For example, the authors show interesting and important findings that SGD could indeed obtain a global optimum even in the non-smooth non-convex over-parameter setting.\nI have some concerns.\n(1) This paper studies square loss. Can these results be generalized to general losses?\n(2) What\u2019s the difference between $\\tilde{\\nabla}$ and $\\nabla$. subdifferential?\n(3) Can the authors provide some numerical experiments to support the theory in this paper?\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6137/Reviewer_XXfc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6137/Reviewer_XXfc"
        ]
    },
    {
        "id": "F1TInnzHUR7",
        "original": null,
        "number": 3,
        "cdate": 1666650424799,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650424799,
        "tmdate": 1666650424799,
        "tddate": null,
        "forum": "raSbs1AFoX3",
        "replyto": "raSbs1AFoX3",
        "invitation": "ICLR.cc/2023/Conference/Paper6137/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper utilizes a method based on Clarke subdifferential and Lyapunov stability to show that SGD for an over-parametrized model converges to the global optimum almost surely under arbitrary initial value and some mild assumptions on the loss function. In addition, it is also shown that if the learning rate is larger than a value depending on the structure of a global minimum, the probability of converging to this global optimum is zero.",
            "strength_and_weaknesses": "Strength:\nThis paper considers two fundamental problems in learning, i.e., does SGD provably find the global optimum with an over-parametrized model, and why SGD prefers a flat global minimum? If all the claims (Thm 3.1, 3.2, 3.3) in this paper are correct, it is indeed a breakthrough in this field.  \n\nWeaknesses:\nHowever, it is hard for me to judge the correctness of the draft due to the following writing issue. \n\n1. There are so many notations used without definition. For example, $\\tilde{\\nabla}$ in assumption 2.1 (it means subdifferential?), $N_0$ in the proof of Claim 2.2 (number of samples in a batch?), $K_0$ in equation (5) and $\\zeta_n$ in equation (9).\n\n2. I did not see the importance of Claims 2.1 and 2.2, but they took up much space in the paper. Moreover, the sketch of Thm 3.1 seems to be disconnected from the preliminaries in section 2. \n\nI would recommend the authors add more insights and intuitions of the proof instead of the proof details and provide some high-level comparisons on why the proposed techniques could show such strong results. This paper considers two fundamental problems in learning, i.e., does SGD provably find the global optimum with an over-parametrized model, and why SGD prefers a flat global minimum? If all the claims (Thm 3.1, 3.2, 3.3) in this paper are correct, it is a breakthrough in this field.  \n\nSome related references are missing. I would like the authors to address how their approach and results differ from these papers. \n\nVaswani, Sharan, Francis Bach, and Mark Schmidt. \"Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron.\" In The 22nd international conference on artificial intelligence and statistics, pp. 1195-1204. PMLR, 2019.\n\nChizat, Lenaic, and Francis Bach. \"On the global convergence of gradient descent for over-parameterized models using optimal transport.\" Advances in neural information processing systems 31 (2018).\n",
            "clarity,_quality,_novelty_and_reproducibility": "See the above comments.",
            "summary_of_the_review": "This paper considers a fundamental problem in the convergence guarantee of SGD in an over-parametrized model. However, the paper is not well-written in its current form, which makes the reviewer hard to verify its correctness.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6137/Reviewer_zzGL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6137/Reviewer_zzGL"
        ]
    },
    {
        "id": "uJTxJ6dvzv2",
        "original": null,
        "number": 4,
        "cdate": 1666758219668,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666758219668,
        "tmdate": 1666758219668,
        "tddate": null,
        "forum": "raSbs1AFoX3",
        "replyto": "raSbs1AFoX3",
        "invitation": "ICLR.cc/2023/Conference/Paper6137/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides theoretical results for the asymptotic convergence of SGD algorithm under an over-parameterized setting. It shows a set of assumptions that can guarantee the global convergence of SGD almost surely in some non-convex setting. ",
            "strength_and_weaknesses": "The strengths of this paper are the theoretical results for the global convergence of SGD almost surely. The authors consider the regular sampling scheme and propose a new scheme (sampling noise with global stable guarantee). They prove the asymptotic convergence for SGD under a set of assumptions. \n\nThe weaknesses of this paper are: \n- This paper did not explain the intuition why SGD converge globally very well. The assumptions are not clearly motivated. The authors should explain why they have two set of assumptions on the gradient/ sample gradient of g. One is Assumption 2.1 part 4, line 2 where there is a lower bound on the liminf of gradient, the other is Assumption 2.2 where we put an upper bound on the sample gradient. Please add a discussion why the theory needs these assumptions and make sure they do not contradict each other. In addition, Assumption 2.3 is not natural when it asks that the constants $c_\\theta, \\hat{c}_\\theta$ are bounded away from 0 and by a constant of $\\theta$. \n- The presentation of this paper is poor. There are many notations and variables that were mentioned before the authors define them in the draft. For example: $M_0$ and $a$ were referred in Assumption 2.1 but only defined until 2.2, $\\tilde{\\nabla} g$ has no definition, 'global stable guarantee' was referred before the explanation,... Most of the time, the sketch proofs are confusing and they did not help to understand the thought process to prove the theorems. \n\nQuestion: In Theorem 3.4, what is 'the variant of Assumption 2.3 described immediately preceding this statement'?",
            "clarity,_quality,_novelty_and_reproducibility": "The results of this paper seem to be new and the approach is different from prior work. However, its clarity and presentation is not good. ",
            "summary_of_the_review": "Although this paper show interesting results, I am not fully convinced by the assumptions and the intuition/reasoning behind the proofs. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6137/Reviewer_5yze"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6137/Reviewer_5yze"
        ]
    }
]