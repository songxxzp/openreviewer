[
    {
        "id": "XxncKUuupLZ",
        "original": null,
        "number": 1,
        "cdate": 1666592776308,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666592776308,
        "tmdate": 1666763656022,
        "tddate": null,
        "forum": "wFOGJB88Y5",
        "replyto": "wFOGJB88Y5",
        "invitation": "ICLR.cc/2023/Conference/Paper2106/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "- The motivation of this paper is that Lyapunov Spectrum (LS)-based distance in the early training stage can represent the final performance.\n- Based on the above observation, this paper illustrates hyperparameter optimization method for RNN pruning using LS distance.\n- Specifically, the candidate set of configurations (i.e., sparse RNN) are initialized and trained for E epochs.\n- The LS is computed for each candidate and dense model through training: eq 1, 2, and 3.\n- All the LS values of candidates and dense models over training epochs are then used to obtain embedding (e.g. PCA).\n- Then the candidate is eliminated based on the L2 distance between candidate embedding and dense model embedding of the last epoch.\n- The experiments are conducted on PTB dataset using stacked-LSTM and RHN.\n- The proposed method outperforms baselines including dense model.",
            "strength_and_weaknesses": "Strength:\n1. The main idea of this paper is intuitive: using LS-distance as a metric for eliminating unpromising sparse network candidates. It naturally supports the usage of reference (dense) model as a target, which is ambiguous for the original Loss-based metric.\n2. The experimental results show the effectiveness of the proposed method.\n\nWeakness:\n1. The computational overhead of Lyapunov Spectrum is not discussed in the paper. In equation 2, QR decomposition and Jacobian matrix computation are required. This makes the proposed method more computation- and memory-intensive than Loss-based metric.\n2. The reason why one should use 2D-PCA as a embedding function is rarely discussed. For instance, one can use either tsne embedding instead of 2D-PCA or the LS-distance itself without any modifications. At least, the reason why authors set 2D for PCA can be discussed. \n3. The writing can be improved, e.g., some paragraphs are too long, notations are overlapped (x is used for both input and embedding).\n4. The experiment is too narrow to validate the general effectiveness of proposed method. RNNs are widely used for tons of NLP tasks (e.g. machine translation as explained in introduction), and the proposed method is not limited to any specific NLP tasks.\n5. I am not expert of neither RNN pruning or LS-distance, but I think this method can be applied to Transformer-based neural networks. Transformer is recently widely used for any NLP tasks, and pruning is one of attractive research directions for these large-scale models. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe clarity of this paper can be improved: see weakness 3.\n\nQuality:\nThe discussion of main technical components are missing: see weakness 1 and 2.\n\nNovelty:\nI am not expert of neither RNN pruning or LS literature, but I think this paper is novel in that it is the first one which introduces LS-distance for RNN pruning.\n\nReproducibility:\nThis paper provides all the experimental setups to reproduce results.\n\n\n",
            "summary_of_the_review": "This paper proposes a RNN pruning method, which eliminates sparse network candidates based on LS-distance metric. The idea is novel and motivating, but the discussion of main technical components are missing and experiments are too narrow to evaluate of general effectiveness of the proposed method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2106/Reviewer_X75X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2106/Reviewer_X75X"
        ]
    },
    {
        "id": "Y5UmF_GQhGn",
        "original": null,
        "number": 2,
        "cdate": 1666762489973,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666762489973,
        "tmdate": 1666762489973,
        "tddate": null,
        "forum": "wFOGJB88Y5",
        "replyto": "wFOGJB88Y5",
        "invitation": "ICLR.cc/2023/Conference/Paper2106/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a new distance based on Lyapunov Spectrum (LS) for neural network hyperpruning. Here, the authors define \u2018hyperpruning\u2019 as finding a suitable pruning method with optimal hyperparameter configuration. The authors apply this method to RNN language model benchmarks and show good results on Penn Treebank Dataset.",
            "strength_and_weaknesses": "Using Lyapunov Spectrum for hyperparameter learning is an emerging method in the deep learning area. In this paper, the author proposed to use the Lyapunov Spectrum for channel pruning. The results show the proposed method achieves higher results than the SOTA.\n\nI\u2019m curious how is the LS distance compared with the L2 distance? What is the performance difference compared to using L2 distance?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clear, but the reference format doesn\u2019t align with the ICLR requirement. The authors can double-check how to make the reference format correct. Moreover, some references are missing, e.g., in the second paragraph of the introduction, when introducing Dynamic Sparse Training (DST), authors need to cite the relevant papers.  \n\nThe quality is moderate. I\u2019m not very familiar with Lyapunov Spectrum (LS) or the language tasks. So it is hard to evaluate the novelty.\n",
            "summary_of_the_review": "Overall this paper proposed a plausible method that achieves good results on language tasks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2106/Reviewer_cX2r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2106/Reviewer_cX2r"
        ]
    },
    {
        "id": "IIMfKw1hz3l",
        "original": null,
        "number": 3,
        "cdate": 1666884841396,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666884841396,
        "tmdate": 1670001662395,
        "tddate": null,
        "forum": "wFOGJB88Y5",
        "replyto": "wFOGJB88Y5",
        "invitation": "ICLR.cc/2023/Conference/Paper2106/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel \"Hyperpruning\" method for RNNs, based on the Lyapunov spectrum. The idea is to select good pruned candidate networks after a few training steps based on their distance in the 'Lyapunov spectrum space' from the unpruned network.\nThe paper claims to achieve with this method state-of-the-art accuracy on the PTB benchmark with stacked LSTM and RHN,  while using fewer training epochs.\n",
            "strength_and_weaknesses": "Strength:\n* The proposed hyperpruning method is novel.\n* The submission bridges tools from different fields (network pruning, Lyapunov Spectrum, Language modelling)\n\nWeakness:\n1.) Many details are missing, which makes the submission hard to evaluate and impossible to reproduce. E.g. \n- How exactly are the variants generated? The relevant paragraph says \"after the elimination, given the remaining n candidates in T, the knowledge of their configurations and distances s serves as an initial history/archive for Bayesian hyperparameter optimization methods such as Tree Parzen Estimators (TPE) and Adaptive TPE (APTE) to generate new candidates. For example, TPE uses this initial knowledge to build a surrogate and generate new configuration candidates via Expected Improvement.\" \nThis is not very specific, what are the exact hyperparameter configurations that are being utilized? Important details seem to be missing here.\n- Details are also missing for the LS Computation: which hidden states are being used? What is f respectively for LSTM and RHN, how is J obtained respectively? What was your choice of T and K? \n2.) No code is provided, which adds to the irreproducibility.\n3.) No theory is provided and it doesn't become clear from the submission why the LS-based metric is working. There is an heuristic explanation in figure 1, but the claim that \"While early perplexity is not indicative of the estimated accuracy, LS-based distance on the other hand appears to be consistent [...]\", seems only anecdotal. It would be great to show that more systematic, e.g., a scatter plot of early vs late perplexity vs a scatter plot of LS-distance vs late perplexity and the corresponding correlations. \n\n4.) The paper claims that the hyperpruning methods would beat the state of the art, but the architectures being used (LSTM and LSH networks) seem not to be state of the art for language modeling, it would be interesting to see how LSH would perform on actual state of the art models. Moreover, the PTB benchmark is a very small data set, so it is unclear if the hyperpruning method would still perform on par when used on more recent benchmarks and architectures. Of course, the result is still of academic interest.\n5.) The submission doesn't discuss the computational complexity. LS calculation involves QR-decomposion (which is cubic in Jacobian size), and also matrix-matrix multiplication (also cubic), so if I am not mistaken, this should become more expensive for larger problems, correct? Would the computational advantage still persist for state-of-the-art language models? \n\nMinor issues\n* Figure 1 A: x-axes labels/ticks are missing, is that just an illustration or actual LS curves?\n* Figure 1B: \n* How does the 2D LS-space look like? It would be good to add a (supplement) figure to plot embeddings in the LS space.\n* Why is there an increase o the perplexity from the pruning ration of 0.6 to 0.5 in figure 3?\n* It would be good to plot the test perplexity as a function of epochs\n* Not clear what hyperparameters are actually used\n* baseline fair? Maybe compare to something else than LS-based metric.\n* Why increase perplexity from a pruning ration of 0.6 to 0.5?\n* Shouldn't time efficiency (figure 3 right) be also compared with selfish RNN?\n* (This might also just be a misunderstanding on my side.) It seems that the optimization of e_i and e_0 was only done for the LS-based metric and not for the loss-based metric. I was wondering what would happen if e_i and e_0 were optimized for the loss-based metric instead and then tested on the LS-based metric. \n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: \nThe introduction is very clear, the section on LS calculation and LS-based hyperpruning could use more details. For example, it doesn't become clear to me, how exactly new candidates are being generated. Which features exactly are being fed into TPE or ATPE? \n\nQuality:\nThe submission contains a good amount of work on comparisons with previous methods. (However, I am not an expert on efficient pruning, so I can't fully certify the quality of that part.\nThe Quality of the LS-based hyperpruning is hard to estimate, because no code is provided, so it is difficult to evaluate how the embedding and LS calculation were done exactly.\n* For table 1, no confidence intervals are reported.\n\nNovelty:\nThe proposed hyperpruning method seems novel. It seems an innovative addition to previous methods (e.g., Selfish RNN, APTE, GS) that seems to improve performance and reduce computational demand.\n\nReproducibility: \nBecause details are missing, and no code is provided, the results are unfortunately not reproducible in the current form. \n",
            "summary_of_the_review": "\nThe submission suggests an innovative and novel hyperpruning method for RNNs based on Lyapunov exponents after a few training steps, resulting in higher performance and reduced computational cost.\nThe main issue of the submission are a lack of a theoretical understanding of how and why it works (including limitations/when it would break down) and a lack of scalability to real-world problems.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2106/Reviewer_mZ4i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2106/Reviewer_mZ4i"
        ]
    }
]