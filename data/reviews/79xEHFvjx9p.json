[
    {
        "id": "g8Y8rw2mYIz",
        "original": null,
        "number": 1,
        "cdate": 1666644862052,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644862052,
        "tmdate": 1666670142195,
        "tddate": null,
        "forum": "79xEHFvjx9p",
        "replyto": "79xEHFvjx9p",
        "invitation": "ICLR.cc/2023/Conference/Paper2188/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper describes an approach for generating video of a talking face from a reference image using either speech as input, or using a video sequence of another person as input.  The architecture uses a separate encoder each for learning a representation of the acoustic features, the lip motion, and the identity.  These learned representations are then concatenated and used as input to StyleGAN2 models for generating the output sequence.",
            "strength_and_weaknesses": "+The work compares against a number of baseline approaches.\n\n+StyleGAN models have shown to work well for multiple problems.\n\n-The paper lacks a lot of detail that would be required to reproduce the method.  Information about the architectures, the feature processing, and so on is all missing.\n\n-It is difficult to evaluate the quality of the output from the way the results are presented.  There are some objective measures included, but these relate more to the image quality and not the visual speech content captured in the video.\n",
            "clarity,_quality,_novelty_and_reproducibility": "For the acoustic features, what is the window size for computing the MFCCs, and what is the hop size?  I assume you use something fairly typical like a 25ms window with 10ms hop size, but you should be explicit.  \n\nIs the frame rate of the audio and video the same?  If not, how is the audio and the video aligned?\n\nThe model produces the output frame-by-frame, so how much acoustic context is provided to generate a frame of video? Is the system causal so only uses past context, or is it non-causal using both past and future context?\n\nI am not sure what Figure 1 is trying to show.  The left figure is labelled the \u201cparameter dimension map\u201d \u2014 is this a visualization of the MFCCs, where the color relates to the value?  The amplitude plot in Figure 1 also does not really provide any information.\n\nFor Figure 2, you have the same person as input for the ID encoder and the lip encoder.  At test-time, these would be different individuals, so I would use different people in the example.\n\nExpand the figure/table captions to completely describe the content independently of the main text.  Include the main takeaway for the reader.\n\nIn the approach section (Section 3), at the end of the first sentence you reference Figure 3.  This should be Figure 2.\n\nThe sub-sections in Section 3 are missing important details about the architectures.  For example, in Section 3.2 you mention modifying FAN, but there are no details about the modifications.  In Section 3.3 there are no details about the architecture for encoding the MFCCs.\n\nIn Figure 4 the caption states you are showing an audio-drive talking face, then the rows have labels \u201caudio\u201d, \u201cfake\u201d, and \u201cground-truth\u201d.  What is fake?  Is this a generated face that should match the speech?  If so the lip shapes are different from the ground-truth.  If the objective is not to match the ground-truth, we have no way of evaluating the output looking at the figure because there is no information about what is being said (and so having some idea of the target lip shapes).\n\nExplain all of the terms in your equations.\n\nOrder the rows consistently in Figures 4 and 5 \u2014 the Fake and Ground-truth are swapped.  Again, the lip shapes do not match in the Fake and Ground-truth rows.\n\nLabel the rows in Figure 8.\n\nHow are missing input modalities handled?  For example, if I wanted to generate the visual speech to align with an input audio sequence, but I do not have the accompanying video for the lip encoder?  \n\n",
            "summary_of_the_review": "The approach taken in the paper seems to be a reasonable one.  Speech-driven talking faces have seen success elsewhere, as has StyleGAN2 for generating content.  However, with so many gaps in the information provided, it is impossible to gauge the quality of the work.  Likewise, the choice of objective metrics makes it difficult to understand how effective is the model at producing visual speech as the measures relate more to image quality.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2188/Reviewer_34UQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2188/Reviewer_34UQ"
        ]
    },
    {
        "id": "P2Qltn_gqL",
        "original": null,
        "number": 2,
        "cdate": 1666667218171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667218171,
        "tmdate": 1666667218171,
        "tddate": null,
        "forum": "79xEHFvjx9p",
        "replyto": "79xEHFvjx9p",
        "invitation": "ICLR.cc/2023/Conference/Paper2188/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work is for talking animation video generation. The authors propose a generative pipeline via different feature extraction like ID, Lip and audio. Also, the authors did some experiments and ablation study for evaluation.",
            "strength_and_weaknesses": "The idea of using combine audio and image feature for generation task is interesting, ",
            "clarity,_quality,_novelty_and_reproducibility": "The structure of this paper as well as the novelty are not clear. I could not find any concrete contribution. ",
            "summary_of_the_review": "This paper presents a fact that using combined audio features with facial feature could improve facial animation. \nHowever, there is not any novel design or insight about this statement. \nUsing StyleGAN2 to generate image is not a contribution. \nStyleGAN2 might not be applicable for video generation due to lack of speech details.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2188/Reviewer_gYGv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2188/Reviewer_gYGv"
        ]
    },
    {
        "id": "k4XgXfwXXNe",
        "original": null,
        "number": 3,
        "cdate": 1666697085824,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697085824,
        "tmdate": 1666697085824,
        "tddate": null,
        "forum": "79xEHFvjx9p",
        "replyto": "79xEHFvjx9p",
        "invitation": "ICLR.cc/2023/Conference/Paper2188/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposed to use StyleGAN2 as the generator to synthesize talking head videos, taking as input the ID feature and the video or audio feature. The consistency of video and audio features is constrained by an adversarial loss. The experiments are conducted on the LRW and CREAM-D datasets.",
            "strength_and_weaknesses": "Strength:\n1. The reason for using StyleGAN2 as the generator is discussed with an illustration.\n\nWeaknesses:\n1. The language fails to convey the meaning. Please revise the paper with the help of English professionals.\n2. The cited state-of-the-arts (SOTAs) are out-of-date. Please cite and make a comparison with more recent papers, such as [1][2][3][4].\n3. The method is proposed unclearly. For example, why are there two StyleGAN2 in Figure 2? Moreover, the Decoder mentioned in section 3.1 is missing in Figure 2.\n4. The novelty of the proposed method has not been discussed in the paper. \n5. The generated results should be compared with SOTAs visually. \n6. Minor issues:\n    a) Mcc Encoder in Figure 2 should be MFCC Encoder.\n    b) The second line in section 3.4, \"... as shown in the figure appears\" should be Figure 3.\n    c) To be formal, I suggest the author use \"adversarial\" rather than \"antagonistic\" and use \"concatenate\" rather than \"splice\".\n\n[1] Wu, Haozhe, et al. \"Imitating arbitrary talking style for realistic audio-driven talking face synthesis.\" Proceedings of the 29th ACM International Conference on Multimedia. 2021.   \n[2] Zhou, Hang, et al. \"Pose-controllable talking face generation by implicitly modularized audio-visual representation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.   \n[3] Ye, Zipeng, et al. \"Audio-driven talking face video generation with dynamic convolution kernels.\" IEEE Transactions on Multimedia (2022).   \n[4] Ji, Xinya, et al. \"EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model.\" SIGGRAPH, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "1. The clarity and quality in terms of language and logic are very bad. \n2. This paper lacks novelty. Using an existing StyleGAN2 is not novel. \n3. Reproducing is not possible since the detailed structures of the networks are missing.",
            "summary_of_the_review": "For the above reasons, this paper is entirely below the acceptance threshold.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2188/Reviewer_FwS3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2188/Reviewer_FwS3"
        ]
    },
    {
        "id": "VxvLJAGDatW",
        "original": null,
        "number": 4,
        "cdate": 1666906628688,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666906628688,
        "tmdate": 1666906628688,
        "tddate": null,
        "forum": "79xEHFvjx9p",
        "replyto": "79xEHFvjx9p",
        "invitation": "ICLR.cc/2023/Conference/Paper2188/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the problem of generating a talking face animation video driven by audio (or video), conditioned on a target face appearance. The proposed approach consists of a StyleGan2-like generator conditioned on output from identity, video and audio encoders. ",
            "strength_and_weaknesses": "Pros:\n- An interesting approach for generating talking faces\nCons:\n- The paper requires proof reading\n- Limited novelty: most of the components of the proposed solution have already been published\n- Limited evaluation and ablation analysis\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper proposes to combine existing techniques into a StyleGan2-like generator to solve the problem of video generation, conditioned on target identity, audio signal (which should drive the mouth expressions) or video signal. \n\nI found the proposed method being only an incremental extension to existing techniques (Bulat et al. 2017, Zhou et al. 2019 etc), whereas an important discussion on details/modifications have been neglected (e.g. in sec. 3.5 there is no information on how the discriminator is used for optimising the sequence). \n\nThe discussion in the evaluation section is quite limited without proper explanations of metrics and what's the impact of different proposed components (e.g. the impact of the synchronisation discriminator is not measured with any of the metrics, only a few pictures are provided)\n\nFinally, the paper requires proof reading. It contains multiple English grammar errors and uses non-scientific language. Examples:\n\u201cFool the discriminator maximally\u201d\n\u201cOur approach is feasible\u201d - missing proper context\n\u201c generation from speech began to flourish\u201d\nFigure 2 is referred as Figure 3 (Section 3, the first sentence)\nDifferent sections introduce variables that are never referred to, or are referred after multiple sections (see. 3.3. Audio encoder, E_a)\nThe legend in Fig 6. has too small font size.\nWe spliced the features -> combined?",
            "summary_of_the_review": "Recommend to Reject.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2188/Reviewer_MS16"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2188/Reviewer_MS16"
        ]
    }
]