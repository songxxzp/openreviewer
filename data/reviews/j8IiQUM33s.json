[
    {
        "id": "A_3vlRIYP9L",
        "original": null,
        "number": 1,
        "cdate": 1666690109781,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690109781,
        "tmdate": 1666690109781,
        "tddate": null,
        "forum": "j8IiQUM33s",
        "replyto": "j8IiQUM33s",
        "invitation": "ICLR.cc/2023/Conference/Paper4851/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nThe paper has a good motivation of transferring a pretrained model into diverse tasks by tackling the problem of negative transfer. The solution is having an abstraction layer of semantic, where input is grouped into different clusters based on their semantics before routing to experts. Experiments show a very good improvement of 2.54% on ImageNet. \n",
            "strength_and_weaknesses": "Strength: 1/ Having a good motivation on an important application, where we should take advantage of pretrained models to save cost.  2/ The testing efficiency is really good. \n\nSome Weaknesses\n\n1/ About cost saving: the proposed solution has a non-negligible overhead of data clustering, which is itself an optimization problem. My understanding is this overhead is not included in the training analysis (please correct if I am wrong). \n\n2/ It\u2019s not entirely clear about the location and number of MoCE layers chosen even with Fig 4b and Table 6,  e.g. the intuition as to why placing them on, say, 10th and 12th layers are good but not before that? This is also along with the claim of hierarchical learning by the model concerning Fig 4b. Why don\u2019t we try a couple of other patterns such as interleaving odd and even layers, or early in the layers while the input is already arranged in clusters, which is somewhat more intuitive? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and easy to follow. \n\nIt\u2019s somewhat surprising to see TokenMoE is much lower than MAE (Table 2). Do you have any intuition as to why? Taking a look at, e.g. Fig 3a, something might happen with TokenMoE, leading to imbalancing. \n\nAny reason why not using, say, t-SNE to visualize the clusters? \n\nNo code or appendix is provided so it\u2019s hard to judge the reproducibility. \n",
            "summary_of_the_review": "\nThe paper motivates a good problem in transferring a vision task and proposes a good solution in which semantics is used to train the mixture-of-expert model, and perform well with diverse tasks. Although there are some parts that need more clarification, generally it\u2019s a good and well written paper. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4851/Reviewer_Emir"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4851/Reviewer_Emir"
        ]
    },
    {
        "id": "9nNtVFkmTYu",
        "original": null,
        "number": 2,
        "cdate": 1666698190585,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666698190585,
        "tmdate": 1669265666048,
        "tddate": null,
        "forum": "j8IiQUM33s",
        "replyto": "j8IiQUM33s",
        "invitation": "ICLR.cc/2023/Conference/Paper4851/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates practical techniques to apply the Mixture of Experts (MoE) model to the MAE-based pre-training paradigm. \n\nTaking a supervised MoE model for ViT as a baseline, the authors introduced data clustering as a strong baseline for experts' partition and assignment. \n\nSeveral additional terms, such as imbalance regularization and distill loss, are also proposed in the self-supervised MoE design. \n\nOn 11 downstream tasks, the proposed method outperforms MAE, supervised TokenMoE and several recent self-supervised single model methods (non-MoE). \n",
            "strength_and_weaknesses": "# Strength\n- This paper proposed practical techniques to address the expert collapse issue in TokenMoE. \n- The proposed MoCE improves the baseline performance on 11 downstream tasks, showing the effectiveness of the proposed method. \n- Reasonable visualization of the expert routing and expert visualization is shown in the experiment. \n- Several ablation studies are conducted in Table 4-7. \n\n\n# Weaknesses\n- Although better performance is achieved on the benchmark dataset, the technical contribution is kind of incremental. The main algorithm re-uses multiple design components of (Liu et al., 2022) and applies them to a slightly different scenario, i.e. MAE self-supervised training with a Mixture of Experts. Compared with (Liu et al., 2022), the improvements are (1) exploring the new self-supervised paradigm MAE, (2) applying to the Transformer backbone, and (3) different routing mechanisms with MoE. Both (1) and (2) are straightforward extensions to new technologies. As to (3), it offers some insights and benefits on self-supervised pre-training. \n- As mentioned above, the proposed method heavily relies on design modules from (Liu et al., 2022), e.g. Data Clustering (Eq. (3)), Distillation term. However, most of the methodology and experimental comparison are conducted with respect to TokenMoE, a specific supervised MoE model. As also verified by the authors in Figure 3(a), TokenMoE is not specifically designed for the setting of self-supervised pre-training. Therefore, more comparisons and discussions should be made w.r.t. (Liu et al., 2022) rather than TokenMoE. \n- TokenMoE used a load loss to balance the expert assignment, so it looks a little strange in Figure 3(a) the expert collapsing. \n- In Table 3, the results for MAE are missing. \n- It seems the results of DeepClusterV2 and BYOL in Table 1 are different from the results from (Liu et al., 2022), both on Resnet 50. \n- Other minors:   \n> \"decades(Jacobs et al.,\" and \"processing(Lepikhin et al., 2020\", missing space before left parentheses.   \n> Equ. 5 is not so common, usually Eqn. 5 or Eq. 5.   \n> In Eq. (3), $Tr(Q^TA), Q\\in \\mathbb{R}^{n\\times m}, A\\in \\mathbb{R}^{n\\times m}$, please check if this is correct. A more common notation for Sinkhorn is using matrix product $<Q, A>$.   \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is based on two previous works, i.e. (Liu et al., 2022) and TokenMoE. It offers some insights into the Mixture of Experts on MAE pre-training. The overall technical novelty is kind of incremental. \n\nThere is no code attached to this paper, but it will be possible to reproduce the results based on the descriptions and experimental details. \n\n\n",
            "summary_of_the_review": "Although this paper obtains improved results over MAE and other self-supervised methods, the technical contribution is kind of incremental w.r.t (Liu et al. 2022) and TokenMoE.   \nAnd there is a lack of discussion and comparison w.r.t (Liu et al. 2022)   \n\nOverall, this paper is below the acceptance threshold. \n\n------------------------------\nAfter Rebuttal:   \nThe authors addressed most of my concerns and made significant efforts. I would like to see this paper accepted and increased the overall score. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4851/Reviewer_Wv7b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4851/Reviewer_Wv7b"
        ]
    },
    {
        "id": "hf1llfdmaK",
        "original": null,
        "number": 3,
        "cdate": 1666797631228,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666797631228,
        "tmdate": 1668669390628,
        "tddate": null,
        "forum": "j8IiQUM33s",
        "replyto": "j8IiQUM33s",
        "invitation": "ICLR.cc/2023/Conference/Paper4851/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies the negative transfer phenomenon of MAE pre-trained models. Authors first experimentally show the existence of negative transfer and then show that natively using MoE can not solve the problem. Then, a clustering-based method is proposed to solve the problem.",
            "strength_and_weaknesses": "Generally, the paper is well-organized and easy to follow. The research topic is relatively new and interesting. The proposed MoCE seems correct and is well supported by the experimental results. \nHowever, I also have the following concerns:\n1. In the abstract and introduction, the authors position this work as a pretraining method. However, MoCE is based on a pre-trained MAE. Considering the extremely heavy cost of training foundation models, it would not be feasible. \n2. Based on 1, the cost of training MoCE would be twice of MAE. To make a fair comparison, what if we train MAE with a dataset of twice scale or train MAE with more epochs?\n3. If we focus on the negative transfer phenomenon, not combining MoE, there would be other transfer learning and model adaptation methods (e.g., prompt tuning) that could ease the negative transfer phenomenon. Could you compare it with some of them?\n4. Since the input of Eq.5 is cluster centroid and there are only m clusters, is it enough to learn W_g well?\n5. For Eq.6, is the loss implemented over all samples in the dataset or a batch? \nIn the experiment, only 11 small-scale classification tasks are evaluated, how about other more important tasks, e.g., object detection, semantic and instance segmentation?",
            "clarity,_quality,_novelty_and_reproducibility": "Generally, the paper is presented clearly. The method should be original.",
            "summary_of_the_review": "I like the idea but believe the current experiments are not sufficient. If my concerns are solved, I will be willing to raise the score.\n\n\n------ After rebuttal --------\nIn general, some of my concerns have been solved, e.g., training from scratch, and transferring to det and seg tasks. Thus, I would like to raise my score to borderline accept considering the interesting design of MOE.\n\nHowever, I still have some concerns and hope the authors can address them in the final version. First, the added supervised pertaining results remind me that comparing MoCE with [1] would be necessary, which shows that simple MLPs can significantly improve the transferring performance of supervised pretraining.\nSecond, as you claim/argue the method is pretraining not transferring, it would be better to update all the results of MoCE with training from scratch instead of based on MAE to avoid confusion. This issue has also been pointed out by other reviewers (I believe it's feasible for the authors to train MoCE 1600 epochs from scratch since they already show some results about 200 epochs).\n\n[1] Revisiting the Transferability of Supervised Pretraining: an MLP Perspective, CVPR 2022",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4851/Reviewer_ycEm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4851/Reviewer_ycEm"
        ]
    },
    {
        "id": "Tz3lJ2LOTeC",
        "original": null,
        "number": 4,
        "cdate": 1667222163491,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667222163491,
        "tmdate": 1669240640203,
        "tddate": null,
        "forum": "j8IiQUM33s",
        "replyto": "j8IiQUM33s",
        "invitation": "ICLR.cc/2023/Conference/Paper4851/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper initiates an interesting exploration of MAE with mixture of experts (MoE). The method is quite well-motivated, with interesting and fairness-in-mind designs, and a good amount of experiments devoted to it. The final results are reported using a suite of 11 downstream classification tasks, typically used to evaluate self-supervised learning methods. The proposed method is shown to significantly improve the overall accuracy, while maintaining efficiency.",
            "strength_and_weaknesses": "Strengths:\n- This is one of the first works that I am aware to study MAE and MoE jointly. The exploration on this direction is interesting and of significance.\n- The paper motivates the approach by pointing to an empirical result of MAE suffering from the negative transfer phenomenon, and the final approach (guided by the motivation) is able to significantly outperform the MAE baseline. Overall it looks like a healthy research cycle to me.\n\nWeaknesses:\n- One major concern I am having with MoE-kind approach is about larger models. While used in a sparse way, MoE is still having a lot of parameters, and if directly comparing against MAE of the same backbone, it does not sound too fair to me. So I would like to see two things: 1) with a larger MAE model that roughly has the same number of parameters as a model with MoE, what's the comparison? and 2) Whether the improvements of the current model can transfer to larger models. Related: in table 3, I am not able to find the column for MAE. All I find are columns for TokenMoE and MoCE, while the caption says MAE is under comparison.\n- Related to the above, I am not sure the \"negative transfer\" phenomenon still exists with larger and larger MAE models? The hypothesis here is that maybe with larger models, it can capture both the man-made objects and the natural ones from ImageNet?\n- Maybe it is demanding too much computation overhead, but I noticed that the method in the paper is built from pre-trained MAE, and not training from scratch.  So I am wondering what would MoCE be like when pre-trained from scratch?\n- For the distillation loss, I am not sure why it still maintains an efficiency advantage over TokenMoE because the \"whole\" network is used as a \"teacher\", and presumably it can be costly to forward through all the parameters.\n- (minor as I don't know where to put it in the review) Why does MoCo v3 achieve so good results on Table 1? Given that MoCo v3 is better than MAE, isn't it more reasonable to start MoE explorations from MoCo v3 given the results listed in the table? In the same table, I am also curious to see what the supervised ViT will achieve on these downstream datasets. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Overall the paper is a good read, and it is reasonably clear. One thing not clear to me: for data clustering, what features do it use? I am thinking the features are important, and if they are computed online, their FLOPs should also be counted? One could compute it offline, but it is still a computation budget over there if we want to compare overall pre-training cost fairly. Another (potentially minor) thing: for PSNR comparison in Fig 4, is it evaluating the reconstruction quality of the pre-training (auto-encoding) task? Or is it something else?\n\nQuality: I think this is a useful exploration marrying MAE and MoE. The paper has good illustrations, nice storyline, and sufficient amount of experiments. One pity is that the MAE w/ MoE model is not pre-trained from scratch, and this may cause some noise in the signals, but the presented work is done with high-quality.\n\nNovelty: While the techniques in the paper (how to do clustering, how to do gating in MoE) do appear to be existing ones, I think the work has directional novelty in exploring MAE w/ MoEs.\n\nReproducibility: If there is no difficult constraints, I hope the code of the paper is released to facilitate reproducing the results.",
            "summary_of_the_review": "I would be on the acceptance side for the paper given the pros and cons listed above. I hope the authors can address the clarification questions I have, and try to tackle the empirical study (e.g. with larger models) as best as they can.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4851/Reviewer_uKwM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4851/Reviewer_uKwM"
        ]
    }
]