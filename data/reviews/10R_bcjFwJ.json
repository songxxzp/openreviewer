[
    {
        "id": "Nc8sM1Im4Ka",
        "original": null,
        "number": 1,
        "cdate": 1666522910585,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666522910585,
        "tmdate": 1666522910585,
        "tddate": null,
        "forum": "10R_bcjFwJ",
        "replyto": "10R_bcjFwJ",
        "invitation": "ICLR.cc/2023/Conference/Paper11/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is about unsupervised learning of image classification. It is focused on patch-level contrastive by an attention mechanism. Experiments on ImageNet-1k, MSCOCO, ADE-20k verify the effectiveness. ",
            "strength_and_weaknesses": "--Strength\n\nThe paper introduces ADCLR, a novel patch-level contrastive learning method, improves the quality of dense representation of unsupervised learning. The experiments show the competitive performance of the proposed method. The paper provides detailed information for reproduction, and opens a new direction for future research.\nThe improvement is marginal comparing to iBOT on ImageNet. \n\n--Weakness\n\nFigure 1 does not describe the method well, e.g., what do we get after the augmentation image goes through Transformer Encoder? Reconstructed image or deep features?\n\nIn page 4, \"...By feeding the whole sequence of two views xA and xB to...\"  I cannot figure out how to get xA and XB.  It seems z is input, and goes through of attention layer, but where does x comes from? what is two views?",
            "clarity,_quality,_novelty_and_reproducibility": "Some presentation is not clear.",
            "summary_of_the_review": "The idea and experiments seem good, but the description of the method is not self-contain and clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper11/Reviewer_7W9J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper11/Reviewer_7W9J"
        ]
    },
    {
        "id": "ZhqPq7C6Cpd",
        "original": null,
        "number": 2,
        "cdate": 1666556348594,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556348594,
        "tmdate": 1666556348594,
        "tddate": null,
        "forum": "10R_bcjFwJ",
        "replyto": "10R_bcjFwJ",
        "invitation": "ICLR.cc/2023/Conference/Paper11/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on learning dense contrastive representation based on vision transformer backbone. To achieve this goal, the authors propose a clean, simple and effective framework. In detail, they perform the cross-attention between query patches and raw patches. Then the updated query patch features are positive pairs. Despite the local contrastive loss, they also add global contrastive loss. The proposed method outperforms the previous methods.",
            "strength_and_weaknesses": "Strengh\n1. The framework is clean, simple, and effective.\n2. This proposed method does not introduce too many extra parameters.\n3. The method outperforms the previous methods.\n\nWeakness\n1. This paper claims that [CLS] token may extract mismatched information. However, the query patch token may also has this problem, if the patch does not exist in one augmented view. \n2. How to select the query patches? Previous works like SoCo use selective search to select good patches. Selective search may also be used in this paper.\n\nOther questions\n1. In Table 7, are the local views used as raw patches or query patches?\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general the quality of this work is good. The idea is clean. The experiments are solid. \nThe proposed method has some novelty in the transformer based patch-level contrastive learning.",
            "summary_of_the_review": "I like the idea that perform attention between query patches and raw patches. Then contrast the query patch features.\nI also like the analyses related to the collapse.\nThere are some details are not doing well, such as selecting the query patches.\nIn summary, I prefer to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper11/Reviewer_D4eS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper11/Reviewer_D4eS"
        ]
    },
    {
        "id": "IF1NNzIVtpx",
        "original": null,
        "number": 3,
        "cdate": 1666819831449,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666819831449,
        "tmdate": 1666819831449,
        "tddate": null,
        "forum": "10R_bcjFwJ",
        "replyto": "10R_bcjFwJ",
        "invitation": "ICLR.cc/2023/Conference/Paper11/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents ADCLR, a method for contrastive and self supervised representation learning. The main target task is dense prediction i.e. semantic segmentation and object detection, however the framework fits the classification task as well.\n\nWithin the general framework of DINO Caron et al. (2021)  authors state their contributions to be (i) cross-views are used for contrasting learning, (ii) the unidirectional cross attention module, and (iii) achieving SOTA.\n",
            "strength_and_weaknesses": "The first natural application is image classification, so ADCLR is benchmarked for that task. Common benchmarks in the field include (i) k-NN and (ii) top-K linear accuracy on imagenet and (iii) comparison against a strong supervised classifier on smaller datasets. The selection of methods (CNN and ViT) and datasets is comprehensive and shows  SOTA improvement.\n\nHowever, the central application is dense prediction i.e. semantic segmentation and object detection, so ADCLR is also benchmarked for those tasks.\n",
            "clarity,_quality,_novelty_and_reproducibility": "There are two typos, on section 3.2 (i) the the same objects and (ii) tiken -> token. Otherwise the paper is very well written. ",
            "summary_of_the_review": "My recommendation is to accept this submission, the paper is sound and the results do support the claims.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper11/Reviewer_kB6d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper11/Reviewer_kB6d"
        ]
    },
    {
        "id": "Xmu7HH8x2jz",
        "original": null,
        "number": 4,
        "cdate": 1666863328028,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666863328028,
        "tmdate": 1666863328028,
        "tddate": null,
        "forum": "10R_bcjFwJ",
        "replyto": "10R_bcjFwJ",
        "invitation": "ICLR.cc/2023/Conference/Paper11/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a dense contrastive representation learning framework for visual feature learning in a self-supervised manner. The proposed framework considers a local patch query strategy for local contrasting together with a global contrasting. The method achieves superior performances on multiple challenging benchmarks for dense predictions. \n\n",
            "strength_and_weaknesses": "Pros:\n+ the idea of combing local and global contrastive learning is interesting, and also important for dense prediction tasks, as local and global contexts provide different semantic information of the image. \n+ the results are strong. \n\nCons:\n- As shown in Table 7, the local patch query introduces a lot of extra memory. It seems that the method sacrifices the memory overhead to gain performance. How about a larger-batch size? Would the method still be able to train with a reasonable memory consumption?\n- The performance on ImageNet-1k seems to be minor compared to DINO and iBOT (only 0.3%). But from the comparison with state-of-the-arts on the dense prediction tasks, the gain is more clear. Could the authors explain the performance differences?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and the novelty is moderate. ",
            "summary_of_the_review": "The idea of performing local and global contrasting seems to be interesting, but on the other hand, this would also bring extra computational overhead, especially when the number of image crops is high. The authors need to further clarify their advantage on this point. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper11/Reviewer_wYqQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper11/Reviewer_wYqQ"
        ]
    }
]