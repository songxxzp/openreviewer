[
    {
        "id": "8AgYwM3yYO",
        "original": null,
        "number": 1,
        "cdate": 1666560037534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666560037534,
        "tmdate": 1666560037534,
        "tddate": null,
        "forum": "ED3WvUgu09",
        "replyto": "ED3WvUgu09",
        "invitation": "ICLR.cc/2023/Conference/Paper5407/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work enables kernel regression with the so-called neural tangent kernel to work with significantly larger datasets compared to before (order of million datapoints) by building on top of the framework of [1] for Gaussian processes. The key difference to [1] stems from the difference of computational costs to obtain the kernel matrix; while for GPs, simpler kernels such as the RBF are often used, calculating the Gram matrix of the NTK associated with a convolutional neural network marks the bottleneck, instead of the linear system that needs to be solved. The authors distribute the computation of the kernel over many devices and store it on disk. To perform the matrix-vector product needed in conjugate gradient descent, the authors use a smart way to read chunks of the kernel efficiently, avoiding thus the need to recompute the kernel at every step. The more powerful method enables experiments that were not possible before, such as large scaling law experiments, experiments on TinyImageNet and heavy data augmentation leading to SOTA kernel performance on CIFAR10. Finally, the authors also show that these kernels can reach very strong performance on smaller datasets including molecular tasks.",
            "strength_and_weaknesses": "**Strengths**:\n1. The paper is very well-written and very easy to follow. I really enjoyed reading this paper.\n2. The resulting methodology in my opinion enables a lot of interesting experiments (provided enough compute) that were simply infeasible before. The obtained results provide a better and more fair comparison between finite and infinite networks, showing for instance that when standard training components such as data augmentation are included for kernels, very competitive accuracies can be achieved with the kernel method as well. It also marks a first step towards evaluating such kernels on ImageNet, which further facilitates comparisons.\n3. The experiments regarding scaling laws are very interesting and important as they provide a more fine-grained look into performance. To my knowledge, such experiments are missing in the literature. It would have been even more interesting to compare the kernel scaling laws with the corresponding finite network.\n\n**Weaknesses**\n1. The methodology largely relies on previous work (i.e. [1]) with the only difference that the kernel is stored on disk instead of computed on the fly. While the smart reading strategy of the chunks is certainly not trivial, the technical novelty of the approach is somewhat limited.\n2. While the protein/molecule experiments are very interesting in their own right, I don\u2019t see how they fit into this paper. Most datasets are only moderately large and the involved methodology developed in this paper is thus not needed to perform accurate inference. \n\n[1] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kilian Q Weinberger, and Andrew Gordon Wilson. Exact gaussian processes on a million data points. In Advances in Neural Information Processing Systems, 2019. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and the challenges to move from the Gaussian process setting to the neural tangent kernel are clearly described. The experiments are well-motivated and very relevant to the community. What remained unclear to me is how many GPUs/CPUs, RAM etc are needed for certain types of experiments. How many GPUs does one need to train on CIFAR10 with augmentations for instance? Including these numbers would be helpful for the reader who plans to use the framework for his own experiments. \nAs mentioned above, this work is largely built on top of the work [1], which reduces the originality of the framework. The obtained extension still remains non-trivial however.\n\n[1] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kilian Q Weinberger, and Andrew Gordon Wilson. Exact gaussian processes on a million data points. In Advances in Neural Information Processing Systems, 2019. ",
            "summary_of_the_review": "In summary, I would like to see this work accepted at ICLR. The framework developed in this work enables important experiments with NTK that were infeasible before. Scaling laws, usage of data augmentation and the possibility to scale to larger datasets such as TinyImageNet facilitate comparing finite and infinite width networks, which still remains an open problem.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5407/Reviewer_7cjF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5407/Reviewer_7cjF"
        ]
    },
    {
        "id": "_mrgjG5X9Z8",
        "original": null,
        "number": 2,
        "cdate": 1666691028752,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691028752,
        "tmdate": 1666691248480,
        "tddate": null,
        "forum": "ED3WvUgu09",
        "replyto": "ED3WvUgu09",
        "invitation": "ICLR.cc/2023/Conference/Paper5407/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper empirically investigates the performance of million-scale kernel regression with infinite-width neural networks using the preconditioned conjugate gradient method from the Gaussian process literature (Wang, 2019). The method is applied to several domains including testing scaling laws on CIFAR-5m, enabling data augmentation on CIFAR-10 by a factor of 20, and sequence/molecule classification tasks using convolutional/graph convolutional architectures.",
            "strength_and_weaknesses": "## Strengths\n\n* The paper makes a nice attempt to scaling infinite-width neural-network kernels. This is an important problem faced by the area and a solution would be of interest to a wide audience. \n\n* The empirical study is comprehensive and covers several data modalities including images, sequences and graphs, all showing competitive performance against strong baselines.\n\n## Weaknesses\n\n* The results are good, but not surprising. Given the existing work of Wang et al. (2019) that already applies GPs to 1 million data points, the scaling in this work was not that far from reach. In fact, the paper is using the same preconditioning as Wang et al. (2019) and from the text I did not see any additional innovations needed to achieve the 5 million scale. Achieving 91% on CIFAR 10 with data augmentation that increases the training set by a factor of 20 does not seem surprising, either. \n\n* One big question i had in mind while reading this paper is---what is the central message? The focus in each section seems completely orthogonal to each other and the whole piece of work feels more like an experiment report. Simply showing that infinite-neural-network kernels achieves predictable good (but not state-of-the-art) results does not qualify a scientific contribution. \n\n* Some details are unclear about the 5 million experiment. What are the tolerance level needed to solve this linear system? One big concern I had about Wang et al. (2019) is that they used a very large tolerance (1 if I remember it correctly) during training and did not bother to get the GP hyperparameters right (partly the reason why the training is super fast/scalable). During test time, to ensure good performance, they had to switch to a much smaller tolerance level. Do you have to do the same? What is total training + test time when compared to a standard sparse method like Nystrom? (Nystrom-like ideas  is not totally infeasible for these kernels, see Deng, Z., Shi, J., & Zhu, J. (2022). NeuralEF: Deconstructing Kernels by Deep Neural Networks. arXiv preprint arXiv:2205.00165 \nwhere they show a nice recovery of NTK using sparse eigenfunctions. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is very clear in what approach it takes and what the result is. However, it is unclear what the central message is. The originality is limited given the prior work on scaling GPs (the authors might want to expand the discussion on distributed computation to show if there is originality there). ",
            "summary_of_the_review": "Given the lack of central message and limited amount of contribution compared to prior literature, I would recommend rejection. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5407/Reviewer_wb1p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5407/Reviewer_wb1p"
        ]
    },
    {
        "id": "cmoxR1dmAor",
        "original": null,
        "number": 3,
        "cdate": 1667304878784,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667304878784,
        "tmdate": 1667304878784,
        "tddate": null,
        "forum": "ED3WvUgu09",
        "replyto": "ED3WvUgu09",
        "invitation": "ICLR.cc/2023/Conference/Paper5407/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores further scaling up preconditioned conjugate gradient (PC-CG) based GP regression by parallelizing the computation of the Gram matrix as a one-off and storing it to disk in \"sub-blocks\" to be loaded in the RAM in a multi-threaded fashion as/when needed. \n\nThis contrasts with standard PC-CG where the Gram matrix is never computed entirely but rather partially recomputed on the fly as needed. \n\nThe standard PC-CG approach works reasonably well when the kernel is easy to compute (e.g. the square-exponential kernel), but breaks down when the kernel is more complex. \n\nThe authors demonstrate the effectiveness of the proposed approach using neural kernels, especially compared to alternatives consisting of approximating the linear system (as opposed to solving it within an error CG-style), and propose additional empirical studies such as how performance of GP regression under neural kernels scale with data sizes.\n",
            "strength_and_weaknesses": "- **Strength**: Scalability is one of the major hurdles in bridging the gap between  GP methods and deep learning. This paper's attempt to doing so is laudable, and experimental results are encouraging.\n\n- **Weaknesses**: Experimental results seem to be missing error bars/intervals, hardware specs, and running durations. Overall the contribution seems too light to be worthy of publication.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- **Clarity**: The paper reads well.\n- **Quality**: Experimental results could be strengthened by adding error bars and running durations.\n- **Novelty**: The main idea here, namely to parallelize the computation of the Gram matrix and store it to disk, isn't particularly novel.\n- **Reproducibility**: While the authors provided Python code, specifications of hardware used would have eased reproducibility.\n",
            "summary_of_the_review": "Overall the main contribution is too simple to be worthy of a paper and, in any case, empirical evaluation should be strengthened.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5407/Reviewer_c8Ru"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5407/Reviewer_c8Ru"
        ]
    },
    {
        "id": "P-vNKuwkeCa",
        "original": null,
        "number": 4,
        "cdate": 1667511806181,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667511806181,
        "tmdate": 1667511806181,
        "tddate": null,
        "forum": "ED3WvUgu09",
        "replyto": "ED3WvUgu09",
        "invitation": "ICLR.cc/2023/Conference/Paper5407/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors consider the large-scale implementation of 'neural kernels', kernel\nmethods that are derived from the architecture of a specific neural network and\nthe correspondence between randomly-initialized neural networks and gaussian\nprocesses. In contrast to existing large-scale kernel methods for use with\nkernels like the RBF kernel, the fact that these neural kernels are\ncomputationally-expensive to compute necessitates additional parallelization.\nThe authors review methods for large-scale kernel regression, implement their\nown method in the context of neural kernels, and show experiments on various\nlarge datasets (CIFAR-5m, Tiny ImageNet) where these kernels achieve\nperformance beyond past kernel methods. They also demonstrate their approach on\nvarious basic science tasks, where it outperforms existing kernel methods.\n\n",
            "strength_and_weaknesses": "## Strengths\n\n- The engineering contribution directly enables new evaluation of neural\n  kernels at novel scales, enabling their use on new datasets where they set\n  new record for performance by kernel methods.\n- The paper is well-written and does a good job of organizing various\n  approaches to large-scale kernel regression in section 1.2 and section 2. One\n  comes away with an excellent understanding of the landscape of approaches to\n  this problem, which should be useful for future work.\n- The authors do a good job of showing how the engineering advancements open\n  new opportunities for kernel methods. They demonstrate that neural kernels\n  achieve excellent performance on molecular prediction/classification tasks\n  compared to existing methods, and show that data augmentation can give good\n  improvements in performance for kernel methods, given the ability to work\n  with larger kernels.\n\n## Weaknesses\n\n- The precise methodological contributions seemed somewhat unclear to me -- is\n  the main technical contribution to identify the most relevant existing\n  large-scale kernel regression approaches for working with neural kernels (as\n  outlined in section 2), and then to develop and implement a new parallelization\n  scheme on top of these methods to facilitate computing neural kernels? The\n  latter feels like a somewhat limited (but nonetheless significant) technical\n  contribution, given that the paper's description of the parallelization scheme\n  seems to be mostly restricted to showing Figure 1.\n- It might be better to have a demonstration of the method on low-resolution\n  datasets other than CIFAR-5m, since this dataset is essentially synthetic. I\n  think the authors' experiments on Tiny ImageNet are more than appropriate in\n  this connection, I just point this out because the description of CIFAR-5m at\n  the bottom of page 5 as \"...ensuring that the additional data are sampled\n  i.i.d.\" is not correct and somewhat misleading (since the dataset is\n  generated from a trained generative model).\n- I think the section about neural scaling laws could be improved\n  by a more precise discussion of the issues -- once one is working with linear\n  models (whether or not the kernel is \"neural\"), one can make much more\n  precise statements about the asymptotic scaling of the test error as a\n  function of the number of samples given various distributional assumptions,\n  and it does not seem to be appropriate to me to treat this as a purely\n  empirical endeavor.\n  These types of questions have been studied in the literature on nonparametric\n  regression in RKHSes; under simple distributional models for the data (e.g.,\n  the target function is sufficiently smooth and the data come from a\n  distribution on a $d$-dimensional manifold), the RKHS associated to a given\n  kernel can be characterized in terms of differentiability properties of the\n  kernel (which are, in turn, inherited from the architectural choices and the\n  random initialization scheme), and the corresponding scaling behavior (i.e.,\n  the exponent) of the test error becomes a function of this RKHS, the\n  dimension $d$, and the target function's smoothness.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, with clear presentation of experimental results and\nample references to prior work. The work enables new applications of neural\nkernels at scale (Tiny ImageNet) and in various applications (the performance\ngains over traditional kernels give justification for the authors' work in\nthese contexts). \n\nIt would be preferable for reproducibility to have code released to reproduce\nexperiments, rather than just the architecture specifications used to generate\nthe kernels (appendix A).\n\n",
            "summary_of_the_review": "\nThe engineering contribution and its consequences are significant, and the\nauthors verify these well through various experiments. One hopes the code will\nbe released.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5407/Reviewer_nR21"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5407/Reviewer_nR21"
        ]
    }
]