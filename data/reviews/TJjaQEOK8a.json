[
    {
        "id": "2KhzOLUMgHH",
        "original": null,
        "number": 1,
        "cdate": 1666238575900,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666238575900,
        "tmdate": 1668141283635,
        "tddate": null,
        "forum": "TJjaQEOK8a",
        "replyto": "TJjaQEOK8a",
        "invitation": "ICLR.cc/2023/Conference/Paper25/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies curriculum learning in reinforcement learning. To achieve this, the paper formulates a teacher-student framework, where the teacher is responsible to generate the tasks for the student to learn. The key idea is two-folded. First, the student should not learn from the tasks that are outside of the difficulty scope. Second, the teacher should be rewarded for generating tasks that cause larger parameters update of the policy/value networks of the student. To implement this idea, the paper proposes REJECT, which simply rejects the tasks that are too easy or too hard based on the rollout reward, and GRAD, which makes the amount of the parameters update as one of the rewards for the teacher. Experiments show that the two strategies can improve efficiency.",
            "strength_and_weaknesses": "Strengths:\n\n1. The proposed ideas are simple and intuitive.\n2. The idea of using the amount of student model updates to guide the teacher is interesting. While there are some similar ideas in curriculum learning, it is nice to see it is effective in RL.\n\nWeaknesses:\n\n1. It is unclear to me why REJECT helps. It simply drops the tasks that are too difficult or easy. However, it still requires generating data with the student model. So it is still expensive. I am also curious whether such dropped samples are counted in the learning curve.\n2. The improvement over the baselines is not significant. It is only comparable with goal GAN.\n3. While the focus of the paper is curriculum learning, the paper should consider baselines that use intrinsic rewards in the experiment. The selected environments are mainly hard-exploration environments, so intrinsic reward methods (such as count-based intrinsic reward) could also perform well in these environments. So I would like to see a comparison.\n4. Various exploration methods (such as intrinsic rewards) are not discussed in the related work. They try to tackle similar challenges in different ways.",
            "clarity,_quality,_novelty_and_reproducibility": "Yes",
            "summary_of_the_review": "This paper presents some interesting ideas. However, I have concerns about the REJECT. Particularly, it is unclear to me whether the dropped samples are counted when drawing the learning curve. I would like to ask the authors to clarify. If not counted, the comparison could be unfair. The experiments also need improvement.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concern.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper25/Reviewer_BAdD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper25/Reviewer_BAdD"
        ]
    },
    {
        "id": "kGY_0922vVW",
        "original": null,
        "number": 2,
        "cdate": 1666561412119,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666561412119,
        "tmdate": 1666561412119,
        "tddate": null,
        "forum": "TJjaQEOK8a",
        "replyto": "TJjaQEOK8a",
        "invitation": "ICLR.cc/2023/Conference/Paper25/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study to problem of finding and using the right balance in curriculum learning applied to reinforcement learning (RL). More specifically, at each training iteration of a student network, the proposed method aims to generate an appropriate, i.e. not too easy, not too hard, task for the student to learn. The goal is to achieve faster convergence and better performance. The approach is applied on top of two existing methods and tested on two benchmarks.",
            "strength_and_weaknesses": "Strengths:\n- The paper is well written.\n\nWeaknesses:\n- Experimental results are mixed (sometimes better, sometimes worse). Thus, the benefit of the proposed method is not clearly shown.\n- The title should be clearly tied to reinforcement learning. The title makes it look like the idea is more generic, while being tested only on RL. If the authors prefer to keep the title as is, they should demonstrate applicability in other areas, e.g. classification.\n- There are many competing curriculum learning approaches for RL, see [A], missed by the authors in their discussion.\n\n[A] Soviany et al., \"Curriculum learning: A survey\", International Journal of Computer Vision, 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, but I am not confident about the novelty of the proposed method.",
            "summary_of_the_review": "Due to the mixed results and the missed related works, I am not confident about accepting the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper25/Reviewer_9WXc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper25/Reviewer_9WXc"
        ]
    },
    {
        "id": "85jh59AKMB",
        "original": null,
        "number": 3,
        "cdate": 1666636161092,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636161092,
        "tmdate": 1667456117433,
        "tddate": null,
        "forum": "TJjaQEOK8a",
        "replyto": "TJjaQEOK8a",
        "invitation": "ICLR.cc/2023/Conference/Paper25/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to improve a known application of curriculum generation with two new techniques. One is to select tasks according to the difficulties to the student. The other one is to reward the teacher according to the student\u2019s gradient norm such that the teacher can generate tasks which enhance the learner more. The authors add these two techniques to existing algorithms, PAIRED and Goal GAN and experimentally test the performances with these two techniques.\n",
            "strength_and_weaknesses": "The paper develops an analytical objective for task generation in Equation (1)-(5), clearly defining the problem. Also, the two techniques are well described, and the proposed algorithm is easy to follow and easy to apply. The experiments are well designed, including comparisons with two representative baselines and analysis of generated tasks.\n\nHowever, there are still some improvements that could be made to the paper:\n\n(1) This paper does not bring in enough novelty. The problem is controlling the difficulty of tasks in the zone of proximal development is already considered in the two baselines. There is no new approach to control the difficulty proposed. The first proposed technique, REJECT, is based on the developed methodologies of the baselines and is already developed in the algorithm POET, proposed by Wang et al. (2019). POET already proposes to reject or delete new environments that are too hard or too easy for the agent.\n\n(2) The motivation of the approach needs more clarification. Neither of the two proposed techniques is proven to maximize the derived objective. Meanwhile, the paper is inspired by generating tasks in the zone of proximal development, which is not too hard nor too easy for the student. However, maximizing the student's gradient norm is unrelated to this main idea of generating ZPD tasks. \n\n(3) More description of the setting is needed. It is unclear whether the paper focuses on a generalization objective to maximize expected returns among a given environment distribution or what.\n\n(4) Modelling the probability in the objective in section 3.3 on page 5 is unclear and technically incorrect. The paper claims that the first technique, REJECT, models the probability $p(r_{curr}|c_{curr},\\theta_{curr})$ but how? Furthermore, this probability is decided by the dynamics of the MDP and cannot be influenced by the teacher. This modelling claim is technically incorrect.\n\n(5) The test on the gradient maximization idea is limited. Some ablation studies to test the gradient maximization idea alone would be interesting, for example, whether gradient maximization is better than regret maximization proposed by PAIRED and difficulty control by Goal GAN.\n\n(6) Some descriptions of experiments are missing. For example, how to set the gradient norm and returns scale when applying the gradient maximization technique? what contexts are all algorithms tested on for tasks (a)-(g) in Figure 2 on page 7? How the rejection rate is computed in Figure 5 on page 9? \n\n(7) I have a few questions about the experimental results:\n\n(a) We see improvements in PAIRED-based algorithms. But PAIRED generates tasks which maximize the current regret. So why are there so many tasks given by the PAIRED rejected? Does PAIRED generate many negative regret tasks?\n\n(b) It is shown in Table 1 on page 9 that PAIRED+REJECT+GRAD gives hard tasks from the beginning. Does it break the increasing difficulty set by the curriculum learning? Is it good?\n\n(c) REJECT does not improve Goal GAN much. Can we say PAIRED is not controlling the difficulty of tasks well as claimed, but Goal GAN already fulfills the job?\n\n(d) Why do PAIRED-based algorithms seem to have a larger variance in the acceptance rate?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper lacks enough novelty and needs more clarification on the motivation. Meanwhile, more clarifications on the setting, modelling and experiments are needed.",
            "summary_of_the_review": "This paper is suggested to be rejected because (1) the algorithm does not propose new measurements on if a task is in the zone of proximal development and does not develop novel techniques to solve the problem, and (2) the paper needs more clarification.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper25/Reviewer_3apF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper25/Reviewer_3apF"
        ]
    },
    {
        "id": "aeu_M-Ibm1",
        "original": null,
        "number": 4,
        "cdate": 1666681624602,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681624602,
        "tmdate": 1670505914993,
        "tddate": null,
        "forum": "TJjaQEOK8a",
        "replyto": "TJjaQEOK8a",
        "invitation": "ICLR.cc/2023/Conference/Paper25/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes ZONE, a computational framework of curriculum task generation for reinforcement learning based on a concept from developmental psychology, i.e., zone of proximal development (ZPD). The target of the framework is to measure and schedule difficulty and progression of a student model. The approach is composed of two techniques, REJECT and GRAD. REJECT is to replace too easy or too hard tasks generated by teacher with proper tasks. GRAD is to maximize the norm of student model\u2019s gradient, which reflects student progression. To evaluate the effectiveness, the authors conduct experiments on two baselines and their corresponding settings.",
            "strength_and_weaknesses": "Strengths:\n1.\tThe paper is well written and easy to understand.\n2.\tThe motivation of the method is intuitive and backed by psychology.\nWeaknesses:\n1.\tThe transition from psychological concepts to computational metrics seems not that natural, which causes some problems. For instance, the difficulty needs manual design based on different RL models, i.e., a fixed range for Goal GAN and a changeable range for PAIRED. Besides, the GRAD technique cannot be applied when the teacher is not trained with rewards.\n2.\tThe number of baselines is a bit small, which degrades its universality and generality.\n3.\tThe analysis of the experiment is not convincing. Figure 5a shows that ZONE generates more complex tasks, instead of better tasks matching the learning situation of a student. The more obstacle-filled or hallways-filled tasks are not necessarily better. Figure 5c shows that the gradient norm of student model without GRAD is larger than that with GRAD in the later stage of training, which seems weird and strange.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is good in clarity and novelty, and seems easy to reproducibility. However, the experiment part is not sufficient and there is still room for improvement in experimental analysis. ",
            "summary_of_the_review": "To summarize, the paper is good except for the experiments part. The motivation is interesting, based on developmental psychology. The method is well described and easy to understand. However, the experimental results are not so ideal and the analysis is not so convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper25/Reviewer_Go3r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper25/Reviewer_Go3r"
        ]
    }
]