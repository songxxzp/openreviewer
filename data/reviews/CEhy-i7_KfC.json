[
    {
        "id": "8TKM0fFDJb9",
        "original": null,
        "number": 1,
        "cdate": 1666546855438,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666546855438,
        "tmdate": 1666546855438,
        "tddate": null,
        "forum": "CEhy-i7_KfC",
        "replyto": "CEhy-i7_KfC",
        "invitation": "ICLR.cc/2023/Conference/Paper3071/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies if Vision Transformers (ViTs) could outperform CNNs in vision-based RL tasks. The authors compared existing self-supervised strategies and proposed a new self-supervised approach designed for the sequential observations in RL. The newly proposed method combines previous ideas from VICReg (Bardes et al, 2021) and Shuffle-and-Learn (Misra et al, 2016). The authors show that the new approach TOV-VICReg outperforms other SOTA self-supervised learning methods in the RL setting. However, from the authors experiments, CNNs still outperform ViTs trained with TOV-VICReg in eight out of the ten tested Atari games. ",
            "strength_and_weaknesses": "### Strength\n\n1. The proposed self-supervised learning method is well suited for RL tasks and experiment results show that it improves performance of self-supervised learning methods that does not capture the temporal relationship.\n\n### Weaknesses\n\n1. This paper lacks novelty in terms of the proposed self-supervised learning method. Also, the presented results are mostly well-known in the community.   \n2. Temporal order verification is one type of self-supervision loss that aims to capture temporal dependency. This paper does not justify the choice of Shuffle-and-Learn against other methods [1, 2, 3]. I\u2019d recommend the authors to add a discussion on relevant self-supervised learning literature for temporal relationship learning and provide a justification on the choice of Shuffle-and-Learn.   \n3. The number of steps used for temporal order verification loss is an important hyperparameter that would affect the quality of the learned representations. This paper does not provide experimental results showing its impact. I\u2019d encourage the authors to add an experiment varying the number of steps.\n\n### References\n\n[1] Lee, Hsin-Ying, et al. \"Unsupervised representation learning by sorting sequences.\"\u00a0*Proceedings of the IEEE international conference on computer vision*. 2017.\n\n[2] Xu, Dejing, et al. \"Self-supervised spatiotemporal learning via video clip order prediction.\"\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2019.\n\n[3] Yao, Yuan, et al. \"Video playback rate perception for self-supervised spatio-temporal representation learning.\"\u00a0*Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "### Quality\n\nThis paper should be improved in terms of clairty and experiments before being accepted into ICLR. \n\n### Novelty:\n\nThe paper presents an incremental contribution where ViTs are trained by combining the temporal order verification loss (Misra et al, 2016) and the VICReg loss (Bardes et al, 2021). Both the methods and the results do not present significantly contributions.\n\n### Reproducibility\n\nThe authors provided their code in the supplemental material. \n\n### Clarity\n\n1. Figure 4 and 5 are confusing becuase of the different scales of the color map. \n2. The desription of the baseline methods is not clear. For the SGI ResNet Large, was it pretrained with self-supervision loss? If so, did the pretraining follow the original SGI paper? \n\nMinor Comments:\n\n1. The DQN algorithm details on page 3 can be presented in an appendix and formatted with Latex algorithm packages. \n2. VICReg is a published ICLR 2022 paper. Please update the reference accordingly. \n3. For audience unfamiliar with VICReg, it would be helpful to explain how expanders work.",
            "summary_of_the_review": "I\u2019m inclined to reject this paper mainly because: 1) the experiments do not justify the motivation of this work, i.e., use large models to solve complex RL tasks where smaller models struggle, as stated in the conclusion of this paper. Clearly, the chosen environments are not complex enough to demonstrate the potential benefits of pretrained ViTs. 2), the proposed method lack novelty and the results do not bring significant new insights to the community, 3) the paper does properly compare other self-supervised learning methods that aim to capture the temporal relations  4) the paper presentation needs further polishing.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3071/Reviewer_X82N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3071/Reviewer_X82N"
        ]
    },
    {
        "id": "6hgzpDMvtr",
        "original": null,
        "number": 2,
        "cdate": 1667062197301,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667062197301,
        "tmdate": 1667062197301,
        "tddate": null,
        "forum": "CEhy-i7_KfC",
        "replyto": "CEhy-i7_KfC",
        "invitation": "ICLR.cc/2023/Conference/Paper3071/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "*I am a vision person and an emergency reviewer.*\n\nThis paper proposes to combine VICReg and a temporal verification loss for video representation learning, and inspect its impact on the data-efficiency of down-streaming reinforcement learning tasks. It is shown that the proposed method out-performs VICReg, MOCO and DINO while still under-performing (randomly initialized?) CNNs. Further analyses show that the proposed variant of VICReg shows better properties in terms of collapse and feature distribution.",
            "strength_and_weaknesses": "Strengths:\n+ The proposed training framework that combines VICReg and temporal verification is new to my knowledge.\n+ Experimental results show that the variant out-performs alternative representation learning methods in the inspected RL setting.\n+ A set of analysis shows that the method shows better statistical properties w.r.t. collapse and feature similarity.\n\nWeaknesses:\n- Most of the experimental results seem not related to reinforcement learning and the only RL evaluation reports inferior performance than (randomly initialized?) CNNs. I recommend the authors to consider generic video classification evaluation protocols in [A], which may be a more promising thing to pursue.\n- The language is fine but the organization is pool IMHO. Too much space is spent on describing standard things like MDP, DQN or VICReg.\n- Generally I am not convinced by the motivation. I think transformers are widely considered less data-efficient and I don't understand why authors expect it to behave the opposite way in reinforcement learning.\n- Typos:\ncomponents that allows us -> allow\nthe increasing interest -> and the increasing interest\nViT presents weaker image-specific inductive biases which allow the CNNs for much sample-efficient learning; allows CNNs?? what does this mean\n\n[A] VideoMoCo: Contrastive video representation learning with temporally adversarial examples, CVPR 2021",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: I am not very sure about the RL protocol. Is the representations trained in an offline setting that pre-stores frames?\nQuality: not good enough for both technical depth or presentation quality.\nNovelty: The training framework is somewhat new.\nReproducibility: Codes are provided.",
            "summary_of_the_review": "This is an empirical study yet I am not convinced by the motivation or the significance of empirical results.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3071/Reviewer_Wxq5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3071/Reviewer_Wxq5"
        ]
    },
    {
        "id": "p2TVmXbGBNQ",
        "original": null,
        "number": 3,
        "cdate": 1667102954550,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667102954550,
        "tmdate": 1667102954550,
        "tddate": null,
        "forum": "CEhy-i7_KfC",
        "replyto": "CEhy-i7_KfC",
        "invitation": "ICLR.cc/2023/Conference/Paper3071/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to introduce visual transformers to extract feature representations for reinforcement learning. It proposed a method, which is a combination of VICReg and temporal ordering prediction, to pretrain the ViT network. Experiments are done on Atari 100K using the visual transformer pretrained by the proposed method. ",
            "strength_and_weaknesses": "** Strength: **\n1. It is reasonable to combine the image only pretraining method (specifically, VICReg) with temporal ordering prediction as the overall objectives covers both image and temporal information. \n2. The paper analyzes the property of representation from a few different aspects in the experiments. \n\n** Weaknesses: **\n1. It is unclear how the pretraining is specific to deep reinforcement learning. It looks like a generic video-based pretraining method, and its connection with reinforcement learning seems a bit loose. \n2. The novelty of the method is limited as combining two established self-supervised learning methods in the same training objective is not something difficult to come up with. \n3. Q: it seems Atari 100K has 26 games (please correct me if I am wrong), why only 10 games are used?\n4. The experimental results are not strong. Especially, using visual transformer does not seem to be that competitive to using CNN, as the paper claimed. \n5. The clarity of the paper (especially in the experiment section) can be improved. ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and writing quality of the paper can be improved. One example, the mode performance analysis in Sec 6.1 is unclear. It is not self-contained enough for one to understand the metric. And it is confusing if the results focus only on \u201csampling efficiency\u201d or actually looks at the model\u2019s game performance. \n\nThe papers have a few broken links. \n\nWhile it could claim the method is novel for RL, but the specific connection between the proposed method and RL is not clear to me. And a generic method to pretrain a feature extractor on video. Such a combination of image-only objectives and temporal ordering prediction is marginally novel. \n\nThe amount of details in the paper is sufficient for one to understand the main idea but may not be sufficient for one to reproduce the results fully. ",
            "summary_of_the_review": "The paper has a good motivation of using a well-pretrained visual transformer in deep RL. However, the method design seems to be generic and of marginal novelty. The experimental results are a bit confusing and do not seem to be staring enough to address the paper\u2019s motivation. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3071/Reviewer_Wuo3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3071/Reviewer_Wuo3"
        ]
    },
    {
        "id": "ULVxcWrhNb",
        "original": null,
        "number": 4,
        "cdate": 1667197589295,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667197589295,
        "tmdate": 1667197589295,
        "tddate": null,
        "forum": "CEhy-i7_KfC",
        "replyto": "CEhy-i7_KfC",
        "invitation": "ICLR.cc/2023/Conference/Paper3071/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper mainly targets usage of ViTs in Reinforcement Learning. The authors conduct several experiments with different training strategies for ViTs, showing the effectiveness of self-supervised learning for ViTs in RL. Besides, the authors propose a new self-supervised objective function based on VICReg, which involves the temporal order prediction for learning better temporal relations.",
            "strength_and_weaknesses": "Strength:\n1. The authors present interesting and trials with ViTs for RL.\n\nWeaknesses:\n1. The novelty is limited. The proposed method is made up of several previous methods.\n2. Even though the proposed TOV-VICReg is more effective than the other self-supervised methods, it is still only comparable to the CNN based models with no significant advantage. In other words, the ViT based models remain impractical for RL.\n3. It would be better if the authors can consider reconstruction based self-supervision like MAE and SimMIM in their experiments, which adopts a quite different objective compared with DINO and MoCo.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is generally clear. The novelty is limited due to no originated method is proposed in this paper. The TOV-VICReg is a combination of two existing methods. The proposed method should be easy to reproduce based on the authors' code.",
            "summary_of_the_review": "The authors present good technical analysis of using ViTs for RL problems. The novelty and significance are limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3071/Reviewer_9PJR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3071/Reviewer_9PJR"
        ]
    },
    {
        "id": "JiL4Dwri0y",
        "original": null,
        "number": 5,
        "cdate": 1667290663501,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667290663501,
        "tmdate": 1667290663501,
        "tddate": null,
        "forum": "CEhy-i7_KfC",
        "replyto": "CEhy-i7_KfC",
        "invitation": "ICLR.cc/2023/Conference/Paper3071/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the effect of self-supervised pre-training for Vision Transformer based RL agents. It shows the effect of temporal order verification and VICReg on 10 Atari games. The proposed methods improves final return on Atari games.",
            "strength_and_weaknesses": "[Strengths]\n- The paper is easy to follow.\n- The proposed method makes intuitive sense to me.\n\n[Weaknesses]\n- Most importantly, the main contribution of the submission is not very clear to me.\n- Also, the technical novelty and empirical novelty is quite limited. TOV and VICReg are existing self-supervised learning methods. Also, it is well-known that the use of pre-training is helpful for vision-based reinforcement learning.\n- The analysis in Section 7 (representation collapse, dimension collapse, etc) are very generic and do not provide much insight on using self-supervised learning for reinforcement learning.\n- It seems that the CNN result is without self-supervised training and ViT result is with self-supervised training (although it is unclear to me). Can authors add more on this?\n- I don't see the learning curve of return vs environment steps. This is pretty important for demonstrating sample efficiency in RL.",
            "clarity,_quality,_novelty_and_reproducibility": "- The core contribution is not very clear to me.\n- Regarding the use of self-supervised learning for RL, \"CURL: Contrastive Unsupervised Representations for Reinforcement Learning\" is one of the work that popularized this. The authors should cite and compare against this work.\n- In light of the existing work, more work should be done to advance the field. Just trying self-supervised learning on reinforcement learning is not sufficient for publication at ICLR.\n- The contribution may be the use of self-supervised learning on ViT based image RL. The proposed method does not contain modifications for ViTs: TOV and VICReg are very generic methods, and they can be applied to any deep nets that take images (regardless of CNN vs ViT, RL vs supervised learning problems). Can authors provide why the proposed method is significantly novel or overcomes challenges in using self-supervised learning for RL?\n- Also, could authors provide the results of applying the proposed methods on CNNs? If the CNN performance can't be improved, why so?",
            "summary_of_the_review": "The submission is limited in terms of both theoretical novelty and empirical novelty. Especially, the empirical validity is pretty limited due to small set of experiments, missing experiment details, etc. More work should be done to warrant the publication of this work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3071/Reviewer_rKFV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3071/Reviewer_rKFV"
        ]
    }
]