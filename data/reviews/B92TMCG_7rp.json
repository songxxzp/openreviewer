[
    {
        "id": "ZVKRtykqKk_",
        "original": null,
        "number": 1,
        "cdate": 1666640528137,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640528137,
        "tmdate": 1666640528137,
        "tddate": null,
        "forum": "B92TMCG_7rp",
        "replyto": "B92TMCG_7rp",
        "invitation": "ICLR.cc/2023/Conference/Paper1626/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes incorporating model-specific prior knowledge into optimizers by modifying the gradients according to a set of model-specific hyper-parameters. The proposed methods are employed on VGG architectures using SGD and AdamW optimizers for image classification and segmentation datasets.",
            "strength_and_weaknesses": "The paper introduces a new approach to reparameterize gradients and parameters according to the prior knowledge \u201cadding up the inputs and outputs of several branches weighted by diverse scales\u201d. \n\nAlthough this proposal is interesting, it is more like a general idea instead of a particular method. Therefore, the main weakness of the paper is the limitation of generalization of the proposal to different models with different architectures, optimizers and tasks. The proposed method should be also compared with other other reparameterization and optimizer design methods as well.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The definition of the prior knowledge is unclear since it has vague arguments. As the authors mention, the proposed approach requires employment of the prior knowledge via modifying the gradients according to a set of model-specific hyper-parameters. Therefore, it is not clear how to apply the proposed method to other architectures, models, optimizers and tasks. \n\nFor instance, I am not sure how to apply the proposed prior knowledge template to WRNs, U-Nets, MobileNets, Transformers, LSTMs/RNNs, GANs etc. for image synthesis/super-resolution, pose estimation, NLP and speech processing/recognition tasks in addition to image classification/detection/segmentation. \n\nMoreover, the proposed approach should be explored together with other optimization methods, like vanilla Adam, RMSProp, etc. and manifold valued optimizers such as Riemannian SGD and Adam, since these optimizers also re-parameterize gradients during optimization.\n\nTheoretical analyses should be further improved. For instance, the proposition in the appendix only tells that there exists a transformation which can provide similar outputs for scaled kernels. However, it does not tell how estimate the optimal transformation. Indeed, a trivial solution would be a transformation rescaled by inverse scaling parameters. However, this solution does not exhibit an improvement.",
            "summary_of_the_review": "The paper introduces a new approach to reparameterize gradients and parameters according to the prior knowledge \u201cadding up the inputs and outputs of several branches weighted by diverse scales\u201d.\n\nHowever, there are various major and minor issues with the paper. Therefore, the paper is not ready for publication without fixing these issues.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1626/Reviewer_96Hr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1626/Reviewer_96Hr"
        ]
    },
    {
        "id": "CLlJrC1lphV",
        "original": null,
        "number": 2,
        "cdate": 1666689503645,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689503645,
        "tmdate": 1666689503645,
        "tddate": null,
        "forum": "B92TMCG_7rp",
        "replyto": "B92TMCG_7rp",
        "invitation": "ICLR.cc/2023/Conference/Paper1626/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper describes a way to impose model-specific prior knowledge into optimizers for training neural networks. Given CSLA structure inspired by the prior, authors seek for parameters of Grad Mult tensor using Hyper-Search. The Grad Mult tensor changes back-propagation gradients directly. It allows us to use the same model architecture for training and testing.",
            "strength_and_weaknesses": "Strength:\n\nThe approach of this paper is quite novel. Authors pointed out that existing works mainly focus on structural re-parameterization (SR). Compared to SR, the clear advantage of the proposed RepOptimizer is that the model architecture stays the same during train and test. It makes training faster and lighter without decreasing accuracy (Table 2 and Table 3). The efficacy of Hyper-Search is validated in Table 4 and Table 5. It is impressive to me that the found constants from different search datasets using Hyper-Search ensures similar accuracies on target datasets. The paper showcased two priors, adding up branches (RepOpt-VGG) and locality (RepOpt-MLP), which can be handled by the proposed algorithm on different tasks such as image classification, detection, and segmentation (Table 7). In addition, the parameter distribution in Figure 3 shows that the RepOptimizer is robust to parameter quantization error. Overall, the paper is well-written and presents valuable insights on optimizers with supporting experiments.\n\nWeakness:\n\nAs authors mentioned in the conclusion section, the proposed method relies on the linearity of operations. Hence, the prior should be able to be represented using linear operations in CSLA structure. It might limit the applicable domains of this approach. It would be great if authors show that the non-linear operations can be approximated using RepOptimizer without sacrificing accuracy too much. The Hyper-Search requires another training on an additional dataset. It might not be concretely fair to make comparisons to other approaches that use a single target dataset. I am also curious if the proposed method is scalable to large-scale distributed training scenarios.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity/quality:\nThe paper is well-written.\n\nNovelty:\nsee above\n\nReproducibility:\nAuthors presented details of experiments and mentioned that they plan to release code. Therefore, it seems that reproducing results would not be difficult.",
            "summary_of_the_review": "See above. Overall the paper is novel, well-written and claims supported.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1626/Reviewer_yhDx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1626/Reviewer_yhDx"
        ]
    },
    {
        "id": "98DppeCWQYO",
        "original": null,
        "number": 3,
        "cdate": 1667064014762,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667064014762,
        "tmdate": 1667064014762,
        "tddate": null,
        "forum": "B92TMCG_7rp",
        "replyto": "B92TMCG_7rp",
        "invitation": "ICLR.cc/2023/Conference/Paper1626/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method they call Gradient Reparamerization which allows them to efficiently add priors to the model without additional training structures other than adding a Gradient Multiplier generated based on tuned hyperparameters. They are able to train models that perform on par or better than recent models with a network with a simple structure, and high training and inference efficiency. They achieve comparable results to RepVGG with significantly faster and more memory efficient training with the same inference model. They demonstrate that their models are naturally much easier to quantize compared to structurally re-parameterized models. They derive their method by proposing that based on prior work performance improvements can be made with parallel convolutions multiplied by a single channelwise scaler which are then summed. They derive that the training dynamics are exactly equivalent to by channelwise rescaling the gradients for a single convolution and propose Gradient Reparamerization which involves this rescaling as well as a scale-aware initialization. This method does require hyperparameter tuning of the multipliers, but they find that they are dataset agnostic even to different sized inputs and need only be specialized for each model architecture. They tune the hyperparameter setting the channelwise scalers to be trainable instead of fixed and optimizing it along with the weight parameters. They demonstrate that they can achieve good results on Imagenet by first hyperparameter tuning on cifar10 which theoretically should only require approximately 0.18% of training on full Imagenet.",
            "strength_and_weaknesses": "Strength:\n- Proposes a novel method for gradient reparameterization which allows incorporation of certain model priors equivalent to changes in the training architecture more efficiently.\n- Achieves strongs results on improving training speed while achieving results comparable to prior work while also being easier to quantize.\n- Demonstrated that the hyperparameters seem to be dataset agnostic even with different input sizes.\n\n\nWeakness:\n- The method requires relatively expensive hyperparameter tuning on a per-model basis.\n- Does not conduct significant analysis on the actual values of these important hyperparameters.\n\n\nComments:\nThe paper may benefit from more runtime analysis of the hyperparameter search method in terms of real time measurements as the major empirical improvement of this method is on training speed.\n\nThe paper may benefit from stronger analysis of the search process for hyperparameters. While the reinitialization is justified with the connection to NAS and it could be understandable how it alters the training dynamics, it would benefit from further analysis. Is my understanding correct that you conduct the hyper-parameter search similar to the first order approximation of DARTS no the second order search method? It would be interesting to how the model performs if you reinitialize the network weight, but still allow those parameters to be tuned. Mainly it would greatly improve the understanding of this hyperparameter which is integral to the method if you analyzed how the values and distribution of those hyperparameters changed during the search process as well as how they differ for different layers at different depths and channel sizes.\n",
            "clarity,_quality,_novelty_and_reproducibility": "They intend to make the code available. The results are novel and the paper is clear and understandable.",
            "summary_of_the_review": "Overall this paper makes a strong contribution. Their novel method to shift architecture priors into the optimizer opens up a new direction to consider for researchers. They demonstrate strong empirical results in improving training efficiency. The main downside is that to achieve best results, their method requires a hyperparameter tuning step and the work would benefit significantly from more analysis of this area. The presentation is clear and they intend to make the code publicly available.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1626/Reviewer_FagT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1626/Reviewer_FagT"
        ]
    },
    {
        "id": "NdanWa7To-w",
        "original": null,
        "number": 4,
        "cdate": 1667199452415,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667199452415,
        "tmdate": 1667199452415,
        "tddate": null,
        "forum": "B92TMCG_7rp",
        "replyto": "B92TMCG_7rp",
        "invitation": "ICLR.cc/2023/Conference/Paper1626/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to incorporate model-specific prior knowledge into optimizers by modifying the gradients according to a set of model-specific hyper-parameters. They name the method as Gradient Re-parameterization, and the optimizers are named RepOptimizers. They show that a VGG-style plain network can be trained with the proposed optimizer to perform on par with or better than the recent well-designed models. \n",
            "strength_and_weaknesses": "Strength\n- the idea of integrating prior knowledge of architectural design into the design of optimizer is interesting and worth exploring.\n- the authors provide a concrete example of implementing the idea with a plain CNN network, specifically demonstrating the effectiveness of RepOptimizers by showing RepOpt-VGG closely matches the accuracy of RepVGG. \n\nWeakness\n- the model prior is not well defined.\n- the advantage of the proposed method in comparison with traditional architecture design is not quite clear.\n- It is not clear whether all architecture design can be transferred to equivalent optimization design. The authors only provide examples such as converting residual connections, but it is not clear for other architectures whether the RepOptimizers can perform the same. If not, it makes the application of the proposed approach quite limited.\n- The hyper-parameters searched on CIFAR-100 are transferable to ImageNet may not be enough to support the claim that \u201cthe RepOptimizers may be model-specific but dataset-agnostic\u201d.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well structured but a bit difficult to read due to either long paragraphs or too much detail. It is not clear whether the approach can be easily reproduced given the complexity of the approach and hyper-search. The originality of the work is good.",
            "summary_of_the_review": "The authors explore the space of optimizer design that is equivalent to architecture prior. This paper performs initial exploration and demonstration replacing the residual connections, which could inspire future works in this direction.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1626/Reviewer_Hu4S"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1626/Reviewer_Hu4S"
        ]
    }
]