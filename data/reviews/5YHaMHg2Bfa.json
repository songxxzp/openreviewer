[
    {
        "id": "8S9LF0n1W_",
        "original": null,
        "number": 1,
        "cdate": 1665765319071,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665765319071,
        "tmdate": 1665814779330,
        "tddate": null,
        "forum": "5YHaMHg2Bfa",
        "replyto": "5YHaMHg2Bfa",
        "invitation": "ICLR.cc/2023/Conference/Paper6217/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes the convergence of stochastic gradient descent (SGD). It also concerns the study of the minimum amount of randomness that needs to be added to non-stochastic gradient descent (GD) so that it can converge.\n\nFirst, it defines the *accuracy discrepancy* as the cumulative difference (measured with the relative entropy)  between the accuracy of the model in the batches seen until iteration $j$ and the accuracy of the model at iteration $j$ of a certain epoch. \n\nThen, it shows that if the average of accuracy discrepancies over the last $t$ epochs is over a certain threshold, then SGD converges to 100\\% accuracy with high probability (w.h.p.) within $\\mathcal{O}(1)$ epochs. This result is architecture-independent, in contrast with the current literature.\n\n* An extra contribution of this result is its analysis, since it is uncommon in this area. It relies on Kolmogorov complexity, which is a measure of compression. The idea is to consider the randomness used to select the permutation of the batches in SGD. Pure randomness cannot be compressed, thus giving a lower bound on the Kolmogorov complexity. After observing the model at a certain iteration and the data, such randomness can be compressed and the paper shows an upper bound that depends on the accuracy discrepancy. Combining those they show that w.h.p. if at epoch $t$ the algorithm did not terminate then the accuracy discrepancy is smaller than a certain threshold. Hence, if the accuracy discrepancy is larger than such a threshold, it must terminate. \n\nFinally, they argue that GD does not converge if the amount of randomness included can be represented with $o(n)$ bits and the amount of epochs is $\\text{poly}(d,n)$, where $d$ is the parameters' dimension and $n$ is the number of data samples. That is, if the randomness is sublinear with $n$, there is a dataset for which the amount of epochs needed for convergence is superpolynomial (e.g. exponential).",
            "strength_and_weaknesses": "**Strengths**\n\n* The results in this paper are architecture-independent. \\\nThis is good since the current understanding of the global convergence of SGD is for specific models that usually require over-parameterization. The only requirements for this theory are that the loss is differentiable, $L$-smooth, and that the step size is smaller than $1/L$, all of which are reasonable.\n\n* The result of the necessity of superlinear randomness with respect to the number of samples for the convergence of GD is insightful. It opens a bridge between why SGD generalizes and GD does not in certain situations and opens the door to the design of the randomness in other versions of GD where the randomization is included differently such as stochastic gradient Langevin dynamics, or even the design of new GD-based stochastic algorithms. \\\nI believe that it could be beneficial for the article to stress this fact since the importance of this result seems overshadowed by Section 3.\n\n**Weaknesses**\n\n* The main statement of the convergence of SGD says that if the average accuracy discrepancies until epoch $t$ is larger than a certain threshold, then the algorithm converges within $\\mathcal{O}(1)$ epochs. \\\nThis statement is interesting on its own in the sense that it gives a sufficient metric to determine the convergence of SGD. That is, if one can see when the average accuracy discrepancy will exceed the threshold, then one knows when SGD will converge. \\\nHowever, it still does not give any convergence result for SGD, since it is still needed to run the algorithm to be able to calculate the accuracy discrepancies. That is, to determine the quantity that tells if SGD converges, one needs to run SGD until convergence.\n  \n  * The paper may be improved with an acknowledgement of this fact and placing the contribution in a different fashion: that is, as a finding of a sufficient condition for SGD convergence. \\\n  Otherwise, and more strongly, the paper would benefit from an analysis of the accuracy discrepancy and when it exceeds said threshold. Though I understand this may be the work of a future follow-up project. \n\n* The paper is hard to read and sometimes leaves some useful information unwritten.\n\n  * In the related work section it mentions the work from *Zhang et al. (2022)* requires *certain assumptions*. It would be helpful to state such conditions. Similarly, when discussing *Jin et al. (2022)* it is said that the randomness required for the perturbations on this work is beyond the considered threshold. This claim is not recovered in Section 4, so we are to believe such a statement.\n\n  * Theorem 2.3 deals with the conditional Kolmogorov complexity $K(x|y)$ before it is introduced int eh following paragraphs. It would be useful to describe this property of random strings right after the general properties of the Kolmogorov complexity.\n\n  * How does one go from the first to the second line of equations in the proof of Lemma 3.1?\n\n  * Also in the proof of Lema 3.1, the papepr goes back and forth with the notation $B_{i,j}$ knowing $X_{i,j}$ and $B_{i,j-1}$ knowing $X_{i,j-1}$. Although equivalent, it would aid readability to maintain one indexing. The same happens in the definition of the variables of interest in the beginning of Section 3.\n\n  * In proof of the Corollary 3.3, what is the exact value of $\\beta(n,b)$? Also, what is $\\gamma$? I could not follow the proof without that knowledge.\n\n  * In the proof of Lemma 4.2, how does one go from the first to the second line of equations? I understand that Pinsker's inequality is employed. My question regards the $\\Theta(n/b)$ term, based on the equation it seems it says that $|I_g| = \\epsilon \\Theta(n/b)$. If so, how do we know that? \n\n* The KL divergence (or relative entropy) $D_{\\text{KL}}(p \\lVert q)$ extension is unjustified. The extremes with respect to $p$ are set to zero when there are real finite limiting points, and the extremes with respect to $q$ are set to some finite value when they diverge.\n\n  * A suggestion is to use the standard agreement that $0 \\log 0 = 0$ by continuity and let $D_{\\text{KL}}(1 \\lVert q) = \\log(1/q)$ and $D_{\\text{KL}}(0 \\lVert q) = \\log(1/(1-q))$, and $D_{\\text{KL}}(p \\lVert 0) \\to \\infty$ and $D_{\\text{KL}}(p \\lVert 1) \\to \\infty$. This should not be a problem for your theory since the relative entropy is only employed in the accuracy disagreement and we have that if $\\lambda_{i,j}' \\neq 0,1$ then $\\varphi_{i,j} \\neq 0,1$ \n\n* In the Appendix B, it would be helpful if some of the figures had the y-axis adjusted to have a better visual comparison between them. Without a closer look, in Figure 2 for batch size 200 and 10 hidden units, one may get the opposite idea.",
            "clarity,_quality,_novelty_and_reproducibility": "* **Clarity**: As mentioned in the weaknesses above, the paper is not very clear and is hard to read. \\\nHowever, there is time for a careful revision of the manuscript to improve these issues.\n\n* **Quality**: The quality of the paper is good. The ideas are interesting and well-executed.\n\n* **Novelty**: As far as I know, employing Kolmogorov complexity (or similar compression arguments) to show the convergence of SGD or other algorithms is novel. This could spark new analyses of different algorithms.\n\n* **Reproducibility**: \\\n*Theory*: I reproduced all the proofs except for the questions that I placed the authors in the weaknesses. I believe that after they answer them and clarify those in the manuscript every theoretical aspect should be reproducible. \\\nSome further notes in this regard are:\n  * In the paragraph *Bounding $t$*: is not the probability $1 - 1/n^t$. Of course, this also implies a probability of at least $1 - 1/n^2$, but I believe that that is what one obtains after applying Theorem 2.3.\n  * I could not find an open-access resource for the Kolmogorov complexity property $K(x|z) + \\mathcal{O}(1) \\leq K(x,y|z) + \\mathcal{O}(1)$ for any three strings $x,y,z$. Therefore, I trusted this inequality as an axiom to reproduce their results. Could the authors link to an open-access source for this result?\n\n  *Experiments*: There is no code to reproduce the experiments. They are easy to replicate due to the small size of the datasets and the simplicity of the experiment. Nonetheless, it would be helpful to include this resource.",
            "summary_of_the_review": "This studies and gives insights in two important phenomena in the convergence of gradient-based algorithms:\n\n* If finds a sufficient condition for the convergence of SGD. As long as the accuracy discrepancy (introduced in this paper) is larger than a certain threshold the algorithm terminates within $\\mathcal{O}(1)$ epochs. Hence, understanding when such a quantity crosses the threshold is enough to understand when SGD will converge. This can serve to derive new theory to characterize the convergence of SGD.\n* It shows that a necessary condition to ensure GD convergence is to have some randomness with a superlinear Kolmogorov complexity with respect to the number of samples. This is an insighful result that can serve to better understand and design current algorithms and to develop new ones.\n\nTo obtain these results the paper employs Kolmogorov complexity, which is a novel technique in convergence analysis as far as I know.\n\nThe paper is not very clear and has some claims that need further support, as I mention in the weaknesses section of the review.\n\nDue to the lack of clarity, I must lean to rejection at the moment. The reason is that unless some of my questions and concerns are addressed, I cannot verify that the proofs are correct. Nonetheless, I am open to changing my rating to accept once they are successfully answered. This is because even though the convergence analysis is not direct, I believe that the paper contributes to a better understanding of the convergence of gradient-based methods and introduces an interesting proof technique that can be used in further work. \n\n**Minor comments and nitpicks that did not impact the score of the review**\n\n* Usually, with high probability (w.h.p.) always means that the probability tends to 1. I am not so sure the footnote is needed, then. Also, in your case, you prove a stronger convergence rate than $1 - 1/n$.\n* 7th line, page 2: (and wide) ~~the~~ GD.\n* Shouldn't it be $acc(W,A): \\mathbb{R}^d \\times 2^X \\to[0,1]$?\n* Before Lemma 4.1, in the parenthesis shouldn't it be \"(by iterating over all values of $\\mathbb{F}^d$)\"? \n* In the proof of Lemma 4.2, it should be $D_{\\text{KL}}(\\lambda_{i,j}' \\lVert \\varphi_{i,j})$.\n* Figures 4 and 5 are hard to see for a colorblind person. Could you change the colors, please?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_T4KT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_T4KT"
        ]
    },
    {
        "id": "FBkz6Roi1u",
        "original": null,
        "number": 2,
        "cdate": 1666287288872,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666287288872,
        "tmdate": 1666287322660,
        "tddate": null,
        "forum": "5YHaMHg2Bfa",
        "replyto": "5YHaMHg2Bfa",
        "invitation": "ICLR.cc/2023/Conference/Paper6217/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proves that SGD fits the training data whenever the average accuracy discrepancy over epoches, defined as the sum of accuracy improvement over batches, is large enough. Using a similar idea, this paper also proves GD needs a certain amount of randomness to escape local minimas efficiently.",
            "strength_and_weaknesses": "Strength:\n\nThe idea of using entropy compression to analyze SGD is novel and interesting.\n\nWeaknesses:\n\nThe assumption of accuracy discrepancy isn't natural: unlike other \"prior\" assumptions, this assumption involves the process of optimization. The assumption is felt to be designed especially for using the entropy compression tool.\n\nThe bound $O(\\frac{n \\log n}{b^2})$ implies $b=\\Omega(\\sqrt{n})$ because $\\rho_i$ is bounded by a constant, likely $\\log 2$ in practice for this binary classification problem. I wonder whether a batch size as large as $\\Omega(\\sqrt{n})$ is practical, or already implies good convergence results itself.\n\nCorollary 3.3 uses a stronger assumption than what's actually needed: it seems to be this only needs to hold on average.\n\nThe setting of section 4 seems artificial and I don't see any close relationship to the theme of section 3, except the common tool they use.",
            "clarity,_quality,_novelty_and_reproducibility": "Writing of this paper needs improvement. Many mathematical statements seem casual and the structure of paper isn't clear (for example, should there be a formal statement of main theorem in section 3?), let alone typos. The presentation of this paper isn't up to the standards a ML audience expects for this conference.",
            "summary_of_the_review": "The idea of this paper is very novel and has the potential to be make impacts. However the assumption and result need more clarification/justification and the writing can be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_xLLJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_xLLJ"
        ]
    },
    {
        "id": "IyzEN6Xa0xX",
        "original": null,
        "number": 3,
        "cdate": 1666862448229,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666862448229,
        "tmdate": 1666955931024,
        "tddate": null,
        "forum": "5YHaMHg2Bfa",
        "replyto": "5YHaMHg2Bfa",
        "invitation": "ICLR.cc/2023/Conference/Paper6217/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is about analyzing the behavior of stochastic gradient descent (SGD) using Kolmogorov complexity.  \nThe authors are using an \u201centropy compression argument\u201d to prove the termination (and convergence) of SGD under weaker assumptions as well as provide a lower bound on the amount of randomness needed for SGD to escape local minima using perturbations. \nThe main argument originates from a constructive proof of the Lovasz Local Lemma by Moser, and the idea is that: \u201cSuppose that by looking at the execution of a randomized algorithm, one is able to produce a string shorter than the string of random bits that the algorithm used, albeit it is possible to use that shorter string in order to reconstruct the input random string, then the algorithm should terminate as soon as that happens since it is not possible to compress a string of i.i.d random bits.\u201c \nFor the first result, the data is split in batches and the algorithm terminates when the accuracy reaches some value<1 (e.g 90%), while for each batch the accuracy is 100%. This allows a more efficient \u201cencoding\u201d of the batch samples since only the subset of correctly classified data could be used. This enables the entropy compression argument to go through and prove termination of the SGD algorithm.\nFor the second result, it is assumed that the algorithm used $\\ell$ uniformly random bits to choose among $2^\\ell$ different perturbations. The authors use a similar argument or random string reconstruction and show that a using a sublinear in the size of the dataset number of random bits, it is not possible for SGD to terminate in polynomial time and non-trivial accuracy.\n\n \n",
            "strength_and_weaknesses": "Strengths:\n- The paper proposes a novel use of a powerful idea, which even though it comes from a different area, it is demonstrated to potentially have interesting applications in optimization and machine learning. \n- It is shown that this technique can be used both for positive and negative results. \n\nWeaknesses\n- The presentation of the paper could be improved. For example, the results need to be more clearly stated in the introduction and I would also suggest that the main results are stated earlier in each of the sections. \n\n\nMinor comments:\n- Page 5, second bullet: there is a comma missing in the lhs.\n- In section 4: I am a bit confused with the use of $m$ and $n$. It seems that they should both mean the size of the dataset. So, the notation needs to be merged. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The main ideas are quite novel end clearly explained. However, the presentation in the technical parts should be improved. ",
            "summary_of_the_review": "I believe the paper makes a solid conceptual contribution and deserves to be accepted if the writing is improved.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_EpmW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_EpmW"
        ]
    },
    {
        "id": "pSpL_eLMc9Y",
        "original": null,
        "number": 4,
        "cdate": 1666987053971,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666987053971,
        "tmdate": 1666987053971,
        "tddate": null,
        "forum": "5YHaMHg2Bfa",
        "replyto": "5YHaMHg2Bfa",
        "invitation": "ICLR.cc/2023/Conference/Paper6217/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies batch SGD using the theory of Kolmogorov complexity to deduce novel observations about the behavior of SGD. For instance, the paper shows that certain patterns of classification accuracy obtained on the batches would allow the randomness of the batches (the permutation used to select the batches) to be compressed, and this implies a bound on the number of iterations SGD requires before achieving 100% test accuracy on the entire training data set. The paper also considers a lower bound on the amount of randomness needed to escape local minima.",
            "strength_and_weaknesses": "I found the paper to be very interesting and refreshing, as it adopts a perspective on SGD that I had not considered before. I also appreciate the focus on minimal assumptions, as it seems important to understand the general behavior of SGD.\n\nI believe that the main weakness of the paper is that it is quite abstract, and it is not clear to what extent the findings are applicable to real-world situations. To this extent, more concrete examples in the main text would be appreciated (e.g., are there concrete learning tasks for which the results of the paper are easily illustrated?). Also, it is worth noting that the setting may be too general, and the results of the paper may not be typical for situations encountered in practice.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very novel and generally of good quality. However, the clarity of the paper could be improved by discussing the implications more explicitly. I also point out some typos:\n- Throughout the paper: \u201cminimas\u201d should be \u201cminima\u201d\n- Pg. 1 footnote, \u201cmean\u201d -> \u201cmeans\u201d\n- Pg. 2: \u201cperturbe\u201d\n- Pg. 2, \u201cThe randomness we compress are the bits\u201d -> \u201cThe randomness we compress is the bits\u201d\n- Pg. 3, \u201cShowing that indeed\u2026\u201d is a sentence fragment\n- Pg. 3, \u201cpertrubes\u201d\n- Pg. 3, by R[0,1] do you mean [0, 1]?\n- Pg. 5, \u201cprivate case of Pinsker\u2019s inequality\u201d what does this mean?\n- Pg. 6, \u201cWhere in the above\u2026\u201d the word \u201cWhere\u201d should not be capitalized; similarly in the paragraph below\n- Pg. 8, \u201cEven for extremely\u2026\u201d is a sentence fragment\n- Pg. 8, \u201cthere must be instances with exponential running time\u201d This is not clear from the paragraph; just because something is not polynomial does not necessarily mean it is exponential.\n- Thm. 4.3, \u201cEven if\u2026\u201d is a sentence fragment",
            "summary_of_the_review": "Despite the lack of concrete implications, I like the novelty and I recommend the paper for acceptance. I hope that it will generate interesting follow-up work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_GiuD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_GiuD"
        ]
    },
    {
        "id": "fDJNIrxQ60V",
        "original": null,
        "number": 5,
        "cdate": 1667016999846,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667016999846,
        "tmdate": 1667017637713,
        "tddate": null,
        "forum": "5YHaMHg2Bfa",
        "replyto": "5YHaMHg2Bfa",
        "invitation": "ICLR.cc/2023/Conference/Paper6217/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the convergence property of SGD using tools from Kolmogorov Complexity.  A key quantity considered here is the accuracy discrepancy---the gap between the model accuracy on a training batch vs the rest of the data set. \n\nThe paper provides two results. \n\n* Section 3 shows that during training if the accuracy discrepancy is consistently beyond a global threshold $\\beta(n,b)$, depending only on batch size $b$ and data size $n$, then training will converge to full accuracy in constant epochs (namely, after constant cycles over the dataset). \n* Section 4 considers randomly perturbing the inputs to allow SGD to terminate with high accuracy. It's shown that SGD may take exponential time unless the amount of randomness used in the perturbations is sufficiently high. ",
            "strength_and_weaknesses": "The paper provides some interesting ideas for studying SGD convergence. To the best of my knowledge, the argument in Section 3 is novel. I think the result is intuitive. That is, if any model training can consistently improve the accuracy on the current batch during SGD, relative to the full dataset, then it is capable of fitting all data. I appreciate the technical part where this idea is formalized. \n\nWhile the theoretical claim of section 3 is interesting, I have concerns about the setting and motivation. First, the argument hinges upon the assumption that the SGD/GD is run with a sufficiently small step size below $1/L$.  I am not convinced that this holds in practice. In particular, modern neural networks are typically trained with large step sizes at a initial stage, higher than what's suggested by optimization theory, then later annealed as training loss improves. See the literature on \"learning rate decay\" and \"learning rate schedule\". This may break the reversibility lemma in section 2 and affect the main claim of section 3. \n\nFor section 4, as the author(s) suggested, one motivation is to study whether SGD can escape local minima. It's unclear this is an interesting question (either in theory or practice). In practice, large networks are highly parametrized and capable of fitting the entire train set. And it's observed that standard optimization techniques (SGD and its variants, such as Adam) can converge. \nThe claim of section 4 seems to be that we need to perturb the input points to avoid getting stuck at a highly sub-optimal local min (with 1/2+$\\epsilon$ accuracy). I encourage the author(s) to expand on the conceptual  message here, since as far as I know, this is not required in practical settings.\n\nThe main result in sec 3 applies to reshuffling SGD where each epoch uses a fresh random permutation. It's also common in practice that there's a single random permutation (sampled upfront) used by all epochs. (This is called single shuffle SGD; see e.g. Can Single-Shuffle SGD be Better than Reshuffling SGD and GD? by Chulhee Yun, Suvrit Sra, Ali Jadbabaie.) Could the author(s) comment on this setting?\n\nSome parts of the paper can be better written.  In the introduction (especially in the \u201coutline of our techniques\u201d section), it should be clarified what are the random bits that the reversal procedure tries to reconstruct. From early discussion (top of page 2), there are random bits in the initialization of the network, random bits for drawing SGD samples per step, and random perturbations added to the input points. The author(s) should specify what the main target here is. \n\nFinally, the experiments appear somewhat limited, conducted only on shallow neural networks trained with MNIST dataset. \n  \nMinor comments\n---\n1. Page 1: \u201cFor every batch, the accuracy of the model after the Gradient Descent (GD) step on the batch is 100%.\u201d When I read it first, it wasn\u2019t clear if this is a claim or a thought experiment. Maybe add: \u201cSuppose hypothetically.\u201d\n2. Page 1: \u201c, however\u201d -> \u201c. However\u201d\n3. Page 2: \u201cperturbe\u201d -> \u201cperturb\u201d\n4. Page 2: The precise termination condition seems confusing here. In one paragraph, it is written that \u201cthus, in our case, termination implies 100% accuracy\u201d. In a later paragraph: \u201cWe apply this approach to SGD with an added termination condition when the accuracy over the entire dataset is beyond some threshold. Thus, termination in our case guarantees good accuracy\u201d. Which one is actually applied here?\n5. Page 2: \u201c a sufficiently small GD step\u201d -> \u201csufficiently small step size\u201d\n6. Page 3: \u201cwe can sort X by IDs\u201d--- I am not sure this is necessary to say?\n7. Page 3: \u201cpertrubes\u201d -> \u201cperturbs\u201d\n8. Page 3: \u201cApplying this reasoning to data with random labels we arrive at a contradiction\u201d \u2014 I don\u2019t understand this point: what\u2019s the contradiction \n9. Section 3: W_ij should be defined explicitly. (I understand it\u2019s clear from the picture.)\n10. Section 4: I think it\u2019s better to state the goal of this section at its beginning\n",
            "clarity,_quality,_novelty_and_reproducibility": "At a technical level, the main theoretical claims of the paper appear sound. As I discussed, though, they either are under restricted assumptions or appear insufficiently motivated in ML practice.\n\nI have a few suggests regarding writing presentation (see above).",
            "summary_of_the_review": "Overall I think the paper has some interesting ideas. However, I am not convinced about the applicability of the claims. The paper can also be improved in terms of clarity and scope of experiments. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_w9AC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_w9AC"
        ]
    },
    {
        "id": "jIOBaEBnx7",
        "original": null,
        "number": 6,
        "cdate": 1667067684574,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667067684574,
        "tmdate": 1667067684574,
        "tddate": null,
        "forum": "5YHaMHg2Bfa",
        "replyto": "5YHaMHg2Bfa",
        "invitation": "ICLR.cc/2023/Conference/Paper6217/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers dynamics of stochastic gradient descent (SGD) and relates accuracy with a notion of \"accuracy discrepancy\". The paper shows that if the \"accuracy discrepancy\" is large enough, then SGD can find a model with perfect accuracy, while on the other hand if the \"accuracy discrepancy\" is small, there exists inputs for which a specific gradient descent component within an SGD algorithm terminates in exponential time.",
            "strength_and_weaknesses": "Overall, the paper is interesting but suffers from clarity issues.\n\nStrengths of the paper:\n- The paper discusses SGD, which is an important and relevant topic for the learning community. \n- The paper gives many examples for the introduction / motivation\n- The usage of Kolmogorov complexity in the analysis of SGD is interesting\n\nSome comments on weaknesses include:\na. It appears that the \"reversibility\" of Lemma 2.1 and other usages in the paper requires that the smoothness constant L satisfy L>1, but it was not clearly mentioned in the paper. Are there any comments on this and how it affects or puts limitations on the main result?\nb. There are parts in the paper that leave a confusing impression. For example, in page 2 \"Outline of our techniques\" it is stated that \"if the accuracy on the entire dataset is 100% we terminate\" but immediately at the top of page 3 the paper asks the reader to consider a scenario where \"we terminate our algorithm when we achieve 90% accuracy on the entire dataset\". This, being in the introduction section, may cause confusion for readers.\nc. Following the above point on confusion, there are also other sentences that may require revision. For example, on page 5, what does \"private case of Pinsker's inequality\" mean? And in the \"Representing sets\" section, it is said that \"some useful bounds\" are to be stated there, but there is only one bound Lemma 2.4., is something missing here?\nd. Does the result only hold for specific scenarios of SGD? For example, does the results of Section 4 only hold when we need convergence/termination in Alg. 2? What about the scenario when SGD only has one step of GD (such as similar to Algorithm 1)? This seems to show that the results are fairly limited and only suitable for specific scenarios.\ne. The main results and flow of the discussion is not very clear and concise. This is also shown through the fact that there are a lot of Lemmas but there is no main Theorem in Section 3 and only one main Theorem at the very end of page 9 in Section 4. The clarity of the paper would be greatly improved with more discussions on the flow of proofs and more interpretations of lemmas.\nf. The experiment results were not discussed in detail.\ng. Some comments on typos: There are several occurrences in the paper where \"perturb\" is spelled wrong. Another point is: Is the description of Algorithm 3 missing?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, there is room for clarity and quality. Please refer to the section above for comments on clarity and quality. In terms of originality, the paper provides some discussion of SGD and prior literature but does not provide many comparisons against prior work or prior techniques. Some additional discussion / comments in the paper regarding novelty of techniques would be helpful.",
            "summary_of_the_review": "The paper is interesting but has several concerns including those listed in the sections above. Revisions on the paper would help with its clarity and quality.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_5jcV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_5jcV"
        ]
    },
    {
        "id": "yP5C0U-GbA",
        "original": null,
        "number": 7,
        "cdate": 1667220376736,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667220376736,
        "tmdate": 1667220376736,
        "tddate": null,
        "forum": "5YHaMHg2Bfa",
        "replyto": "5YHaMHg2Bfa",
        "invitation": "ICLR.cc/2023/Conference/Paper6217/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers studies dynamics of mini-batch SGD using Kolmogorov complexity. They define a notion of accuracy discrepancy as a KL-divergence between accuracy of the model on previous batches in the current epoch and on the last batch. Using the fact that the random strings used for generating the epoch\u2019s permutation are incompressible, they bound the accuracy discrepancy throughout the algorithm execution.\n",
            "strength_and_weaknesses": "I have the following concerns about the paper:\n\n-- As I understand, the discussion near Theorem 2.2 is really important, since it shows that the provess is invertible. Moreover, the algorithm actually relies on the fact that the set of possible values is finite, to find the original point in finite time.\n\nHowever, the conditions in Theorem 2.2 don\u2019t necessarily hold in practice. To give an example, for f(x)=x^2 / 2, with the step size 1e-16, for the double type, the gradient step starting from points 1e16 + 2 and 1e16 produce the same result.\n\nWhile I understand that the example is artificial, it suffices to show that the process is, in general, not invertible (when working in doubles). I\u2019m not sure, but it might be important for the rest of the paper, since you do rely on the invertibility. I think that it should be clarified why the conditions of Theorem 2.2 actually hold.\n\n-- \u201cIt is clear that w.h.p we cannot train a model with d parameters that achieves any accuracy better than 1/2 + o(1) on X\u201d - I\u2019m not sure this statement is true. You probably should use non-random labels and counting argument.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Minor issues:\n-- Theorem 4.3: I don\u2019t understand what \u00bd + \u03b8(1) means\n-- Instead of $\\tilde{\\nabla f_{A}}$ (and others), I would write $\\tilde{\\nabla f}_{A}$. The long tilde looks really weird\n-- I think \u201cw.h.p.\u201d is commonly defined as 1 - n^-c for an arbitrary large c\n-- Page 6:\u201d Where in the above we used Stirling\u2019s approximation\u201d and \u201cWhere the efficiency of reconstruction is expressed via \u03c1_i\u201d: should start commas (instead of being new sentences).\n-- I think the relation between f and acc was never mentioned\n",
            "summary_of_the_review": "Solid paper, accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_dQLw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6217/Reviewer_dQLw"
        ]
    }
]