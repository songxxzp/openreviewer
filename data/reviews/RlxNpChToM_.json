[
    {
        "id": "zrVZMRuWUTp",
        "original": null,
        "number": 1,
        "cdate": 1666633783002,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633783002,
        "tmdate": 1666633783002,
        "tddate": null,
        "forum": "RlxNpChToM_",
        "replyto": "RlxNpChToM_",
        "invitation": "ICLR.cc/2023/Conference/Paper5708/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyses representations learned by neural network. The analysis is done via isometric mapping of the probabilistic model into a lower dimensional maniforld such that it preserves the pairwise distance between models. They propose a distance metric to compare different learning trajectories.  They also use InPCA to embed the model to a lower dimensional space. By doing so they can visualize a trajectory of the learning process and compare trajectories of different models.  \nThe main findings are: 1)the manifold of  the models trained on different tasks are low-dimensional 2)supervised learning help training of dissimilar tasks especially if it has diverse classes. 3) pre-training for longer epochs could hinder fine-tuning of the downstream task. 4) episodic meta-learning fits similar models than supervised learning even if they take different trajectories. 5) contrastive learning trained on different datasets learn similar representations. \n",
            "strength_and_weaknesses": "Strengths:\n- I generally find this sort of analysis interesting which can help us better understand what goes on the learning process. \n- Having a quantitative method to compare learning trajectories is very interesting. \n- The paper is generally organized well. The experiments support the contribution claims. \n\nWeakness:\n- The authors mention in the abstract that they have surprising findings, however, all their findings are inline with the literature.\n- They could apply their method on more diverse dataset instead of sampling classes from the same dataset or 2 relatively similar datasets (i.e. CIFAR10 and Imagenet)\n- CIFAR10 is a very small dataset to apply self-supervised learning. \n- It would have been more interesting if they had conducted experiments on more self-supervsing methods and to compare different self-superviseing methods. \n- Fig 4 does not seem to be referenced in the text. \n- Organization of results is very condense. It could be done better. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is generally clear and easy to follow. Some notions in the paper such as \"Progress\" are not defined before being used. \n- The main novelty of the paper seems to be quantitative metric to computer distance between mean trajectories. \n",
            "summary_of_the_review": "I think the paper has merits for publication. Analysis of deep learning representations and models can help us better understand what goes on the learning process. It seems to me the main novelty of the paper is distance metric between training trajectories which can help us measure similarity of models and tasks. The paper could have performed more extensive experiments (see weaknesses for details. )",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5708/Reviewer_hj5S"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5708/Reviewer_hj5S"
        ]
    },
    {
        "id": "FupZKjCijwt",
        "original": null,
        "number": 2,
        "cdate": 1666637336135,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637336135,
        "tmdate": 1666637336135,
        "tddate": null,
        "forum": "RlxNpChToM_",
        "replyto": "RlxNpChToM_",
        "invitation": "ICLR.cc/2023/Conference/Paper5708/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors use the Bhattacharyya distance to define notions like the distance between learning trajectories on classification tasks. They also define a simple notion of transfer learning, and use that to compute learning trajectory distances across different transfer tasks and ",
            "strength_and_weaknesses": "Using the Bhattacharyya distance gives a nice way to generate normalized comparisons between different approaches. In addition, mapping sub-tasks to the union task gives a simple way to compare aspects of representation learning.\n\nIt is not clear to me that the time parameterization using the geodesic from the random initialization to the final solution is a good normalization in all situations. It may provide too much weight to early time learning dynamics. However, the fact that the authors derived a relatively simple normalization procedure is to their credit.\n\nThe claim of result 1 feels like an overreach. Dimensionality reduction techniques should not be conflated with dimensionality counting techniques. In addition, 33% of the variance explained is not a large quantity. While low dimensional dynamics exist in many ML settings (including perhaps this one), it is not clear what fractions of the dimensions are necessary to capture the relevant information. For example if much of the captured variance is early in the trajectories (when accuracy of the classifier is low), the low-dimensional picture may not capture the more interesting aspects of feature learning later in the trajectories.\n\nResult 2 is perhaps less surprising, given results that the early layers of convolutional networks learn similar features in different tasks (e.g. https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53). Result 3 is interesting in principle, but is very hard to see from the plot - again suggesting that the normalization may not be well-suited to answer certain questions.\n\nResult 4 is also known. Result 5 is potentially interesting; however, the fact that the models arrive at similar points in information space is not surprising if both methods are known to produce solutions with similar accuracy. Result 6 is also potentially interesting, but I had trouble understanding the text and the figure, and their relationship.",
            "clarity,_quality,_novelty_and_reproducibility": "Sections 1 and 2 were generally clear. When defining the distance metric, \"we define the Bhattacharyya distance\" could be interpreted to mean that this paper came up with the distance metric to a reader unfamiliar with information geometry. Also, $\\tau$ may not be the best notation for a trajectory - in classical differential geometry, $\\tau$ is often used as a normalized time coordinate to parameterize a curve or manifold. $\\gamma$ is a more standard notation for a curve.\n\nIn the section \"Embedding a probabilistic model in lower-dimensions\", I didn't understand the distinction between the InPCA method (\"preserves global distances\") and other dimensionality reduction methods (\"preserves local distances\"). Don't global distances get distorted when a small subspace is plotted? Or is the point that, like regular PCA, InPCA is based on reduction of the global set of distances?\n\nSection 3 was very difficult to parse. There are many figures/subfigures, each with complicated explanations. Perhaps the text would be better suited to having fewer results, explained in more detail, with some moved to the appendix. In particular Figures 1 (c) and 3 were difficult to parse/interpret.\n\nI believe Result 6 and Figure 4 were meant to be linked, but don't seem to be in the main text.\n\nI believe the method is novel, but I am not well-versed in the fields of information geometry and contrastive learning, so I will defer to the other reviewers on that point. Many of the results are not novel. There is value of recapitulating known results with a new method, if that method can then be used to understand new phenomenology; however, in this work it's not clear if new results can be derived using this methodology.",
            "summary_of_the_review": "Overall, the use of information geometry + InPCA embedding is a very nice way to visualize learning trajectories with different setups. However, the overall results seem to mostly be known, and the presentation, particularly for the figures, can be hard to parse.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5708/Reviewer_opzC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5708/Reviewer_opzC"
        ]
    },
    {
        "id": "YWldBsucStU",
        "original": null,
        "number": 3,
        "cdate": 1666654702801,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654702801,
        "tmdate": 1666654702801,
        "tddate": null,
        "forum": "RlxNpChToM_",
        "replyto": "RlxNpChToM_",
        "invitation": "ICLR.cc/2023/Conference/Paper5708/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper develops a set of tools to measure model distances, and visualize model embeddings/training trajectories, and thus make measurements on different tasks' relationship. Techniques used includes Bhattacharyya distance, linear layer imprinting, InPCA and others. The paper then uses these tools to make several interesting observations on self-supervised learning, meta learning, supervised learning and fine-tuning and find the representations are quite shared. ",
            "strength_and_weaknesses": "Strengths:\n\n-- The paper proposes a set of tools that can be used to analyze neural network training, trajectories and similarities in general, even for models on different tasks.\n\n-- The paper makes several interesting observations on the representations learned.\n\nWeaknesses:\n\n-- The metric Bhattacharyya distance relies only on the output y, so even if the intermediate representation is very different it still calculates two models as very close. This may not be ideal, especially when two models agree on the output but not representations. The later tools all rely on this distance so this affects all metrics and observations in this paper.\n\n-- Given the point above, when comparing models trained on the same task (e.g., supervised learning and meta-learning), it may not come as a surprise that their eventual similarity is high, since both of them will get high accuracy in the end. \n\n-- I feel the presentation and clarity can be greatly improved, especially for audience without a prior background in this topic. \n\n-- The paper's introduction does not cite any prior work which is weird. I also feel the six listed findings do not have to repeat in introduction and abstract, maybe only one place is enough.\n\nOther comments:\n-- Remark 1: \"Note that if we were to train the classifier w2 (with backbone w1 fixed) using samples from the other task under the constraint that rows of w2 have unit l2 norm, then we would obtain the imprinted weights as our solution.\" I get that imprinting is a way to substitute linear probing, but Is there a mathematical proof for this statement?\n\n-- Equation (2's) \"*\" step is still not that clear to me. Maybe it could be made more clear by having an additional intermediate step. \n\n-- \"where Lij = \u03b4ij \u2212 1/n is the centered version of D.\" The centered version of D seems to be W.\n\n-- The last equation in Section 2, can you make it more clear why it indicates isometry?\n\n-- The axes and ticks in figures can be made thinner and more transparent to improve the visualization and highlight the trajectories. \n\n-- I don't quite understand the \"n\" for equation (9) and the trajectories plotted in figure 1. Given that each epoch produces a standalone model, for an intermediate model, where to select the other n-1 models to construct a joint model embedding?\n\nTo summarize, the paper proposes a set of useful tools to analyze and compare different models, and present interesting findings with them from the perspective of task relationships. But the paper also has many points to improve on. Therefore I will recommend \"marginally above\".",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity can be improved - see comments above. Methods and tools seem novel. Code is not submitted and code release is not discussed which is a weak point. ",
            "summary_of_the_review": "To summarize, the paper proposes a set of useful tools to analyze and compare different models, and present interesting findings with them from the perspective of task relationships. But the paper also has many points to improve on. Therefore I will recommend \"marginally above\".",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5708/Reviewer_wHo9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5708/Reviewer_wHo9"
        ]
    }
]