[
    {
        "id": "Gu6PN6eS30",
        "original": null,
        "number": 1,
        "cdate": 1666312612572,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666312612572,
        "tmdate": 1668887959498,
        "tddate": null,
        "forum": "LE5LxBgjB4V",
        "replyto": "LE5LxBgjB4V",
        "invitation": "ICLR.cc/2023/Conference/Paper3409/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers micro-batches, which are disjoint smaller subsets of mini-batch. Empirical evidence is provided to suggest that, although a large mini-batch does not lead to generalization as good as a small mini-batch, the introduction of micro-batches together with a large mini-batch helps recover the generalization corresponding to a small mini-batch.",
            "strength_and_weaknesses": "Strength: using micro-batch as an estimator of the gradient norm and a regularizer is a very interesting idea. It seems to have a lot of potential.\n\nWeakness: \n\n(1) very little theoretical justification is provided, but the empirical results are not comprehensive enough to support the claims. \n\n(2) if I didn\u2019t misunderstand, one central goal of this paper is to suggest that small-batch SGD has an implicit bias that regularizes gradient norm. I\u2019d like to better understand why this wasn\u2019t investigated directly, but rather via an indirect route based on the extra complication of micro-batch. I\u2019m relatively convinced that micro-batch has this implicit bias, but curious about why this serves a demonstration of the small-batch SGD story.\n\n(3) a traditional belief is that smaller batch size leads to behaviors similar to those under larger learning rate. The implicit bias of large learning rate, in terms of regularizing the gradient, for example, has been studied already, not only empirically but also theoretically. In addition, the implicit bias of small batch size has also been studied, see e.g., [Smith et al. ICLR 2021], which was already cited. Therefore, it is not completely clear to me what\u2019s new in terms of SGD\u2019s implicit bias. The micro-batch part is still interesting though.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I wish how this work positions in the literature could be clarified more. It could just be a clarity issue, but without better understanding this, I\u2019m incapable of assessing the originality.\n\nI also hope the authors could clarify about whether/how micro-batch is better than small mini-batch in practice? Is it computationally more efficient? It could just be me but I feel this part is not so clear to me.",
            "summary_of_the_review": "Overall, I feel this paper definitely proposed an interesting idea. However, I hope to understand better what is the gain, as well as to see more evidence of its effectiveness (if not theoretically, at least empirically, and more comprehensively).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3409/Reviewer_rQEx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3409/Reviewer_rQEx"
        ]
    },
    {
        "id": "g6PRStUZrs",
        "original": null,
        "number": 2,
        "cdate": 1666623870358,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623870358,
        "tmdate": 1666623870358,
        "tddate": null,
        "forum": "LE5LxBgjB4V",
        "replyto": "LE5LxBgjB4V",
        "invitation": "ICLR.cc/2023/Conference/Paper3409/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper does a thorough simulations to clarify and verify the existing hypothesis about why small batch SGD outperform its large batch counterpart. They derive many interesting results. First, they show that adding regularizer based on gradient norm or the Fisher Information Matrix trace can recover small-batch SGD generalization, while Jacobian-based regularizations fail. On the other hand, they show that the gradient norm, in some case, is not a good indicator of generalization, i.e., there exists some algorithms achieving good generalization but with large gradient norm. Last, they show that some tricks work on small dataset like CIFAR10 might fail on large dataset like CIFAR100, Anti-PGD, for example.",
            "strength_and_weaknesses": "## Strength\n- Though this paper is purely empirical, it provides many important intuitions for future work. For example, the failure of the Jacobian-based regularizations shows that this might not the reason for the good generalization of small-batch SGD. On the other hand, the counterexample in Figure 4 shows that we should seek other explanations beyond gradient norm regularization. The last observation that some tricks might fail on large datasets calls for verifying all the previous tricks only verified on small datasets and rethinking the importance of dataset structure. \n- The paper is well structured. I enjoy the process of reading this paper.  \n\n\n## Weaknesses\n- The performance reported in the paper is not the SOTA. I suspect the reason is that they do not use weight decay and momentum, as mentioned in the appendix. I can understand that weight decay or momentum can be regarded as some kind of regularization. Moreover, I did not find the stepsize regime. Do you use constant stepsize? In the reality, we always add weight and momentum, together with stepsize decay (or cosine learning rate) to improve the generalization. I will be very happy to increase my score if you can provide simulations showing that gradient norm regularization can also recover the generalization after adding all these common tricks.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has good writing and is easy to read. Some of its results are novel, which I believe will broadly impact the community.",
            "summary_of_the_review": "Please see the strength and weaknesses part.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3409/Reviewer_i18E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3409/Reviewer_i18E"
        ]
    },
    {
        "id": "lfhYZ4HqY54",
        "original": null,
        "number": 3,
        "cdate": 1666633651379,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633651379,
        "tmdate": 1666633651379,
        "tddate": null,
        "forum": "LE5LxBgjB4V",
        "replyto": "LE5LxBgjB4V",
        "invitation": "ICLR.cc/2023/Conference/Paper3409/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies the implicit bias of SGD by investigating which kind of explicit regularization can help large batch SGD/GD match the performance of (small batch) SGD. Several new insights are drawn from a systematic set of experiments. \n\nFirstly, the effect of several explicit regularizer are studied, including (1) micro-batch gradient norm regularization, (2) micro-batch fisher trace regularization, (3) micro-batch average Jacobian regularization and (4) micro-batch random and unit Jacobian regularization. It is shown that the first two regularizers are more effective than the last two. \n\nSecondly, the work turns to looks at some extreme use cases of micro-batch gradient norm regularization: (1) when the micro batch size approaches the scale of the batch size of \"large batch SGD\", the regularizer no longer works well; (2) when the micro-batch gradient norm regularization is approximated by a sample micro-batch gradient norm regularization, the latter regularizer works well when he regularization strength is property turned. \n\nFinally, the paper considered a very interesting set of experiments that involve gradient grafting, which mixes the direction and magnitude from two separate (or related) optimizers. It is shown that, for some CIFAR-10 settings, gradient grafting can recover the performance of SGD, indicating that perhaps the direction of large batch SGD update is good enough but its magnitude could have been improved. Yet, this conclusion does not carry over to harder settings, where improving the magnitude of large batch SGD update might not be sufficient for it to generalize well. \n\nBased on the last result, an inconsistent performance of one algorithm in easy/hard deep learning application, the paper discusses some limitations of current scientific study of deep learning, where most experimental evidence is from relatively easy tasks and might not migrate to harder tasks.  ",
            "strength_and_weaknesses": "# Strength\n+ The main contributions are summarized in the Summary section. Some other strength of this work includes:\n+ The paper is overall very well organized. I have a good time readying most of the paper (except for a few places please see below). \n+ As a scientific paper, more experiments are always welcome. With that being said, I think the conclusions presented in this work are well backed up by its experimental design. At this point I did not find a major shortcoming in terms of experimental design. \n+ I really like the design of Section 5 which disentangles the effect of the magnitude and direction of the update. Such a perspective is novel to my knowledge for understanding SGD (despite it has been studied from a more theory work), and I think it may inspire a deeper understanding of SGD. \n\n\n# Weakness\n- A large portion of the work focuses on verifying which type of explicit regularization for large batch SGD/GD can recover the performance of small batch SGD. I would like to see some discussions on why finding the right explicit regularization (that corresponds to the implicit regularization of SGD) is important or useful. \n- Note that the explicit regularization term introduces several new hyperparameters. For example the penalty strength $\\\\lambda$. I would like to see some discussion on how hard it is to tune the penalty strength $\\\\lambda$. Partly due to the new hyperparameters, I somehow feel the finding of this work might not be very useful for practitioners. \n- I find Section 3.2, paragraph (ii) is very hard to interpret. Would you mind elaborate a bit? Perhaps written down some math could help. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good. \n\nQuality is solid. \n\nNovelty is fine. Sections 3 and 4 might not be very novel. But Section 5 proposes a new perspective to me. \n\nReproducibility is good. ",
            "summary_of_the_review": "Please see above. At this point I would like to vote for a weak acceptance. I will consider increasing the score based on the authors reply to my questions. I might also consider decreasing the score if I or other reviewers find a major vulnerability in the experimental design that cannot be satisfactorily addressed. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3409/Reviewer_fWma"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3409/Reviewer_fWma"
        ]
    },
    {
        "id": "A8kTnzHP42",
        "original": null,
        "number": 4,
        "cdate": 1666792480726,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666792480726,
        "tmdate": 1666792480726,
        "tddate": null,
        "forum": "LE5LxBgjB4V",
        "replyto": "LE5LxBgjB4V",
        "invitation": "ICLR.cc/2023/Conference/Paper3409/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides an empirical study comparing a range of proposed mechanisms by which small batch SGD enhances generalization. \n\nTo achieve this, the authors split large mini-batches into a set of \"micro-batches\". They then evaluate a range of proposed implicit regularization terms on each micro-batch before summing across micro-batches to form a single large batch update. They show that applying the gradient norm/Fisher trace regularizers proposed by Smith et al. [2021] and Jastrzebski et al. [2020] to these micro-batches consistently closes the generalization gap between small and large batch training, while Jacobian regularization does not. Additionally, as predicted by prior work, they confirm that the benefits of gradient norm regularization diminish as the micro-batch size rises.",
            "strength_and_weaknesses": "Strengths:\n\n1) The micro-batch concept enables the authors to directly minimize the modified loss proposed in Smith et al. while increasing the batch size. This is also practical, since large batch training is usually achieved by parallelizing across multiple devices.\n2) The authors provide experiments across a range of models/datasets, which consistently show that gradient norm and Fisher trace regularization closes the gap between small and large batch training, thus supporting the claim that small batch SGD implicitly regularizes these quantities.\n3) The authors confirm empirically that these regularizers perform poorly when the micro-batch size is too large. \n\nWeaknesses:\n1) The novelty of the work is quite limited, however I think this is fine for an empirical evaluation.\n2) It would have been nice to include an ImageNet experiment.\n3) I didn't understand how the grafting experiments connected to the main theme of the paper?\n4) A longer background section or appendix describing the relevant prior work in detail would help readers less familiar with the topic.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper was mostly clear\n\nQuality:\nThe quality of the work is reasonably high\n\nNovelty:\nThe novelty of the submission is limited, but this is reasonable for an empirical study\n\nReproducibility:\nIt would be relatively straightforward to replicate the results.\n\nOther comments:\n1) I do not understand point three in the list of main contributions. Please could the authors clarify what they mean?\n2) Note that all of the mechanisms in the first paragraph of section 2 are closely related.\n3) Section 2.1 first paragraph: Barrett and Dherin study full batch gradient descent, the correct citation here is Smith et al., which extended Barrett and Dherin's results to SGD.",
            "summary_of_the_review": "The authors present some useful experiments clarifying which proposed mechanisms for the implicit regularization of small mini-batches are most effective in practice at closing the generalization gap between small and large batch training.\n\nI am currently recommending marginal accept since I think this is a helpful contribution. The experiments are reasonably thorough but no large scale datasets are included, and the novelty/originality of the submission is not very high.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3409/Reviewer_QhHW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3409/Reviewer_QhHW"
        ]
    }
]