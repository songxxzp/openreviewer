[
    {
        "id": "rKW4YE9yxX",
        "original": null,
        "number": 1,
        "cdate": 1666622584921,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622584921,
        "tmdate": 1666622584921,
        "tddate": null,
        "forum": "m0fEJ2bvwpw",
        "replyto": "m0fEJ2bvwpw",
        "invitation": "ICLR.cc/2023/Conference/Paper4194/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a generative model that draws on the diffusion-based and score-based models that generate samples through a sequence of intermediate distributions, but proposes different parameterisation, training criteria and sampling procedure. The model is motivated largely by the fact that prior models treat time (that parameterises intermediate distributions) as discrete and do not benefit from parameter sharing between these distributions. Limited model evaluation is presented.",
            "strength_and_weaknesses": "**Strengths**\n* The paper proposes an interesting view on diffusion and score-based models that leads to a different formulation and training criteria.\n* The paper is well-written and clearly motivated.\n* Empirical evaluation appears to be sound.\n\n**Weaknesses**\n* The motivation of lack of parameter sharing between models of $p_z(z|t)$ is not convincing. For example, this does not seem to take into account the existence of continuous time diffusion- and score-based models, or the fact that in practice $p_z(z|t)$ are often implemented with neural networks that share most of the parameters.\n* Computation efficiency of the proposed model is not discussed. For example, taking K (which runs up to 100) steps of MCMC in each training iteration seems rather expensive, especially when comparing this to diffusion-based models that can generate a sample from an intermediate distribution required for training in a single step. This aspect of the model should be discussed.\n* Despite clear motivation the benefits of the proposed method are not obvious form the empirical evaluation. For example, the proposed model has one of the worst FID scores among the competing methods and is far behind the SOTA methods in terms of Inception and FID scores. Why is that?",
            "clarity,_quality,_novelty_and_reproducibility": "* Clearly written.\n* An interesting take on diffusion-/score-based modelling.",
            "summary_of_the_review": "Interesting well-motivated generative model that needs stronger empirical results.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4194/Reviewer_KRWa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4194/Reviewer_KRWa"
        ]
    },
    {
        "id": "Ez7llmHGEB3",
        "original": null,
        "number": 2,
        "cdate": 1666632964215,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632964215,
        "tmdate": 1666632964215,
        "tddate": null,
        "forum": "m0fEJ2bvwpw",
        "replyto": "m0fEJ2bvwpw",
        "invitation": "ICLR.cc/2023/Conference/Paper4194/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to treat \"time\" in an energy-based model as a random variable and learns a joint distribution of time and samples. Concretely, this results in a parameterized function that outputs the joint log probability of the samples and time indices. Two methods of training such a model have been presented: a) using product of conditional and marginal where training is achieved by the optimization of the contrastive divergence and cross entropy losses; b) using the joint distribution directly. The proposed model offers slight improvements over baseline models trained via constrastive divergence.",
            "strength_and_weaknesses": "[Strengths]\n* I found the idea of modeling a joint distribution by making the neural network output log probabilities for every time index interesting.\n\n[Weaknesses]\n\nMotivation/Claims/Significance:\n* The proposed approach of treating time as a random variable is not well-motivated in the introduction. It is unclear why such a treatment is needed or even useful. Furthermore, it is unclear what treating the time as random variable even means in the context of energy-based models. Sampling methods, such as Langevin Monte Carlo, follow the steepest descent curve in the Wasserstein space (given by the Fokker-Plank equation) to draw from the Boltzmann distribution. This curve of probability densities can be indexed by time but I am not sure what treating time as a random variable would imply.\n* The only potential benefit that's mentioned in the introduction is that it leads to \"improved results\". However, the improvement is over a specific class of EBM and is marginal.\n* It is claimed that the method can be used \"in conjunction with almost any explicit distribution learning algorithm\"; however, only a specific variant has been studied in the paper. It is unclear how it would be applicable to other algorithms/models and how that would help.\n\nWriteup:\n* Some things in the paper are unclear:\n    * Is Eq. (1) just an example that motivates the discussion or is it the form that's used in the experiments. I am not sure if previous works have studied intermediate distributions that are linear combinations in the data space.\n    * What is meant by \"which the standard approach\"? As per my understanding, EBMs learn energy of the Boltzmann distribution and are not indexed or conditioned on time. \n    * Why did the authors choose to initialize the MCMC process \"with a sample from the dataset\"? Can the authors explain what they meant by \"vanilla contrastive divergence\"?\n\nTechnical Contribution and Empirical Evaluation:\n* The overall technical contribution of this work is limited and it doesn't offer any new insights. \n* The sampling procedure now has an inner loop which appears to increase the computational complexity. \n* The authors state that the method results in \"substantially improved results\". However, scores have been reported only for one dataset and the improvement is marginal, that too over the specific class of CD-based EBMs. CelebA scores are conspicuously missing. ",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is written reasonably well but some sections are unclear (see weaknesses). \n* Please refer to weaknesses for the evaluation of quality.\n* The specific method proposed in this work is novel but its significance is limited.\n* The proposed method, as described in the paper, doesn't appear straightforward to implement. In the absence of specific details and/or code, the reproducibility of this work is low.",
            "summary_of_the_review": "The key proposal of treating time as a random variable lacks clarity and motivation. The overall technical contribution and significance of this work is limited. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4194/Reviewer_k6Mq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4194/Reviewer_k6Mq"
        ]
    },
    {
        "id": "mqmSzooNXLL",
        "original": null,
        "number": 3,
        "cdate": 1666697897322,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697897322,
        "tmdate": 1666697897322,
        "tddate": null,
        "forum": "m0fEJ2bvwpw",
        "replyto": "m0fEJ2bvwpw",
        "invitation": "ICLR.cc/2023/Conference/Paper4194/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to train EBMs that can model the distribution defined by uniformly sampling a linear interpolation, called time, between a noise distribution and the data distribution. The joint distribution of the interpolated image distribution and the time random variable is the target density, and samples from the target density can be obtained by jointly sampling from noise, data, and the interpolation proportion. Two methods are proposed to learn the joint distribution. The first uses joint MCMC sampling to obtain the negative samples and standard CD loss. The second uses a JEM-like objective where the EBM parameterizes the marginal of the image distribution via a log sum exp over the output of the EBM output, which is a vector of the conditional densities for each class. One can draw samples of the model by initializing states from noise and performing a few Langevin updates on at each level of time until the time step corresponding to the data distribution is reached. Experiments evaluating the method are performed on CIFAR-10 and Celeb-A.",
            "strength_and_weaknesses": "STRENGTHS:\n\n1. The technical innovations in this work are an interesting application of design principles in diffusion models to EBM learning. The CD+CE method is an elegant way of training the joint density that relates nicely to existing work. \n\nWEAKNESSES:\n\n1. I am not sure that the claims about the method being the first successful CD-based method are valid (paragraph right before Section 2). The Multigrid method (Gao et al. 2018) and Diffusion Recovery (Gao et al. 2020) use transformed data samples for both initial MCMC samples and for the positive samples. One could make the argument that these works use CD if we broaden the definition of CD to include samples that can be created by transforming the data, as done in the present work.\n2. The empirical results in terms of FID score are solid for EBMs but not exceptional. I have some doubt about the reported Inception scores of 8.4 and 8.5, which are much higher than the Inception scores of models with much better FID. Although I realize that Inception and FID can be at odds, this is not generally the case on simple datasets like CIFAR-10. Further experiments at a higher resolution beyond 32x32 or with more diverse datasets would improve the experimental evaluation.\n3. EBM training times are already a critical issue, and further increases in runtime are a significant drawback.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The core idea is novel and interesting and the presentation in clear. Experiments show a modest improvement over some recent EBMs but the quality of the experimental results is average.",
            "summary_of_the_review": "Overall, I found this work interesting but not exceptional. The core idea of the proposed method is the biggest strength of the paper, and it could prompt further design of diffusion-like sequences of distributions from a reference distribution to the data. The experimental results are reasonable but not extremely compelling and the gap between EBMs and diffusion models remains wide. The high computational cost could be an obstacle towards experiments with more complex datasets. I am on the borderline but inclined to recommend against accepting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4194/Reviewer_qaem"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4194/Reviewer_qaem"
        ]
    }
]