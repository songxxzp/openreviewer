[
    {
        "id": "YWGAcgqRrI",
        "original": null,
        "number": 1,
        "cdate": 1666483482222,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666483482222,
        "tmdate": 1670380925926,
        "tddate": null,
        "forum": "p9zz7hLzH-4",
        "replyto": "p9zz7hLzH-4",
        "invitation": "ICLR.cc/2023/Conference/Paper4400/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In order to increase the expressive power of GNNs, a lot of strategies used in many research have been seen, e.g., augmenting node/edge features by position/structural information. This paper focuses on the affinity metrics to achieve this purpose. Specifically the paper considers the statistics that arise from random walks in graphs, by exploring hitting/commute time between pairs of vertices, so that such a feature can be incorporated in the standard graph message passing neural network. It has been found that the richer vector-valued resistive embedding is quite effective in discriminating nodes etc.  In fact the distances between these embedding can reveal the hitting time etc, or in other words, the embedding method provides an alternative way to calculate the hitting/commute time according to the theoretical proof presented in the paper.",
            "strength_and_weaknesses": "Strength:\n\n1. By defining the resistive embedding, the paper claims that the resistive embedding can be used to calculate the hitting time (Lemma 4.3)\n2. Thus instead of directly using hitting/commute time as node/edge features, the embedding has replace them in MPNN\n\nWeakness:\n\n1. The novelty is incremental. All the conclusion can trace back to Lovasz (1993), that is, the very important conclusion Res(u,v) = K_{uv}/2M and Res(u,v) = (1_u-1_v)L^*(1_u-1_v) where L^* is the pseudo-inverse of L.  As Res(u,v) = (1_u-1_v) L^{*/2}L^{*/2}(1_u-1_v), thus Definition 3.1 is straightforward. It is in fact r_v = L^{*/2}1_v or simply r_v = L^{-1/2}1_v.  Thus, all the conclusions in Section 4 are trivial.\n\n2. Although the paper uses an example in Figure 1 to show that a MPNN using Res(u,v) can distinguish nodes in this particular graph, this does not prove it will be more expressive than MPNN without using these features. The reason is, we cannot simply say in all cases MPNN using Res(u,v) are always better than MPNN without using Res(u,v).  Unless I have missed something, I believe that the general claim (Theorem 3.7) cannot hold.\n\n3. In the experiments, it is not clear how the new features are used in MPNN.  It is better to have detailed description. For example, (3)-7) in Section 5 do not clearly specify where the new features Res(u,v) and Embedding are used.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has good readibility. In general the paper is well written. As pointed in the above Weakness 1, the originality of the work is limited.\n\nCouple of minor comments for correction:\n\n1. In Lemma 4.1, the last 1\\leq j\\leq m should be 1\\leq i\\leq m??\n\n2. On page 16 of the Appendix, Lemma 4.3 and its proof are repeated and mixed up. Please clear it up.\n\n3. It is not clear when a random projection in embedding is applied, how this is dealt in training (fixed or randomly applying each train step)?\n",
            "summary_of_the_review": "Overall, the paper does not provide sufficient novelty for ICLR. It can be regarded as re-interpretation of the early work from Lovasz (1993). ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4400/Reviewer_siZ1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4400/Reviewer_siZ1"
        ]
    },
    {
        "id": "jUJYKkW_7T",
        "original": null,
        "number": 2,
        "cdate": 1666659636534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659636534,
        "tmdate": 1666659636534,
        "tddate": null,
        "forum": "p9zz7hLzH-4",
        "replyto": "p9zz7hLzH-4",
        "invitation": "ICLR.cc/2023/Conference/Paper4400/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose using using hitting times, commute times and resistive embeddings as features in a GNN. They show that using using this makes the network more powerful than 1-WL. They also show that this can be approximated by sketching techniques in large graphs. Then they show that empirically these networks do better on four datasets.",
            "strength_and_weaknesses": "The idea is neat, as is the counter example showing that the networks beat 1-WL. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written. ",
            "summary_of_the_review": "This paper presents a new bunch of features that can be used in GNN. It is well written. I recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4400/Reviewer_gd3D"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4400/Reviewer_gd3D"
        ]
    },
    {
        "id": "gqEqFeItgA",
        "original": null,
        "number": 3,
        "cdate": 1666808697740,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666808697740,
        "tmdate": 1666808697740,
        "tddate": null,
        "forum": "p9zz7hLzH-4",
        "replyto": "p9zz7hLzH-4",
        "invitation": "ICLR.cc/2023/Conference/Paper4400/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new MPNN variant that leverages affinity-based features. Specifically, they consider random walk-based statistics such as hitting time between nodes in order to augment edge features in the graph. To make this approach scalable to large datasets, the paper proposes to employ random projections using Gaussian matrices for these affinity-based features to remain of manageable dimensionality. The paper claims that expressivity beyond WL-1 can be achieved, which is a nice property, however there are existing GNN variants that achieve this as well, so the novelty of this property for a new GNN variant is limited.\nIn the experiments, improvements over the vanilla MPNN can be seen, while the method seems to perform reasonable across several datasets, in particular showing new SOTA results on the OGBG-PCQM4Mv1 benchmark which I think is interesting. At the same time, the OGB benchmark page states that v1 is outdated, and that v2 should be used. So it would be interesting to see, if the proposed method also performs competitive on the current v2 (where probably most new methods are tested on), compared to the older v1 version.\n\n\n\n\n",
            "strength_and_weaknesses": "Strengths:\n- Good contribution to scale random walk-based features to large-scale GNNs\n- Paper is clearly written.\n- Mixed-Ok experiments, but new SOTA on a large-scale benchmark (although it's an outdated version)\n\nWeaknesses:\n- Slightly out of touch with literature, amongst others e.g. Topological Graph Neural Networks have previously shown expressivity beyond 1-WL, plausibly could also solve the same example graph (Fig 1). This paper used a mini experiment on synthetic graphs to demonstrate that it can distinguish graphs that previous methods couldn't. This would be an easy way to credibly demonstrate the superiority of the proposed method against existing methods that improve expressivity. If this is not the case, then the claims about improving expressivty need to be toned down, e.g. it could be a nice property, but is it not a novel contribution to the GNN literature.\n- previous Random walk based methods are not discussed. At least refer to Random walk kernels, and ideally compare to previous GNN variant that leverage random walks, e.g. Random Walk Graph Neural Networks (Neurips 2020). \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. One thing that would drastically increase the exposition and understanding \"at a glance\" of the method would be a clear overview figure.\n\nTo my knowledge, the proposed method has some novelty.\n\nDue to late submission, I did not have time to check code for reproducibility.\n\n\nMinor details: \n3.3. \"where W are the non-negative edge weights\" --> state dimensionality of W\nL4.1 --> this looks weird. E.g. o(1) I suppose should be O(1)? I would reformulate in the standard way using bounds instead of equal signs (i'm not even sure if that is correct this way).\n5.3 why are there almost no baslines in OGBG-MOLPCBA? I would expect to see at least GIN, GAT, GCN.\n5.4 The number of layers is imho quite useless. If possible, it would be more informative to show runtime (at training or inference), memory costs, flops, CO2, #parameters, basically anything but #layers.",
            "summary_of_the_review": "\nOverall, the paper makes an interesting point that random-walk based features can be used to augment GNNs to improve performance. Random walks have been around in Graph ML for decades, e.g. with random walk kernels, which suffered from excessive runtimes. Therefore, the fact that random-walked based features show beneficial results on a large-scale benchmark is indeed promising.\nHowever, there are several issues with the current submission, that I believe the authors need to address before publication is warranted:\n\n- Either show (e.g. with synthetic experiment on graphs that are hard to distinguish) that the proposed method performs better than previous methods that increase expressivity (e.g. topology-aware GNN variants) to then more strongly claim that this method (of increased expressivity) is superior to existing works. Else, tone down the expressivity content, as simply going beyond 1-WL on its own is not a major novel finding at this time, albeit a nice side-property to mention.\n\n- Several related pieces of literature are not mentioned nor compared to: in terms of expressiveness, TopoGNN (ICLR 2022), re random walks: random walk kernels, and random walk GNNs (Neurips 2020). It would be interesting to see how random walk GNNs perform as a baseline. Also, GIN would be a stronger baseline than simply GCN. \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4400/Reviewer_cQm4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4400/Reviewer_cQm4"
        ]
    },
    {
        "id": "fvPNTZ4PvA",
        "original": null,
        "number": 4,
        "cdate": 1667106474952,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667106474952,
        "tmdate": 1667106474952,
        "tddate": null,
        "forum": "p9zz7hLzH-4",
        "replyto": "p9zz7hLzH-4",
        "invitation": "ICLR.cc/2023/Conference/Paper4400/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors propose a strategy to include scalar and vector features reliant on random walks to message passing GNNs. The authors the show that these features can be computed/ approximated efficiently and provide results of augmenting MPNNs with their affinity metrics on multiple graph datasets.",
            "strength_and_weaknesses": "**Strengths:**\n1. The paper is well written and easy to understand\n2. The idea to capture affinity without the use of anchor sets - and how to incorporate them into existing MPNNs is novel\n3. The authors provide principled techniques to efficiently compute the aforementioned affinity metrics\n\n**Weakness, corresponding questions and suggestions:**\n1. Looking at the results (for instance the PNA dataset) where the authors have multiple variant models - in the case of an unseen test - it is unclear how we pick the variant to use. That is, what should we looking at in the data to identify which variant to use?\n2. Lack of clarity in section 3.5 and how the performance is compared with what are referred to SPD based methods. Please explicitly elucidate them in the tables\n3. Add comparison with methods which include subgraph features as nodes features (e.g. Bouritsas, et al. 2020). Also in this case, provide an increase in performance by combining affinity metrics with such methods (i.e. increase in perf when adding affinity metrics to the above method compared to without it)\n4. Provide timing numbers associated with the efficient computation of affinity measures (Sec 4) for the different datasets.\n5. Given that most real world graphs are obtained from crawls on the web - and the data is extremely noisy - the use of random walk approaches can further enhance the effect of noise - it would be nice if the authors can provide an analysis (either qualitative/ quantitative) on the impact of noise in the graph (e.g. one way could be deliberately corrupt the adjacency structure of the graph and look at the impact on performance - all else remaining the same)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read. Currently code is not provided - so reproducibility is not checked.\nSome elements of the work are novel as listed in the strengths above.\n\n",
            "summary_of_the_review": "Currently the weaknesses slightly out weigh the strengths of the paper - but if the authors can address the weaknesses and questions listed above - I would be happy to increase my scores appropriately.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4400/Reviewer_46iR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4400/Reviewer_46iR"
        ]
    }
]