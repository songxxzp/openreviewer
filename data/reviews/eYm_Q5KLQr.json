[
    {
        "id": "vvOCl-MYx1B",
        "original": null,
        "number": 1,
        "cdate": 1666272529745,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666272529745,
        "tmdate": 1668766799894,
        "tddate": null,
        "forum": "eYm_Q5KLQr",
        "replyto": "eYm_Q5KLQr",
        "invitation": "ICLR.cc/2023/Conference/Paper4505/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper presents a theoretical framework for automatic curriculum generation in the setting of zero-sum games using the Q-value variance as an implicit signal of the learning progress toward the convergence criterion (Nash Equilibrium).\nThe theoretical derivation solves a problem of distribution matching for multi-agent RL, optimizing energy-based policies due to a statistical distance to an LSBRE optimal policy. The proposed method ZACL (Zero-sum Automatic Curriculum Learning) optimizes Jensen's gap between the Q-function of the individual policies and the average policy, which is equivalent to the value disagreement proposed for goal-directed problems in [1].\nThe variance of the value is approximated using an ensemble of value functions. Tasks are sampled from the value uncertainty distribution using a non-parametric particle-based sampler. The method uses the MAPPO algorithm to train the agents, with some tricks to improve exploration and control the replay buffer.\nThe experimental section evaluates the method in two environments; MPE and HnS. The results suggest performance gain and verify most of the claims in the paper.\n\n[1] Zhang, Y., Abbeel, P., & Pinto, L. (2020). Automatic curriculum learning through value disagreement. Advances in Neural Information Processing Systems, 33, 7648-7659.",
            "strength_and_weaknesses": "The authors haven\u2019t mentioned nor compared to Neural Auto-Curricula [1], which is also a method for automatic curriculum generation in zero-sum games. This is essential given the claim in the abstract that ZACL is the first method to tackle the problem.\nIn related work, the paper states that the method is faster than emergent behavior methods, this claim is not supported in the results.\nIn the paragraph after equation 2, the energy-based formulation should be approximate, not exact. Equation 8 the Q function needs +-.\nIn the experimental section, the comparison to baseline methods was only in the easy MPE environment, while we see only comparisons to MAPPO and ablations in HnS.\nIt would be useful to report the time of the training time when assuring using a single desktop machine as a major contribution (given the method needs 2.0 billion samples in figure 5).\n\n[1] Feng, X., Slumbers, O., Wan, Z., Liu, B., McAleer, S., Wen, Y., ... & Yang, Y. (2021). Neural auto-curricula in two-player zero-sum games. Advances in Neural Information Processing Systems, 34, 3504-3517.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the writing is clear. The paper should present the gist of the experiments in the main paper instead of deferring everything to the appendix. The theoretical derivation is correct and the presentation quality is good. The idea of using the value disagreement is not novel, the authors pointed that in the related work. The theoretical formulation for zero-sum games is somehow novel but not significant. No code submission to reproduce the results.\n",
            "summary_of_the_review": "Some of the claims of the paper weren\u2019t evaluated, like the ones mentioned in the comments.\nThe technical contributions are marginal and not significant, given the previous work on using the value disagreement [1].\nThe experimental contributions are marginal as well, where the evaluation is similar to the VARL [2] paper.\nGiven that, the paper is marginally below the acceptance threshold. \n\n[1] Zhang, Y., Abbeel, P., & Pinto, L. (2020). Automatic curriculum learning through value disagreement. Advances in Neural Information Processing Systems, 33, 7648-7659.\n\n[2] Chen, J., Zhang, Y., Xu, Y., Ma, H., Yang, H., Song, J., ... & Wu, Y. (2021). Variational Automatic Curriculum Learning for Sparse-Reward Cooperative Multi-Agent Problems. Advances in Neural Information Processing Systems, 34, 9681-9693.\n\nPost-rebuttal feedback\n------------------------------\nI have read the revised version and found few significant improvement. Thus, I'll keep my score.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4505/Reviewer_EQHM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4505/Reviewer_EQHM"
        ]
    },
    {
        "id": "Al9PZmG1VZt",
        "original": null,
        "number": 2,
        "cdate": 1666629338419,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629338419,
        "tmdate": 1666633610264,
        "tddate": null,
        "forum": "eYm_Q5KLQr",
        "replyto": "eYm_Q5KLQr",
        "invitation": "ICLR.cc/2023/Conference/Paper4505/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a method for choosing an auto curriculum strategy: a big buffer with all visited states is preserved, and the initial state for starting a trajectory is sampled from it proportionally to the variance of a number of value functions trained jointly with the policy.\n\nThe method is motivated by the lack of progression metric in 0-sum games, where the absolute value of the reward doesn't correspond to the progress in training where multiple agents are training in parallel.\n\nAuthors also provide a mathematical motivation of using the Q-value variance, under the assumption of using a MaxEnt policy with a fixed $\\lambda$ parameter.\n\nThe experiments consist of a simple Predator-Prey environment where the proposed method (ZACL) is compared against a number of 0-sum alternatives, and 4 rounds in the hide-and-seek (HnS) environment (where the goal is to converge to a Nash equilibrium as quickly as  possible), where MAPPO is used as a baseline.",
            "strength_and_weaknesses": "## Strengths\n\nThe results on hide and seek sound impressive, and the ones for Predator-Prey are evaluated against good baselines.\n\nThe proposal of using convergence time on HnS is an interesting idea, and a useful contribution on its own, as it could become a common tool for evaluating curriculum learning methods.\n\n## Weaknesses\n\nI don't find the argumentation behind concentrating on 0-sum (the rewards are not growing) convincing, as there is a lot of work that define progress in 0-sum games this way or another (apart from the cited PSRO-derived ones, eg. [1] claims size of a NE is a useful measure of progress). Furthermore, the mathematical motivation behind the method nor the method itself doesn't use the assumption of the game being 0-sum in any way, which makes the 0-sum part unintuitive.\n\nThe mathematical analysis provides a motivation for a significantly different method:\n\n1. One that uses Q functions, and not value functions. There is no explanation provided on why using V provides an accurate approximation to Q. I don't understand why authors couldn't use Q-function (with actions sampled from the policy) instead, following their motivation.\n2. A Maximum entropy one.\n3. One with a fixed entropy weight, which on its own is a relatively strong assumption in a Curriculum Learning setting, where we may want to decrease entropy penalty over the course of training as the policy gets better and better.\n\nThe comparison with MAPPO on HnS doesn't provide enough detail to be properly judged: it's not clear what machine is used in both cases: the phrasing of the paper makes me assume that for ZACL there is a single GPU, able to process 2B samples in a single batch, but that seems beyond the current hardware possibilities. It's not clear why MAPPO uses 9600 workers while ZACL 256 and why the batch sizes are significantly different between the methods. Finally, it would be useful to test one of the other SOTA methods (eg. from the ones tested on Predator-Prey) on this task for a reliable comparison.\n\nAs authors correctly cite, a very similar method for optimizing the tasks based on maximizing a Q-value variance was proposed in 2020.\n\n## Future work suggestion\n\nOne unaddressed issue of the maximizing-variance method is the variance coming from the environment. If there are tasks which inherently possess such variance (eg. there are two doors chosen randomly and if the agent chooses wrongly, it's not able to get to the goal), they will always be on top of the priority queue, even when with the optimal policy of the agent, wasting environment interaction on tasks where nothing else could be improved.\n\n[1]: Czarnecki et al. [Real World Games Look Like Spinning Tops](https://arxiv.org/abs/2004.09468)",
            "clarity,_quality,_novelty_and_reproducibility": "I find the work relatively clear (with the issues expanded in the previous section), and of high quality.\n\nI find the novelty of the method itself limited, but I find the proposed task to be of high experimental novelty.\n\nI find the first experiment reproducible, the second one: not so much (again, details in the previous section).",
            "summary_of_the_review": "Overall, the proposed method is reasonable, but with little novelty. I find the authors' claim on \"complete theoretical derivation\" of the work to be an overstatement, given that the empirically evaluated method differs significantly from the one being derived. Similarly, I don't find the 0-sum motivation matching the content of the paper.\n\nI find the proposed evaluation task very interesting, but provided results are not detailed enough to be confident that it provides a useful signal to evaluate CL methods on.\n\nedit: after the usage of HnS task was pointed out by another reviewer, I decided to decrease the score from 5 to 3, given the (unchanged) review above, claiming the task is the novel part of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4505/Reviewer_5rst"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4505/Reviewer_5rst"
        ]
    },
    {
        "id": "qvgW-yCb2wf",
        "original": null,
        "number": 3,
        "cdate": 1666681882661,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681882661,
        "tmdate": 1666681882661,
        "tddate": null,
        "forum": "eYm_Q5KLQr",
        "replyto": "eYm_Q5KLQr",
        "invitation": "ICLR.cc/2023/Conference/Paper4505/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper ZACL, a curriculum method for self play, where the policy is incentivized by the variance of the Q function",
            "strength_and_weaknesses": "I liked that this paper was able to replicate and improve on the results from the OpenAI hide and seek paper with much less compute. However, there are a number of issues with this paper. \nFirst, I don\u2019t think the motivation is there. If you are already doing self play, why do you need a curriculum? Usually the idea is that you need to use a curriculum to scale up to hard tasks, but the point of the OpenAI paper was to show that self play is an automatic curriculum. \nSelf play isn\u2019t guaranteed to converge to Nash in two-player zero-sum games, and MAPPO is a cooperative RL algorithm\nI don\u2019t understand why PSRO is exploitable by a best response. By design, upon convergence PSRO is not exploitable by a best response. How many iterations did you train it? Could you show approximate exploitability over time for all algorithms? Also, comparisons to CFR-based methods such as ESCHER would help the evaluation. \n",
            "clarity,_quality,_novelty_and_reproducibility": "I thought the paper was well-written. The novelty is fine: to my knowledge I don't know of any papers looking at curriculum in two-player zero-sum games. The algorithm seems reproducible. ",
            "summary_of_the_review": "Overall, I think the method is not well-motivated, and the experiments do not convince me that it indeed (1) finds a NE, and (2) converges faster than baselines. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4505/Reviewer_L9ZZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4505/Reviewer_L9ZZ"
        ]
    }
]