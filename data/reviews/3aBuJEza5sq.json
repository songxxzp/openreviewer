[
    {
        "id": "l5ZMyUIu2E",
        "original": null,
        "number": 1,
        "cdate": 1666604371730,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604371730,
        "tmdate": 1666604371730,
        "tddate": null,
        "forum": "3aBuJEza5sq",
        "replyto": "3aBuJEza5sq",
        "invitation": "ICLR.cc/2023/Conference/Paper860/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work introduces FedTHE(+), a way to allow for test-time personalization of federated learning models. FedTHE works by introducing and training a scalar that interpolates global and local classifier predictions. This scalar is tuned at test time by minimising the entropy of the interpolated logits while being regularised by an alignment loss between the features. The latter giudes the scalar according to the difference of the representation of the test point to the local and global feature means; if the representation is closer to the global one it increases the scalar otherwise it decreases it. To balance the two losses, the authors propose to use the cosine similarity between the local and global logits as a \u201cweight\u201d that linearly interpolates the two. After this specific scalar is optimized, the other parameters of the network are also fine-tuned using an entropy-minimisation method, which constitutes the FedTHE+. \n",
            "strength_and_weaknesses": "Strengths\n- Extensive evaluation and ablation studies\n- Hyperparameter tuning for the baselines as well\n- Simple method and computationally efficient to apply, which is an important aspect for FL\n- Good results\n\nWeaknesses\n- A lot of moving parts\n- The motivation behind some choices is a bit unclear",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is mostly clear and well written. One minor comment is that there seems to be a bit excessive use of \\vspace that makes the text look cramped, so I would suggest for some things to be moved to the appendix so that the paper looks better. Apart from that, another minor comment is that MEMO (G) needs to be bold at Table 1 for Dir(0.1), OoC local test and Dir(1.0) OoC local test. \n\nWhile this work does have a lot of moving parts, the authors do extensive experimental evaluation and ablation studies to show the effects of each one. Besides that, the authors also perform an extensive hyperparameter optimization procedure for the baselines as well. I believe that there is enough information in this work for reproducibility. \n\nAs far as I am aware, FedTHE is novel. Furthermore, its simplicity coupled with the extensive distribution shifts considered, makes it quite appealing for FL type of scenarios. One nitpick on the loss is that the \u201cfeature alignment\u201d is a bit misleading, as the features are frozen. Something like adapting the scalar according to the feature distance would be more appropriate. The similarity-guided loss weighting is also an interesting and intuitive contribution. FedTHE+ is not novel per se, as the authors just use existing test-time adaptation methods (i.e., MEMO) to fine-tune the model. One catch for FedTHE though is that it needs to send summary statistics of the client data to the server ($h_l$), which I believe should be discussed in the context of the baselines (e.g., do baselines employ such summary statistics?) and in the context of the privacy / security aspects of FL.\n\n",
            "summary_of_the_review": "This is an interesting work and discusses a relatively simple method FedTHE and its extension FedTHE+. FedTHE is novel and the experimental evaluation is quite extensive, including multiple distribution shift scenarios, with good results and the authors do extensive ablations to show the effects of each of their choices in FedTHE, which is appreciated. For this reason, I am recommending acceptance of this work. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper860/Reviewer_129X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper860/Reviewer_129X"
        ]
    },
    {
        "id": "Ig20LuwOYfl",
        "original": null,
        "number": 2,
        "cdate": 1666707248279,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666707248279,
        "tmdate": 1666707248279,
        "tddate": null,
        "forum": "3aBuJEza5sq",
        "replyto": "3aBuJEza5sq",
        "invitation": "ICLR.cc/2023/Conference/Paper860/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the OOD generalization problem in the FL setting. Specifically, the author presents to use a test-time-training based strategy to ensemble the global and local prediction heads. The object function use in the TTT phase consists of two parts (1) the entropy of the predictive distribution. (2) feature space constrains. The empirical results have shown significant improvements over various baseline models.",
            "strength_and_weaknesses": "# Pros\n1. The idea of using test-time-training to improve the model robustnees in FL is novel and effective.\n2. The empirical results are comprehensive and can validate the superiority of the proposed method.\n# Cons\n1. Acctually, this paper is not directly for OOD generalization, i.e., the local and global training is still based on standard FedAVG. There are some previous work [1,2] focus on optimizing the worst-case local loss needed to be discussed. In addition, is it possible to train different prediction heads by distributionnaly robust optimization?\n2. There are many critical hyperparameters need to be tuned.\n# Comments\n1. It seems that this paper is a re-submission of NIPS, can the author provide the critical difference between the current and previous version?\n# Refs\n[1] Distributionally Robust Federated Averaging. \n[2] DRFLM: Distributionally Robust Federated Learning with Inter-client Noise via Local Mixup",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "See above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper860/Reviewer_bhMe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper860/Reviewer_bhMe"
        ]
    },
    {
        "id": "wi-mAAIpSt",
        "original": null,
        "number": 3,
        "cdate": 1666800980758,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666800980758,
        "tmdate": 1666800980758,
        "tddate": null,
        "forum": "3aBuJEza5sq",
        "replyto": "3aBuJEza5sq",
        "invitation": "ICLR.cc/2023/Conference/Paper860/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the federated learning problem under the test-time distribution shift. Federated learning is a distributed learning paradigm with a server and many clients such that each client has its private data, and the server aggregates the trained model from clients without revealing any private information. Many previous works focused on the performance of the server model, but recently personalization on FL has been studied. However, most works assume distribution matching between training and testing, whereas, in practice, the class distribution can change, unseen classes can appear, and common corruptions or natural distribution shifts can happen. Therefore, it is crucial to design robust FL algorithms for dealing with such distribution shifts. In this work, the authors propose Federated Test-time Head Ensemble plus tuning (FedTHE+), which personalizes FL models robustly to the distribution shifts. They evaluate FedTHE+ with various neural architectures on CIFAR10 and ImageNet with diverse test distributions. ",
            "strength_and_weaknesses": "Strengths\n\n-This paper combines test time training and federated learning, which is an interesting direction.\n\n-The authors provide a benchmark for FL robustness\n\n-The proposed algorithm, FedTHE+, outperforms baselines in most of the scenarios.\n\n\nWeakness\n\n- The authors do not clearly explain why each step of the algorithm is required and for what reason the algorithm can be good for FL. \n\n- FedTHE, the proposed algorithm, consists of many parts. Many parts have synergy between them, and the synergy produces good performances. However, such a complicated structure makes it difficult to understand which part is the key to dealing with distribution shifts. \n\n- Since this paper has too much content with a page limit, many details are omitted in the main body and deferred to the appendix. So it took a lot of work to read.",
            "clarity,_quality,_novelty_and_reproducibility": "Due to the page limit, the algorithm is not precisely described in the main text. Many detailed steps are deferred to the appendix. \n\nThey provide the source code, although I have not tried to run the code. ",
            "summary_of_the_review": "This paper suggests an interesting direction, FL robustness. However, this paper does not provide a good intuition for the topic.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper860/Reviewer_wWXU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper860/Reviewer_wWXU"
        ]
    },
    {
        "id": "SDMldC4Ffwo",
        "original": null,
        "number": 4,
        "cdate": 1667540121711,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667540121711,
        "tmdate": 1668882690537,
        "tddate": null,
        "forum": "3aBuJEza5sq",
        "replyto": "3aBuJEza5sq",
        "invitation": "ICLR.cc/2023/Conference/Paper860/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a new personalization method in federated learning: Federated Test-time\nHead Ensemble plus tuning (FedTHE+). FedTHE trained a global feature extractor, a global head and local heads for classification, and during the personalization time, a scalar e is learnt to interpolate between predictions by global and local heads. \n",
            "strength_and_weaknesses": "Distribution shift/robust personalization is an important problem that only draws attention recently. \n\nThe proposed method of interpolation between local and global heads sounds intuitive. \n\nI appreciate the extensive experiments, but also have some concerns on clarity and reproducibility listed below. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Is this method stateful, or stateless (A Field Guide to Federated Optimization Section 7.5 https://arxiv.org/abs/2107.06917)? Is the local head w^l maintained on each client? How many clients in experiments, and is there client sampling for each round?\n\nHow is h^{history} computed? Is it intra-client, or inter-client? Why is moving average used, which makes it depending on the order of \u201chistory\u201d?\n\nThough I appreciate the extensive results, table 2 is a bit hard to interpret. Maybe I missed it, I would personally see more analysis on e, which is the key of the proposed method. For example, what is the distribution of e for various clients, and in various distribution settings? Why did the proposed method work for various shifts while the previous methods fail. \n\nBoth the proposed FedTHE+ and the dataset in section 5 sound somewhat complicated. Would the authors release both dataset and code for reproducibility? \n",
            "summary_of_the_review": "I would be happy to discuss the clarification questions with the authors. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper860/Reviewer_JEHY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper860/Reviewer_JEHY"
        ]
    }
]