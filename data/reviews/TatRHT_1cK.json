[
    {
        "id": "0bxJHIbQybg",
        "original": null,
        "number": 1,
        "cdate": 1666532710533,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666532710533,
        "tmdate": 1669211515547,
        "tddate": null,
        "forum": "TatRHT_1cK",
        "replyto": "TatRHT_1cK",
        "invitation": "ICLR.cc/2023/Conference/Paper3116/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper investigates the reproduction of training examples from pre-trained language models.\nThe amount of memorization is measured across 1) different model sizes, 2) the number of times an example is duplicated in the training data, 3) the length of prompt given as input.\nIt is concluded that larger models, more duplication and longer prompt all increase the likelihood of reproducing the training example.\n",
            "strength_and_weaknesses": "Strengths:\n\n* Investigation of the foundational language models is an important topic.\n* The experiments are systematic and more thorough than previous work.\n\nWeaknesses: \n\n* The same properties of pre-trained language models have all been investigated and demonstrated by previous work. There don't seem to be any novel hypotheses or findings.\n* Some of the experiments have questionable setups (more details below).",
            "clarity,_quality,_novelty_and_reproducibility": "The conclusion of the paper is that larger models, more duplication and longer prompts all increase the likelihood of reproducing the training example. All of these properties have been investigated before by previous work, and several of these works are also cited in this paper. The experiments are somewhat more thorough than previous work (although not without issues), but neither the hypotheses nor the findings are novel. Therefore the novelty of the paper seems to be limited to scaling up previous diagnostic experiments.\n\nOne of the contributions highlighted in the introduction is the finding that GPT-J memorizes at least 1% of its dataset, compared to a previous lower measurement for GPT-2.\nHowever, these are two different models, and more importantly they have been trained on different datasets. The experiments show that the number of duplicates present in the training set leads to more examples being memorized. So it seems that the simplest explanation to GPT-J memorizing more than GPT-2 would be that the training data for GPT-J contained more duplicates.\n\nThe main set of results are presented on a dataset that controls both the distribution of duplication and the distribution of length. This would be suitable for presenting results at different duplication and length levels, but the results with different model size or prompt length become less meaningful due to the artificial nature of the distributions. Furthermore, because the data is controlled for two distributions at the same time, it is unclear how these two distributions interact and what kind of artefacts that may create. Luckily, results on a random distribution are also reported as auxiliary experiments.\n\nGPT-2 is reported as a baseline on the same dataset of examples from the training set of GPT-J. However, it is a different model trained on a different dataset. We also don't know what percentage of those particular examples were present in the GPT-2 training set and how many times they were duplicated there, so it's unclear what the baseline is meant to show.\n\nExperiments with other models (e.g. T5) find lower rates of memorization depending on the duplication counts. But these experiments are performed on the same data that was constructed specifically from the duplicated examples of GPT-J? It seems that result is explained simply by those examples not being duplicated in the T5 training data.\n\nWhen experimenting with different prompt lengths, is seems different sets of examples are chosen so that they would match the correct prompt length. That would mean the results are on different texts, and the prompt length is not the only thing that is varied. There may be other properties of the texts that are correlated with their length, particularly if that dataset also favours highly duplicated examples.\n\nThe practical applicability of these experiments needs some more discussion. First, it's unclear what would be the realistic scenario where an attacker would know 100+ exact tokens from the training set but not the next token. The paper also discusses that the results are skewed by correctly generating long sequences of whitespace, which doesn't seem like a practical case of memorization.\n\nIt is said that most models use random sampling to generate, but only greedy sampling is investigated in this paper. It seems results with random sampling would then have more practical usefulness. The choice of restricting to greedy sampling could at least be motivated more.",
            "summary_of_the_review": "The investigated area is important and the experiments are more thorough than in previous work.\nHowever, the novelty of the experiments and the findings is limited and it is not clear that all the experiments show what they are claiming to show.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3116/Reviewer_6k5t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3116/Reviewer_6k5t"
        ]
    },
    {
        "id": "LjntxbGxna_",
        "original": null,
        "number": 2,
        "cdate": 1666594201732,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594201732,
        "tmdate": 1666594201732,
        "tddate": null,
        "forum": "TatRHT_1cK",
        "replyto": "TatRHT_1cK",
        "invitation": "ICLR.cc/2023/Conference/Paper3116/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper extensively studies the memorization of training data in neural language models. First, it defines memorization as \"given a prefix, the model can generation a continuation that exists in the training data\". Then, it focuses on three aspects that affect a model's memorization rate: duplication times, length of prefix and model scales. It draws three main conclusions from a series experiments: (1) bigger models memorize more; (2) data that has more duplicates are memorized more; (3) longer prefixes can more easily extract memorized text. Furthermore, authors extend their experiments to other types of language models and models that are trained on deduplicated training data.",
            "strength_and_weaknesses": "**Strength**\n1. Paper is very well written and easy to understand.\n2. The experiments conducted in this paper are quite extensive and results are convincing.\n3. I appreciate that authors perform experiments on recently released models, e.g. OPT, although its training data is not fully transparent to the public.\n\n**Weakness**\n1. Duplication is a very important factor in the paper when measuring memorization, but I didn't find how a sequence of length l is obtained /extracted from the training data. Did you consider all the substrings of length l in the training data? Did the segmentation you used to obtain substrings match that of those deduplication methods used for pre-training (section 5.2)? Are there overlap between sequences of length 100 and sequences of length 200?\n2. As discussed in the paper, the biased sampling method normalized by length and duplication times is more biased towards duplicated sequences, although there are experiments on uniformly sampled subsets, I think doing experiments on a subsets without any duplications is necessary. Again, duplication is dependent on how the substring is defined, which is worth discussing.\n3. The definition of memorization is pretty strict in this paper, but in reality a paraphrased continuation which might not exactly exist in training corpus can still leak data. I think studying the generation in a relaxed setting is more interesting, e.g. consider using paraphrased prefixes or perturb the prefixes, as considering the paraphrased continuation can be hard to quantify the extractability.\n4. Although authors provide many qualitative memorization examples, it's still hard for me to have a clear idea what types of sequences are more easily extracted, I think a dedicated section",
            "clarity,_quality,_novelty_and_reproducibility": "This paper provides insights to pre-training language models: model can memorize even when using deduplicated training data.\nAs pointed in the weakness section, it's not clear how a substring is defined in this paper and how this relates to deduplication method. Thus, I find this paper lacking guidance or insights on how we can reduce repetition, which makes this paper not a huge significant contribution compared to previous papers that also reveal similar observations.\n\nA missing reference: Memorization Without Overfitting: Analyzing the\nTraining Dynamics of Large Language Models, as the other important dimension when measuring memorization is the order of training data presented to the language models.",
            "summary_of_the_review": "The empirical results presents in this paper provide insights to pre-training language models: model can memorize even when using deduplicated training data. But it lacks guidance or insights on how to reduce memorization and the explicit complications brought by memorization. Thus I feel that this paper has marginal contributions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3116/Reviewer_CQAU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3116/Reviewer_CQAU"
        ]
    },
    {
        "id": "XHJ74Ybfi_",
        "original": null,
        "number": 3,
        "cdate": 1666619731374,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619731374,
        "tmdate": 1666619731374,
        "tddate": null,
        "forum": "TatRHT_1cK",
        "replyto": "TatRHT_1cK",
        "invitation": "ICLR.cc/2023/Conference/Paper3116/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies memorization in large language models and tries to quantify memorization rates depending on prefix length, duplication rate in training data and model size. The authors define an example memorised (given prefix of some length) if greedy decoding conditioned on this prefix reproduces the training example verbatim. This allows to estimate memorization rate in a more thorough manner differently from e.g. extraction attacks done in previous work. Experiments are conducted for different datasets (Pile, C4), models (GPT, T5, etc) and settings (e.g., with or without deduplication). The authors find that memorization rate is log-linear with respect to model size, prefix length and duplication rate in training.",
            "strength_and_weaknesses": "Strengths:\n\n1) The problem is important\n\n2) The methodology is solid, the experiments are thorough\n\n3) The definition of memorization is reasonable and makes a lot of sense when dealing with language models: the paper considers generated examples and not e.g. token-level accuracy or loss\n\nWeaknesses:\n\nThe results are expected and some of the trends in one way or another were reported in previous work. \nHowever, I believe that overall the paper gives a solid ground for future work on memorization.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Some comments:\n\nIntro, paragraph 3: \u201cWhile McCoy et al. (2021) broadly study the extent to which language models memorize, their focus is on how to avoid the problem and ensure novelty of model outputs, rather than on studying model risk through identifying the maximal amount of data memorization.\u201d - I believe this description of the work by McCoy is not accurate. They focus on evaluating novelty rather than ensuring it.\n\nTypos:\n\nAbstract: \u201cmodels continues\u201d\n\nSection 3.2, paragraph 1: \u201cthroughput\u201d",
            "summary_of_the_review": "The paper measures memorization for NLMs by looking at how often output of greedy decoding given prefix reproduces training examples verbatim. The experiments show log-linear memorization rate with respect to model size, prefix length and duplication rate in training. The results are not surprising, but given solid methodology and thorough experiments I believe overall the paper is of good quality and can give a solid ground for future work on memorisation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3116/Reviewer_G66g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3116/Reviewer_G66g"
        ]
    },
    {
        "id": "yDrjElUUT-",
        "original": null,
        "number": 4,
        "cdate": 1666651021557,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651021557,
        "tmdate": 1666651021557,
        "tddate": null,
        "forum": "TatRHT_1cK",
        "replyto": "TatRHT_1cK",
        "invitation": "ICLR.cc/2023/Conference/Paper3116/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a simple method to measure how well LLMs memorize training data. The authors found that memorization increases when 1) the model size is increased 2) the sample size is increased 3) the context of the example is increased. \nConsidering the level of memorization of T5 and comparing it with causal language models of similar size is the most interesting part of this paper for me. ",
            "strength_and_weaknesses": "Strength:It's an interesting and intuitive idea in the paperI found the paper to be easy to read and understand.\u00a0Very\u00a0interesting results and analysis.\n\nWeaknesses:\n- Limited novelty",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and reproducible. \nIt has limited novelty. ",
            "summary_of_the_review": "This is an important topic, and the paper is interesting. Although the idea is not that novel, I think this is a good addition to the conference. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3116/Reviewer_vuN8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3116/Reviewer_vuN8"
        ]
    }
]