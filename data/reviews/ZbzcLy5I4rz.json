[
    {
        "id": "ABg93W0PwQ",
        "original": null,
        "number": 1,
        "cdate": 1666311674417,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666311674417,
        "tmdate": 1666311674417,
        "tddate": null,
        "forum": "ZbzcLy5I4rz",
        "replyto": "ZbzcLy5I4rz",
        "invitation": "ICLR.cc/2023/Conference/Paper1411/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper enhances two previous algorithms for non-convex finite sum minimization, SARAH and L-SVRG, by adding a diagonal preconditioner from Jahani et al. The authors prove that the preconditioned methods, which they call Scaled SARAH and Scaled L-SVRG, both converge. The convergence bounds are better than those of previous algorithms. \nThe authors then show the results of their methods on several LibSVM datasets.",
            "strength_and_weaknesses": "The paper combines two well known algorithms with a previous preconditioner. The key theoretical result (which I did not check in detail) is that the preconditioned algorithms have a better convergence bound than previous algorithms such as AdaGrad, Adam etc.\n\nPoints to improve:\n1. Please compare the convergence bounds of the Scaled algorithms with the unscaled versions. Also for Adagrad and Adam, please do point out the specific theorems in the papers where the epsilon^{-4} bounds are shown.\n2. Some discussion of the importance of these bounds should be included. While the given algorithms are theoretically much more efficient than AdaGrad/Adam, the difference is not visible in the experimental results.\n3. The experiments are on very small datasets. The big advantage of Adagrad and Adam is that they run on very large datasets, it would be definitely desirable to see the performance of Scaled SARAH and Scaled L-SVRG on some moderate sized datasets.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written quite clearly.\nThe novelty is mainly in the convergence bounds --- the algorithms themselves combine a previously known technique with two known algorithms.",
            "summary_of_the_review": "I would like to see the weaknesses in the paper corrected before it is ready for publication. As of now, it makes strong claims, but they are not backed up by enough discussion of their significance, or by experimental results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1411/Reviewer_QfWd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1411/Reviewer_QfWd"
        ]
    },
    {
        "id": "GdouHgXNFtE",
        "original": null,
        "number": 2,
        "cdate": 1666519549541,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666519549541,
        "tmdate": 1666519549541,
        "tddate": null,
        "forum": "ZbzcLy5I4rz",
        "replyto": "ZbzcLy5I4rz",
        "invitation": "ICLR.cc/2023/Conference/Paper1411/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a preconditioned update to stochastic gradient methods\uff0c based upon Hutchinson\u2019s approach to approximating the diagonal of the Hessian. ",
            "strength_and_weaknesses": "Strength: introducing a new method to improve the convergence of the stochastic method \n\nWeaknesses: (1) The theoretical complexity is only for the smooth objective function. \n\n                       (2) The function under the PL condition is equivalent to the strongly convex function, but the linear convergence   from Nesterov's method is classical.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper presents very clearly. But the originality is limited, which still needs to be strengthened. ",
            "summary_of_the_review": "The paper proposes a preconditioned update to stochastic gradient methods, but the theoretical results still need to be strengthened. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1411/Reviewer_ayos"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1411/Reviewer_ayos"
        ]
    },
    {
        "id": "IdPvdn4HlO",
        "original": null,
        "number": 3,
        "cdate": 1666563013499,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666563013499,
        "tmdate": 1666563013499,
        "tddate": null,
        "forum": "ZbzcLy5I4rz",
        "replyto": "ZbzcLy5I4rz",
        "invitation": "ICLR.cc/2023/Conference/Paper1411/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper provides a generic pre-conditioning method for non-convex finite-sum minimization, which is an important task in machine learning. It combines this technique with two mainstream algorithms for finite-sum minimization, namely SARAH and L-SVRG. They provide both theoretical analysis for these algorithms, showing it achieves 1/eps^2 convergence rate under smoothness condition and log(1/eps) convergence rate under PL condition. They also corroborate the favorable performance of this pre-conditioning technique in practice by comparing it with Adam, another algorithm that incorporates second order information adaptively.",
            "strength_and_weaknesses": "Strength:\n1) I think the paper is well-presented. The authors explain the motivation and idea pretty clearly, thus making it easy to follow. The author also did a thorough literature review.\n\n2) I think the problem they study is interesting and of important: i.e. study the effect of preconditioning, or incorporating little second-order information for finite-sum minimization-type methods. This is an important topic.\n\n3)The empirical performance show favorable improvement over other SOTA optimizer like Adam, which demonstrates potential impact of this preconditioning idea to other mainstream first-order optimizers in training neural networks.\n\nWeakness:\n1) The main confusion I have is I don't think the theoretical results in this paper corroborate the main argument of the paper, i.e. pre-conditioning technique is useful in practice. In particular, the theoretical analysis with preconditioning all just suffer from an additional L/alpha in their runtimes (compared with standard SARAH and SVRG), which is basically the condition number in worst case of the approximate diagonal matrix you construct, right? This is not a very interesting theoretical result in my mind. In particular, it doesn't explain why you choose such an estimator for diagonal rescaling at all. I believe for any arbitrary diagonal preconditioning bounded by condition number L/alpha, you can prove the same result. Please correct me if I am wrong.\n\n2) Following the above point, I feel the claim in the introduction about showing an eps^{-2} theoretical result, which improve over eps^{-4} is a bit misleading. Really you probably want to compare with un-preconditioned SARAH and other methods. The eps^{-4} result is for general case not finite sum minimization, and they also don't scale additionally with the number of function n, right? (please correct me if I am mistaken)\n\n2) In its current way written, it is hard to compare this paper's theoretical result with some prior art. In particular in these settings a natural metric is the gradient complexity so adding a remark for Theorem 3.1, 3.2 etc. on that would be helpful. Also, it seems Theorem 3.1 right now doesn't require an additive n in the final bound? Is that possible, or in other words would this violate some lower bound?",
            "clarity,_quality,_novelty_and_reproducibility": "I think the clarity and quality of writing is good. See above for my additional comments.",
            "summary_of_the_review": "I like the main idea of the paper a lot. And I think the experiment performance looks quite interesting too.\n\nI just feel theoretical analysis could probably be improved, or if not the main contributions and comparison with prior art shall be made clearer in the paper. Otherwise the theory part of the paper really doesn't feel like making much point in the current form.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1411/Reviewer_8yK7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1411/Reviewer_8yK7"
        ]
    }
]