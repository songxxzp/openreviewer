[
    {
        "id": "YyWWMgY10XW",
        "original": null,
        "number": 1,
        "cdate": 1666146885279,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666146885279,
        "tmdate": 1666146885279,
        "tddate": null,
        "forum": "2Fb-h04mt5I",
        "replyto": "2Fb-h04mt5I",
        "invitation": "ICLR.cc/2023/Conference/Paper3776/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper advocates to use robust kernel density estimation in the self-attention mechanism to mitigate the issue of data contamination. In particular, the idea is to down-weight the weight of bad samples in the estimation process. The authors conduct empirical experiments to demonstrate the effectiveness of the proposal.",
            "strength_and_weaknesses": "Strength:\n1. The problem and issues to address are well motivated.\n\nWeakness:\n1. The paper appears to be largely overlapped with the existing literature of Kim and Scott (2012), which is intensively cited in the manuscript.\n2. The robust self-attention mechanism does not have strong theoretical support ",
            "clarity,_quality,_novelty_and_reproducibility": "I fail to identify the original of this manuscript over existing literature.",
            "summary_of_the_review": "I fail to identify the technical contribution of this manuscript over existing literature.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3776/Reviewer_cLTJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3776/Reviewer_cLTJ"
        ]
    },
    {
        "id": "e3VUbMykV-x",
        "original": null,
        "number": 2,
        "cdate": 1666519213856,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666519213856,
        "tmdate": 1670390022530,
        "tddate": null,
        "forum": "2Fb-h04mt5I",
        "replyto": "2Fb-h04mt5I",
        "invitation": "ICLR.cc/2023/Conference/Paper3776/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a robust variant of the standard Transformer named Transformer-RKDE to improve the robustness of data with contaminated samples. The idea is based on the interpretation that the self-attention in transformer can be viewed as a non-parametric estimator based on the kernel density estimation (KDE). The authors re-interpret KDE as a regression in the Reproducing Kernel Hilbert Space (RKHS) and indicate that the vanilla KDE is sensitive to data corruption. Therefore, the authors introduce a robust version of the KDE and use the robust KDEs to construct the robust self-attention mechanism for the Transformer.",
            "strength_and_weaknesses": "Strengths:\n+ The idea seems interesting. Starting from the inherent problem of self-attention (the KDE is not robust to the outliers), the authors introduce RKDE, the robust version of the KDE, to construct robust attention in transformer.\n+ The experiments verify the effectiveness of the proposed model in both language modeling and image classification tasks.\n \nWeaknesses:\n+ Lack of quantitative analysis on the efficiency of the proposed model. Since additional weight calculation requires iteration, I wonder whether it will affect the efficiency of the algorithm.\n+ Because the experimental verification is mainly designed for different adversarial attacks, I wonder how the final effect of the proposed method is compared with other models designed for adversarial attacks. This can better show the significance of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear and well-organized. The contributions are significant and somewhat new. The authors provide the code to reproduce their results. ",
            "summary_of_the_review": "Overall, the innovation is somewhat interesting to me. However, I have some concerns about its efficiency and practical application value.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3776/Reviewer_NtCQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3776/Reviewer_NtCQ"
        ]
    },
    {
        "id": "-QnnTmWvFt6",
        "original": null,
        "number": 3,
        "cdate": 1666540103396,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666540103396,
        "tmdate": 1666540103396,
        "tddate": null,
        "forum": "2Fb-h04mt5I",
        "replyto": "2Fb-h04mt5I",
        "invitation": "ICLR.cc/2023/Conference/Paper3776/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authoor leverage the robust kernel density estimation (RKDE) in the self-attention mechanism, to alleviate the issue of the contamination of data by down-weighting the weight of bad samples in the estimation process.  Empirical results on language modeling and image classification tasks have demonstrated the effectiveness of this approach. The motivation is clear, but still have few concerns.",
            "strength_and_weaknesses": "strengths:\n1. propose a novel robust transformer framework which replace the dotproduct attention by an attention arising from the robust kernel density estimators (RKDE) associated with the robust kernel regression problem.\n2. the proposed method only requires computing an extra set of weights.\n\nweaknesses:\n1. My most critical concern is regarding the review of the related works. Several robust transformer methods have been proposed, I suggest that the author should review those works in the related works, please see an incomplete list here, Mao X, Qi G, Chen Y, et al. Towards robust vision transformer[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 12042-12051. Li N, Liu Y, Wu Y, et al. Robutrans: A robust transformer-based text-to-speech model[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(05): 8228-8235. Liu J, Singhal T, Blessing L T M, et al. Crisisbert: a robust transformer for crisis classification and contextual crisis embedding[C]//Proceedings of the 32nd ACM Conference on Hypertext and Social Media. 2021: 133-141. Yang J, Gupta A, Upadhyay S, et al. TableFormer: Robust Transformer Modeling for Table-Text Encoding[J]. arXiv preprint arXiv:2203.00274, 2022.\n2. The second concern is the baselines. The current baseline methods seem insufficient. Why does the author choose these methods as the baseline? Why does comparison with these methods can prove the effectiveness of the proposed method? I think the authors should select some similar methods (see weaknesses 1) as a baseline to prove the effectiveness of the proposed method.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this paper is good, the problem is clearly stated, but the novelty seems limited.",
            "summary_of_the_review": "Motivation of this paper is clear, but the reviwe of the related works and baselines are insufficient.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3776/Reviewer_N1gC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3776/Reviewer_N1gC"
        ]
    },
    {
        "id": "CVZAO-xREhK",
        "original": null,
        "number": 4,
        "cdate": 1666746077845,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666746077845,
        "tmdate": 1666746077845,
        "tddate": null,
        "forum": "2Fb-h04mt5I",
        "replyto": "2Fb-h04mt5I",
        "invitation": "ICLR.cc/2023/Conference/Paper3776/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a robustified transformer model motivated by the robust kernel density estimation. The proposed model alleviates the influence of bad data (e.g., outliers). The empirical studies on NLP and CV tasks demonstrate the effectiveness of the proposed methods.",
            "strength_and_weaknesses": "Strength:\nThe proposed method enjoys promising numerical performance when the noise level is high.\n\nWeakness:\n\n1.  The writing could be significantly improved. For example, equations (5-11) in subsection 2.2 on page 3 are almost analogous to equations (3-8) in Nguyen et al. (2022). It may not necessary to use a whole page to restate the existing result in the literature. \nMoreover, in Aglrotihm 1 on page 6, a) the $\\hat{p}_{\\mathrm{robust}}^{(k)}(k)$ is used without declaration, in the input\nline the initial weights of $\\omega\\_{\\mathrm{marginal}}^{(0)}$ and $\\omega\\_{\\mathrm{joint}}^{(0)}$  are not used, and I don't find the definition of $\\omega_j^{\\mathrm{joint}}$.\n\n2. The bond between theoretical motivation and real implementation is kind of weak. For example, the theoretical analysis requires the solve for $\\hat{p}_{\\mathrm{robust}}^{(k)}$ via an iterative method (e.g., KIRWLS). However, when checking the code, the RBFMultiHeadAttn class in Line 391 of src/mem_transformer.py, I don't find the for loop in the forward function and it seems the attention \nbeing computed via only a single path...  If it is the case, the $\\hat{p}\\_{\\mathrm{robust}}^{(k)}$ can be far away from its underlying *optimal* value. If it is not the case, I suggest authors report the computing cost of the proposed method since the multi-round of the iterative method can increase the computation time by a significant amount. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The current presentation could be improved.\n\nQuality: Based on the current presentation, the overall quality doesn't meet the bar of the top ML conference like ICLR.\n\nNovelty: This paper considered an interesting attention variant motivated by robust kernel estimation.\n\nReproducibility: The code is provided. ",
            "summary_of_the_review": "This paper proposes a variant of Transformer motivated by robust kernel estimation. In particular, a decayed weight $\\omega$ is computed to minimize the influence of the outliers, and in high noise settings, the proposed method reaches promising numerical performance.\n\nMy major concern is the writing of the paper which could be significantly improved. I also suggest authors to further highlight the computation cost of the iterative approach or show the single path can yield an accurate enough estimation of $\\hat{p}_{\\mathrm{robust}}^{(k)}$.\n\n\nMinor Issue: I suggest authors use \\eqref instead of \\ref when referring to the equation number.\n\n\nAt the current stage, I tend to reject this paper but I'm willing to change my evaluation after rebuttal.\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3776/Reviewer_tAEV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3776/Reviewer_tAEV"
        ]
    },
    {
        "id": "By05fSexVi",
        "original": null,
        "number": 5,
        "cdate": 1667236827297,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667236827297,
        "tmdate": 1667236827297,
        "tddate": null,
        "forum": "2Fb-h04mt5I",
        "replyto": "2Fb-h04mt5I",
        "invitation": "ICLR.cc/2023/Conference/Paper3776/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper develops transformer-RKDE by leveraging robust kernel density estimation as a replacement of dot-product attention. The authors show that the optimal estimation of density functions via robust KDE requires computing a set of weights by solving an iterative re-weighted least-square problem. Multiple experiments including NLP and vision demonstrate the proposed method on performance and efficiency improvement. ",
            "strength_and_weaknesses": "Strength \n1. The topic is interesting and important \n2. The paper is clear to understand and easy to follow \n3. Related works and backgrounds are well discussed and explained.\n4. Strong experiments are shown the benefits of the proposed method\n\nWeakness\n1. The novelty is weak and lacks of a major contribution\n2. The comparison is not strong and lacks of other robust transformer models instead of KDE-based models ",
            "clarity,_quality,_novelty_and_reproducibility": "clarity, quality and reproducibility are good\nnovelty is weak \n",
            "summary_of_the_review": "The paper simply employs the robust KDE (20212 JMRL) for transformer models. The idea is great and the connection is natural and straightforward but the major contribution seems marginal. \n\nI also expect to see the performance benefits compared with other robust transformer models not only KDE-based vanilla models. That's important to demonstrate the necessity of the proposed robust KDE. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no concerns ",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3776/Reviewer_3QyS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3776/Reviewer_3QyS"
        ]
    }
]