[
    {
        "id": "_utws6R49Od",
        "original": null,
        "number": 1,
        "cdate": 1666534970164,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666534970164,
        "tmdate": 1666534970164,
        "tddate": null,
        "forum": "UhEJz3wgLnG",
        "replyto": "UhEJz3wgLnG",
        "invitation": "ICLR.cc/2023/Conference/Paper1465/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper explores single-frame training for video-and language tasks. While simple, this approach can achieve state-of-the-art performance on a range of datasets. This paper also proposes an early-fusion strategy at inference which boosts performance. ",
            "strength_and_weaknesses": "Strengths:\n1. The single-frame training strategy is highly efficient while effective, which only needs to train one frame during pre-training and fine-tuning. \n2. The proposed model achieves state-of-the-art among the video-text retrieval tasks on VideoQA tasks.\n3. This paper is clearly written and easy to follow.\n\nWeaknesses:\n1. The single-frame pre-training strategy (see Figure 6) is not fully convincing to me. Although the author claim that \u201cwhen pre-trained on a sufficient amount of data, the performance of models trained with single frames might be very close to models trained with multiple frames\u201d. However, there only exists 2.5M video-text data among the full 17M pre-training dataset. I think the reason is that with the growth of the #PT images/videos, the large-scale image-text dataset is enough for the model to achieve a good performance on downstream tasks (Note that a lot of recent works [1] have demonstrated that a pure image-text dataset is enough for video-text downstream tasks).\n2. Meanwhile, with the growth of the pre-training video-text dataset, there inevitably exists more noisy information and irrelevant frames in the video. Will the single-frame strategy still work on these large-scale web video datasets? I think an additional experiment on a pure large-scale web video dataset (instead of a pre-training dataset mix of videos and large portion images) will further enhance the reliability of this paper (e.g., HowTo100M).\n3. The proposed early-fusion strategy is simple and effective, which I do appreciate. However, compared with late-fusion, early-fusion strategy seems to need to input an Nx longer frame feature sequence into the multi-modal encoder. And this may bring an Nx higher memory/inference times cost compared with the late-fusion strategy. The author could further provide a memory/time cost comparison in Figure 4. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and easy to follow. The author also provides the code of the paper. ",
            "summary_of_the_review": "Although a lot of paper has proposed the single-frame pre-training strategy (on the pure image-text pre-training dataset), the single-frame finetuning strategy is still novel to me. My main concerns are the reliability of the single-frame strategy on the pure large-scale web video datasets and the memory/time cost of the early-fusion strategy. I will raise my score if these concerns are addressed.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1465/Reviewer_VHWz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1465/Reviewer_VHWz"
        ]
    },
    {
        "id": "HQ_r4DEr3GR",
        "original": null,
        "number": 2,
        "cdate": 1666619045733,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619045733,
        "tmdate": 1666619045733,
        "tddate": null,
        "forum": "UhEJz3wgLnG",
        "replyto": "UhEJz3wgLnG",
        "invitation": "ICLR.cc/2023/Conference/Paper1465/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies a video-language model trained using only a single frame, finding that is can do quite well on many existing benchmarks. The paper then proposes new benchmarks on something-something, where more temporal understanding is required.",
            "strength_and_weaknesses": "The paper is well written and easy to understand. The experiments are well done. The finding that many current tasks can be done with a single frame training is interesting and useful. For the proposed task, it would be good to add more details. For example, how many unique labels are there?  It also be valuable to evaluate more existing methods on this benchmark to better understand how existing approachs perform on it.\n\nMinor details: \nThe bolding is misleading in Tables 1 and 2.\nShould use citep command for references to make it easier to read.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well written. It has details needed to reproduce the results. It isn't really novel, but has important findings. The proposed new benchmark is interesting and valuable, however the new benchmark is totally clear how useful it is.",
            "summary_of_the_review": "The paper studies an important and interesting problem, finding that single frames are good enough for many video language tasks and proposes a solution to it.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1465/Reviewer_DUem"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1465/Reviewer_DUem"
        ]
    },
    {
        "id": "SU_rE5ABgH",
        "original": null,
        "number": 3,
        "cdate": 1666853417999,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666853417999,
        "tmdate": 1666853417999,
        "tddate": null,
        "forum": "UhEJz3wgLnG",
        "replyto": "UhEJz3wgLnG",
        "invitation": "ICLR.cc/2023/Conference/Paper1465/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the domain of video-and-language tasks and analyses the methods and datasets in the space. Through empirical studies, authors show that using a single frame (randomly sampled) from the entire video is enough to reach similar performance as existing SOTA methods on these datasets. The claim is that such tasks do not need temporal understanding, merely static appearance/image understanding. Further, two new datasets are proposed in this space that require more temporal understanding.",
            "strength_and_weaknesses": "**Strengths**\n\n-- The empirical studies presented in the work are insightful and will help propel more understanding in this relatively new domain of text-video understanding.\n\n\n**Weaknesses**\n\n-- While the empirical studies are certainly insightful, the paper's contributions are limited to just that. There are no other significant technical contributions.\n\n-- While the training is done with single frame, inference is still conducted using multiple sampled frames, which makes the setup a bit hard to fairly evaluate.\n\n-- The claim in the paper that existing datasets in the space (ActivityNet, MSRVTT) do not require any temporal understanding is only partially correct. Given that R1 results are still ~50%, it means that these are not solved datasets. The results point more towards the limitation of existing methods in fully learning/exploiting the temporal information in the videos rather a strict limitation of the datasets.\n\n-- The two new proposed datasets SSv2-{label|template} also have very high (and comparable) R1 scores with just sampling single frame which means those 2 new datasets are roughly comparable with other existing datasets.\n\n-- Overall, since the technical contribution in the paper is quite limited, the Sec 5 (Analysis) does not present any interesting studies and felt forced in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written for the most part. There is limited novelty in this work.",
            "summary_of_the_review": "Overall, not very convinced the paper is ready in its current form, especially as a long-form paper. The technical contributions are limited. The empirical studies do point to limitations of existing methods (and datasets to some extent) but do not propose any effective solutions to overcome such limitations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1465/Reviewer_kab7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1465/Reviewer_kab7"
        ]
    },
    {
        "id": "n1wYh34ZGbb",
        "original": null,
        "number": 4,
        "cdate": 1667328404161,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667328404161,
        "tmdate": 1669390084820,
        "tddate": null,
        "forum": "UhEJz3wgLnG",
        "replyto": "UhEJz3wgLnG",
        "invitation": "ICLR.cc/2023/Conference/Paper1465/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates whether using multiple frames for training is necessary for video-language downstream tasks. The authors propose a single-frame framework for video-language understanding. Results indicate that with large-scale pre-training and a proper frame ensemble strategy at inference time, a single-frame trained model that does not consider temporal information can achieve better performance than existing methods that use multiple frames for training. This result reveals strong static appearance bais in current video-langauge datasets. To allow for a more comprehensive evaluation of video-and-language models, the authors propose two new retrieval tasks based on existing fine-grained action recognition datasets that encourage temporal modeling. ",
            "strength_and_weaknesses": "Strengths:\n1. The paper is clearly written and easy to follow.\n2. It reveals the single-frame bias for current video-language datasets.\n3. The proposed approach is simple and effective, and the authors prove that with the simple, single-frame model, the video-language models can already achieve good performance.\n\nWeaknesses:\n1. The novelty is limited. The single-frame model applies the basic image-language architectural design with self-attention and cross-attention.\n2. The single-frame bias for current video datasets is not new. Researchers have realized that most video datasets (except for SSv2) do not require much temporal information for understanding the contents.\n3. The SSv2 retrieval tasks are only naive extensions of the SSv2 dataset.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The novelty is limited. The authors provide details of the proposed method to reproduce the results.",
            "summary_of_the_review": "My main concern is the limited novelty, as it has been investigated by previous works that most video datasets do not need too much temporal information for understanding. The proposed single-frame architecture is a basic structure with self-attention and cross-attention, and the novelty is limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1465/Reviewer_pLom"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1465/Reviewer_pLom"
        ]
    }
]