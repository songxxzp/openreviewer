[
    {
        "id": "tkaah9MHmSB",
        "original": null,
        "number": 1,
        "cdate": 1666644445313,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644445313,
        "tmdate": 1666644445313,
        "tddate": null,
        "forum": "1FxRPKrH8bw",
        "replyto": "1FxRPKrH8bw",
        "invitation": "ICLR.cc/2023/Conference/Paper301/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a system for training of machine learning classification models. The main focus\nof the paper is to reduce the overal cost of training. The total cost considered in the paper is a\ncombination of the cost due to human labeling and the cost due to computation (e.g. renting a GPU on\nAmazon Cloud).\n\nIn order to reduce the training cost the paper proposes to rely on combination of active learning and\nautomatic labeling.\n\nThe training in the proposed MCAL approach proceeds iteratively. On each iteration the approach\nselects which examples should be labeled by human annotators, and which examples are assigned labels\nautomatically using a classifier trained on previously labeled data (using automatic + human labels).",
            "strength_and_weaknesses": "I generally have a negative impression about the paper. I can see that the approach is trying to\naddress a valid practical problem. The overall approach sounds very much like a common sense, i.e.\nlet's label some data points with already pre-trained model, and let human's label \"interesting\"\nexamples. The paper relies on standard metrics to assess what to give humans for labeling and how to\nchoose examples for automatic labels (i.e. examples where classifier is confident about the label).\n\nThe main technical contribution of the paper appears to be the use of the power law to predict how\nmany examples to label automatically (Sec. 3.1) and the overall system (Sec. 4).\n\nAfter going through the paper several times, what I am left with is the following: The paper\npresents an idea that sounds like a common sense, which is followed by hard to comprehend notation\nand cost model (Sec. 2,3), followed by description of standard mechanisms of sample selection (Sec.\n3.3), followed by hard to understand Sec. 4 that describes the algorithm.\n\nThe text does not communicate the ideas well unfortunately, and at least in my case it leaves the\nreader unexcited about the approach. Perhaps using clear real-world examples would help here.\n\nFor example, I don't understand why power law in Sec. 4 even applies to the approach presented in\nthe paper. I can see how such power law works for increasingly larger training sets labeled by\nhumans. However, why does this law apply to automatically labeled examples, where adding larger\nnumber of examples would result in adding more of the incorrectly labeled examples.\n\nI addition I am not taking it for granted that \"auto-labeling\" generally works. Why is automatically\nlabeling examples where classifier is already confident going to help with classifying examples\nwhere classifier is not confident? I understand that it might be the case, but I disagree that this\ncan be taken for granted.\n\nPrior to reading this paper I believed that the main questions for the type of approaches proposed\nin the paper is how to choose what to label manually, and how to choose what to label automatically.\nIf we knew how to do that well we would be in good shape to construct such systems. The paper does\nnot contribute to addressing these questions, and the paper didn't convince me that it makes other\nvaluable contributions.\n\nI found some statements in the paper which I would like to have clarified:\n1. \"it chooses to label CIFAR-100 mostly using humans\",\nI am very curious why this is the case, and why similar outcome does not happen for ImageNet.\n\n2. \"MCAL\u2019s goal is to select samples to train a classifier that can machine label the largest number\nof samples possible.\"\nThat is certainly a good goal, but I do not understand how the approach achieves that. All I see in\nthe text is some application of the power law, whereas the problem above appears to be too complex to be\naddressed by a power law.\n\n3. \"When MCAL uses margin or least confidence for L(.), the samples selected have high accuracy (close to 100%, Fig. 5)\n. The samples selected by a core-set based algorithm such as k-centers is poorly correlated\nwith accuracy (Fig. 5) and margin (Fig. 6).\"\nI din't quite understand what is meant by these sentences. Why is accuracy relevant here if we are\nselecting examples to be labeled and to be used for training of the next-iteration classifiers?\nAlso, if we select examples that have high confidence already why would such examples improve the\nclassifier?\n\nFinally, I also doubt that the topic of the paper is within the scope of ICLR. For me ICLR stands\nfor interesting new representations (i.e. neural network models), or perhaps interesting techniques\nfor training neural networks. This paper sounds like a system paper that would perhaps be a good fit\nfor WACV (Winter Conference on Applications of Computer Vision) or some other more applied\nconference (e.g. International Conference on Computer Vision Systems).\n\nPositive:\nThe overall observation that in addition to human labeling cost there is also a computational cost is\nvaluable.\n",
            "clarity,_quality,_novelty_and_reproducibility": "As I mentioned above I didn't find the paper clearly written.",
            "summary_of_the_review": "I refer to my text in the \"Strength And Weaknesses\" section. I would like to avoid repeating myself here.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper301/Reviewer_hXWK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper301/Reviewer_hXWK"
        ]
    },
    {
        "id": "HAIh53XkBS",
        "original": null,
        "number": 2,
        "cdate": 1666650035659,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650035659,
        "tmdate": 1666650035659,
        "tddate": null,
        "forum": "1FxRPKrH8bw",
        "replyto": "1FxRPKrH8bw",
        "invitation": "ICLR.cc/2023/Conference/Paper301/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors presents a novel active learning setup using a \"hybrid human-machine labeling\" framework to reduce the cost of labeling/annotating large datasets. They compared their approach on a number of public datasets and claims to provide a 6x overall reduction in cost to entire human labeling of the dataset.",
            "strength_and_weaknesses": "Some of the key strengths of the paper are as follows:\n\n- The authors presented a general solution to a very important problem. With the impressive performance of AI/DL models for various tasks, the proposed method could be very important in operationalizing AI for real-world problems. Further, the MCAL Algorithm is not dependent on the underlying DL architecture / task and thus could be applied as a meta learner over multiple problems\n- The authors have presented evaluations across multiple datasets/publicly available benchmarks to support the claim about the effectiveness of the solution\n- Further, the detailed analysis about the reduction in labeling cost from multiple view points provides better insights for the method to be operationalized\n\nThe paper could improve upon the following aspects\n\n- The presentation of the paper can be improved upon. The two components of MCAL algorithm is hard to follow. Symbols are sometime introduced before they are properly defined (e.g. delta in section 3.1, 3rd line from end of the section; first introduced in 3.2) and intuitions behind certain selections not presented well\n- While the authors provide interesting insights about the drivers of the reduced cost, the discussion is lacking depth when considering the problem in a holistic manner. For example, in addition to labeling cost, other metrics of interest can include turnover time which can be affected as part of this multi-loop process\n- In a similar manner, more details/analysis on which samples are selected during the AL phase would be useful. The authors presented a comparative analysis of the sample selection candidates - however, the intuition and effect of the selected samples may need further analysis. This could also be useful for downstream tasks in identifying \"good enough\" solutions under budget constraints",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed solution is simple but intuitive - including the model training objective under the AL task can be intuitively thought of being more \"intelligent\" about the selected samples while adhering to the realities of increasingly costly DL models. While the results are impressive, the clarity of presentation could be improved upon. This will also help with reproducing the results across domains.\n\n",
            "summary_of_the_review": "Overall, the paper discusses an intuitive and simple solution to gather ground truths for large datasets and reduce the overall labeling costs for the process. While there are avenues for improving the paper, the results are impressive and general enough to have a significant impact.\n\nApart from the previously highlighted strengths and weakness, the authors may also want to address the following:\n\n- Section 3.1, the intuition for the upper-truncated power-law could be motivated better. For example, a plot showing the effects of various values of the parameters from equation (3) could be useful to understand the desired shaping of the rewards\n- MCAL Algorithm -provide better details about the fitting procedure. Discuss about the cost for the optimization function\n- Provide intuitions behind the sample selection - e.g. how does the ranking of samples selected change with reduced labeling cost etc. Investigate possible issues with fairness / group fairness in selection of samples\n- In recent years, a number of papers have identified labeling/data issues in the original benchmarks. The authors may want to compare if their proposed solution is able to circumvent those issues",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "While there are no direct ethical concerns, there may be unintended consequences in sample selection in proposed method - e.g does the algorithm provide equal exposure to labels for protected sub-groups for human labeling.\n\n",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper301/Reviewer_9gqM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper301/Reviewer_9gqM"
        ]
    },
    {
        "id": "jO-FlwTQX6X",
        "original": null,
        "number": 3,
        "cdate": 1666803986373,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666803986373,
        "tmdate": 1666803986373,
        "tddate": null,
        "forum": "1FxRPKrH8bw",
        "replyto": "1FxRPKrH8bw",
        "invitation": "ICLR.cc/2023/Conference/Paper301/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents an \u2018active labeling\u2019 (inspired from active learning) based approach to automatically label parts of a dataset given some error bounds by jointly optimizing for human labeling and machine labeling costs (eg. training costs) together. It formulates the problem as a minimum-cost labeling problem in an optimization framework that minimizes total cost by jointly selecting which samples to human label and which to machine label. A major assumption of this work is that it considers the truncated power-law distribution to mimic how model error behaves with training set size. ",
            "strength_and_weaknesses": "Strength: \n- The paper presents approach that allows control over accuracy and cost with the cost minimization objective to launch data labeling task for developing a high accuracy model training.\n- The approach scales well with complexity of the data and also suggests (with a \u2018exploration tax\u2019) if it\u2019s feasible to automatically label the data or not (given the size and complexity)\n- The approach can use any active learning metric for selecting samples for human-labeling. Also they found empirically that some popular metrics (like coreset) don\u2019t work as well as others with this objective. \n- The approach is extensible to allow multiple model architectures \n\nWeakness:\n- It is not clear if the approach would work with a dataset containing imbalanced distributions and/or open classes to decide which ones to human-level vs machine-label. Also not clear if the power-law distribution would follow in the situation. \n- Fig. 4 needs more explanation - the role of batch size for model errors is low which is unintuitive. \n- The human-labeling is assumed to be oracle in this work - how realistic is this assumption - if the model can accommodate imperfect oracles in human labels, the method would become more realistic. \n- Fig. 6 needs more explanation \n- It is not clear how does \u2018fast search\u2019 work in the method. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper presents very interesting approach but there is no comparison with other existing cost sensitive active labeling methods (expect with active learning method).  A comparison to other weak labeling methods with active learning would have made a good comparative study in this work. \n",
            "summary_of_the_review": "This paper presents some practical ideas for cost reduction for data labeling which is growing need today. The work presents a solid extensible framework where one can set various thresholds for performing the labeling tasks. The methods have been testing on multiple datasets and understanding the role of various parameters have been explored and discussed well in the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper301/Reviewer_Dg4t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper301/Reviewer_Dg4t"
        ]
    }
]