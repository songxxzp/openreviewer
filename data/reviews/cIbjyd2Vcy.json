[
    {
        "id": "YdmciVs9KF",
        "original": null,
        "number": 1,
        "cdate": 1666651579612,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651579612,
        "tmdate": 1668842164554,
        "tddate": null,
        "forum": "cIbjyd2Vcy",
        "replyto": "cIbjyd2Vcy",
        "invitation": "ICLR.cc/2023/Conference/Paper4500/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies how representation collapse is avoided in non-contrastive methods that employ certain \u201casymmetrical tricks\u201d rather than using negative samples, despite the existence of trivial solutions.\nIt proposes an explanation through an idea of \u201crank differential\u201d created by asymmetric designs, i.e. the effective rank of representations from the target branch is consistently higher than that of the online branch, as training progresses.\nThis is empirically verified for methods like BYOL, SimSiam, SwAV, DINO.\nThe paper hypothesizes that this rank differential leads to a continuous increase in the effective rank of representation and thus prevents dimensional collapse; a combination of theory and experiments are used to justify this.\nThe paper further attempts to explain the existence of such a rank differential through a \u201clow-pass filtering\u201d effect of asymmetric designs on the representation spectrum.\nMotivated by these ideas, new non-contrastive variants (that directly set the prediction head) are proposed that achieve comparable performance to existing methods on CIFAR and ImageNet benchmarks.",
            "strength_and_weaknesses": "**Strengths**\n\nThe paper makes many interesting observations about non-contrastive learning, a promising alternative to contrastive learning that is more computationally efficient and scalable.\n- Symmetric architectures, with feature centering, are shown to avoid complete collapse but not dimension collapse\n- The existence of rank differential in many non-contrastive methods is a new and interesting finding in my opinion\n- The overall story, with eigenspace alignment -> rank difference -> rank improvement seems plausible given the results of the paper. Many claims are supported either by some experiments or some theory, albeit with many assumptions at times to make analysis more convenient.\n  - Eigenspace alignment is empirically verified\n  - Rank difference is empirically verified and justified from eigenspace alignment by further assuming a \u201clow-pass filter\u201d assumption (that is empirically verified)\n  - Finally Theorem 4 proves rank improvement as a consequence of the above two (potentially significant issues with this in weaknesses).\n- The rank differential and low/high pass filtering view inspire new non-contrastive variants that directly set the prediction head rather than training them (similar to DirectPred). These methods perform comparably to previous methods, sometimes outperforming them with fewer epochs.\n\nOverall many interesting insights are provided which can help understand non-contrastive methods better.\n\n\n**Weaknesses**\n\n- My main concern is with Theorem 4 that tries to explain why eigenspace alignment + rank differential can lead to increase in effective rank. In particular, I'm not sure that the result is correct, due to a potential bug in Eq. 22 in the Appendix. While it is assumed that the (unnormalized) covariances $\\Gamma_{t}$ and $\\Gamma_{z}$ of $p$ and $z$ respectively are aligned, that does not imply that $\\mathbb{E} [p^{\\top} z] = \\left(\\Gamma_{p} \\Gamma_{z}\\right)^{1/2}$, as assumed in Eq. 22. (Simple counter example is when $p$ and $z$ are independent, and zero-mean Gaussians say). For the result to hold, you might something stronger that the cross-covariance of $p$ and $z$ are also aligned, which is a much stronger assumption and will change the expression for Eq 4. and Theorem 4 in general.\n\n- Most of the theoretical results are not end-to-end (which is fine) and make many assumptions as the paper goes along, which is sometimes hard to follow. For e.g. why does the assumption in Theorem 5 that $z$ and $\\bar{z}$ have aligned eigenspaces makes sense?\n\nOther comments/questions:\n\n- While Theorem 4 is meant to show that the effective rank keeps increasing, what stops the rank to asymptotically stagnate at a small value and lead to dimension collapse? Furthermore, is there any intuition for why rank differential should always remain high as training progresses (besides empirical evidence)?\n\n- The use of the terms \u201clow/high pass filters\u201d can be confusing at times. If these terms are directly inspired from signal processing (or another field), then the connection should be made clearer. The notation for $g$ and $h$ are also ambiguous, since they are just functions of a single $\\lambda^{z}\\_{i}$ but their definitions also involve $\\lambda^{p}\\_{i}$.\n\n- Figure 4: typo \u201conline branch is more biased towards larger values\u201d. For SimSiam, plot (a) suggests that the ratio of $\\lambda^{p}\\_{i} / \\lambda^{z}\\_{i}$ is not monotonic, but figure (b) doesn\u2019t agree since the ratio seems to be 1 at the ends. Do the spectral filter function plots not include all indices $i$? Is this also true for other plots like Figures 5 and 6?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clearly written, besides some issues with the presentation of the theoretical results that are described above. The findings are novel, to the best of my knowledge.",
            "summary_of_the_review": "Overall I believe that the paper contributes many interesting insights about non-contrastive methods through the lens of rank differences, and also suggest ways to empirically leverage these findings. However currently I am not convinced about the technical correctness and conclusions about one of the main results (Theorem 4) that aims to prove why rank differential can help avoid dimension collapse, which is one of the main contributions of the paper. Keeping this and some other issues in mind, I am inclined to assign a score of weak reject (keeping in mind all the other contributions).",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4500/Reviewer_rcoc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4500/Reviewer_rcoc"
        ]
    },
    {
        "id": "UHkj_5uubqz",
        "original": null,
        "number": 2,
        "cdate": 1666675104478,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675104478,
        "tmdate": 1666675104478,
        "tddate": null,
        "forum": "cIbjyd2Vcy",
        "replyto": "cIbjyd2Vcy",
        "invitation": "ICLR.cc/2023/Conference/Paper4500/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, authors provide a new perspective for non-contrastive learning methods. A rank difference mechanism (RDM) theory is proposed to explain how existing non-constrastive learning methods alleviate dimensional feature collapse. More importantly,  RDM theory also provides practical guidelines for designing many new non-contrastive variants. In this paper, authors demonstrate the low-pass filters and high-pass filters for online and target branches respectively. In the experiments, the theory is consistent with the practical settings.",
            "strength_and_weaknesses": "Strength\n1. This paper proposes a novel perspective for non-contrastive learning methods. This perspective can effectively explain the four main approaches in non-contrastive learning.\n2. Some of the more interesting variants were obtained under the new view. These methods are promising and are able to inspire the community.\n3. The experiments are convincing in my opinion.\n\nWeakness:\n1. Although the authors call the method rank difference, and define an effective rank difference to estimate how uniform the eigenvalues are. It is the eigenvalues difference rather than the rank difference. I don't think that's a suitable name.\n2. The eigenspace alignment assumption is a little bit strong, and all conclusions and methods are based on that. This paper is, therefore, more like an extension of ICML2021(Understanding self-supervised learning dynamics without contrastive pairs).",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is good. I can easily understand this paper, including proofs. The quality is also ok for me. The novelty and originality of the work are the advantages of this work. They focus on a very important topic: dimensional collapse in contrastive learning, and show us something new for this.",
            "summary_of_the_review": "Based on the comments above, I am inclined to accept this paper. However, I will also take full account of other reviewers' comments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4500/Reviewer_xJ6R"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4500/Reviewer_xJ6R"
        ]
    },
    {
        "id": "9hB4LSbZrR4",
        "original": null,
        "number": 3,
        "cdate": 1667468030818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667468030818,
        "tmdate": 1670684506897,
        "tddate": null,
        "forum": "cIbjyd2Vcy",
        "replyto": "cIbjyd2Vcy",
        "invitation": "ICLR.cc/2023/Conference/Paper4500/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes RDM theory, which is applicable to different asymmetric designs (with and without the predictor), and can serve as a unified understanding of existing non-contrastive learning methods. Besides, the RDM theory also provides practical guidelines for designing many new non-contrastive variants. RDM achieves comparable performance to existing methods on benchmark datasets, and some of them even outperform the baselines.",
            "strength_and_weaknesses": "Pros: \n1.\tThe paper analyzes how symmetric and asymmetric frameworks alleviate different collapses (dimensional and complete).\n2.\tRDM designs a new asymmetric design that also work well in practice.\n\nCons:\n1.\tLimited novelty: SymSimSiam is quite similar to the combination of DINO and SimSiam. It seems to me like just borrowing the centering and sharpening techniques of DINO to $l_2$ space (see Algorithm 1).\n2.\tAlignment in eigenspace is similar to whitening operation and the authors may miss some related works [1, 2]. Author may compare RDM with these methods on methodology.\n3.\tThe limited performance on CIFAR and ImageNet. Authors claim the proposed method can get comparable results on these benchmarks. However, I don\u2019t see any result that can lead to this conclusion (For example, with 100 epochs pretraining, RDM gets 85.7% and 58.4% accuracies on CIFAR10 and CIFAR100 datasets). Maybe the authors can comment on this.\n\n[1] Weng X, Huang L, Zhao L, et al. An Investigation into Whitening Loss for Self-supervised Learning[J]. arXiv preprint arXiv:2210.03586, 2022.\n[2] Zhang S, Zhu F, Yan J, et al. Zero-CL: Instance and Feature decorrelation for negative-free symmetric contrastive learning[C]//International Conference on Learning Representations. 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "See the above comments. The evluation is basically good.",
            "summary_of_the_review": "The paper has some merits while I currently still have some concerns and questions as above stated. I would like to see the authors' response.\n\n---post rebuttal:\n\nThe authors have made good job in clarifying their novelty not only to my original concerns but also the other reviewers'. I appreciate the novelty of this work and increase my score from 5 to 6. Yet the performance advantage of this method shall be more carefully and comprehesively discussed. BYOL actually used a non-linear projector while the authors did not show their non-linear version's performance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4500/Reviewer_hvBQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4500/Reviewer_hvBQ"
        ]
    }
]