[
    {
        "id": "TGZmIS1rCr",
        "original": null,
        "number": 1,
        "cdate": 1666239163896,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666239163896,
        "tmdate": 1670827045671,
        "tddate": null,
        "forum": "N-S6pJrlkK",
        "replyto": "N-S6pJrlkK",
        "invitation": "ICLR.cc/2023/Conference/Paper2159/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an inter-sentence pretraining task, Text Structure Prediction (TSP), which encourages the encoder to classify sentence structure relationships. The relationships are manually defined to be among 6 types across two dimensions: hierarchical relation and ordering relation. The hierarchical relation contains three cases: neighbor, same paragraph and same document; the ordering relation contains two cases: original and reversed order. By randomly shuffling a subset of the sentences in a sequence, TSP can be used to train the encoder to classify sentence pairs into the pre-defined 6 relation types. The authors argue that such an inter-sentence pretraining task provides more high-level language understanding signals and thus improves using word-level pretraining tasks only. The major benefit of TSP compared to previously proposed inter-sentence pretraining task, as pointed out by the authors, is that TSP is able to offer performance gain for encoders of different sizes (small, base, large), whereas previous methods usually do not benefit large model sizes. The empirical evaluation is conducted on the SuperGLUE benchmark. The authors show that TSP is more effective than other compared sentence-level tasks when combined with MLM.",
            "strength_and_weaknesses": "Pros:\n* The paper is clearly written and well-organized.\n* The proposed pretraining task TSP is shown to outperform the compared sentence-level pretraining objectives (NSP, SOP and SSO) on SuperGLUE.\n\nCons:\n* The novelty of the method is quite limited. The proposed objective TSP is essentially a combined (and slightly extended) version of NSP and SOP -- the \"hierarchical relation\" classification is extending NSP by further classifying the \"non-neighbor\" case into \"same-paragraph\" and \"same-document\"; the \"order relation\" classification is equivalent to SOP. In this sense, the proposed task is not new and should be compared with multi-task training that uses both NSP and SOP (this seems to be an important baseline that is missing from comparisons).\n* The paper omitted discussions and comparisons with an important family of sentence-level pretraining tasks that use contrastive learning. For example, COCO-LM and CLEAR demonstrated that contrastive learning-based sequence-level tasks can effectively improve NLU task fine-tuning performance, and they can also be combined with word-level pretraining tasks. \n* The empirical evaluation is quite problematic. My major concern comes from the fact that the paper does not follow the conventional pretraining/downstream task settings that have been extensively used by previous studies. (1) The pretraining corpus used in this paper is OpenWebText whereas previous pretraining methods use either Wikipedia + BookCorpus (in BERT) or further expanded corpora that include a wide variety of datasets besides OpenWebText (in RoBERTa). The common pretraining corpora choice could be either of the two above. Even if BookCorpus is not publicly available, Wikipedia should at least be included as a formal language pretraining source. If the pretraining experiments are conducted only on OpenWebText, it's very unclear whether the observed performance gain of TSP can be generalized to common/larger pretraining corpora. (2) Most pretrained language models (e.g., BERT, RoBERTa, ELECTRA) have been evaluated on GLUE whereas this paper only reports SuperGLUE results. This makes it hard to compare with previous pretrained models. While it is good to have SuperGLUE results, I believe it's important to conduct experiments on GLUE as well to facilitate easy comparisons with prior work. (3) The evaluation results only have the best performing results on the test set but not median results across all tasks (Table 6 only has averaged scores). This is problematic because some tasks have very small test sets and the scores could vary a lot. The common practice is to report median results over several random seeds on dev sets of all tasks.\n\nReferences:  \nMeng, Yu et al. \u201cCOCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining.\u201d NeurIPS (2021).  \nWu, Zhuofeng et al. \u201cCLEAR: Contrastive Learning for Sentence Representation.\u201d ArXiv abs/2012.15466 (2020).\n\n---\n**Post-Rebuttal Updates**:  \nI thank the authors for providing responses to my raised concerns. Unfortunately, I'm not convinced by the authors' arguments regarding my major concerns. My detailed comments are as below:\n* Regarding novelty: I never implied in my review that a method needs to be complicated to be novel; instead, simple methods can be novel as long as they provide new perspectives/insights. My critique regarding the novelty of the paper is mainly due to the fact that the proposed objective TSP is very similar to previously proposed sentence-level objectives (NSP and SOP) in formulation. The major difference between TSP and NSP+SOP, according to the authors' response, is that TSP considers paragraph-level relationships. However, the motivation for this idea is very vague -- how can one accurately/unambiguously define the real difference between a sentence and a paragraph, and the difference between a paragraph and a document? For example, in novels, it's very common that one or two sentences can be a paragraph; other times, a paragraph can also be an entire document. If the distinction between sentences, paragraphs and documents is even hard to tell for humans, then I don't see how the so-called \"paragraph-level relationships\" newly introduced in TSP can actually make a big difference compared to a simple combination of previous methods (NSP+SOP).\n* Regarding the comparisons to contrastive learning approaches: I'm not fully convinced by the argument that the comparisons between contrastive learning objectives and the proposed TSP objective can be omitted. I understand that the two sets of methods are based on different principles, but they are indeed for the same goal -- improving the sequence-level representations of PLMs. Therefore, unless TSP has other usages/benefits that contrastive learning fails to bring, I don't see why they cannot be fairly compared to each other. Such comparisons do not exist in prior work probably because it is quite well known that NSP/SOP-style methods do not really work well (also shown in this paper), and many PLMs simply ended up not using any sequence-level objectives (e.g., RoBERTa, ELECTRA), so the contrastive learning papers didn't compare with NSP/SOP (of course, I do hope to see those results in the contrastive learning papers). \n* The empirical gain of TSP does not seem convincing to me: Although TSP provides significant gains on GLUE/SuperGLUE average score, the gains are mainly from small/unstable tasks, like CoLA, MRPC, RTE and STS-B. Fine-tuning on those tasks are notoriously unstable and is subject to high variance. On large tasks like MNLI and SST-2 (note that they are more reliable than the previous tasks; RoBERTa paper uses these two tasks intead of the average GLUE score for ablation studies), the TSP objective barely provides any gain and even worsens the result. Therefore, I suspect that the performance gain of TSP is simply due to randomness in fine-tuning.\n* Regarding the choice of pretraining corpus (OpenWebText): I'm indeed aware that OpenWebText is larger than Wikipedia, but Wikipedia is still a more standard choice for pretraining corpus if only one corpus is used (e.g., base model exploration in BERT, RoBERTa, ELECTRA all used Wikipedia + BookCorpus). This is probably because Wikipedia's text quality is arguably higher than OpenWebText which contains many short/casual/informal texts. Therefore, I'd suggest the authors follow the standard protocol for pretraining; otherwise, the findings and conclusions may not be reliable generalize well. This is not my major concern about the paper though. \n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: The paper is clearly written and well-organized.\n* Quality: The contribution of the paper is not convincing due to the problematic empirical evaluation (see cons above).\n* Novelty: The novelty is quite limited. The proposed objective TSP is essentially a combined version of previous sentence-level tasks (see cons above).\n",
            "summary_of_the_review": "I believe the paper will need quite a lot major revisions to be convincing. (1) There should be more discussions and evaluations regarding what are the real differences from NSP and SOP. (2) The method needs to be compared with contrastive-learning based pretraining tasks. (3) The evaluation needs to follow conventional pretraining/fine-tuning settings.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2159/Reviewer_SQUJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2159/Reviewer_SQUJ"
        ]
    },
    {
        "id": "uvWN_xJJKx",
        "original": null,
        "number": 2,
        "cdate": 1666538571126,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666538571126,
        "tmdate": 1666538571126,
        "tddate": null,
        "forum": "N-S6pJrlkK",
        "replyto": "N-S6pJrlkK",
        "invitation": "ICLR.cc/2023/Conference/Paper2159/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a new inter-sentence training task for language model pertaining, called textual structure prediction. The author designs six structure relationship labels for TSP prediction. The empirical results show that the TSP improves the performance of pretrained language models on NLU tasks.",
            "strength_and_weaknesses": "Strength:\n1. The paper is clearly stated and easy to follow.\n\n2. The performance improves significantly in some of the SuperGLUE downstream tasks.\n\nWeaknesses:\n1. The hierarchical relationship labels are not convincing enough. It is even difficult for humans to predict whether two sentences are from the same document. Enforcing the model to learn the \"same document\" label might cause the overfitting of the sentence representations. The ablation study also shows that order information is more important than hierarchical structure information.\n\n2. The empirical results are not uniformly improved with the proposed TSP methods on the downstream tasks. ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is good. The quality and novelty are fair. The experiments seem reproducible.",
            "summary_of_the_review": "The paper proposed hierarchical structure labels for training language models. The empirical results show performance improvements, while why the hierarchical labels work needs more reasonable explanations.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2159/Reviewer_t9DH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2159/Reviewer_t9DH"
        ]
    },
    {
        "id": "WM1I_d1w5p",
        "original": null,
        "number": 3,
        "cdate": 1666630463994,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630463994,
        "tmdate": 1666630463994,
        "tddate": null,
        "forum": "N-S6pJrlkK",
        "replyto": "N-S6pJrlkK",
        "invitation": "ICLR.cc/2023/Conference/Paper2159/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper argues for adding inter-sentence tasks to language model pretraining. In this direction, the paper introduces Text Structure Prediction (TSP), an improved version of BERT's Next Sentence Prediction where a LM must classify two sentences using one of six labels that track whether sentences belong to the same document, same paragraph, are neighbors and their order (A then B vs. B then A).",
            "strength_and_weaknesses": "## Strengths\n\n* Well-written and easy to follow.\n* The models are properly trained before evaluation. Section 4.1.1 states, \"To ensure fair comparisons, all models at the same scale had almost the same computational cost and number of parameters in both training and inference.\". This is a detail people often forget, so I am happy to see that the paper takes computation into account.\n\n## Weaknesses\n\n* The introduction argues that \"Without text structure, a text becomes a long continuous word sequence, which is hard to read and makes it difficult to identify key concepts and logical relationships.\" While this is probably, intuitively, true for humans, the paper uses the statement to argue against prior unsupervised sentence tasks and projects intuitive understanding of human attention and reading abilities onto machines (i.e., language models) without valid proof. This is a logical fallacy. I do not believe that this is a major weakness for the paper. The paragraph just needs to be rewritten to avoid the incorrect argument.\n* The Related Work / Word-Level Pretraining section is a little too naroow. I understand that the paper's method only applies to transformer encoders, but I believe the paper would benefit from summarizing pretraining methods for encoder-decoders too.\n* Section 4.1.1 cites \"the sentence splitter of NLTK (Bird et al., 2009) for the use of TSP\". NLTK's sentence splitter is Punkt (Kiss and Strunk, 2006), see also https://www.nltk.org/_modules/nltk/tokenize/punkt.html. The paper should cite the Punkt paper together with the NLTK citation. Giving credit to the method's authors is not only fair, but it explicitly names the sentence splitter used, thus improving clarity for readers.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Well-written and easy to follow.\n* The TSP task is not hugely different from prior sentece-based pretraining methods, but it does work.\n* See the point in Weaknesses and the Punkt sentece splitter.\n* The paper contains all the hyperparameters needed to reproduce the experiments.\n",
            "summary_of_the_review": "A good paper with a couple of minor issues that can be easily addressed before a camera ready.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2159/Reviewer_LMAd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2159/Reviewer_LMAd"
        ]
    },
    {
        "id": "fiJNBLL_3h6",
        "original": null,
        "number": 4,
        "cdate": 1666665243711,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665243711,
        "tmdate": 1666665243711,
        "tddate": null,
        "forum": "N-S6pJrlkK",
        "replyto": "N-S6pJrlkK",
        "invitation": "ICLR.cc/2023/Conference/Paper2159/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new way to pre-train language models. The main idea is to include sentence-level hierarchy information during the pre-training. Instead of considering only neighbor sentences, they consider more relations between sentences such as if they are in the same paragraph or if they are in the same document. By training this classification loss along with the original masked token loss, they have better performance on the SuperGLUE downstream tasks.",
            "strength_and_weaknesses": "Strength\n- The motivation is clear and reasonable.\n- The experiments support the claim and seem to be promising\n\nWeaknesses\n- To test the generalizability, I suggest to test on GLUE tasks as well.\n- It's not clear what size of models is using for the ablation study in Table 2 \n- What will happen if we do not shuffle the sentences?",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed method is clearly described. The novelty is enough for me. They propose additional self-supervised loss to improve the language models.",
            "summary_of_the_review": "The motivation is clear and reasonable. Although the proposed method is simple, it's very effective. Ablation study supports their claim.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2159/Reviewer_VZCD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2159/Reviewer_VZCD"
        ]
    }
]