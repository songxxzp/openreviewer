[
    {
        "id": "qCsXZAbPffE",
        "original": null,
        "number": 1,
        "cdate": 1666577310587,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577310587,
        "tmdate": 1666577310587,
        "tddate": null,
        "forum": "IQM-3_Tzldw",
        "replyto": "IQM-3_Tzldw",
        "invitation": "ICLR.cc/2023/Conference/Paper6577/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "1. The paper tackles the problem of client drift (locally) in federated learning (FL) due to heterogeneous client data distributions. Besides, they target the period drift (globally), which is the inter-communication heterogeneity of data distributions.\n2. They propose a learning based parameterized aggregator called FEDPA, debiasing model aggregation under client drift and period drift in a unified framework. Their key approach is to learn an adaptive calibration parameter to approximate the global objective.\n3. The input of the framework includes the intra-communication client parameters.\n",
            "strength_and_weaknesses": "Strength\n1. The 2 levels of drift they tackle is indeed important for FL.\n2. They can achieve better training performance on the MovieLens dataset and FEMNIST dataset.\nWeaknesses\n1. Some small typos, grammar issues, inconsistency issues as stated below.\n2. The key idea should be agnostic to the dataset and types of tasks. It would be good to show the performance on other LEAF datasets.\n3. It would be good if the overhead on the server can be quantified. If the method can not be applied at scale (equation 7 seems to be iterative but more clarification would be good), it is not a perfect match for FL.\n",
            "clarity,_quality,_novelty_and_reproducibility": "0.Could you explain how the drift issue in FL (stated in Figure 1) is different from similar convergence issues in non-FL training pipelines? Explaining this is important for people to understand that this is indeed a new problem.\n1. At the beginning of section 2, \u201cFederated learning with non-iid data Federated Learning with non-iid Data\u201d is duplicated.\n2. \u201cImplementation\u201d does not end with \u201c.\u201d in section 4.1.\n3. \u201cWe compare FEDPA with the baselines that using proxy dataset\u201d should be \u201cWe compare FEDPA with the baselines that use proxy datasets\u201d in section 4.2.\n4. The idea is inspired by the control theory and dynamic systems. They add a regularization term, which is determined by the client weight updates, weight matrix and the parameters of the learnable aggregator.\n5. Could you explain Equation 7 regarding how the dense() operator actually functions?\n6. Plan for open source code?\n",
            "summary_of_the_review": "The introduction and related work part is in good shape and I enjoyed reading it. But the quality for writing could be improved. (See the above session for more details)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6577/Reviewer_5zVx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6577/Reviewer_5zVx"
        ]
    },
    {
        "id": "fwKl_Hf83b7",
        "original": null,
        "number": 2,
        "cdate": 1666678536295,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678536295,
        "tmdate": 1666678536295,
        "tddate": null,
        "forum": "IQM-3_Tzldw",
        "replyto": "IQM-3_Tzldw",
        "invitation": "ICLR.cc/2023/Conference/Paper6577/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a method named FedPA to learn aggregators in federated learning. When aggregating model updates from clients, instead of uniform or weighted by number of examples as in the popular FedAvg, FedPA will feed both the global model and the client model updates to a neural network before \u201caggregating\u201d/averaging. The aggregator is trained on the server with a proxy dataset. Experiments on EMNIST and MovieLens show the advantage of FedPA. \n",
            "strength_and_weaknesses": "The general idea of \u201cmeta learning\u201d and \u201clearning to aggregate\u201d makes sense. \n\nHowever, as the authors commented, though interesting, having a proxy dataset is a strong assumption in problem setting.  \n\nIn addition, the server seems to have access to the model updates of each individual client, which makes it hard to be consistent with FL privacy principles, and other privacy techniques like SecAgg and differential privacy [Federated Learning and Privacy https://queue.acm.org/detail.cfm?id=3501293]\n\nMy major concern is that the proposed method seems to be ad-hoc. It is hard for me to connect the motivation of \u201cperiod shift\u201d to the proposed FedPA method. Instead of learning neural networks for aggregation, I am wondering if there are easier approaches to use the proxy data. For example, we can simply \u201clearn\u201d a scalar weight to do weighted aggregating/averaging of client model updates. I would strongly suggest some more ablation studies of the proposed FedPA method.\n\nA second major concern is the experiment performance. The accuracy on FEMNIST seems to be lower than expected. For example, [Adaptive Federated Optimization https://arxiv.org/abs/2003.00295] reports >80% for natural user non-IID. \n\nI would also appreciate some more comments on hyperparameter tuning. For example, how are 100 communication rounds, 5 epochs, learning rate \\eta_l=0.01 chosen? How are training epochs (5 for MovieLens, 30 for FEMNIST) and learning reate \\eta_g chosen?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "AFAIK, the idea is novel.\n\nNeed improvement and clarification: the intra/inter-communication arguments look inaccurate to me. The global model and control variates are shared \u201cinter\u201d rounds, for example, in SCAFFOLD [Karimireddy et al. 2021]. Some previous work also assume all clients can participate in training, and I would strongly encourage the authors to clarify the source of \u201cintra-round heterogeneity\u201d\n\nCould you clarify \u201csince many companies like Google, Facebook remains previous data at the turning point of legislation for privacy\u201d for motivating the proxy dataset?\n\nI may have missed it, is the code open sourced? The authors mention they implement the algorithms in PyTorch. Using a FL framework, or based on previous released code can significantly help reproducibility. \n\nMinor issue:\nThe citation format does not seem to be consistent. I would suggest the authors carefully consider the usage of `\\citep` and `\\citet`. \nI cannot understand why Kairouz et al 2021 is cited for Figure 1. \nSome grammatical errors might need to be corrected. \n",
            "summary_of_the_review": "The idea is interesting, but the draft itself needs improvement. Ablation study and experimental performance are my main concerns. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6577/Reviewer_o7iB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6577/Reviewer_o7iB"
        ]
    },
    {
        "id": "TU68d2sMrk",
        "original": null,
        "number": 3,
        "cdate": 1667191219609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667191219609,
        "tmdate": 1667191219609,
        "tddate": null,
        "forum": "IQM-3_Tzldw",
        "replyto": "IQM-3_Tzldw",
        "invitation": "ICLR.cc/2023/Conference/Paper6577/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a method called FedPA that deals with client and period drift problems. The period drift problem is caused by the asynchronized updates of each client, leading to extra bias in model aggregation. The authors proposed a learning-based aggregation strategy, that parameterizes the aggregation function using neural network models. The models are trained under a meta-learning framework, which treats the global model as a meta-learner and each client as a specific task. Experimental results have shown that FedPA can account for the additional bias induced by both client and period drift and therefore demonstrate superior performance over other FL baselines in various tasks.",
            "strength_and_weaknesses": "Strength:\n1. Drifts in FL arise in time and space, while most existing works only address the heterogeneity of client data distributions. This paper has discovered this practically important problem and proposed the notion of period drift that can facilitate further research.\n2. The authors have conducted comprehensive experiments in different settings. Results have shown that FedPA has a superior advantage over baselines in different categories.\n\nWeakness:\n1. This paper lacks an in-depth discussion on why meta-learning frameworks are particularly suited for the period drift problem. It seems like both client and period drift influence the model performance by introducing extra bias in model aggregation. In that case, why not use a regularization-based approach incorporated with a temporal dimension? Moreover, it seems like this paper [1] have studied a similar problem, the authors could consider comparing FedPA with their work as an additional baseline.\n2. The dynamic system analogy seems useless in section 3. The authors are not using significant knowledge from this area. I would recommend adding more discussions or simply removing this part to avoid confusion.\n3. From my understanding, FedPA accounts for additional bias via controlling $\\Delta w_t^k$ through $u_t^k$, then why do we need two separate neural networks for both $w$ and $\\Delta w$? The authors need to be more specific on the choice of NN architectures.\n\nMinor:\n1. Please add a discussion on FedDL in section 2.\n2. Please move the definition of $n_k$ and $n$ to the beginning of section 3.\n\n[1] Jothimurugesan et al., Federated Learning under Distributed Concept Drift, 2022",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-motivated and easy to follow. The technical novelty is ok but not much. It is also ambiguous whether the proposed method is the best solution for this problem (please see weakness).",
            "summary_of_the_review": "This work can benefit from adding more in-depth discussion on the unique advantages of the proposed method and further polishing the writing. I am leaning toward rejecting this paper at this time.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6577/Reviewer_L63q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6577/Reviewer_L63q"
        ]
    },
    {
        "id": "ILzsJyUYfN",
        "original": null,
        "number": 4,
        "cdate": 1667364578178,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667364578178,
        "tmdate": 1667364578178,
        "tddate": null,
        "forum": "IQM-3_Tzldw",
        "replyto": "IQM-3_Tzldw",
        "invitation": "ICLR.cc/2023/Conference/Paper6577/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a learnable aggregation scheme in the context of federated learning. The paper achieves this using meta-learning to generalize the parameters of the aggregator with a proxy dataset. The paper identifies 'period drift' in the current federated learning setup and presents the meta-learning-based aggregator as a way to overcome this issue. The paper follows up with experimental results showing increased accuracy for different methods and heterogeneity rates across two datasets.",
            "strength_and_weaknesses": "Strengths\n1. The paper identifies a possible source of client drift\n2. The paper proposes a novel aggregation scheme.\n\nWeaknesses\n1. The paper does not do enough to discriminate between regular client drift and the so called period drift either theoretically or through experiments.\n2. The aggregation strategy uses a proxy dataset which limits use cases. Also, it is very similar to other knowledge distillation-based techniques like FedET[1] and DS-FL[2]. A comparison of performance with these methods should be shown to justify its usefulness.\n3. There is no ablation study showing the effect of the data distribution in the proxy data on model performance.\n4. The experimental settings are not strong. The datasets and models are too simple. I suggest including results on CIFAR-100 and Stack Overflow datasets.\n\n\n[1] Cho, Y. J., Manoel, A., Joshi, G., Sim, R., & Dimitriadis, D. (2022). Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning. arXiv preprint arXiv:2204.12703.\n\n[2] Itahara, S., Nishio, T., Koda, Y., Morikura, M., & Yamamoto, K. (2020). Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data. arXiv preprint arXiv:2008.06180.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is not very well written and has some grammatical mistakes.\nQuality: The paper quality needs to be improved. The axes font in the figures is too small to read and overall the writing needs to be updated.\nNovelty: The paper has limited novelty.\nReproducibility: No code was given.",
            "summary_of_the_review": "The paper proposes a meta-learning-based aggregation scheme. However, it does not show enough theoretical or experimental justification to highlight the effectiveness of the algorithm. Additionally, the paper lacks enough ablation studies on the different aspects of the algorithm like the data distribution of proxy data, the influence of the size of the aggregator model, etc.  Furthermore, the paper's concept of 'period drift' is not well defined despite being a key motivation of the algorithm.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6577/Reviewer_7sGj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6577/Reviewer_7sGj"
        ]
    }
]