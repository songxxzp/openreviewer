[
    {
        "id": "yf0Ws90COO",
        "original": null,
        "number": 1,
        "cdate": 1666552232689,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666552232689,
        "tmdate": 1666552232689,
        "tddate": null,
        "forum": "O5rKg7IRQIO",
        "replyto": "O5rKg7IRQIO",
        "invitation": "ICLR.cc/2023/Conference/Paper6202/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel algorithm to learn from imperfect teacher's policy and achieves super-teacher performance. It provides theoretical justification, performance bounds and experimental results with comparison against strong baselines.",
            "strength_and_weaknesses": "Strengths:\n- The paper addresses a key problem of learning from an imperfect teacher policy\n- It provides performance bounds and thorough empirical evidence\n\nWeaknesses:\n- The results are shown only on one environment\n- Would be nice to have more details and experiments on how to choose epsilon and if it has to be manually chosen every time based on the environment?\n\nTypos:\n- There is a '5' at the beginning of the introduction section\n",
            "clarity,_quality,_novelty_and_reproducibility": "Well written paper, novel contribution and most details for reproducing the results are shared. ",
            "summary_of_the_review": "I recommend to accept the paper ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6202/Reviewer_rkHD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6202/Reviewer_rkHD"
        ]
    },
    {
        "id": "3YfjRHDoVfM",
        "original": null,
        "number": 2,
        "cdate": 1666835805236,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666835805236,
        "tmdate": 1670861658537,
        "tddate": null,
        "forum": "O5rKg7IRQIO",
        "replyto": "O5rKg7IRQIO",
        "invitation": "ICLR.cc/2023/Conference/Paper6202/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers the setting where one is provided a reference policy and we assume that we could be able to estimate or access the value function of the reference policy. The paper thus proposes the method called TS2C where they construct a mixture policy of the student policy and the reference policy and proposes a heuristic to decide which policy to use at run time. The experiment results show that the student policy indeed can overperform the reference policy in practice. ",
            "strength_and_weaknesses": "# Strength\n* The paper is easy to follow, and the proposed algorithm is simple. \n* The experiment results indeed confirm that the student policy could eventually outperform the reference policy, validating the intuition of the algorithm.\n\n# Weakness\n* The paper should consider a more diverse set of benchmarks.\n* The theory results in the paper are not very illuminating: for example, I am having a hard time understanding the significance of Thm. 3.3: $H$ should also depend on $\\epsilon$ because $\\epsilon$ defines $\\pi_b$. Thus what exactly does the term $\\sqrt{H-\\epsilon}$ mean?\n* The clearest issue of the paper is that the proposed algorithm seems like that the proposed algorithm is highly relevant to AggreVaTe [1] and LOLS [2]. The overall algorithm framework is very similar, and I would say the differences seem rather insignificant. I would assume that works such as [1,2] are overlooked in the literature search, so it would be great to compare and argue in the paper why the contribution of TS2C given the existence these previous works. \n\n[1] Ross, Stephane, and J. Andrew Bagnell. \"Reinforcement and imitation learning via interactive no-regret learning.\"\n\n[2] Chang, Kai-Wei, et al. \"Learning to search better than your teacher.\" ",
            "clarity,_quality,_novelty_and_reproducibility": "* The clarity of quality of the paper is good.\n* The novelty is greatly compromised given quite similar previous works that are not discussed in the paper.\n* Reproducibility is good since the paper gives detailed experiment setups and the submission includes codes for the experiments. ",
            "summary_of_the_review": "The writing and the experimental results of the paper are good, but at the current stage, I am not convinced that the paper actually makes enough technical contribution given similar previous works. I would recommend a rejection.\n\n======================\nIncreased the score after the rebuttal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6202/Reviewer_wVZY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6202/Reviewer_wVZY"
        ]
    },
    {
        "id": "5k6dstFPgBf",
        "original": null,
        "number": 3,
        "cdate": 1667247413367,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667247413367,
        "tmdate": 1667248232686,
        "tddate": null,
        "forum": "O5rKg7IRQIO",
        "replyto": "O5rKg7IRQIO",
        "invitation": "ICLR.cc/2023/Conference/Paper6202/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors proposed a teacher-student framework (TSF), where a\nteacher agent or human expert guards the training of a student agent by intervening and providing online demonstrations that the teacher policy may not be optimal. This is a more realistic setting than standard TSF in which the teacher policy is always assumed to be near-optimal. Specifically, they develop a new method that can incorporate arbitrary teacher policies with\nmodest performance and utilize an off-policy RL method known as Teacher-Student Shared Control (TS2C) to effectively update the student policy. In this method the main novelty is to incorporate teacher intervention based on trajectory-based value estimation. Theoretical analysis justifies also that the proposed algorithm attains efficient exploration and lower-bound safety guarantee without being affected by the teacher\u2019s\nown performance, and this algorithmic finding is also furthered validated with several benchmark experimentations (especially with autonomous driving simulation), showcasing that TS2C can exploit teacher policies at any performance level, maintain lower\ntraining cost and the corresponding student policy can often outperform the imperfect teacher policy.",
            "strength_and_weaknesses": "Strengths:\nThe paper is well-written with concepts clearly explained. \nThe problems studied in this work is important and the concerns of existing TSF work where the teacher is always optimal is a realistic one with good real-world implications. \nExperiment is thorough, validating the student policy can outperform the teacher in the TS2C algorithm. It is also backed with some theoretical analysis to justify this approach\n\nWeaknesses:\nDiscussions of some experimental parameters are still missing. For example it is still unclear how the epsilon parameter is chosen. \nThe theory part of this work is very straight-forward, the bound developed is pretty much a direct result from existing work.\nespecially lemma 1 and theorem 3.2 are pretty much standard results from occupancy distribution matching and policy-value differences in RL (e.g., TRPO-style proofs).\nHow tight is the bound in Theorem 3.3 (it seems with the (1-\\gamma)^2 in the denominator the bound is rather loose), and how to effectively determine the average entropy in practice to validate the tightness of this bound?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, with both algorithms and experimental setup quite clearly explained. In terms of contributions, the main novelty is TS2C, which is a flexible TSF framework that combines with off-policy RL student improvement and a value-aware methodology of deciding when a teacher policy should intervene the student.\n\nTS2C can work with a wide range of teacher policies with different performance. Finally the authors evaluate the TS2C algorithm on a wide range of benchmark experiments and detailedly demonstrate the ability of TS2C in an autonomous driving scenario, which shows the algorithms' ability not only on imitation learning (of teacher) but also performance improvement guarantees. Contribution is slightly more on the empirical sense in which a new TSF framework is proposed to solve the problem of imperfect teacher. Algorithmically the novelty is moderately significant and most experimental hyperparameters and setup are explained in great details in the paper and its appendix to help readers better understand the experiment. However, the source code is not available reproducibility.",
            "summary_of_the_review": "The work is quite neat in terms of addressing the teacher-student framework in RL problem in which the teacher does not necessarily need to be near-optimal. The TS2C framework is also quite flexible and can be adapted to different off-policy RL algorithms. Experiments show the superiority of this algorithm in terms of imitating the teacher, deciding when the teacher should intervene the student during training and having the learned student. outperforming the teacher.\n\nNovelty wise I think the underlying technique has merits in terms of problem formulation, algorithms and experimentation, albeit the theoretical analysis being a bit straight-forward, which is my main concern of the technicality of this work. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6202/Reviewer_oviQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6202/Reviewer_oviQ"
        ]
    },
    {
        "id": "Xg-DyImwM18",
        "original": null,
        "number": 4,
        "cdate": 1667528967016,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667528967016,
        "tmdate": 1667528967016,
        "tddate": null,
        "forum": "O5rKg7IRQIO",
        "replyto": "O5rKg7IRQIO",
        "invitation": "ICLR.cc/2023/Conference/Paper6202/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the Teacher-Student Framework, in the case when the teacher is potentially sub-optimal. Learning here is based on an ensemble off-policy method. The core concept is to develop a teach intervention function that is based on the estimated sub-optimality of student actions with respect to the teacher's value function. The advantage of the proposed method is that it allows for continuous student improvement, to the point that the student takes complete control in the case of sub-optimal teacher. The authors provide theoretical bounds for the proposed method and evaluate it on a driving simulator. The proposed method 1) outperforms baselines across evaluation scenarios and 2) demonstrates the student capability to solve the task, even with a significantly sub-optimal teacher. ",
            "strength_and_weaknesses": "Strengths:\n\n1. The proposed algorithm is intuitive and straight-forward\n2. It asymptotically outperforms baselines\n3. The proposed method is backed by theoretical analysis\n\nWeaknesses\n\n1. It seems that the algorithm takes some time before it starts improving and is initially substantially outperformed by EGPO. Is this because at the start of the training the switch allows most student actions, while EGPO mostly uses the teacher?\n\nQuestion:\n\nWhat is the effect of the optimization algorithm here? The proposed method uses Q-ensembles, which combined with more often training have demonstrated significant improvement in sample complexity (RedQ). What update frequency was used in this paper and is this directly comparable to the baseline methods? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and clear. The core concept of agent switches here is not new, but the particular implementation allows the student to learn and eventually outperform a sub-optimal teacher, unlike baseline methods. ",
            "summary_of_the_review": "Simple and intuitive method to allow student agent to learn and outperform a potentially sub-optimal teacher. Clear writing with good theoretical backing. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6202/Reviewer_L3ge"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6202/Reviewer_L3ge"
        ]
    }
]