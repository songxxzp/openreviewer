[
    {
        "id": "y75KQCpCKCw",
        "original": null,
        "number": 1,
        "cdate": 1665855327919,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665855327919,
        "tmdate": 1665940175463,
        "tddate": null,
        "forum": "3Y5Uhf5KgGK",
        "replyto": "3Y5Uhf5KgGK",
        "invitation": "ICLR.cc/2023/Conference/Paper510/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is about the performance of models trained on ImageNet-1K. The authors take the premise that performance on IN-1K and on other transfer tasks are desirable, and they examine how to achieve these two objectives simultaneously. They have traditionally been achieve distinctly by supervised and self-supervised approaches, respectively. The main concrete contribution is a suite of enhancements to a supervised model.",
            "strength_and_weaknesses": "### Strengths (S) and weaknesses (W)\n\n- S1: Interesting setting: focus on the *trade-off* between performance on training task / transfer tasks.\n- S2: The empirical evaluation is extensive and do adequately support the claims.\n- S3: The proposed enhancements to a supervised model lead to two SOTA models on each of the two settings, with competitive performance on the other respectively.\n\n- (W1) This is mostly an empirical/engineering paper. A lot of different pieces are put together and evaluated empirically. There is surely value in these results and SOTA models, but I am not sure what findings are applicable/relevant beyond IN-1k. Can the authors comment on this?\n- (W2) What does \"cosine\" mean in \"cosine softmax\"? The only difference with a standard softmax seems to be a summation over crops, and an additional vector of learned class-wise weights, that follow an L2 normalization of the outputs.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is very good and clear, as is the organization of the paper. The summarization the findings makes things very clear for the reader.\n",
            "summary_of_the_review": "Not sure about the scientific value of this paper. But the contributions should nevertheless be of interest to some of the audience, and I can see no obvious flaws. Therefore I recommend acceptance!",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper510/Reviewer_p7Yg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper510/Reviewer_p7Yg"
        ]
    },
    {
        "id": "_1A1N8qAyNW",
        "original": null,
        "number": 2,
        "cdate": 1666500451717,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666500451717,
        "tmdate": 1668607984931,
        "tddate": null,
        "forum": "3Y5Uhf5KgGK",
        "replyto": "3Y5Uhf5KgGK",
        "invitation": "ICLR.cc/2023/Conference/Paper510/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors revisit the paradigm of supervised learning with recent successful breakthroughs in self-supervised learning, including multi-crop data augmentation [Caron et al., 2020], expendable projector head [Chen et al., 2020a], and a variant of nearest class means classifier [Mensink et al., 2012]. They found that those components, while working well with self-supervised settings, also work well with conventional supervised settings. Combined with those 3 components, the authors show improvement over other supervised, self-supervised, and semi-supervised approaches for ResNet-50.",
            "strength_and_weaknesses": "Strength:\n- The proposed modifications are straightforward and easy to follow. They are easily reproducible as most of the components have been released by previous works.\n- Various figures (2 & 3) are helpful in comprehending the proposed changes.\n- I appreciate the ablation study of those 3 components with detailed performance analysis.\n\nWeaknesses\n- As discussed in related work by the authors themselves, all 3 components have been proven useful. The multi-crop data augmentation and expendable projector head are now widely used in various self-supervised learning approaches. Directly applying and combining 3 well known components in my opinion does not necessarily guarantee meeting the novelty bar at ICLR.\n- For example, since AlexNet, VGG, and ResNet, various different image cropping methods have been proposed. With the recent semi-supervised learning advances, we now know that more aggressive image cropping can generally make the training task harder, making the network learn harder and generalize better. The authors found that multi-crop data augmentation from [Caron et al 2020] also works well on ResNet under supervised learning setting should not be a huge surprise.\n- Also, expendable projector head essentially forces the downstream tasks to use feature representations from the earlier layer (not the last layer) has also been largely explored such as [A1, A2]. One can essentially use any combination of any previous layers for the downstream task training. \n- The major challenge in transferring features learned from supervised learning is that it generally does not perform well on class-imbalanced downstream tasks. This is due to during supervised learning, the features are learned to tailor the class distribution from the pre-training set. It would be great that the authors can shed more light on this for any future improvement of the manuscript.\n\n[A1] Lin et al. Feature Pyramid Networks for Object Detection. CVPR\u201917\n[A2] Lee et al. Deeply-Supervised Nets. AISTATS\u201915\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear but with much redundant information. For example, the introduction section uses 1 page of the space but essentially the message is that the authors try to combine 3 well-known components in a supervised learning setting. I encourage the authors to spend more space on the analysis of class-imbalanced downstream tasks and show whether supervised learning still can outperform self-supervised learning for transfer learning.",
            "summary_of_the_review": "Please provide a short summary justifying your recommendation of the paper.\nThe proposed modifications are simple and easy to understand. 3 components used in this work are well known from self-supervised learning and other fields. I do not see major novelty in the current form of the manuscript and encourage the authors to expand more on the class-imbalanced setting to see if the proposed changes can benefit for that scenario. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper510/Reviewer_yEpr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper510/Reviewer_yEpr"
        ]
    },
    {
        "id": "ZBL060eAc3",
        "original": null,
        "number": 3,
        "cdate": 1666551012713,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666551012713,
        "tmdate": 1666551012713,
        "tddate": null,
        "forum": "3Y5Uhf5KgGK",
        "replyto": "3Y5Uhf5KgGK",
        "invitation": "ICLR.cc/2023/Conference/Paper510/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a supervised learning framework, named T-Rex, which is a combination of several training strategies, including 1) multi-scale cropping for data augmentation 2) a better designed projection head 3) an auxiliary loss function based on protypical contrastive learning. With the combination of these training strategies, the proposed method achieved improved performance on transfer learning and supervised learning benchmark on image classification tasks. The paper further analyse each component of the proposed training strategy and its corresponding hyper-parameters, shines some insights of the effectiveness of the overall improvement.",
            "strength_and_weaknesses": "For the full transparency, I reviewed the prior version of this work. The paper was rejected was mainly because of i) lack of insights of the motivation of each design and ii) lack the detailed analysis of each method. I am glad the current version has mostly resolved these issues and expanded the experiments significantly with more analysis and visualisations.\n\nStrength.\n\n++ A comprehensive analysis of training tricks for supervised image classification, which may benefits the community.\n\n++ A detailed hyper-parameter ablative study on each training strategy to understand its contribution to the overall improvement.\n\nLimitation.\n\n-- All proposed training strategy is not new, and therefore the technical contribution is rather limited.\n\n--The comparison for DINO and PAWS might be a bit unfair, since they are unsupervised learning methods?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall analysis and paper structure are quite clear. The novelty is limited mainly due to the state of this paper being an analysis of existing training strategies rather than proposing a new framework. I have no questions on the reproduciblity.",
            "summary_of_the_review": "The paper is improved compared to the last version. The paper is clear in terms of explaining and providing detailed experiments on each training trick. The technical contribution is limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper510/Reviewer_o6eH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper510/Reviewer_o6eH"
        ]
    },
    {
        "id": "wtyz2CJkgB",
        "original": null,
        "number": 4,
        "cdate": 1666641310664,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641310664,
        "tmdate": 1669015933038,
        "tddate": null,
        "forum": "3Y5Uhf5KgGK",
        "replyto": "3Y5Uhf5KgGK",
        "invitation": "ICLR.cc/2023/Conference/Paper510/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Motivated both by the fact that Self-Supervised Learning (SSL) models have better transferability than supervised models and by the expectation that additional information on labels should not impair generalization performance, this paper attempts to achieve both good supervised classification accuracy and good transferability to other tasks. Regarding the trade-off between good performance on the training task and transferability, the paper carried out extensive experiments and claims that multi-crop data augmentation is one of the key ingredients for transferability. It also finds that the trade-off can be controlled by the design of the projector head. Deeper head leads to better generalization and shallower one leads to better IN1K performance. The paper also incorporates a class prototype to improve performance and achieves SOTA regarding training-versus-transfer performance. ",
            "strength_and_weaknesses": "The strengths of this paper are (1) finding that generalization can be obtained from multi-crops data augmentation (DA) that SSL is based on, (2) finding that deeper head leads to better generalization and shallower head leads to better IN1K performance, (3) presenting SOTA models (t-ReX and t-ReX*) for the training task vs transfer task performance, and (4) an interesting comparison between the use of class weights and the use of prototypes for training, showing that prototypes with a memory bank performs slightly better.\n\nThe first and second findings are supported by several careful analyses as well as direct experiments. They show that the intra-class L2 distance between samples increases with the projector and decreases with the multicrop DA, and that the fraction of feature dimensions close to zero (which they call sparsity) is larger without the projector. These results indicate that deeper projectors improve generalization performance because the representation is more distributed and less sparse. Those analyses were interesting to me. The SOTA models will be released, and I think the models as well as the details of the learning presented in this paper will be valuable to the community.\n\nThe weaknesses of this paper are that the methodology is one that the community is already familiar with and the findings are still based on experiments, so the findings have not been shown to be theoretically correct. We can make hypotheses, but we do not know if the findings are correct for other large data sets.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and I enjoyed reading it. \n\nI would probably use u and w for Figure 2 instead of mu and omega, as they are consistent with the equations in the text. \n\nThe discussion about gradient similarity and the middle figure in Figure 6 were not very intuitive to me. Intuitively, I would expect the gradients to be similar for easy samples and to be diverse for challenging samples; since Base+Mc would be more difficult than Base, the high similarity of Base+Mc would seem to contradict this. The middle figure in Figure 6 may just show that Base converges faster. I would like to ask the authors why the gradient similarity is higher in Base+Mc than in Base.\n",
            "summary_of_the_review": "Overall, the paper was an enjoyable read. Although only empirical findings are presented, the experiments are careful and reliable. The findings on transfer learning will be valuable to a wide range of audience.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper510/Reviewer_63Ba"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper510/Reviewer_63Ba"
        ]
    }
]