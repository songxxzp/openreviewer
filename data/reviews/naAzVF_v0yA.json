[
    {
        "id": "XlwByJ0C6ff",
        "original": null,
        "number": 1,
        "cdate": 1666489217524,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666489217524,
        "tmdate": 1666722556265,
        "tddate": null,
        "forum": "naAzVF_v0yA",
        "replyto": "naAzVF_v0yA",
        "invitation": "ICLR.cc/2023/Conference/Paper699/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a metric to measure generalization ability for transformer models that is not data-dependent. Their proposed metric is based entirely on a spectral analysis of the weight matrices, and the authors seek to show that this metric correlates directly with test accuracy (as a proxy for model generalization) rather than the 'generalization gap' considered by other works. They demonstrate the effectiveness of their generalization metric on transformers trained with a wide variety of hyperparameter settings on WMT '14, as well as analyzing its predictions on a number of pretrained models from the literature such as BERT, GPT-2 and T5.",
            "strength_and_weaknesses": "**Strengths**\n\nThe idea of producing a comprehensive study of generalization metrics for NLP models is an interesting and valuable contribution, and the proposed metric does indeed perform well in the thorough empirical studies they performed. The authors tested a wide variety of hyperparameter settings and convincingly showed strong performance on predicting generalization ability. The analysis of existing pretrained models also led to some interesting results, and considered a wide variety of models from the literature with different variants.\n\t\n**Weaknesses**\n\nMy primary criticism of this paper is that almost all this analysis was performed only on a single dataset:  the WMT '14 english to german translation dataset. As such, it is difficult to be entirely confident that the results shown in the paper are truly general, and not specific to either this dataset or machine translation models on the whole. An analysis of multiple datasets with diverse task settings and dataset sizes that could show consistent results would be significantly more convincing. I certainly understand that this must already have been an enormous computational undertaking, but surely some of the resources used in the broad hyperparameter search might be better spent on replicating these results with other datasets. This is particularly critical given that this paper's primary contribution seems to be the broad scope of empirical analysis, rather than technical novelty.\n\nMy one other criticism is that I believe it is extremely important that papers relying on large-scale experiments accurately report the computational cost of their experiments and particularly the approximate greenhouse gas emissions of said experiments (there are a number of calculators online for estimating this). It is important for us to be cognizant of the environmental implications of our research and I believe transparency about the environmental cost of our experiments is ethically significant.\n\nQuestions/Minor errors:\n1) I don\u2019t believe ESD is defined anywhere in the introduction of the paper outside of the abstract\n2) Figure 2 is a little bit unclear. You state that \"the shape metrics always correctly predict the model quality, i.e., the BLEU scores should be higher when the metric values are smaller\". Is this true only for the black starred models (the optimal configurations)? The wording makes it seem like this holds more generally, but this is not visually obvious and is contradicted by the caption.\n3) In Figure 4 the 'scale' metrics seem near-perfectly anticorrelated with model quality. Shouldn't that mean these metrics are actually *more* predictive of model quality than the shape metrics, as a perfect negative indicator should be just as valuable as a perfect positive one?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is generally clear and well-written (with a few small exceptions), and makes a significant contribution to the field. It is not wholly original as it does not propose any technically novel methods, but the scope and thoroughness of the study nevertheless constitute valuable and novel contributions. My only qualm with this is the limitation of focus to only a single dataset, which may make the results of questionable value outside the scope of that one dataset and task.",
            "summary_of_the_review": "Overall I think the paper makes a valuable contribution in thoroughly cataloguing generalization metrics for NLP models and demonstrating the effectiveness of HT-SR methods at this task in a data-agnostic way. If these results had been demonstrated on a broader array of natural language tasks and datasets I would have no qualms about recommending it be accepted. As is, I believe this is a significant enough limitation for a work whose primary novelty is in the scope of its empirical studies that I view it as more of a borderline paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper699/Reviewer_oVEt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper699/Reviewer_oVEt"
        ]
    },
    {
        "id": "01NOQIjMmYu",
        "original": null,
        "number": 2,
        "cdate": 1666497636530,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666497636530,
        "tmdate": 1666497636530,
        "tddate": null,
        "forum": "naAzVF_v0yA",
        "replyto": "naAzVF_v0yA",
        "invitation": "ICLR.cc/2023/Conference/Paper699/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides a very comprehensive empirical analysis of predicting the test set performance on NLP tasks, and reveals several interesting properties like shape based metrics are generally better than scale based ones, and some fitting strategies (for empirical spectral densities of a weight matric) like E-TPL could alleviate the issues of other typical fitting methods like pure PL based. ",
            "strength_and_weaknesses": "**Strengths**\n\n1. The problem at hand is indeed meaningful: getting the full-set dev set performance for real NLP tasks, especially that web-scale ones, are very time and resource consuming, hence having a data-free surrogate metrics for model selection would be quite useful, especially for real industry use cases;\n\n2. The study here is quite solid which delivers convincing insights for the community. \n\n3. The paper is generally written in a clear way.\n\n**Weaknesses**\n\n1. The raw innovation of this paper seems limited given its natural of being an empirical study focused work.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nIt is impressive that this paper is stated in such a clear way that I feel easy to absorb quite a few information it wants to deliver.  In particular it gives a good literature survey which lays down the overall background.\n\nI only have one question which puzzled me: I cannot figure out why *Then even if we have i) access to both models\u2019 training errors, and ii) a metric which is guaranteed to perfectly rank correlate with the generalization gap, then we still cannot determine which model as smaller test error.*\n\n**Quality**\n\nThe study here is quite solid. For example different settings of NLP experiments are studied like across different well trained models, across different hyper-parameters and across different training phases/time. I believe it could be very useful for NLP researchers who care a lot about real-world performances.\n\n**Novelty**\n\nWhile this paper proposes several new metrics to study, its technical novelty seems not comparable with several previous works. On the other hand there is a certain degree of novelty in terms of problem itself, that is driving some data free metric to predict test set performances in NLP which would be very useful in real-world.\n\n**Reproducibility**\n\nGood.\n\n ",
            "summary_of_the_review": "As stated above, despite that lacking of technical novelty, this paper is generally good given its solid empirical study which could be of good/common interest to NLP researchers. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper699/Reviewer_zKbc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper699/Reviewer_zKbc"
        ]
    },
    {
        "id": "m0OYStysLPD",
        "original": null,
        "number": 3,
        "cdate": 1666648451263,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648451263,
        "tmdate": 1666648451263,
        "tddate": null,
        "forum": "naAzVF_v0yA",
        "replyto": "naAzVF_v0yA",
        "invitation": "ICLR.cc/2023/Conference/Paper699/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors provide the first systematic empirical study on various generalization metrics in NLP, including 400 transformers trained with varying hyperparameters. The authors measure the correlation between 28 generalization metrics and the model quality. The authors find that shape metrics consistently perform better than scale metrics for predicting model quality.\n\n",
            "strength_and_weaknesses": "Strength:\n1. The authors have done widely exploration of generalization metrics on machine translation tasks.\n2. The metric can be used for machine translation model evaluation. \n\nWeakness:\n1. The paper over-claimed the contribution. The authors only work on machine translation task but claim \"Evaluating natural language processing models \" in the title. I would suggest authors work on GLUE/SuperGLUE/AdvGLUE dataset to further explore the robustness.\n2. Not sure why the authors want to emphasize \"do not need access to any training or testing data\". It is ambiguous. Some scores like BERTScore (Evaluating Text Generation with BERT), BARTScore (BARTScore: Evaluating Generated Text as Text Generation) also don't need. Can the metric evaluate model only based on the model initialization?",
            "clarity,_quality,_novelty_and_reproducibility": "Transferring the metric from CV to machine translation. The novelty looks a bit limited, but experiments are solid.",
            "summary_of_the_review": "Overall, I think the authors over-claimed too much on the contribution. I would suggest the authors either add more NLP tasks in the experiments or change the title. I appreciate the large-scale experiments on machine translation tasks, while the metric mainly comes from CV and the novelty is little bit limited.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper699/Reviewer_j1Vx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper699/Reviewer_j1Vx"
        ]
    },
    {
        "id": "2dq0Y8Bssf",
        "original": null,
        "number": 4,
        "cdate": 1667486317807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667486317807,
        "tmdate": 1667486317807,
        "tddate": null,
        "forum": "naAzVF_v0yA",
        "replyto": "naAzVF_v0yA",
        "invitation": "ICLR.cc/2023/Conference/Paper699/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper empirically measures correlations between various generalization metrics and model quality (BLEU score) on a machine translation dataset across a range of hyperparameters. Compare to prior work, the main differences are:\n- Focusing on NLP, rather than computer vision\n- Focusing on metrics that predict test error rather than generalization error\n- Focusing on a class of metrics based on whether the empirical spectral density (ESD) of the weight matrix follows a power law.",
            "strength_and_weaknesses": "**Strengths**\n- This paper extends the study of generalization metrics to NLP. This is a valuable contribution, because unlike in computer vision, NLP models are not typically trained to zero training error. As the authors note, even if a generalization metric is rank-correlated with the generalization gap, it may not be correlated with the test error due to differences in training error. There are also differences in neural network architecture (transformers vs. CNNs).\n- The paper considers a fairly wide range of metrics and hyperparameters.\n\n**Weaknesses**\n- There are two main components in the paper (1) Evaluating natural language processing models with generalization metrics, and (2) advocating the use of a certain class of generalization metrics based on heavy-tail self-regularization theory (HT-SR), including several new metrics based on fitting exponentially truncated power laws to the ESD of the weight matrix. In my opinion, the second point is largely orthogonal to the first, and it's difficult to assess whether these HT-SR metrics are preferable to existing metrics due to methodological differences with prior work [1, 2]. I think the paper would be stronger if it placed less emphasis on shape metrics and focused more on evaluating generalization metrics for NLP, using a similar evaluation methodology to prior work.\n- Interpretation of Figure 4 (comparing generalization metrics): Figure 4 indicates that scale-based metrics are positively correlated with generalization gap but have a strong negative correlation with model quality, meaning a lower value of the generalization metric is associated with higher value of model quality. Isn\u2019t this to be expected? We would expect that a low generalization gap is correlated with a high quality score. From this point of view, the metrics at the bottom of Figure 4a seem better. The other evaluations in the main paper only compare shape metrics, or compare correlations with hyperparameters rather than quality (see next point), so overall I am not convinced that shape metrics are preferable to other metrics.\n- The evaluation setting in 3.3 is puzzling to me. From what I can understand, this section does not measure the correlation between generalization metrics and model quality, but instead measures correlation with architecture parameters like depth and hidden dimension. It's not clear how this section supports the argument of the paper. This section might be more convincing if it compared models that were trained on the same data and then evaluated on a consistent quality metric.\n- Justification for HT-SR metrics: In particular, is there any theoretical justification for these metrics? The authors propose several different shape metrics (based on PLs and E-TPLs, different parameters, and goodness-of-fit metrics). Which of these should be preferred?\n- Methodological differences from prior work: Prior work [1, 2] has explored methodological questions about comparing generalization measures and proposed metrics and methodologies aimed at controlling for noise and unwanted correlations---in particular using Kendall's tau correlation and controlling for Monte Carlo noise across environments. This paper reports Spearman's rank correlation and does not explicitly address the questions raised by [1, 2] about how to robustly evaluate generalization metrics.\n- \"Generalization metrics that do not need access to any training or testing data\": The authors argue that shape metrics are preferable \"without access to training or testing data\". It would be valuable to be more clear about how existing metrics depend on the training data: to my understanding, metrics based on generalization bounds depend only the number of samples in the training data, which is a relatively mild requirement (you need to know the size of the data, but don't need access to the data itself). Also, none of the other generalization metrics considered use the testing data, so that should not be considered a special property of shape metrics.\n- The authors note that \"NLP pretraining datasets are typically web-scale and are challenging to access\", but their evaluation is focused on a machine translation dataset (where the data is available), so the argument about NLP pretraining is not entirely relevant.\n- Correlation evaluated on optimally trained models: It's not clear to me that this is a reasonable evaluation setting. How are you supposed to determine which model is optimally trained without access to testing data? If you can determine whether a model is optimally trained, why do you need a generalization metric?\n\n- The paper only considers one dataset and quality metric (BLEU score). It would be valuable to consider a wider range of commonly used NLP benchmarks, such as the GLUE benchmark.\n- It's not entirely clear to me which weight matrices are being considered in these metrics, and how the scores of different weight matrices are aggregated. More generally, it would be valuable to discuss another difference from prior work, which is that the models considered here are based on the transformer architecture, while prior work has mainly studied CNNs (and most theoretical metrics have been developed for feed-forward networks, but see [3].)\n\n[1] Jiang et al., 2019. Fantastic generalization measures and where to find them.\n[2] Dziugaite et al., 2020. In search of robust measures of generalization.\n[3] Long and Sedghi, 2019. Size-free generalization bounds for convolutional neural networks.",
            "clarity,_quality,_novelty_and_reproducibility": "*Quality*: The paper conducts an extensive evaluation of generalization metrics, but I have some doubts about the choice of evaluation and the interpretation of the conclusions. In particular, the first set of evaluations (figures 2 and 3) only only compares shape metrics; the second set of evaluations (figure 4) may indicate that scale-based metrics are actually preferable (see above); and final set of evaluations (figure 5) may have limited relevance to the paper, because it is comparing correlation with model hyperparameters rather than a quality score.\n\n*Clarity/reproducibility*: The paper is clearly written for the most part, but I have methodological questions about how the scores from different weight matrices are combined to calculate the PL metrics.\n\n*Novelty*: To my knowledge, evaluating generalization metrics for NLP has not been addressed in prior work, and this is a valuable new contribution.",
            "summary_of_the_review": "My recommendation is to reject this paper. The paper addresses an interesting problem, evaluating generalization metrics for NLP, but it is difficult to interpret the results due to the secondary focus of the paper on advocating the use of a certain class of metrics, and I find that some of the analysis does not seem to support the main claims of the paper. I think the paper would be stronger if it placed less emphasis on shape metrics and focused more on evaluating generalization metrics for NLP, using a similar evaluation methodology to prior work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper699/Reviewer_rJjc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper699/Reviewer_rJjc"
        ]
    }
]