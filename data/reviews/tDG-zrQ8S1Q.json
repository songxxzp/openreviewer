[
    {
        "id": "spuSfP1thkD",
        "original": null,
        "number": 1,
        "cdate": 1665824527491,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665824527491,
        "tmdate": 1665825138153,
        "tddate": null,
        "forum": "tDG-zrQ8S1Q",
        "replyto": "tDG-zrQ8S1Q",
        "invitation": "ICLR.cc/2023/Conference/Paper4964/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a scalable offline policy pre-training approach based on natural language instructions.\n\nIt enables automatic augmentation of pre-training data with large language models to relabel and chain across trajectories.\n\nIt empirically shows that the proposed approach outperforms prior pre-training approaches.",
            "strength_and_weaknesses": "***Strength***\n\n- It can populate pre-training data without additional human labeling. It only requires labeling sub-trajectories (maybe with crowd-sourcing), and the approach will combine the trajectories for new data.\n\n- The evaluation results show its advantage over prior methods.\n\n***Weakness***\n\n**The weakness is mainly in cross-trajectory chaining.**\n\n**1. The primary concern is the change of MDP**\n\nThe cross-trajectories chaining changes the original Markov Decision Process in the transition probability distribution P. More precisely, the value of chaining transition s_TA x a_TA x s_0B is changed.\nFor example, if sub-trajectory A ends in a conference room, and sub-trajectory B starts in a kitchen, then the last action of A warps the agent in the new MDP.\nIt encourages the agent to try warp when it wants to go to another room (after finishing a sub-trajectory).\nIt seems the reward of Q(s, a, z_B) is not designed to correct the problem of MDP change.\n\nIt might need explanations if the change of MDP is supposed not to have significant influences in general.\n\n**2. The online change of Q**\n\nAs mentioned in the paper, the reward Q(s, a, z_B) changes during training and is updated online.\nIt requires the RL algorithm to accept online changes in rewards.\nSo, the paper may need more arguments on what RL algorithms are compatible (it says \"any offline RL algorithm\" in Section 4.1).\n\n**3. The novelty**\n\n- The instruction aggregation and using LLM for it might be novel. However, existing LLMs are used.\n\n- The use of consecutive sub-trajectories seems straightforward.\n\n- Cross-trajectory chaining has novelty, though many of its techniques are inspired by goal-conditioned RL approaches (e.g., offline goal chaining [Chebotar et al., 2021]), as mentioned in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThis paper is clear to read.\n\nQuality:\n\nThis paper may have some significant concerns.\n\nNovelty:\n\nThis paper contains novelty.\nPlease see the last section.\n\nReproductivity:\n\nThe paper does not mention (anonymous) source codes.",
            "summary_of_the_review": "This paper proposes a scalable offline policy pre-training approach that outperforms conventional ones.\nIt combines sub-trajectories to augment data without extra human annotation.\n\nHowever, it still has significant concerns, as mentioned earlier.\nMore explanations might help.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4964/Reviewer_QEPp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4964/Reviewer_QEPp"
        ]
    },
    {
        "id": "THrT3e72pRH",
        "original": null,
        "number": 2,
        "cdate": 1666186704656,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666186704656,
        "tmdate": 1668898383608,
        "tddate": null,
        "forum": "tDG-zrQ8S1Q",
        "replyto": "tDG-zrQ8S1Q",
        "invitation": "ICLR.cc/2023/Conference/Paper4964/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes two data augmentation schemes for instruction following training via offline RL:\n\n(1) **same-trajectory instruction aggregation**: Given a trajectory with multiple sub-trajectories, aggregate adjacent sub-trajectories into a longer trajectory and relabel its natural language annotation with a summary of the individual instructions. This summary is generated via a large language model (LLM) prompted with few-shot examples.\n\n(2) **cross-trajectory chaining**: concatenate two random trajectories, and their instructions via \u201cand\u201d. \n\nThe proposed approach, SPRINT (Scalable Pre-training via Relabeling Language INsTructions), is evaluated on ALFRED benchmark, where authors create a set of 100 unseen\nlong-horizon evaluation instructions (EVAL 100), and a set of 20 evaluation commands that test the agent\u2019s chaining capabilities (EVAL CHAIN). On these sets, SPRINT is shown to outperform imitation learning (L-BC) and offline RL (AM) baselines in zero-shot and finetuning setups.",
            "strength_and_weaknesses": "**Strength**: The proposed approach is simple and effective on authors\u2019 test sets of ALFRED, with potential to apply to more instruction following tasks with long-horizon challenges.\n\n**Weakness**: My main concern is that experiments are only done in one benchmark with test sets specifically created by authors that would intuitively benefit from such compositional data augmentations. Is it also possible to also provide results on standard ALFRED evaluation/test sets, which should be more than 100 selected task instances? \n\n\nSome other suggestions/questions:\n- Maybe mention the domain (ALFRED) and learning method (offline RL, instead of more vague \u201cpolicy pre-training\u201d) in abstract? \n- Can benchmark setups be better explained? The first paragraph of Experiment sections talks about unseen task/language/environment, but then EVAL100 is said to be taken from the training set (why not eval set?), and EVALCHAIN unclear. A better explanation of task splits in original ALFRED and author created test sets is needed.\n- Learning setups: Is comparison to baselines fair - do they have similar architecture (size), training steps, number of training language tokens? Are two data augmentation subschemes mixed 1:1 or other proportions? I tried to check some appendix but they are not clear, and maybe including them in main paper would be nice.\n- The cross-trajectory scheme seems only applicable to offline RL, while the same-trajectory one also applies to imitation learning. Would be interesting how the latter alone might benefit IL, if authors want to claim usefulness beyond offline RL but for general policy pre-training.",
            "clarity,_quality,_novelty_and_reproducibility": "The method part is clear and easy to follow, but I have some clarification questions about experiment setups (see above). \n\nThe method seems fairly novel and interesting to me, though I cannot confidently judge the novelty due to my lack of knowledge in related work.",
            "summary_of_the_review": "The approach is simple, interesting, and potentially useful for more instruction following tasks, but more evaluations and explanations of setups would help better justify it.\n\n==\n\nAfter rebuttal, authors addressed many clarification questions and I raise my score from 5 to 6 in light of these.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4964/Reviewer_pfEp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4964/Reviewer_pfEp"
        ]
    },
    {
        "id": "98Btwa-o1U",
        "original": null,
        "number": 3,
        "cdate": 1666727999494,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727999494,
        "tmdate": 1666727999494,
        "tddate": null,
        "forum": "tDG-zrQ8S1Q",
        "replyto": "tDG-zrQ8S1Q",
        "invitation": "ICLR.cc/2023/Conference/Paper4964/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The proposed method is a offline language-based goal-conditioned reinforcement learning. It needs human annotator for labeling basic tasks with languages. It implements large languages models for relabeling and cross-trajectory chaining, in order to increase the diversity of the task set. Experiments are conducted to show the proposed method exceeds previous approaches in terms of task completion rate, more efficient finetuning and 0-shot generalization.",
            "strength_and_weaknesses": "> Strength\n\n1. Generally well writen, structure of the article is well organized. Illustrative figures are intuitive and do a good job to help reader understand the main ideas. Important details are included in the appendix. Every claims of the authors are well-supported by their experiments\n2. Experiments are well conducted on ALFRED household task benchmark. The results support the claimed points of the proposed. Comparisons are provided to show the superior performance of proposed methods. Ablation study is conducted to show all parts of the proposed method are useful.\n3. This work has good motivation. It pretrains generalization agent with interpretable skills. The proposed method leverages easy-to-collect natural language instructions to generate unseen tasks by taking advantage of language semantics. The learned agents have skills that are semantically meaningful. \n\n> Weaknesses\n\n1. The idea of generating unseen language-based tasks is not novel. Previous work has already used the idea of generating new language-based tasks. The paper IMAGINE used Construction Grammar Heuristic for this job while this paper uses large language model. Therefore, the technical innovation is limited. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing and figures are very clear, and the paper is of high quality. The proposed method is of mediocre novelty but the experiments are complete and well-conducted to show the advantages of proposed method. ",
            "summary_of_the_review": "It is a well-written paper with high completeness. The method leverages language models for efficient pretraining. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4964/Reviewer_bhG4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4964/Reviewer_bhG4"
        ]
    },
    {
        "id": "Fhis7esceJ",
        "original": null,
        "number": 4,
        "cdate": 1666732115187,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666732115187,
        "tmdate": 1669151767657,
        "tddate": null,
        "forum": "tDG-zrQ8S1Q",
        "replyto": "tDG-zrQ8S1Q",
        "invitation": "ICLR.cc/2023/Conference/Paper4964/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a data-augmentation strategy to learn from demonstrations in an embodied agent setting. Two types of augmentation strategies are proposed - relabeling language instructions with a language model and changing together different tasks/trajectories to obtain a new task/trajectory. The proposed method claims to be able to learn a diverse set of skills with these augmentation strategies. The method outperforms two baselines on an evaluation set constructed by the authors based on the ALFRED dataset. ",
            "strength_and_weaknesses": "Pros\n* Paper is generally easy to follow\n* Interesting use of language models for instruction re-labeling and summarization\n\nCons\n* Limited technical novelty\n* The motivations for the paper are not clear \n* Details about experimental setup are vague/missing and the results do not look particularly convincing\n",
            "clarity,_quality,_novelty_and_reproducibility": "One of the biggest concerns I had about this paper is the evaluation setting. The authors refer to their methods as a \u2018pre-training method\u2019 and claim to be able to learn new tasks more efficiently by fine-tuning on them. I found this perspective quite unconventional. Taking the ALFRED setting as an example, one trains a goal conditioned policy on training demonstrations and expects it to generalize to new tasks, which is a supervised learning problem. The authors need to motivate the setting better and make it more meaningful. \n\nWhy do the authors need to resort to offline RL approaches for this problem setting? Have you considered learning from the augmented demonstrations with imitation learning? If the proposed method is more advantageous, was the imitation learning (on the augmented data) baseline considered? \n\nThe design choices in eq(4) and eq(5) need better motivation. What is the motivation for assigning non-zero rewards to only the final state of each sub-trajectory? \n\nThe evaluation section needs better clarity for readers to understand the exact train/test setup.\nThe construction of EVAL_100 is vaguely described as \u2018sampling sequences of 1 to 7 instructions\u2019. I did not understand what this means (are they tasks? subgoals?). \n\nDo the baselines benefit from your data augmentation strategies? Or are they only trained on the original set of demonstrations? I would assume the comparison is unfair in the latter case. \n\nI found it unsatisfactory that the proposed method was not compared against state of the art methods on the ALFRED task, which makes it hard to contextualize this work. How well does the proposed method perform compared to these methods?\n\nFigure 1 can be improved to make the approach more intuitive.\n",
            "summary_of_the_review": "Raising score to 5 post-rebuttal.\n\nThe paper suffers from lack of strong motivations for the problem setting considered and several aspects of the experimental results are lacking. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4964/Reviewer_QcsA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4964/Reviewer_QcsA"
        ]
    }
]