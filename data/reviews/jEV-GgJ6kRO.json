[
    {
        "id": "O5yM9EzDoN",
        "original": null,
        "number": 1,
        "cdate": 1666554825592,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666554825592,
        "tmdate": 1670607551009,
        "tddate": null,
        "forum": "jEV-GgJ6kRO",
        "replyto": "jEV-GgJ6kRO",
        "invitation": "ICLR.cc/2023/Conference/Paper2582/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work the authors propose an optimal transport framework algorithm called Entire Space Counter-Factual Regression (ESCFR). The method is interesting in itself, with the additions the authors propose (specially the PFOR) add some light on how neural representations could be better used for counterfactual estimation",
            "strength_and_weaknesses": "Strengths:\n\n- PFOR is a nice technique to mitigate the missing confounder problem\n- Performance is good on test sets\n\nWeaknesses:\n\n- I think PFOR is the most interesting part of this paper. A lot of counterfactual analysis comes down to the issue of missing confounders. It seems assuming a monotonic effect is all PFOR needs but I didn't see any theorem proving this",
            "clarity,_quality,_novelty_and_reproducibility": "The paper isn't very clear and it took me a lot of effort to understand it. For example, how do you align r_i and r_j in Section 3.3? Is the algorithm affected by this at all?\n\nThe PFOR is a nice algorithm that should be interesting to study theoretically. Can the authors clarify how novel the algorithm is?\n\nThe code is not published in the supplementary and I didn't find a link to it - so Id give it a lower score for reproducibility",
            "summary_of_the_review": "Overall, this paper presents a nice idea of how to use neural representations for counterfactual estimation. I really enjoyed the PFOR idea and thats part of why I'm leaning towards an accept\n\nI think more empirical studies are merited - and Id like to see a proof of what is the minimal set of assumptions needed for PFOR to work. Maybe the authors understand this, but even after careful reading - I couldn't really figure it out\n\n---\n\nMany thanks to the authors for their detailed and insightful comments - I have read them and will keep my score as is. I'm still not sure how justified the procedure is without a concrete theoretical result.\n\n---\n\nUpdate post discussion: After extensive reading and discussions with the other reviewers, I share their concerns about clarity and novelty. In particular, it isnt really clear to me what is being added over the paper by Shalit et al (2017). Based on this, I will lower my score",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2582/Reviewer_VvUp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2582/Reviewer_VvUp"
        ]
    },
    {
        "id": "2T-gX2m2Eo",
        "original": null,
        "number": 2,
        "cdate": 1666599119853,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599119853,
        "tmdate": 1670699573480,
        "tddate": null,
        "forum": "jEV-GgJ6kRO",
        "replyto": "jEV-GgJ6kRO",
        "invitation": "ICLR.cc/2023/Conference/Paper2582/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of individual causal estimation under treatment selection bias or missing confounders. The authors proposed a new estimator based on representation learning and optimal transport, which aims to minimize certain regularized discrepancy measure in the representation space. The robustness of the proposed estimator under outliers, and the adjusted regularization for missing confounders are analyzed theoretically. The authors further provided empirical study on the performance of the proposed estimator on two datasets.",
            "strength_and_weaknesses": "Strength:\n- Causal effect estimation under treatment selection bias or missing confounders is an important problem, with many downstream applications. Aligning the distribution of each treatment groups in a \"learnt\" representation space is a natural solution, and has been studied in the prior works. This work builds on works in this line, and provided further adjustments to mitigate the mini-batch sampling bias and unobserved confounder problems, which are all well-motivated and significant research questions to investigate in.\n- Using optimal transport appears to be an elegant solution for aligning two distributions. The proposed estimator using optimal transport exhibits simplicity while outperforming several other baselines.\n- The proposed estimator was compared with a wide selection of baselines in the prior works\n\nWeakness:\n- I think many places in writing for this work can be improved. For example, Figure 2 was actually referenced before Figure 1, and the Figures are in lack of explanations in the caption or text about how the readers should read them. Besides, there were several places in the theoretical analysis where the authors left sentences like \"see more rigorous analysis in the appendix\" w/t further explanations on what are the exact content to look for, and where to look for in the Appendix. While deferring non-urgent details to the appendix is completely fine, such a way of writing gives the reader a sense that the paper was written maybe in a rush or so. \n- It was not very clear what is the key adjustment which avoids the overfitting issue under mini-batches, and the main novelty over (Uri et al 2017) . As the author noted, (Uri et al 2017) minimizes PEHE (eq(3)), which may overfit to the respective group\u2019s properties and thus cannot generalize well to the entire population. However, for the optimal transport solution, suppose that the treated/untreated groups are highly imbalanced, shouldn't that give bias issue to the optimal transport optimization loss too?\n- From table 1 the proposed method not only has lower out-of-sample loss, but also the lowest in-sample loss. It was unclear if the other baselines are tuned and trained to the minimal in-sample loss? Otherwise what is the intuition that the proposed estimator also performs the best in the in-sample regime?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is written in a relative clear manner, however certain places in writing and the presentation can be improved (see details above). There was some novelty concern in comparison to other representation-based methods in the prior works. \nReproducibility: I was not able to find the implementation code for the submission.",
            "summary_of_the_review": "Overall the paper studies an important and open problem. The proposed solution is intuitive and natural, while the theoretical analysis and experimental comparisons can be improved.\n\n\n\n------author rebuttal acknowledgement------\nI thank the authors for the detailed response, revising the manuscripts, and answering the questions I raised during my review. I believe this work proposed very reasonable heuristics based on optimal transport and various regularization terms to address the minibatch sampling effect and unobserved confounders problem in causal effects estimation. I also believe that the presentation and empirical evaluations could be further improved to make this work stronger. I keep my original score after the author feedback.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2582/Reviewer_ECPK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2582/Reviewer_ECPK"
        ]
    },
    {
        "id": "AaS3ct03tl",
        "original": null,
        "number": 3,
        "cdate": 1666601009697,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601009697,
        "tmdate": 1666601009697,
        "tddate": null,
        "forum": "jEV-GgJ6kRO",
        "replyto": "jEV-GgJ6kRO",
        "invitation": "ICLR.cc/2023/Conference/Paper2582/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Treatment effect estimation is a challenging problem that requires generalising to counterfactuals and involves bias and variance due to confounding and lack of overlap between treatment groups. The authors build on the extensive literature studying this problem from the representation learning perspective to propose a new regularisation function with improvements for balancing distributions of treated and untreated populations and dealing with unobserved confounders.",
            "strength_and_weaknesses": "**Strengths**\n- This paper is well-written. All ideas follow intuitively and all quantities are well defined. \n- Treatment effect estimation in the presence of unobserved confounding is challenging (if not impossible without strong prior assumptions). I commend the authors for studying it. \n- The proposed approach substantially outperforms in the presented experiments.\n\n**Weaknesses**\n- Given unobserved confounders, there aren't any assumptions that discuss their influence on the system. For instance, in Sec. 3.3 the authors assume that all variation in outcomes that does not come from observed confounders or treatment, must be due to unobserved confounders. Could it not be due to variables with an independent causal effect on the outcome? I take it that the authors assume the graph in Fig. 2, but still this graph hides the contribution of exogenous variables so that it is not necessarily the case that two units with similar covariates and unobserved confounders have similar outcomes.\n- The datasets used in the experiments do not have unobserved confounders (UC), right? How are you evaluating the UC regularization term?\n",
            "clarity,_quality,_novelty_and_reproducibility": "My evaluation of the quality, clarity and originality of the work is positive although the most novel part in my view: dealing with unobserved confounding, is not sufficiently justified.",
            "summary_of_the_review": "The proposed extensions, in my view, are reasonable heuristics to improve treatment effect estimation although the formalism and theoretical understanding behind them thin (which is understandable given that this problem is challenging). I do believe however that this is a good contribution to the literature.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2582/Reviewer_gP25"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2582/Reviewer_gP25"
        ]
    },
    {
        "id": "VdqCXGg0K_k",
        "original": null,
        "number": 4,
        "cdate": 1666802086934,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666802086934,
        "tmdate": 1666802086934,
        "tddate": null,
        "forum": "jEV-GgJ6kRO",
        "replyto": "jEV-GgJ6kRO",
        "invitation": "ICLR.cc/2023/Conference/Paper2582/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the problem of inferring individual treatment effects from observational data using representation learning via a discrepancy constraint between the representations of the treatment and control groups. In particular, the authors address the problem of dealing with the mini batch sampling which occurs during the deep learning process. ",
            "strength_and_weaknesses": "Strengths: \n(1) This is an interesting insight regarding minibatches and discrepancy estimation. While I find it unlikely that this effect would render existing methods unusable, the increased variance is certainly an item of concern and the authors do a nice job of addressing it.\n(2) The solution is well reasoned and motivated and elegant in execution.\n(3) There are strong empirical results. \n\nWeaknesses:\n(1) The language in this paper is _very_ difficult to follow. In particular, it is exceedingly loose in some places to the point that it is almost misleading. Some examples:\n*  it is always too expensive to conduct randomized experiments. \u2192 I think you meant to say often instead of always here?\n\n* I\u2019m not entirely sure what happened but a number of your citations cite either the first name or middle initial instead of last name (e.g. R, et al., Uri).\n\n* \u201cExisting representation-based methods fail to eliminate the treatment selection bias due to the Unobserved Confounder Effects (UCE). \u201c Is this really what you meant? If so, can you provide specific theoretical evidence of this?\n\n(2) This may partially be an artifact of (1) but the claims in this paper appear to be not terribly well stated and substantiated. This is in contrast with the proposed method which I found to be quite elegant! In particular is the claim that minibatching renders existing discrepancy based learners inconsistent? At which batch sizes? Can we quantify this? The underlying claim that minibatching and the resulting variance is problematic I think is fine, but the authors need to be more careful in the overall language and claims. \n",
            "clarity,_quality,_novelty_and_reproducibility": "See above, I have substantial concerns about the clarity of this paper, which in turn affects the overall quality. However, I do think the paper contains some nice novel ideas. ",
            "summary_of_the_review": "As I stated above, I think this paper overall contains a very interesting idea which serves as a real contribution to the literature. Unfortunately a combination of loose claims and writing make it difficult to engage more fully with the work as it presently stands. To be entirely honest, because of this it wasn't always clear _which_ claims are being made versus are the unfortunate artifact of loose writing. Additionally, I believe that the authors would be well served to clarify exactly what they are correcting for in terms of the _consequences_ of minibatches, not just the presence of minibatching itself. With that being said, I think there is a very nice idea here! I would strongly encourage the authors to make a substantial editing pass to improve the language and narrative of the paper. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2582/Reviewer_2Gnu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2582/Reviewer_2Gnu"
        ]
    }
]