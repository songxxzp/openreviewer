[
    {
        "id": "7DU3tcbo6L",
        "original": null,
        "number": 1,
        "cdate": 1666505750160,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666505750160,
        "tmdate": 1668998729290,
        "tddate": null,
        "forum": "H7M_5K5qKJV",
        "replyto": "H7M_5K5qKJV",
        "invitation": "ICLR.cc/2023/Conference/Paper5286/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper propose a new setting named Few-shot Supervised Multi-source Domain Transfer (FSMDT),  which try to generalize from multiple source domains to a single target domain with few labeled samples. And  authors propose a novel method based on the mix-up trick to create an intermediate mix-up domain.  The experimental results show that the proposed framework outperforms the baselines in two datasets.\n",
            "strength_and_weaknesses": "**Strengths :**\n\n1. The paper is the easy to read in most cases.\n2. The proposed method seems to make sense and it is easy to reproduce.\n3. The experimental results look good and outperforms the baselines  with large margin.\n\n**Weakness or question**\n1. Sec 3.3 why choose the Wasserstein distance as the measurement ?  As far as I know, the the Wasserstein distance is hard to compute and how to compute it in your detail implement?\n\n2. To the best of my knowledge, the proposed Few-shot Supervised Multi-source Domain Transfer (FSMDT) is similarity with the multi-domain few-shot learning \\[1]\\[2][3]   ,  I am very interested that the methods  \\[1]\\[2][3] designed for the multi-domain few-shot learning will work well in setting of FSMDT or not. \n\n3. The performance improvement is significant. But could you design some more useful baselines? From my view, you used many weak baselines. So it is not a fair comparison.\n\n\n\n[1] Liu L, Hamilton W L, Long G, et al. A Universal Representation Transformer Layer for Few-Shot Image Classification[C]//International Conference on Learning Representations. 2021.\n\n[2] Dvornik N, Schmid C, Mairal J. Selecting relevant features from a multi-domain representation for few-shot classification[C]//European Conference on Computer Vision. Springer, Cham, 2020: 769-786.\n\n[3] Triantafillou E, Larochelle H, Zemel R, et al. Learning a universal template for few-shot dataset generalization[C]//International Conference on Machine Learning. PMLR, 2021: 10424-10433.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is the easy to read in most cases.\n2. The proposed method seems to make sense and it is easy to reproduce.\n3. The experimental results look good and outperforms the baselines  with large margin.\n",
            "summary_of_the_review": "In this paper, the authors propose a new setting and a new method.  But I think there are still many details are unclear, or something importance reference and baselines is missing.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5286/Reviewer_DEHW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5286/Reviewer_DEHW"
        ]
    },
    {
        "id": "VnLyoRpQ66",
        "original": null,
        "number": 2,
        "cdate": 1666600043232,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600043232,
        "tmdate": 1669682957786,
        "tddate": null,
        "forum": "H7M_5K5qKJV",
        "replyto": "H7M_5K5qKJV",
        "invitation": "ICLR.cc/2023/Conference/Paper5286/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on multi-source domain adaptation with limited target data.  Since in this setting there are few target data, current methods fail to learn efficient model to model it. In this paper, authors provide a progressive  mix-up method to learn it. The experiment results support the effectiveness of the proposed method. ",
            "strength_and_weaknesses": "Pons:\nThis paper explores new setting which follows practical  application.\n\nThe paper gives clear description about this setting which is convincing. \n\nThis article is well-written and easy to follow.\n\nThe experiment results are sufficient  to support the proposed method.\n\nI have a few concerns which are as following: \n\n1) The proposed method is not vary suitable for multi-source domain adaptation. Why the mix-up technique works well for few-shot learning.  Since the number target dataset is small, the goal should locate the local information which is good for target domain. I just feel weird about the mix-up technique for few-shot learning.\n\n2) Reproducibility is hard, since it refers more detail precessing about the proposed method. Also the code is not provided, thus I am not sure about reproducibility.\n\n3) More datasets should be considered, since two datasets seem to be less. \n\n4) It still works well if the network is transformer, do authors try it?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "see the above section.",
            "summary_of_the_review": "I am not sure why the motivation why the mix-up is suitable for few-shot. Also the reproducibility should be challenge. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5286/Reviewer_dGEs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5286/Reviewer_dGEs"
        ]
    },
    {
        "id": "ymoW0RTL5O",
        "original": null,
        "number": 3,
        "cdate": 1666611160982,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611160982,
        "tmdate": 1666767815631,
        "tddate": null,
        "forum": "H7M_5K5qKJV",
        "replyto": "H7M_5K5qKJV",
        "invitation": "ICLR.cc/2023/Conference/Paper5286/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper targets a research problem of how to transfer knowledge from multiple source domains to a single target domain with very limited target data. To solve the problem, a new method based on mix-up is proposed. It progressively pushes both the source domains and the few-shot target domain aligned to the mix-up domain.",
            "strength_and_weaknesses": "Strength:\n\n+ The research problem is important and may have many practical applications. In real-world machine learning applications, it is possible that the data from the target domain is limited. Improving the generalization of the pre-trained model on the source domain could be important and is a practical setting.\n\n+ Extensive experiments are conducted.  The proposed method demonstrates strong performance and surpasses baselines by a large margin on two benchmark datasets. \n\n+ This paper generally is well-written and easy to follow.\n\nWeakness:\n\n+ The technical insight may not be enough, the proposed method is mainly based on mix-up. Most of the motivation for using techniques is based on intuitive explanations. Transferring the knowledge to the target domain with very limited target data seems to be an ambitious task.  For me, it is not clear whether this method works in general or not. I think it is better to add some insightful analysis. For example, are there any assumptions to make the method useful? When will the proposed method fail to transfer the knowledge?\n+ The related work under the proposed problem settings may not be fruitful. I am wondering that is it possible to compare one or two more baselines with minor tweaks in the related problem settings, (e.g., few-shot learning). \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper leverages mix-up for domain transfer, which can be viewed as an application extended from previous techniques. The written quality overall is high.",
            "summary_of_the_review": "This paper focuses on a practical and challenging domain transfer problem. Using mix-up for reducing transferring knowledge from the source domain to the target domain is an interesting method. My largest concern is that the technical insight may not that strong.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have not found any ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5286/Reviewer_Fa3w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5286/Reviewer_Fa3w"
        ]
    },
    {
        "id": "Ow9fuzuq5aW",
        "original": null,
        "number": 4,
        "cdate": 1667635775603,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667635775603,
        "tmdate": 1672912109039,
        "tddate": null,
        "forum": "H7M_5K5qKJV",
        "replyto": "H7M_5K5qKJV",
        "invitation": "ICLR.cc/2023/Conference/Paper5286/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work introduced a new setting of few-shot supervised mulit-source domain adaptation with a few labeled target samples and labeled multiple source domain data. Under this new setting, they proposed a progressive mix-up method for few-shot supervised multi-source domain transfer. Specifically, it creates an intermediate mix-up domain and gradually updates the mix-up ratio $\\lambda$ to mitigate the domain between the target domain and the source domain. They followed MAML(Finn 2017) to train the model. Their experiments show that the proposed method can outperform the previous methods on the two benchmarks of \"Office-home\" and \"DomainNet\". ",
            "strength_and_weaknesses": "Strength:\n\nThe work introduced a new setting of domain adaptation with a few labeled target samples and labeled multiple source domain data which hadn't been proposed before. The proposed setting is useful in practice. They proposed a  simple progressive mix-up scheme that outperforms the current SOTA on the two benchmarks. The paper is basically well-written and easy to follow.  \n\nWeaknesses:\n- I have some concerns about the method. And, some places of the method are not very clear. \n   - The progressive mix-up formulation of $\\lambda$ in EQ8: there is no grantee that $\\lambda$ is in the range of [0 1]. According to EQ8, the final value of $\\lambda_N = \\lambda_{N-1}$ with $q=0$, where the q ranges from $\\frac{1}{e}$. \n   - In EQ6 and 4, the method uses mixed-up multi-class regression labels. It's not clear in the paper whether they linearly combine the labels across the classes or they only mix the labels across the domains with the same class. Few-shot samples from more classes should provide a more diversified and complete target domain distribution. However, the experiments show the method doesn't get better results with more few-shot samples. What is the reason? \n\n- The experiments are not very strong with a few concerns.\n   - The paper only provided their results on two benchmarks. Usually, more results are required.\n   - The few-shot results with all the class samples are required. It only shows the results with the samples from 10~25 classes.  It is not clear whether the method can deal with a big class number (i.e. will the performance deteriorate with a big class number ?)\n   - More ablation results and discussions about the meta-training process are required, e.g. the split selection of 60%source domains only for meta- train + 40% all domains for meta-test.  \n       \n",
            "clarity,_quality,_novelty_and_reproducibility": "The work introduced a new setting of FSMDA which hadn't been proposed before.  Under this new setting, they proposed a simple progressive mix-up method that outperforms the SOTA on two benchmarks, which has enough novelty. ",
            "summary_of_the_review": "In summary, they proposed a progressive mix-up method with a new setting of FSMDA. However, there are some concerns about the proposed method and the experimental results. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "nil",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5286/Reviewer_6uuP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5286/Reviewer_6uuP"
        ]
    }
]