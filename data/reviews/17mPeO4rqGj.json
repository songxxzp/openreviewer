[
    {
        "id": "D5WchMwo4YJ",
        "original": null,
        "number": 1,
        "cdate": 1666367275225,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666367275225,
        "tmdate": 1669366364726,
        "tddate": null,
        "forum": "17mPeO4rqGj",
        "replyto": "17mPeO4rqGj",
        "invitation": "ICLR.cc/2023/Conference/Paper3580/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors study the cross-modal transfer ability of large-scale pre-trained models. The authors present a general workflow to fast and automatically exploit existing pre-trained models for different tasks. The authors provide some experiments to evaluate the proposed method.",
            "strength_and_weaknesses": "Strengths: \n- The authors have evaluated the proposed method on many different datasets.\n- The motivation of this work is interesting.\n\nWeaknesses:\n- The presentation of the paper is not clear. For example, Figure 1 cannot give us clear information (e.g., how does ORCA work? What represents cross-modal transfer?)\n- This work lacks novelty. It utilizes the existing techniques to fine-tune models on different datasets, which is incremental.\n- Although this paper evaluates the proposed method on many different datasets, it lacks sufficient comparison experiments with state-of-the-art baselines.\n- The authors claim their method could achieve task-specific adaption. Why does it need fine-tune? If it could achieve task-specific adaption on the upstream pre-training, it could achieve good performance on the downstream tasks without finetuning.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not clear and hard to follow. This work is incremental and lacks novelty.",
            "summary_of_the_review": "The paper is not clear and hard to follow. This work is incremental and lacks novelty. The experiments are not solid for comprehensively evaluating the proposed method.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3580/Reviewer_tx7E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3580/Reviewer_tx7E"
        ]
    },
    {
        "id": "nNPQ4odIxhg",
        "original": null,
        "number": 2,
        "cdate": 1666449902082,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666449902082,
        "tmdate": 1669366766926,
        "tddate": null,
        "forum": "17mPeO4rqGj",
        "replyto": "17mPeO4rqGj",
        "invitation": "ICLR.cc/2023/Conference/Paper3580/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies an pratical problem that how to effectively transfer pretrained models for diverse tasks. To this end, the authors proposed ORCA, which conducts task-specific adaptation by minimizing the OTDD between the proxy source embeddings and target embeddings. Extenvise transferring experiments on different tasks have been conducted to verify the effectiveness of the proposed ORCA.",
            "strength_and_weaknesses": "Strengths:\nIn the big model era, how to effectively adapt pre-trained models to different downstream tasks is an interesting and practical problem. Therefore, studying the cross-modal transfer ability of large-scale pre-trained models is significant.\n\nWeaknesses:\n1. The key idea of the proposed method is resorting to the OTDD between the proxy source data and target data to fine-tune the given pre-trained model. However, the definition and usage of OTDD are not clear enough. More importantly, according to the unclear statement, OTDD seems to be a modified version of OT which introduces the label information for complimentary. From this perspective, the reviewer wonders how the OTDD works under the scenario where the source and target data have no share labels. \n2. The authors argue that the proposed ORCA is an effective transfer learning method for pre-trained models compared to vanilla fine-tuning and hand-craft prompt tuning. Although the empirical results on two popular vision and language models (Swin and Roberta) show the superiority of the proposed method compared to the vanilla baselines. The effectiveness of ORCA should be further defended by comparing it to the recently-proposed vision and language prompt tuning methods such as [1].\n[1] Visual Prompting: Modifying Pixel Space to Adapt Pre-trained Models",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The paper is likely to have a modest impact on the community. Clarity: The paper is well organized but the presentation has minor details that could be improved. Originality: The main ideas of the paper are not novel or have limited novelty. Please see the weaknesses. ",
            "summary_of_the_review": "Please see weaknesses",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3580/Reviewer_J4V2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3580/Reviewer_J4V2"
        ]
    },
    {
        "id": "GG8KnFl4Y0",
        "original": null,
        "number": 3,
        "cdate": 1666603216560,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603216560,
        "tmdate": 1668855478457,
        "tddate": null,
        "forum": "17mPeO4rqGj",
        "replyto": "17mPeO4rqGj",
        "invitation": "ICLR.cc/2023/Conference/Paper3580/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on cross-modal transfer learning that aims to utilize the pre-trained models for downstream tasks with diverse modalities. It devises a task specific embedding network to map data distribution to the pertained modality using the optimal transport dataset distance metric. Extensive experiments verify the efficacy of the proposed ORCA.",
            "strength_and_weaknesses": "Strength\n1. ORCA achieves good performance compared to other approaches on 13 tasks with different modalities, input dimensions, and prediction types.\n2. The importance of matching feature distribution is well supported and it is compatible with the in-modality transfer.\n\nWeaknesses\n1. The technical contribution is not significant. Though the performance is good and the effect of OTDD is verified, the main idea of using OTDD as a target for data distribution mapping is somewhat trivial.\n2. The custom embedding network uses a convolution layer for patch embedding like Vit. Does it only process image-like data? If so, it is inconsistent with the state as claimed before.\n3. What is the effect of other metrics in Stage I?",
            "clarity,_quality,_novelty_and_reproducibility": "The key idea and the pipeline are described clearly. According to the detailed instructions, I believe that this idea is easy to reproduce. However, it lacks enough novelty since the whole pipeline is build on previous techniques.",
            "summary_of_the_review": "Though the proposed method achieves good performance on a range of tasks, it is an integration of existing techniques, which weakens the contribution of this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3580/Reviewer_WPBn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3580/Reviewer_WPBn"
        ]
    },
    {
        "id": "i9ZpWm6nY1",
        "original": null,
        "number": 4,
        "cdate": 1666669683818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669683818,
        "tmdate": 1668995531897,
        "tddate": null,
        "forum": "17mPeO4rqGj",
        "replyto": "17mPeO4rqGj",
        "invitation": "ICLR.cc/2023/Conference/Paper3580/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a new approach, ORCA, to tackle the cross-model transfer problem. ORCA equips a pre-trained transformer with a task-specific embedder and a task-specific predictor. When facing a downstream task, the embedder is learned to map data of any dimension to a standard sequence of tokens that is then fed into the pre-trained transformer. The predictor is learned to map the output to the specific label space. The learning objective is an optimal transport dataset distance that is used to match the source and target data distribution. After matching, the whole network is finetuned using standard loss.",
            "strength_and_weaknesses": "Strength:\nAs pre-trained models become increasingly powerful, there exists a desire to apply these models to aid the learning of downstream tasks. But the mismatch of data distribution from different modalities hinders the application. This paper tackles this challenging and important problem and gives a simple but effective solution. The solution of this paper leverages the property of transformer that it has a standard input structure without any inductive bias of data, and proposes to learn an embedder that can map any input to match such structure. To better align different modalities, the authors further use optimal transport to match the source and target datasets. Although being simple, this adaptation scheme is reasonable and the authors give enough empirical evidence of that this approach is effective.\nThe experiments show that ORCA performs generally better than SOTA methods.\n\nWeakness:\nOne concern is the use of pre-trained model, which serve as a potential unfair comparison with other methods. In particular, the pretrained model the author chooses is RoBERTa and SwinTransformer trained on Five English-language corpora and ImageNet-22K, respectively. This may differ from other methods which potentially use different datasets for pre-training. So can authors provide some comparisons with state-of-the-art methods trained on the same dataset using identical architecture? I understand that such large-scale experiments are difficult to reimplement, but such comparisons would be greatly appreciated and would bolster the work.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, quality: The paper is well organized and well written. \nNovelty: As pre-trained models become increasingly powerful, there exists a desire to apply these models to aid the learning of downstream tasks. But the mismatch of data distribution from different modalities hinders the application. This paper tackles this challenging and important problem and gives a simple but effective solution. \nReproducibility: The code and configuration file for reproducing each experiment can be found in the supplementary material.",
            "summary_of_the_review": "Overall, this paper tackles the important and challenging cross-modal transfer problem and gives a feasible solution, despite lack of novelty and potentially unfair comparisons with other methods. Given that the problem and the proposed approach would be interesting to ICLR audience, I would tend to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3580/Reviewer_AGVC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3580/Reviewer_AGVC"
        ]
    },
    {
        "id": "t5f5aWZQhDL",
        "original": null,
        "number": 5,
        "cdate": 1667192525249,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667192525249,
        "tmdate": 1667192525249,
        "tddate": null,
        "forum": "17mPeO4rqGj",
        "replyto": "17mPeO4rqGj",
        "invitation": "ICLR.cc/2023/Conference/Paper3580/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper shows an universal approach of handling transfer learning in terms of cross-modal tasks, by using pre-training the embedding layers using OTDD loss metric. This paper although conduct empirical study on 13 tasks, demonstrating the effectiveness of embedding layer pretraining in cross modality learning. This approach is universal, effective and beats counterparts like NAS solutions, handcraft models, and other cross modal transfer tasks by quite some margin.",
            "strength_and_weaknesses": "Strengthes\nThe paper proposes an general/universal approach to do cross modal transfer learning, by adding a dimension transformation embedding layer and pretrain it using OTDD\nThe paper describe the motivation, challenges and solution clearly, and there are enough empirical results supporting the claim.\nThe comparisons against other cross modal transfer solution is convincing, e.g from NAS based class, hand-crafted expertise class.\nThe paper did some deep dives into the effectiveness of transformation embedding pretaining.\n\nWeaknesses\nSome of the empirical results and statements requires better reasoning, e.g:\nWhy just pretraining the embedding transformation layers will make the final result even better than hand-crafted expertise modes by a big margin in some downstream tasks? like Spherical and JSB Chorales.\nWhy using OTDD when pretraining the transformation embedding layers, the metric and semantic goal is not connected explicitly. \n\nThere are some places requires more deep diving, e.g:\nUsing convolutional network to do dimension transferring from target X -> source X, using it for Orca-swin makes sense because they both belong to vision modeling, but why still use it for Orca-roberta? Why convolutional network is selected, and how does it compare to simple feed-forward network for dimension transformation, their comparison in down stream tasks, etc, to demonstrate the effectiveness of this choice\n\nIt is good to show both Orca-swin/roberta results in Table 2, let people understand the impact of pretrained body as well.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written 3/4,\nclarity/easy to understand 2/4, some ideas needs better reasoning \nand novelty 3/4.",
            "summary_of_the_review": "This is a good empirical paper that the idea makes quite sense, and the authors conduct thorough experiments to support the claim such that pretraining embedding layer served as cross modal transfer is very effective. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3580/Reviewer_uNsA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3580/Reviewer_uNsA"
        ]
    }
]