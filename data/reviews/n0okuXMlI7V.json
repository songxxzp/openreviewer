[
    {
        "id": "cm65VkUqsbT",
        "original": null,
        "number": 1,
        "cdate": 1666600738908,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600738908,
        "tmdate": 1666600738908,
        "tddate": null,
        "forum": "n0okuXMlI7V",
        "replyto": "n0okuXMlI7V",
        "invitation": "ICLR.cc/2023/Conference/Paper4970/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors tried to explain the cause of catastrophic overfitting (CO) during FGSM adversarial training. The authors first showed that CO can be deliberately induced by dataset intervention. Then, using this intervened dataset as a starting point, the authors conducted three well-designed experiments to study the chain of mechanisms behind CO. They conclude that in FGSM-AT:\n1. The network attempts to learn both non-robust and robust features.\n2. The network increases its non-linearity for better learning these two types of features.\n3. The explosion of non-linearity breaks FGSM, which provides a shortcut for networks to only learn from non-robust features, which eventually leads to CO.\n\n",
            "strength_and_weaknesses": "S1. The experiments are well designed and convincing.\nS2. The theory for onset of CO is both empirically and theoretically justified.\nS3. The related work is well cited and discussed.\n",
            "clarity,_quality,_novelty_and_reproducibility": "W1. Over-claim of the contribution in dataset construction. ",
            "summary_of_the_review": "Overall it\u2019s a solid and well written paper. The way the authors design experiments is novel. Both empirical results and theoretical justifications are provided. And they successfully fill the knowledge gap on the onset of CO. The only weakness is that they seem to over-claim their contribution in dataset construction. The authors stated that it\u2019s hard to separate robust features from non-robust features in real datasets, so they used their way of dataset construction. But (Ilyas et al., 2019) seems to have successfully disentangled these two features from neural networks\u2019 perspective. In (Ilyas et al., 2019), a method is provided to extract non-robust features and robust features. So why don't the authors use their methodology?\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4970/Reviewer_GG1w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4970/Reviewer_GG1w"
        ]
    },
    {
        "id": "q-SKnHh1Ad",
        "original": null,
        "number": 2,
        "cdate": 1667378834298,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667378834298,
        "tmdate": 1669315264767,
        "tddate": null,
        "forum": "n0okuXMlI7V",
        "replyto": "n0okuXMlI7V",
        "invitation": "ICLR.cc/2023/Conference/Paper4970/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "the paper investigates possible causes of catastrophic overfitting (CO), a well known state in adversarial training (AT) of robust model which leads to the collapse of model robustness during later stages of AT. The authors propose to augment the data with easy to classify DCT patterns during analisis, in order to gain insights on the properties of the feature space leading to OT.  ",
            "strength_and_weaknesses": "Strengths:\n* the paper addresses an important and interesting question \n* the paper introduces the OT problem well and gives a quite good overview of the related work (missing only a view important works [1-3])\n\n[3] Grabinski et. all: FrequencyLowCut Pooling--Plug & Play against Catastrophic Overfitting, ECCV 2022\n\nWeaknesses:\n\n1) the title is overclaiming: overfitting is NOT a bug and the paper fails to show that OT is indeed caused by features\n2) novelty: the paper fails to show new insights: OT is overfitting - this is in line with all related work, all known counter measures (augmentation, regularization, input smoothing ...) and the presented experiments only confirm this\n3) experimental setup is not suited to answer the questions defined at the beginning of the paper. Adding easy to learn patterns to the data, leading 100% clean ACC is definitely provoking ofervitting. The described case of beta=eps does not allow strong conclusions other than OT is overfitting. Additional results on the nonlinearity and non-smoothness of the optimization space only further confirm this (and have been shown before)   \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clear lack of novelty -> see above",
            "summary_of_the_review": "the initial question of how the feature space representation affects the CO is indeed very interesting and requires further investigation. However, I think that presented approach is not suitable to achieve this goal because the chosen augmentation of training data will trigger clean overfitting and thus prevents an unbiased investigation.\n\nI would suggest that the authors investigate the features at CO without external manipulation. [1+2] provided a large DB of models which could be used to conduct these studies and have already shown that robust features are only different in specific layers of a network...\n\n[1] Gavrikov et. all Adversarial Robustness through the Lens of Convolutional Filters, CVPR-W 2022\n\n[2] Gavrikov et. all. CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters, CVPR 2022",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4970/Reviewer_2rv5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4970/Reviewer_2rv5"
        ]
    },
    {
        "id": "SGitGul_aq",
        "original": null,
        "number": 3,
        "cdate": 1667451844948,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667451844948,
        "tmdate": 1667452061795,
        "tddate": null,
        "forum": "n0okuXMlI7V",
        "replyto": "n0okuXMlI7V",
        "invitation": "ICLR.cc/2023/Conference/Paper4970/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors attempt to understand catastrophic overfitting in adversarial training with an FGSM adversary. To analyze catastrophic overfitting, they proposed to induce it using controlled manipulations of the dataset technique. Depending on the magnitude of the data perturbations, they observed that it is possible to induce catastrophic overfitting to any model given sufficiently large dataset perturbation. The authors discussed the implications of this finding and provided further insights into catastrophic overfitting.",
            "strength_and_weaknesses": "### Strengths\n\n- A novel approach to understanding catastrophic overfitting based on controlled manipulations of the dataset, which provides interesting insights into the problem.\n- A novel explanation of catastrophic overfitting that can be used to guide the development of new defense methods.\n- Detailed extensive empirical analysis of catastrophic overfitting under different dataset manipulations.\n- The paper is really easy and enjoyable to read.\n\n### Weaknesses\n\n- Technical novelty is limited, all results are very intuitive.\n- At the time of the review, the author\u2019s code is not available, which will be hopefully addressed in the revision phase.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and insightful. In general, the paper is original and timely as it tackles an important problem. The proposed approach directly extends the previous work on \u201cAdversarial Examples Are Not Bugs, They Are Features\u201d. Yet, the proposed approach is novel as it addresses the problem of catastrophic overfitting. At the time of the review, the authors haven\u2019t released the source code, which makes it more difficult to reproduce the results. The authors should try to release the source code of the paper.",
            "summary_of_the_review": "This paper provides novel insights into catastrophic overfitting with fast adversarial training. Understanding catastrophic overfitting is a very important problem in adversarial machine learning and addressing it is extremely important. To this end, the authors proposed a novel method to induce catastrophic overfitting in controlled settings, provided possible explanations of this phenomenon, and suggested a few directions to reduce catastrophic overfitting in practice. Overall, the paper is interesting and enjoyable to read and makes few contributions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4970/Reviewer_27L3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4970/Reviewer_27L3"
        ]
    },
    {
        "id": "qqSGXaqSXa",
        "original": null,
        "number": 4,
        "cdate": 1667487779798,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667487779798,
        "tmdate": 1669382677267,
        "tddate": null,
        "forum": "n0okuXMlI7V",
        "replyto": "n0okuXMlI7V",
        "invitation": "ICLR.cc/2023/Conference/Paper4970/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors investigate the phenomenon of catastrophic overfitting (CO) which occurs during FGSM adversarial training (AT). CO describes the \u201coverfitting\u201d of the model to simple adversaries (FGSM) during a certain amount of training epochs while losing robustness against more complex adversaries like PGD.  \n\nThe authors show empirically:\n\n1. that it is possible to induce catastrophic overfitting on smaller \\epsilon values (where \\epsilon is the hyperparameter to determine the strength of the adversarial attack) than before by adding simple discriminative features to the dataset. These injected features are added onto the original image scaled with a hyperparameter $\\beta$\n2. if $\\beta << \\epsilon$ the model behaves normal \n3. if $\\beta >> \\epsilon$ the robust accuracy is high as the model can rely on the simple features\n4. if $\\beta \\approx \\epsilon$ they can induce CO at \\epsilon values at which the model on the clean data would not suffer from CO\n\nThe part where $\\beta \\geq \\epsilon$ is further exploited to analyse the network's training behavior, especially at the point of CO. Further, the authors empirically show:\n\n1. that standard training and FGSM training after CO highly rely on the simple features while PGD training forces the model to learn more robust features\n2. that the curvature of the model\u2019s loss landscape explodes after CO  \n\nThe authors show empirically that models favour easy-to-learn features when no robustness constraints are considered. Further, they claim that a network that needs to learn different features needs to increase its non-linearity and thus opens a shortcut to breaking FGSM with CO.\n\nFurther, the authors analyse two methods which can prevent CO (GradAlign and N-FGSM). They empirically show that these methods can keep low curvature of the loss landscape thus preventing CO. Additionally, the authors try to use a lowpass filter on top of the data but could not prevent CO for large $\\epsilon$ values.\n",
            "strength_and_weaknesses": "**Strength:**\n- The paper is well-written and supported by adequate graphics to demonstrate the author's findings.\n- The authors provide an empirical analysis of the learning preferences of CNNs under different training settings (standard training, FGSM AT and PGD AT)\n- The approach of injecting simple features for the analysis of CO is interesting and can be extended for future explainability studies on CNNs\n\n**Weakness:**\n\n- Prior work/extensiveness: the authors investigated prior work that prevents CO, however, missed prior work which includes additional insights into why CO happens:                                   \n[0] https://arxiv.org/abs/2204.00491 (ECCV 2022): showing that CO is correlated with a vast increase in aliasing and preventing CO by using an aliasing-free downsampling layer\n\n- The authors conducted their empirical study only on low-resolution datasets and for small networks. It would be interesting to see if their approach also applies to high-resolution data and bigger networks.\n- The authors did not provide any code for reproducibility. \n",
            "clarity,_quality,_novelty_and_reproducibility": "- **Clarity and Quality:** The paper is well-written and supported by adequate graphics to demonstrate the author's findings.\n-  **Novelty:** The authors missed one prior work which shows a different perspective on CO. Thus, I would suggest the authors to include this method in their analysis part as well as the insights on CO in their discussion.\n- **Reproducibility:** The code for reproducing the results of the paper should be publicly available to encourage future research on the explainability of CNNs.\n",
            "summary_of_the_review": "The contribution of this paper is a simple, yet interesting empirical analysis of CO. However, the authors missed a new perspective from previous work. Thus, I would like to encourage the authors to include this perspective in their analysis. Further, it would be interesting to see if the presented results hold for high-resolution datasets and bigger networks. \nAdditionally, the authors should provide their code for reproducibility and enablement of future research on the analysis of the explainability of CNNs.\n\nGenerally, the suggested idea and findings are interesting and worse publishing. However, I can not accept the paper for now due to my concerns mentioned above. I highly encourage the authors to address these concerns such that I can increase my score. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4970/Reviewer_F9cZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4970/Reviewer_F9cZ"
        ]
    }
]