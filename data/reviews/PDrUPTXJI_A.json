[
    {
        "id": "0p5LScpToF",
        "original": null,
        "number": 1,
        "cdate": 1666647783274,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647783274,
        "tmdate": 1666647783274,
        "tddate": null,
        "forum": "PDrUPTXJI_A",
        "replyto": "PDrUPTXJI_A",
        "invitation": "ICLR.cc/2023/Conference/Paper3426/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper present an adaptive approach to threshold selection for labeling of unannotated data during semi-supervised model training. The proposed approach combining adaptive threshold selection and a combination of losses leads to state of the art performance for a variety of SSL classification tasks.",
            "strength_and_weaknesses": "The authors to a good job of laying out th problem and presenting their proposed approach. The approach is clean and well motivated and the empirical results are strong.\n\nOne experimental result that would have been helpful to demonstrate the importance/impact of the components of the proposed approach would have been to show performance of the loss function (12) for a fixed threshold selected by optimizing validation performance. This would help show whether the adaptive thresholding scheme is serves to reduce hyperparameter-tuning computation or overall training time (in the event of similar performance for an optimally selected threshold) or whether the adaptive threshold outperforms any fixed threshold. Additionally, this would help demonstrate the improvement from the proposed self-adaptive fairness loss term independent of the adaptive threshold.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The adaptive thresholding scheme presented is novel to my knowledge, as is the proposed formulation for incorporation of fairness in the loss function.",
            "summary_of_the_review": "Overall I'm in favor of accepting this paper as it combines novelty related to both the thresholding of unlabeled data and formulation of the loss function with empirically strong results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3426/Reviewer_axb6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3426/Reviewer_axb6"
        ]
    },
    {
        "id": "RRvVN_fzMH",
        "original": null,
        "number": 2,
        "cdate": 1666685523960,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685523960,
        "tmdate": 1666685706573,
        "tddate": null,
        "forum": "PDrUPTXJI_A",
        "replyto": "PDrUPTXJI_A",
        "invitation": "ICLR.cc/2023/Conference/Paper3426/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "For the semi-supervised learning task, this paper argues that existing methods might fail to utilize the unlabeled data more effectively since they either use a fixed threshold or an ad-hoc threshold adjusting scheme, resulting in inferior performance and slow convergence. Then they propose FreeMatch to adaptively adjust the confidence threshold. Experiments on several related datasets demonstrate the superiority.",
            "strength_and_weaknesses": "Strengths:\n\n(1) The motivation is clear.\n\n(2) The presentation is good, and the paper is easy to follow.\n\n(3) The proposed method is simple and effective.\n\nWeaknesses:\n\n(1) The main contribution lies in learning the threshold in an adaptive way. They assign various thresholds for different categories and iterations. Given the fact FlexMatch already set different weights for different classes, I think the overall novelty is unsatisfying. The contribution of self-adaptive fairness over existing strategy is also marginal.\n\n(2) The improvements over FlexMatch on CIFAR10 and CIFAR100 are marginal.\n\n(3) Please compare with FlexMatch under the same fairness regularization. For example, if you replace the SAF with that in FlexMatch, will the results are still better than FlexMatch?\n\n(4) The authors claim that the proposed method can speed up the convergence. However, according to Figure 3(c), in 500k iteration, the proposed method is still not converged. Please give more explanation.\n\n(5) For different datasets, different training strategy is adopted to improve the performance. For example, the authors define specific training requirement on STL-10, which is not consistent to the so-called self-adaptive property throughout the paper.\n\n(6) In FixMatch and FlexMatch, the predefined threshold is very high to promise that highly-confident samples are selected for training. However, the proposed FreeMatch assigns low threshold at the beginning, which might incorporate some incorrect samples for training. Please explain this.\n\n(7) There are several typos, such as \"Eq equation 6\" in page 5.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to my detailed comments above.",
            "summary_of_the_review": "The overall novelty of self-adaptive threshold learning strategy is unsatisfying. Improvement over FlexMatch on CIFAR is marginal. More experiments are suggested to support some statements.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3426/Reviewer_Ebth"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3426/Reviewer_Ebth"
        ]
    },
    {
        "id": "2XiaingJtXW",
        "original": null,
        "number": 3,
        "cdate": 1666836018139,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666836018139,
        "tmdate": 1666836018139,
        "tddate": null,
        "forum": "PDrUPTXJI_A",
        "replyto": "PDrUPTXJI_A",
        "invitation": "ICLR.cc/2023/Conference/Paper3426/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper the authors have proposed FreeMatch, a novel Semi Supervised Learning (SSL) technique that samples unlabeled data (the goal is to add pseudo labels and use them as supplementary data for model learning) through a Self Adapting (confidence) Thresholding (SAT) mechanism. The hypothesis is that an adaptive threshold (depending on the status of the model) is likely to be more effective than a predefined or ad-hoc threshold,  usually used by existing techniques.  \n\nAt each training iteration, FreeMatch adjusts confidence thresholds (for each class in the context of a classification problem) by leveraging model prediction information from the corresponding step. SAT first estimates a global threshold, then modulates via the local class-specific thresholds, estimated as the Exponential Moving Average (EMA) of the probability for each class. The threshold is kept low at the beginning of the training process to accept as many weak(possibly correct) samples as possible. As the model becomes more and more confident in subsequent iterations, the threshold is gradually increased to identify and reject incorrect samples (which reduces confirmation bias). \n\nThe proposed model has been tested on CIFAR-10/100, SVHN, STL-10, and ImageNet benchmarks and the results look to be encouraging. \n",
            "strength_and_weaknesses": "Strength: Usually, semi-supervised learning (SSL) methods use either some predefined or ad-hoc threshold adjustment mechanisms  which are challenged  by FreeMatch through the idea of Self Adaptive Threshold (SAT). FreeMatch provides an automated iterative unlabelled data sampling methodology using  threshold adjustments based on the learning status of the model and using  prediction confidence scores at the latest. The methodology also ensures class  fairness and learns a generalized solution (in the context of classification).\n \nFreeMatch has been tested on CIFAR-10/100, SVHN, STL-10, and ImageNet benchmarks and compared against existing techniques such as FixMatch, ReMixMatch, and FlexMatch. Reported results are found to be encouraging. \n \nWeakness: Although the reported results in section 5 look promising, they lack some statistical tests. It is suggested that authors perform proper statistical tests to justify whether the achieved gains are (statistically) significant or not.  If we zoom in to the results (Table 1 and 2) we find that gains are marginal for larger (such as CIFAR-100, ImageNet) and more complex (ImageNet) datasets when compared to smaller and simpler datasets such as CIFAR-10 (or data with less number of classes). This may have exhibited scalability limitations of the proposed approach. \n\nAlso, it is surprising that for smaller datasets (such as CIFAR-10) the reported results are sometimes even better than models trained in a fully supervised fashion.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with minor linguistic errors. The idea of the threshold adjustments is first presented through a simple binary conditional gaussian example which helps easily understand the concept and the context. The structure of the paper is well organized and the content is easy to follow. \n \nSelf Adaptive Threshold (SAT), Self Adaptive Fairness, and the overall FreeMatch approach are the major novelty of this work.\n \nThe experiments (plus setup) were thorough and detailed. As the code has been shared, it is expected that the reproduction of the results is feasible.",
            "summary_of_the_review": "I have gone through the paper more than once including the appendices. Overall, the idea is quite sound, well articulated through the document, which is found easy to follow. The experiment is thorough and the reported results look encouraging. Self Adaptive Threshold (SAT), Self Adaptive Fairness, and the FreeMatch approach as a whole may benefit the SSL research. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3426/Reviewer_zMTV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3426/Reviewer_zMTV"
        ]
    }
]