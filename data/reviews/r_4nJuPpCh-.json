[
    {
        "id": "D4AiwO_vpl",
        "original": null,
        "number": 1,
        "cdate": 1666598605455,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666598605455,
        "tmdate": 1666598686645,
        "tddate": null,
        "forum": "r_4nJuPpCh-",
        "replyto": "r_4nJuPpCh-",
        "invitation": "ICLR.cc/2023/Conference/Paper5457/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers node features, graph structure, and node labels as three views, and designs three loss components correspondingly. Moreover, the latent $F$ and the model parameters $\\Theta$ are proposed to be optimized alternatingly to achieve efficiency and scalability.\n",
            "strength_and_weaknesses": "Strength:\n\n1. The proposed multi-view loss (with *lazy propagation*) is an effective method to achieve the trade-off between performance and cost.\n2. The authors conduct extensive ablation experiments to verify each model component.\n\nWeaknesses:\n\n1. The three losses after decoupling are not novel individually. Each component seems to be standard and well-studied in graph semi-supervised learning.\n2. The pseudo-label with certainty and class-balancing in graph semi-supervised learning are also discussed in [1]. And there are other works highly related to the core ideas in this paper, e.g., [2,3,4], should be discussed.\n\nReferences:\n\n[1]. Self-Enhanced GNN: Improving Graph Neural Networks Using Model Outputs. Han Yang, Xiao Yan, Xinyan Dai, Yongqiang Chen, James Cheng. IJCNN '2021\n\n[2]. Rethinking and Scaling Up Graph Contrastive Learning: An Extremely Efficient Approach with Group Discrimination. Yizhen Zheng, Shirui Pan, Vincent Cs Lee, Yu Zheng, Philip S. Yu. NeurIPS \u201922.\n\n[3]. Combining Label Propagation and Simple Models Out-performs Graph Neural Networks. Qian Huang, Horace He, Abhay Singh, Ser-Nam Lim, Austin R. Benson. ICLR \u201921.\n\n[4]. On Graph Neural Networks versus Graph-Augmented MLPs. Lei Chen, Zhengdao Chen, Joan Bruna. ICLR \u201921.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper clearly presents the multi-view loss design and provides solid experiment results. The technical novelty of the proposed method is a bit limited.",
            "summary_of_the_review": "This paper decouples the supervised loss and optimizes it alternatingly to achieve efficiency and scalability. However, the technical contribution is a bit limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5457/Reviewer_xFqB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5457/Reviewer_xFqB"
        ]
    },
    {
        "id": "xpoRDpPnHrp",
        "original": null,
        "number": 2,
        "cdate": 1666607044043,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666607044043,
        "tmdate": 1666607102808,
        "tddate": null,
        "forum": "r_4nJuPpCh-",
        "replyto": "r_4nJuPpCh-",
        "invitation": "ICLR.cc/2023/Conference/Paper5457/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a way to map node features $\\boldsymbol{X_i}$, graph structure $\\boldsymbol{A}$, and node label $\\boldsymbol{Y_i}$ into a shared latent feature space $\\boldsymbol{F_i}$. Each node $i$ is now represented by a vector $\\boldsymbol{F_i}$, which has the same dimension as one-hot encoded label $Y_i$. $\\boldsymbol{F_i}$ is treated as the soft pseudo label of node $i$.  \n\nThe $X_i \\mapsto F_i $ mapping is done by an MLP with learnable $\\Theta$ params. The graph structure $\\boldsymbol{A}$ is enforced by imposing Laplacian smoothness of $\\boldsymbol{F_i}$ against its neighbors $\\boldsymbol{F_j}$. The node label $\\boldsymbol{Y_i}$ is enforced by otimizing MSE between $\\boldsymbol{Y_i}$ and $\\boldsymbol{F_i}$ directly.\n\nThe paper also proposes an alternative training scheme to learn $\\boldsymbol{F}$ and $\\Theta$ with various training tricks such as: initial feature diffusion, MLP pre-training that uses only $\\boldsymbol{Y_i}$, importance sampling based on $\\boldsymbol{F_i}$ entropy, class balancing...\n\nThe experiment results show that the proposed methods yield good performance when the label rate is very low.\n\n",
            "strength_and_weaknesses": "Strength:\n  - The paper is well presented, with good support materials in the Appendix, including speudo code, dataset statistics, and parameter setting.\n  - The paper conducts relevant experiments, including ablation study and efficiency comparison. \n\nWeaknesses:\n  - The whole training scheme is very similar to the Iscen et al. 2019 \"Label Propagation for Deep Semi-supervised Learning\" paper, including most of the important training tricks.\n  - Experiment results only show good improvement on very low label setting (5-60 examples per class). For most of the time, the improvements are not statistically significant.\n  - Hyper-param search space for the proposed method is much larger than baseline models. It includes $\\lambda_1$ and $\\lambda_2$, which together create 25 to 100 settings (Appendix E). On the contrary, the baseline models use fixed learning rate = 0.01. Apart from that, there are many training params that are used in the training algorithm (Appendix B) but are not mentioned in the paper. They are usually also hard to tune right, including $S$ pre-training steps, $p$ times $F$ update and $t$ times $\\Theta$ update per iteration.\n  - The paper claims that alternating optimization provides better flexibility than end-to-end training. This is not a well-supported statement. In practice, end-to-end training is much easier to setup, does not require training tricks, and depends less on hyper-params tuning.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality:\n  - The paper is well presented with clear notations and references. The experiment setups are reasonable.\n\nNovelty:\n  - Not too much, because the whole training scheme is very similar to the Iscen et al. 2019 \"Label Propagation for Deep Semi-supervised Learning\" paper, including most of the important training tricks.",
            "summary_of_the_review": "The paper's novelty is not significantly strong. Most of its empirical positive results are from training tricks that are introduced by Iscen et al. 2019. The paper claims that alternative training is more flexible and more efficient than end-to-end training, which are not well-supported.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5457/Reviewer_ogUi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5457/Reviewer_ogUi"
        ]
    },
    {
        "id": "IXI1wUpE4nk",
        "original": null,
        "number": 3,
        "cdate": 1666651495724,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651495724,
        "tmdate": 1666651495724,
        "tddate": null,
        "forum": "r_4nJuPpCh-",
        "replyto": "r_4nJuPpCh-",
        "invitation": "ICLR.cc/2023/Conference/Paper5457/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes using node features, graph structure, and node labels as three different views and maximizing their agreement.  Therefore, the loss function has three terms and gets optimized with an alternating optimization method. ",
            "strength_and_weaknesses": "**Strengths**\n- The paper is well-written and clear.\n- The method shows strong performance in very low node label scenarios.\n- MULTIVIEW4GNN is memory and time efficient.\n\n**Weaknesses** \n- D_A assumes that graphs have high homophily ratios (the smooth assumption). What would the model perform when the graph is heterophily?\n- The performance is promising mostly in the scenarios where we have very few labels. \n- In my opinion, more rigorous experimentation is needed. The datasets are mostly small, easy to solve, and not diverse. \n- Is it mentioned anywhere in the paper what would be the case if there are no initial node features? ",
            "clarity,_quality,_novelty_and_reproducibility": "The manuscript is clear and written with good quality. \\\nThe approach is relatively novel but it's limited. \\\nMULTIVIEW4GNN is relatively straightforward so reproducibility shouldn't be an issue here.",
            "summary_of_the_review": "Most of the points are mentioned in the strength and weaknesses section but in summary, the authors suggest that we can see different types of information as different \"views\" and try to maximize the agreement between these views with an alternation optimization algorithm. They show that the model is efficient and has good performance, mostly in very low node label scenarios. The datasets in the experiment section are small and not diverse.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5457/Reviewer_YbZy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5457/Reviewer_YbZy"
        ]
    },
    {
        "id": "USb6cx2mn2",
        "original": null,
        "number": 4,
        "cdate": 1667192619178,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667192619178,
        "tmdate": 1667192619178,
        "tddate": null,
        "forum": "r_4nJuPpCh-",
        "replyto": "r_4nJuPpCh-",
        "invitation": "ICLR.cc/2023/Conference/Paper5457/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new perspective of graph learning by regarding node features, graph structures, and node labels as three different views. Different from the previous GNN models, the paper brings in a new latent variable to be learned and shared across all three views. Through a progressive optimization framework, the paper claims the new framework can obtain better computational and memory efficiency. ",
            "strength_and_weaknesses": "The regularization framework  $min_F tr(F^T L F) + ||F-Y||^2$, which is used to derive the graph learning models, is not new and it has been well known for decades. Early works such as manifold ranking [1] already used it; in the era of graph neural networks, as the paper mentioned, some works [2] proposed the unified optimization view of GNNs and also used it; in addition, from the perspective of graph signal processing, there are also some works [3,4] which used it as a special case. These works generally directly obtained the solution expression of F, and then use MLP(X) to take place of F. This paper differs in that it added || MLP(X) \u2013 F||^2 as another regularization and unroll the optimization process (the methodology of [4] is somehow similar but used a different objective). It does provide a new perspective and still looks a little bit trivial. The understanding of updating F as propagation and updating \\Theta as pseudo label generation is interesting.\n\n However, the biggest problem is I did not see much advantage of the new formulation and optimization. The analysis of the time complexity if also doubtable.  If Multiview4GNN and APPNP only have dimension $c$ in time complexity because of the feature dimension transformation. Why do not we use a different step for SGC? $XW$ part can be first calculated (so the feature dimension is changed to c) and then do the feature aggregation with \\sigma(A^n XW). Let us look back at Multiview4GNN, as other methods contains the computation of backpropagation, why is the backpropagation of the MLP in equation (9) not counted in Multiview4GNN? This MLP generally does not only have one layer so the dimension $d$ cannot be omitted, and the backpropagation also takes $t$ steps. \n\n[1] Zhou et al. Learning with Local and Global Consistency. In NIPS 2003.\n[2] Ma et al. A Unified View on Graph Neural Networks as Graph Signal Denoising. In CIKM 2021.\n[3] NT, Maehara. Revisiting graph neural networks: All we have is low-pass filters[J]. arXiv preprint arXiv:1905.09550, 2019.\n[4] Chen et al. BiGCN: a bi-directional low-pass filtering graph neural network. arXiv preprint arXiv:2101.05519, 2021.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The description of the method is clear but the complexity comparison need to be improved. \nNovelty: it provides a new perspective but maybe not so useful. ",
            "summary_of_the_review": "In general, the paper has some interesting insights but the novelty is limited. And the contribution especially the analysis of the advantage of the complexity has some problems. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5457/Reviewer_aMtC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5457/Reviewer_aMtC"
        ]
    }
]