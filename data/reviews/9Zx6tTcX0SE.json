[
    {
        "id": "QHQHM3Z6Wi",
        "original": null,
        "number": 1,
        "cdate": 1666344464232,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666344464232,
        "tmdate": 1666344464232,
        "tddate": null,
        "forum": "9Zx6tTcX0SE",
        "replyto": "9Zx6tTcX0SE",
        "invitation": "ICLR.cc/2023/Conference/Paper6598/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors introduce a biologically inspired neural network architecture that combines a variety of learning and processing mechanisms, with the goal of studying whether the performance of artificial systems might be improved by increasing their computational realism. In particular, the authors explore the implementation of Dale\u2019s principle, Hebbian updates, sparse coding, heterogeneous dropout, synaptic consolidation, and experience replay. The proposed model is tested in continual learning (CL) tasks derived from the popular MNIST benchmark, and results suggest significant performance gains.",
            "strength_and_weaknesses": "One of the strengths of this paper is that it combines a remarkable number of recent ideas and biological processing mechanisms into a unified architecture. The proposed model is also tested in an incremental fashion, which allows to better understand the contribution of each mechanism / biological feature in final performance.\n\nA major weakness is the adoption of fairly \u201ceasy\u201d tasks, since performance is almost at ceiling in most task variants (the only challenging one seems to be the Seq-MNIST). This might not allow to fully appreciate the gain associated with each model variants (e.g., Hebbian Update in Tab. 1). The latter issue is also amplified by the small number of runs (n=3) for each condition: increasing this number to at least 5 or 10 would allow to better clarify the actual gains, at least qualitatively; increasing it even more (maybe in some representative cases) would further allow to verify the gains through statistical testing. It would also be interesting to consider the full \u201cBio-ANN\u201d model without Hebbian Update, to check whether the overall performance gain in the combined model really depends on the inclusion of such principle. Moreover, it might be useful to compare the proposed model with some other state-of-the-art approaches in continual learning to better highlight its potential (and possibly its limits).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is written in a very clear way. The overall framework is properly motivated, the related work is (to the best of my knowledge) appropriately referenced and the results are presented in a comprehensible way.\n\nQuality: The results are interesting, and the methods seem appropriate. As stated above, I would suggest increasing the number of runs for each condition to better establish the statistical significance of the performance gains, and possibly include the comparison with an alternative CL approach and/or a more challenging CL task to more robustly validate the model.\n\nNovelty: The proposed architecture is original since it combines many recent (or even very-recent) ideas into a unified framework.\n\nReproducibility: I think the results are reproducible.",
            "summary_of_the_review": "Overall, my judgment of this paper is positive. The research scope of this paper is broad enough to be of interest to the audience at ICLR and stimulate further interdisciplinary research work. At the same time, I believe that further simulations are required to make the paper stronger.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6598/Reviewer_TqnH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6598/Reviewer_TqnH"
        ]
    },
    {
        "id": "FpYwnAT0vg",
        "original": null,
        "number": 2,
        "cdate": 1666640529597,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640529597,
        "tmdate": 1666640529597,
        "tddate": null,
        "forum": "9Zx6tTcX0SE",
        "replyto": "9Zx6tTcX0SE",
        "invitation": "ICLR.cc/2023/Conference/Paper6598/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors claim that including biological features in artificial neural networks can improve their performance on tasks such as continual learning. The authors propose to include four different biological features into their models and evaluate their relevance: 1) segregation of neurons into populations of excitatory and inhibitory neurons as in Dale's principle. 2) Dendritic-inspired structures as modulators and sparsifiers. 3) Hebbian plasticity rules. 4) Experience replay and regularization. \n\nThe different scenarios are evaluated based on variations of the famous MNIST dataset, showing the influence of the different parameters when considered together or separately. The authors claim that the results obtained offer compelling evidence. ",
            "strength_and_weaknesses": "The paper addresses an important question on the importance of biological features for guiding the design principle of artificial neural networks. \n\nHowever, it is unclear how we can evaluate the importance of such features with the currently limited evaluation. How does it generalize to other datasets? What about synthetic datasets, where one can control for the different structures of the data that the features are supposed to be useful for?  \n\nThe influence of the different parameters is primarily measured in accuracy, but how is that the suitable proxy for measuring the impact of these features? Why are those features even relevant in this context? \n\nWhat is the rationale for combining Dale's principle, pyramidal-like cells, and Hebbian principles? Pyramidal cells are arguably not driven by the Hebbian principle. How is sparsity enforced without interneurons?  \nThe biological plausibility of those features is mostly in name but is vaguely plausible in their current implementation; as such, it cannot be one of the paper's main claims.\nMore rigor in how these concepts are handled would be greatly appreciated. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is relatively well written. The motivations and rigor expected are lacking but various things are suggested in order to possibly improve the current submission. \n\nThe novelties are minor, it is rather incremental work. ",
            "summary_of_the_review": "This paper addresses the important question of the influence of biological inspiration on the design of artificial neural networks. \nThe paper falls short however in how the influence of the different features is evaluated and motivated. \nIt is a bit of a patchwork of biological components rather than a cohesive model. \n\nI greatly encourage the author to further this research as it can provide a great alternative to the blind study of artificial neural networks. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6598/Reviewer_jDQX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6598/Reviewer_jDQX"
        ]
    },
    {
        "id": "eWpE60xEnx",
        "original": null,
        "number": 3,
        "cdate": 1666687147853,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687147853,
        "tmdate": 1666687147853,
        "tddate": null,
        "forum": "9Zx6tTcX0SE",
        "replyto": "9Zx6tTcX0SE",
        "invitation": "ICLR.cc/2023/Conference/Paper6598/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work the authors study the individual and combined effect of a range of biologically inspired mechanisms on continual learning. ",
            "strength_and_weaknesses": "Strengths\n- Interesting and important research for both AI and neuroscience\n- Paper is well written and all approaches are clear and well described \n\nWeaknesses\n- No floor (e.g normal training) and ceiling (e.g. full replay) controls provided in the experiments.\n- Experiments limited to variants of MNIST. Unclear if active dendrites approach & prototypes/context will be as effective with datasets without an exemplar structure, like fashion mnist. \n- Unclear how the difference mechanisms interact with each other, but possibly beyond the scope of a conference paper. \n- Why do active dendrites fail with seq-mnist?\n- Is there biological evidence for context dependent dendritic processing of eq. (4). \n- Heterogeneous dropout does not seem to be biologically motivated. \n- Novelty appears limited to combining existing approaches, the authors could more clearly describe their specific novel contributions. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and of good quality. Novelty appears somewhat limited. \n",
            "summary_of_the_review": "Overall I think this is an important and interesting study, though interactions between mechanisms could be explored better and some controls are missing. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6598/Reviewer_Bd22"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6598/Reviewer_Bd22"
        ]
    },
    {
        "id": "ZyVZ5c3O62",
        "original": null,
        "number": 4,
        "cdate": 1666703938614,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703938614,
        "tmdate": 1666703938614,
        "tddate": null,
        "forum": "9Zx6tTcX0SE",
        "replyto": "9Zx6tTcX0SE",
        "invitation": "ICLR.cc/2023/Conference/Paper6598/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper evaluates previous work on biologically plausible DNNs in the setting of continual learning (CL). Namely, they evaluate ideas around Dale\u2019s principle, Active Dendrites, heterogenous dropout, Hebbian learning, synaptic consolidation and experience replay. They present experimental evidence that these ideas can individually improve the performance on CL, or be combined to achieve greater improvement.",
            "strength_and_weaknesses": "+ This paper provided a useful list of biologically-inspired modifications to DNNs.\n+ I find the main idea interesting (investigating such modifications\u2019 usefulness to CL).\n- The paper is a compilation of already published work.\n- Some of these insights (s.a. dropout being useful for CL) can be found in literature.\n- The experiments are exclusively done on MNIST-based datasets. While they provide evidence to support the claims of this paper, it would be useful to understand whether this translates into other image domains.",
            "clarity,_quality,_novelty_and_reproducibility": "I found the discussion around the properties of biological neural networks a bit confusing. Concretely, I don\u2019t think that the terminology was clearly described.\nNovelty: I do not think that the ideas expressed in this paper are novel. A \u201crelated work\u201d section would be a great way to set the paper apart from similar insights expressed in CL literature, and point out how the ideas of this paper are different.\n",
            "summary_of_the_review": "While the paper presents an interesting discussion, it reuses ideas from other work and presents limited experimental evidence.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6598/Reviewer_Ds8Z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6598/Reviewer_Ds8Z"
        ]
    }
]