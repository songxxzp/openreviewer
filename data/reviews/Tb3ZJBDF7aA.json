[
    {
        "id": "kUbpWWPAhyy",
        "original": null,
        "number": 1,
        "cdate": 1665715503854,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665715503854,
        "tmdate": 1665715503854,
        "tddate": null,
        "forum": "Tb3ZJBDF7aA",
        "replyto": "Tb3ZJBDF7aA",
        "invitation": "ICLR.cc/2023/Conference/Paper3898/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper proposed to use diffusion model as a pretrain model for several downstream tasks.\n\nIt decomposes the structure and uses a DDPM like diffusion for continuous variable and a multimodel diffusion for categorical variable. Experiment shows that the diffusion model serves as a good pretrain model.",
            "strength_and_weaknesses": "Strength:\nUsing diffusion model as a pretrain model seems to be a new application.\n\nWeakness:\n1. I am not an expert of the application area this paper considers. But simply examing the experiment, it seems the improvement is not large. There is no repeated experiment and it's unknown whether the performance is significant.\n\n2. The overall technical novelty is low. It follows a standard design of diffusion model and the contribution seems restricted to diffusion as a pretrain model?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good.\n\nQuality: good.\n\nNovelty: Low.\n\nReproducibility: unknown. No example code was provided.",
            "summary_of_the_review": "See above.\n\nMy evaluation is a educated guess.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3898/Reviewer_UBXw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3898/Reviewer_UBXw"
        ]
    },
    {
        "id": "XC9E6VCah3",
        "original": null,
        "number": 2,
        "cdate": 1666546822740,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666546822740,
        "tmdate": 1666546822740,
        "tddate": null,
        "forum": "Tb3ZJBDF7aA",
        "replyto": "Tb3ZJBDF7aA",
        "invitation": "ICLR.cc/2023/Conference/Paper3898/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose SiamDiff, a method that employs a multimodal diffusion process to simulate the structure-sequence co-diffusion trajectory. SiamDiff maximises the Mutual Information (MI) between paired correlated views of proteins, where one is the native protein and the other is obtained with a structure perturbation. Under this learning framework, the authors argue that the model can acquire the atom- and residue-level interactions underlying protein structural changes, as well as the residue type dependencies. Extensive experiments on diverse and standardised downstream tasks are shown to demonstrate the value of the learned protein representations.",
            "strength_and_weaknesses": "The authors introduce some interesting and novel ideas in this paper. They show that their Siamese multimodal diffusion process can obtain reasonable protein representations that are useful for several different downstream tasks. Modelling the joint distribution of protein sequences and structures through multimodal denoising allows them to capture SiamDiff to capture the atom and residue-level interaction. Another interesting point is their idea of maximising the MI between two Siamese diffusion trajectories, one based on the native view of the protein, and the other view being generated by a structure perturbation.\n\nThe mathematical framework presented is sound. The authors also present the proofs to every proposition they make. They also mention that the full code will be available for reproducibility. \n\nThe experiments shown in the paper are very thorough, where all the tasks have standardised experimental settings in well-established benchmarks. The empirical results are strong enough to prove the effectiveness of the protein representations obtained by SiamDiff.\n\nI feel that the paper is lacking clarity. While I understand that there are a lot of concepts to fit in the manuscript, I believe that this paper could benefit from high-level intuition behind the main ideas of the paper. The notation feels overloaded and it is hard to follow.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well-written and the language is concise. However, as I mentioned before, I feel that the paper is not that clear. I had to go through several readings of the manuscript to fully comprehend the main ideas in the paper. Some better intuition and improving of the structure could be greatly beneficial to this paper. \n\nI believe this is a high-quality paper. The mathematical framework introduced is sound, the experiments are extensive and thorough, and the empirical results obtained are strong.\n\nThis method is a novel application of existing methods, however, the real novelty lies in the ideas applied to learn the protein representations. There are some clear distinctions from previous works, like the multimodal denoising and the Siamese paired views of proteins. \n\nAs for reproducibility, the authors mention that the code will be made available in case the paper gets accepted. The representation learning is well-defined in the paper. However, the models used for downstream tasks, based on the representations, are not that clear. The information can be found in the supplementary material, but I believe that a couple of sentences providing more details about these models should be included in the main paper. In short, releasing the code for both the training of SiamDiff, as well as the pipeline for downstream tasks is critical for the reproducibility of this work.\n",
            "summary_of_the_review": "My recommendation is an accept. I believe that this is a good paper introducing some novel ideas that are very relevant to protein representation learning. The evaluation of the learned representations in downstream tasks was made following well-established benchmarks. Although, the performance achieved is not the highest for every task, the general value of these representations is evident from the empirical results. \nI do expect the authors to address the issues raised in this review, mainly about clarity.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3898/Reviewer_jQVb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3898/Reviewer_jQVb"
        ]
    },
    {
        "id": "vtB3-iVQEzl",
        "original": null,
        "number": 3,
        "cdate": 1666676751205,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676751205,
        "tmdate": 1666676751205,
        "tddate": null,
        "forum": "Tb3ZJBDF7aA",
        "replyto": "Tb3ZJBDF7aA",
        "invitation": "ICLR.cc/2023/Conference/Paper3898/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper applied diffusion model to learn the protein structure from protein sequence. Compared with previous method, they introduced contrast learning and utilized mutual information. The experiment revealed that the proposed method showed marginal performance increase. ",
            "strength_and_weaknesses": "Strength:\nThe proposed method tried to achieve better protein residual presentation by introducing contrast learning and maximize mutual information.\nThe proposed method showed marginal performance increase compared with existing methods.\n\nWeakness:\nAs far as I regard, the proposed method is not quite intuitive. Diffusion model is already a method that apply graduate noise. Such that, introducing another level of random noise for contrast learning is not intuitive to me. The marginal performance increase also suggest some conflict between diffusion model and the contrast learning. I was wondering is there some theoretical analysis between diffusion model and the introduced random noise?\nI also do not quite agree with the claim that added random noise could reflect the conformational fluctuation. Simply add the noise on the structure is not equivalent to random permutation. Sometimes, a random permutation could drastically change the protein structure. Simply add random noise, could never reflect such conformational change. \n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "It is quite clear however the author did not provide code.",
            "summary_of_the_review": "The paper focus on the residual level prediction accuracy. It utilize diffusion model with added mutual information training. However, there lack some clear linkage between diffusion model and synthetic mutual information, which makes the method not intuitive. Moreover, the added noise is quite straightforward, which is not optimal to reflect conformation change. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3898/Reviewer_kp2m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3898/Reviewer_kp2m"
        ]
    },
    {
        "id": "tZNO-VQCnr",
        "original": null,
        "number": 4,
        "cdate": 1667079315185,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667079315185,
        "tmdate": 1667079315185,
        "tddate": null,
        "forum": "Tb3ZJBDF7aA",
        "replyto": "Tb3ZJBDF7aA",
        "invitation": "ICLR.cc/2023/Conference/Paper3898/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the task of unsupervised learning of protein structures. They do so by maximizing the mutual information between two diffusion trajectories that start with a protein and a perturbed copy of the protein. The authors prove a lower bound of the mutual information which resembles an ELBO and specifically depends on the likelihood of one trajectory given the other.\nAs such one trajectory is used to compute the noises in the other trajectory.\nBy doing so, the procedure encodes both the structure and sequences jointly.",
            "strength_and_weaknesses": "### Strengths\n- The paper comes up with a lower bound for the mutual information between two diffusion sequences and uses it to lean an encoder for protein structure. This is an interesting case of theory guiding practice and makes the work interesting.\n- The ablation study shows that all the components of the procedure contribute to improving the model.\n\n### Weaknesses\n- What are correlated views? These show up in the abstract and introduction without a simplified explanation and throw off a reader who is (even very slightly) outside the field.\n- Table 1 and 2 are not color blind friendly, the community seems to use bold and underline to denote first and second, One can also use markers like $\\dagger$ and $\\circ$. Standard deviations are missing. Provide context for $F_{\\text{max}}$, and $\\rho$ in the caption for ease of skimming. In fact I was unable to find a definition of $F_{\\text{max}}$.\n- The abstract talks about comparing with motif based methods, this comparison doesn't show up in the experiments. \n- What do you mean by `We do not include the results of GVP on EC and GearNet-Edge on RES tasks due to their poor performance.`?\n- [Personal nitpick] Page 2 first sentence - shortening Tables to Tabs is non-standard.\n- The writing was pretty dense and I required multiple reads to understand the text. However the figures helped and the fact that this is not my primary field of research is not the authors' fault.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The method is novel and the methods derived in the paper are interesting.\n- The writing is dense and assumes strong domain knowledge in proteins and is not really accessible to the general ML audience. Multiple concepts are not defined and this reviewer would find it hard to reproduce the work. The theoretical proofs are complete and fully reproducible to the point that results from cited papers are re-proved for convenience.",
            "summary_of_the_review": "- I am giving this paper a weak reject due to the minor concerns noted in the above two sections, specifically about reporting on the experimental results and making the writing more accessible to the general audience at ICLR. I am willing to upgrade my rating based on my interaction with the authors during the discussion phase.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3898/Reviewer_yEUV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3898/Reviewer_yEUV"
        ]
    }
]