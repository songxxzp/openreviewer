[
    {
        "id": "dW9f2XDV3Z",
        "original": null,
        "number": 1,
        "cdate": 1666241319966,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666241319966,
        "tmdate": 1666241319966,
        "tddate": null,
        "forum": "yLzLfM-Esnu",
        "replyto": "yLzLfM-Esnu",
        "invitation": "ICLR.cc/2023/Conference/Paper2057/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method to build a Tensor Train (TT) representation of a multidimensional tensor when the computation of the tensor entries has an analytical expression with a tree structure involving derivative functions that apply to single entries and values of their neighbor derivative functions (Fig. 1a). Moreover, the obtained representation is exact and it has low TT-ranks when the domain sets of the derivative functions are discrete and have small cardinality. The advantage of the proposed method is that, once the TT model is available, it is possible to operate with it in an efficient way for example to implement sums or convolutions of high-dimensional tensors directly in the TT format. The authors apply their method to several combinatorial problems, including game theory problems and to the calculation of the permanent of a matrix.",
            "strength_and_weaknesses": "Strengths\n-\tThe idea is novel, and it could have important applications beyond the scope of the present paper.\n\nWeaknesses\n-\tThe assumption that a set of analytical derivative functions is available is a very strong hypothesis so the number of cases where this method can be applied seems limited.\n-\tThe high dimensional tensor can be also compactly represented by the set of derivative functions avoiding the curse of dimensionality, so it is not clear what is the advantage of replacing the original compact representation by the TT representation. Maybe the reason is that in TT-format many operations can be implemented more efficiently. The paper gives not a clear explanation about the necessity of the TT representation in this case.\n-\tIt is not clear in which cases the minimum rank is achieved by the proposed method. Is there a way to check it?\n-\tIn the paper it is mentioned that the obtained core tensors can be rounded to smaller ranks with a given accuracy by clustering the values of the domain sets or imposing some error decision epsilon if the values are not discrete. It is not clear what is, in theory, the effect on the approximation in the full tensor error. Is there any error bound in terms of epsilon?\n-\tThe last two bullets in the list of main contributions and advantages of the proposed approach are not clear to me (Page 2).\n-\tThe method is introduced by an application example using the P_step function (section 2.2). I found this example difficult to follow and maybe not relevant from the point of view of an application. I think, a better option would be to use some problem easier to understand, for example, one application to game theory as it is done later in the paper.\n-\tVery relevant ideas and results are not included in the main paper and referred instead to the Appendix, which makes the paper not well self-contained.\n-\tThe obtained performance in terms of complexity for the calculation of the permanent of a matrix is not better than standard algorithms as commented by the authors (Hamilton walks obtained the result with half of the complexity). It is not clear what is the advantage of the proposed new method for this application.\n-\tThe comparison with the TT-cross method is not clear enough. What is the number of samples taken in the TT-cross method? What is the effect to increase the number of samples in the TT-cross method. I wonder if the accuracy of the TT-cross method can be improved by sampling more entries of the tensor.\n\nMinor issues:\n-\tPage 2: \u201can unified approach\u201d -> \u201ca unified approach\u201d\n-\tPage 2: \u201cand in several examples is Appendix\u201d -> \u201cand in several examples in the Appendix\u201d\n-\tIn page 3, \u201cbasic vector e\u201d is not defined. I think the authors refers to different elements of the canonical base, i.e., vectors containing all zeros except one \u201c1\u201d in a different location. This should be formally introduced somewhere in the paper.\n-\tPage 9: \u201cas an contraction\u201d -> \u201cas a contraction\u201d\n",
            "clarity,_quality,_novelty_and_reproducibility": "The article presents a nice an original method for dealing with high dimensional tensors, although its applicability is limited to a few selected problems.\nThe quality and clarity of the presentation could be largely improved, some parts are not clear enough and many relevant ideas and results are briefly commented in the paper (see my comments in the Weaknesses section).\n",
            "summary_of_the_review": "I think that, although the paper could be highly improved in quality and clarity, the ideas and method introduced are interesting and could have large impact in future applications.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2057/Reviewer_vyEW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2057/Reviewer_vyEW"
        ]
    },
    {
        "id": "zcT0gQIjHAX",
        "original": null,
        "number": 2,
        "cdate": 1666358951480,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666358951480,
        "tmdate": 1666358951480,
        "tddate": null,
        "forum": "yLzLfM-Esnu",
        "replyto": "yLzLfM-Esnu",
        "invitation": "ICLR.cc/2023/Conference/Paper2057/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studied a new way to build the tensor train (TT) representation for a special group of functions. In particular, as proved in the paper, this paper claimed that a functional tensor could be represented in the low-rank TT format if the image of the \u201cderivative functions\u201d is sufficiently small. In the experiments, the proposed method was used for two applications: (1) cooperative games and (2) the computation of the matrix permanent. The proposed method gives a significant improvement in the performance for both two problems.",
            "strength_and_weaknesses": "+Strength:\n\n1. The targeted problem is relatively novel. In machine learning, the problem is rarely discussed in the tensor community.\n2. The applications mentioned in the paper\u2014cooperative games and the computation of matrix permanent\u2014illustrate the potential of tensor methods in new applications.\n\n- Weakness\n\n1. The paper is hard to follow, even though the main idea seems not complicated.\n2. The experimental results for the cooperative game are not convincing.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and quality**\n\nThe clarity and quality of this paper are relatively lower than others I review or read from the same-level conferences.\n\n1. some technical vocabulary used in the paper is not well explained. For example, the definition of \u201canalytic dependence\u201d is not clearly given throughout the paper, but it seems to be a crucial concept to understand the main idea.\n2. The proposed method is only suitable for tensors, of which the image of the involved \u201cderivative function\u201d is small-size (proved in Thm 2.1), but there is no discussion about which families of functions result in such property. The lacking of discussion on this point makes me doubt if the proposed method can be widely applied in real.\n3. I agree that the applications mentioned in the paper are very interesting and would inspire lots of tensor researchers. However, in the experiment of the cooperative games, only the work~(Ballester-Ripoll, 2022) is implemented for comparison. It is difficult to support that the proposed method outperforms SOTAs claimed in conclusion.\n\n**Novelty**:\n\nThe novelty of this paper seems good. The paper focuses on a relatively different setting of the tensor network, unlike tensor decomposition, completion, and parameter compression, which has been widely discussed in the machine learning community.\n\nIn my own opinion, this work is closely similar to studies by Boris N. Khoromskij et al. on quantized tensor approximation (whose paper was also cited by the authors). For example in sec 4 of (Khoromskij, 2018), it theoretically discussed the low TT-rank property of various functions under reshaping. I do not find too much difference between this paper and these existing works. It will be good if the authors highlight the differences in the revision of the manuscript.\n\n*Khoromskij, Boris N. \"Tensor numerical methods in scientific computing.\"\u00a0Tensor Numerical Methods in Scientific Computing. De Gruyter, 2018.*\n\n**Reproducibility**\n\nThe experimental codes were provided.",
            "summary_of_the_review": "It is hard to simply judge if this work is good or bad. It\u2019s quite different from others I have reviewed recently. Both the strength and drawbacks are obvious. It would be a good work if the mentioned problem is revised.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2057/Reviewer_ApYB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2057/Reviewer_ApYB"
        ]
    },
    {
        "id": "3_Js0M_gaY",
        "original": null,
        "number": 3,
        "cdate": 1666448173755,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666448173755,
        "tmdate": 1666448173755,
        "tddate": null,
        "forum": "yLzLfM-Esnu",
        "replyto": "yLzLfM-Esnu",
        "invitation": "ICLR.cc/2023/Conference/Paper2057/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Given a function f : Z^d -> R, the authors construct tensor-train representations for the implicitly defined tensor. They first need to build a computational tree for the entries of the tensor that involves left and right derivative functions. From this, they show that they can construct a TT format for the tensor X_{i1,...,id} = f(i1,...,id) that has a TT-rank vector that depends on the size of the image of each derivative function (see Theorem 2.1 and Algorithm 2). They use analytical techniques to write down the derivative functions. Once the tensor is in TT-format, subsequent tensor computations can be sped up.\n",
            "strength_and_weaknesses": "The main observation in this paper is in Figure 1: A computational tree for the values of a tensor gives you a TT-decomposition. Going in the opposite direction is standard. A TT-decomposition gives you an efficient computational tree to recover any tensor entries. However, the usual challenge is finding a reasonable computational tree. In this paper, the authors give many applications where one can write a reasonably efficient computational tree. Before reading this paper, I always imagined that writing down reasonable computational trees for each entry of a tensor was challenging except in very simple examples. That's why we have to compute the TT format. \n\nHowever, the authors demonstrate this semi-analytic technique on several applications that ends up being quite convincing. In particular, the appendix contains many applications and examples of derived derivative functions. It would be nice to have more discussion on how one can construct a reasonable derivative functions (hence, computational tree). Or, are the authors just writing down anything that works? When one can write down a reasonable set of left and right derivative functions, then the resulting TT-decomposition is usually sparse. \n\nThe big weakness of the paper is the possibility that it is quite a specialized approach that only works on a handful of carefully selected functions, f, for the tensor entries.  It is hard to judge the generality of the approach, but the range of applications in the appendix is highly welcomed. I also assume that it is hard to write down a computational tree, in the setting when you are happy with a lossy compression. \n\nIn some of the applications in the appendix, it was unclear what the computational benefit is in terms of solving a problem.",
            "clarity,_quality,_novelty_and_reproducibility": "I struggled to understand what they are exactly referring to \"their method\". I believe they want Algorithm 2 to be \"their method\" as that is the algorithm that takes the left and right set of integer-valued derivative functions to TT-cores. \n\nWhile there are lots of papers that work with tensor formats of functions and develop techniques for compression, they most work in the lossy setting. The paper is demonstrating the approach in setting for which TT formats are rarely employed. ",
            "summary_of_the_review": "The paper presents a relatively simple, but effective, idea of deriving TT decompositions of tensors by analytically writing down a computational tree. The scope of this paper may be slightly limited, though in the appendix the authors do a good job at demonstrating the approach on a range of problems.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2057/Reviewer_EMLj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2057/Reviewer_EMLj"
        ]
    },
    {
        "id": "zI4oGTe5I-",
        "original": null,
        "number": 4,
        "cdate": 1666897487703,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666897487703,
        "tmdate": 1670517224852,
        "tddate": null,
        "forum": "yLzLfM-Esnu",
        "replyto": "yLzLfM-Esnu",
        "invitation": "ICLR.cc/2023/Conference/Paper2057/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a method for formulating particular multivariate functions based on indices as a particular tensor train decomposition. An overview of the construction is presented, where so-called derivative functions are used to define index dependences as well as the cores of the TT representation. Analysis of the construction is presented, and a number of experimental settings are evaluated where particular problems can be framed within the proposed derivative function TT framework.",
            "strength_and_weaknesses": "\nStrengths\n1) The paper presents an interesting formulation building on existing work modeling functions as tensor representations.\n2) A large number of (cool!) example problems are provided, which are both individually interesting and strongly support the author's claims that the method is quite general and can be variously applied.\n3) All formulations and implementations are extremely detailed, and an interested reader would have no problem recreating this work given the paper and Appendix, as well as the anonymized linked code.\n\nWeaknesses\n1) The organization of the paper could be improved. In the main paper, the Appendix is referenced 11 times. Many of those places seem to allude to significant contributions or problems of interest. Personally, I was able to much more understand the motivation and the construction of the method through some of the examples given in the Appendix (knapsack, n-queens). Throughout the main paper as a reader, it was hard to understand and follow the motivation of why and where this could be valuable or interesting. The cooperative game examples were a bit further outside of my familiarity, and I assume this might be true for other readers; including a classical combinatorial optimization or other Hard problem from the Appendix could connect with a wider audience. Sections 3.3 and 4 could be merged with the introduction, they do not meaningfully add much currently.\n\n2) Theorem 2.1 does not seem to be revealing anything particularly interesting, a discussion of why this is not obvious or why it is valuable would be beneficial over the full proof which could be moved to the Appendix, leaving more space for additional examples.\n\n3) In the end, the practical value is not demonstrated that well. While the construction performs better than the cross-TT approach for the given problems, it's not clear why constructing functions in this way is particularly valuable over the existing, \"non-tensor\" problem formulations or functions. The complexity of the tensorized problem in both space and time seems to only be lower bounded by existing solutions/solvers. This is further demonstrated by the paper's own description that the rank can sometimes be low and sometimes significantly higher than approximation methods. The experiments do not compare solving the problem against traditional solvers.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\nThe paper can be followed by someone who has a strong tensor-train background, but the existing presentation would be difficult for someone without it. The paper might benefit from a slower description and motivation in section 2 and 2.1, where it is somewhat unclear why the \"middle-out\" structure is needed, especially if \"it is more efficient to start the calculation from one end\" (bottom of page 2).\n\nQuality\nWith the appendix, the construction and exploration of different problems is significant.\n\nNovelty\nThe formulation of various games as TT representation seems to be novel and interesting. However, the practical value of the proposed method seems quite low, outside of curiosity and interest. \n\nReproducibility\nI feel confident that, with a careful reading, anyone would be able to fully reproduce the work. The provided code is sufficient to replicate all experiments with relative ease.",
            "summary_of_the_review": "Ultimately the paper is poorly organized, and fails to effectively motivate a reader as to why they should be interested in and value the proposed method. There are some particularly interesting pieces, but they are not communicated effectively in the current presentation.\n\n---\nAfter Rebuttal:\nThe authors have addressed a majority of my concerns (and mistaken confusions) and put significant effort in restructuring the paper for clarity and readability. It is still difficult for me to see the significance (theoretically or practically) outside of what is presented here, but it is interesting work nonetheless. The revision is much clearer to me;  I have increased my score to reflect this.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2057/Reviewer_gzvu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2057/Reviewer_gzvu"
        ]
    }
]