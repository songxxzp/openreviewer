[
    {
        "id": "xX7I1gH7Np",
        "original": null,
        "number": 1,
        "cdate": 1666506588472,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666506588472,
        "tmdate": 1666506588472,
        "tddate": null,
        "forum": "6Wl7-M2BC-",
        "replyto": "6Wl7-M2BC-",
        "invitation": "ICLR.cc/2023/Conference/Paper3552/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed an adapt scheme for sharpness-aware minimization (SAM), in which the algorithm simply perform ERM when landscape is flat and perform SAM which landscape is sharp. This method is based on couple of approximation strategies. The author also establish the theoretical guarantee for their proposed algorithm. Some empirical results are provided to verify the effectiveness of the proposed algorithm.",
            "strength_and_weaknesses": "Strength:\n(1) The proposed method is new and intuitive\n(2) The effectiveness of proposed algorithm is supported by both theoretical and empirical evidence\n\n\nWeakness:\n(1) It is not clear how the author come up with (4) (5) for mean and variance derivation. It seems that this formulation enables establishing convergence result but it is hard to see the intuition behind this formulation.",
            "clarity,_quality,_novelty_and_reproducibility": "As I mentioned in the \"weakness\" in the last section, the author is suggested to clarify how and why they choose the formulation in (4) and (5) for adaptive SAM.",
            "summary_of_the_review": "Overall this paper is well-written and easy to follow. The high level idea of the algorithm proposed in this paper is intuitive and nature and should be interesting to the community. Since i didn't see obvious weakness in both theoretical and empirical results, I will give the score of 6 for now.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3552/Reviewer_4reb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3552/Reviewer_4reb"
        ]
    },
    {
        "id": "hv9RjA6TOk",
        "original": null,
        "number": 2,
        "cdate": 1666528365904,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666528365904,
        "tmdate": 1670643731271,
        "tddate": null,
        "forum": "6Wl7-M2BC-",
        "replyto": "6Wl7-M2BC-",
        "invitation": "ICLR.cc/2023/Conference/Paper3552/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this work, the authors propose a new variant of SAM optimizer, called AE-SAM by mixing SAM and SGD with a carefully designed condition. Moreover, the theoretical convergence of the proposed algorithm is also provided. Preliminary experiments demonstrate the efficacy of the proposed approach. ",
            "strength_and_weaknesses": "The authors propose a new variant of the SAM optimizer. Preliminary experiments demonstrate its effectiveness. However, before this work is accepted, several concerns should be resolved:  \n(i)\tThe proposed AE-SAM falls into the scope of mixing SGD and SAM. Several existing works adopt a similar idea to improve the efficacy of SAM, including SS-SAM, and Gradient penalized SAM (Zhao et al, ICML 2022). The idea is not new, which limits the novelty of the proposed approach.\n(ii)\tAbout the practicality. Several hyperparameters are introduced in AE-SAM. In general, SAM-type methods are sensitive with respect to the perturbation parameters. More hyperparameters may make the tuning process more difficult. \n(iii)\tAbout the theory. In general, mini-batch SGD could achieve the linear speedup property with respect to the mini-batch size. (see Efficient Mini-batch Training for Stochastic Optimization, KDD). However, in Theorem 3.4, the linear speedup property is missing. In addition, can the author prove that the proposed AE-SAM theoretically outperforms randomized SAM (SS-SAM)? \n(iv)\tAbout the sensitivity of the adaptive policy. The proposed adaptive policy is highly related to mini-batch size. When training a deep neural network with a different mini-batch size, it shows different gradient variances. (see Not All Layers Are Equal: A Layer-Wise Adaptive Approach Toward Large-Scale DNN Training, WWW2022). The authors may conduct more experiments to show the consistency/sensitivity of the adaptive policy with respect to mini-batch size. \n(v)\tMore experiments verification on advanced neural network architecture are needed, such as vision transformer (such as Deit). \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written. However, mixing SGD and SAM is not new.  ",
            "summary_of_the_review": "See the comments above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3552/Reviewer_rg2h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3552/Reviewer_rg2h"
        ]
    },
    {
        "id": "PpgniGjX-j",
        "original": null,
        "number": 3,
        "cdate": 1666768390638,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666768390638,
        "tmdate": 1671435630929,
        "tddate": null,
        "forum": "6Wl7-M2BC-",
        "replyto": "6Wl7-M2BC-",
        "invitation": "ICLR.cc/2023/Conference/Paper3552/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Sharpness-aware minimization (SAM), which searches for flat minima by min-max optimization, has been shown to be useful in improving model generalization. However, SAM requires two gradient evaluations per iteration which makes it cost twice more expensive as SGD. One way to reduce this cost is mixing SAM with SGD in an adaptive manner. This paper proposes an adaptation based on the stochastic gradient\u2019s norm.  The main idea is to keep an adaptive threshold (updating it per iterate), and then compare the gradient norm with this threshold: if it is bigger take a SAM update, otherwise take a SGD update.  They show the convergence of their proposed method. \n",
            "strength_and_weaknesses": "Strength: Adaptive thresholding is a new approach to mixing SAM and SGD/ERM \nWeaknesses: choosing this threshold is not well justified. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow and understand ",
            "summary_of_the_review": "Comments: \n1- Assuming that the stochastic gradient follows a normal distribution is not well justified. For example, there exists research in the literature that shows this norm follows heavy-tailed distribution and not normal distribution. \n\n2- In your analysis, the adaptivity of the threshold doesn\u2019t show up and the analysis is very similar to the method which takes SAM step randomly. Specifically, in the deterministic setting, there is no stochastic gradient and hence normal distribution assumption is incorrect. Besides in the empirical result, there is no significant difference between AE-SAM and SS-SAM. \n\n3- It is not explained in the paper why using AE-SAM can improve the robustness against label noises. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3552/Reviewer_qUYU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3552/Reviewer_qUYU"
        ]
    }
]