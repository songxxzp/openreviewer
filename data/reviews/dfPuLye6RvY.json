[
    {
        "id": "y3peFpzM-w",
        "original": null,
        "number": 1,
        "cdate": 1666495090012,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666495090012,
        "tmdate": 1666495090012,
        "tddate": null,
        "forum": "dfPuLye6RvY",
        "replyto": "dfPuLye6RvY",
        "invitation": "ICLR.cc/2023/Conference/Paper1254/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates whether or not light-weight probings to the action between continuous states and reward can measure the pretrained encoder performance on RL tasks. To do this, they pretrained the encoders through self-supervised learning loss with a transition model implemented through recurrent module or recurrent state space modeling with several variations. They tested the pretrained encoder for linear probing and RL tasks. They showed the correlations between the performances on linear probings and RL tasks. They found the linear probing to reward is highly correlated with performance on RL tasks, while the relationship between action linear probing and RL performance is weaker.",
            "strength_and_weaknesses": "Strength\n- The motivation behind this paper is good. Applying the pretrained encoders to RL tasks has been investigated [1,2]. It requires lots of resources. This investigation could be a piece of good evidence to skip the costly evaluation.\n- The many variations and ablations are evaluated. For transition modeling, deterministic recurrent modules and RSSM are validated. For SSL, BYOL and Barlow Twins are tested with various configurations. The ablation studies are reported with and without each objective, such as inverse dynamics modeling and goal-conditioned RL.\n\nWeaknesses\n- They only investigated the SSL methods, not other unsupervised methods such as VAE. I expected they would cover the overall methods from their title, but it is not.\n- They only evaluated nine games. It could not be enough to back up the conclusion.\n- In the equation for reward-reg loss in Reward Probing in section 3.2, should the encoder get $o_{t+1}$ not $o_t$? Because the reward $r_t$ is given when the action is given on the observation $o_t$. \n- In the equation for action-classif loss in Action Prediction in section 3.2, shouldn't the input be $o_t$ and $o_{t+1}$? In Figure 2, the consecutive observations are given to the loss, but not in this equation.\n- In Figure 2, I cannot understand the below sentence. Why is the stacked observation related to data augmentation?\n    - > The observations consist of a stack of 4 frames, to which we apply data augmentation before passing them to a convolutional encoder.\n-  > The action is represented as a 2D one-hot vector and appended to the input to the first convolutional layer.\n    - Why is the action represented through a 2D one-hot vector? The action space is larger than the 2 Dimension.\n- For RSSM, you used the discrete latent. Why didn't you try the continuous latent version [3]? Perhaps, because the discrete version outperforms the continuous latent version in [4], but the discrete latent variable training is more unstable than the continuous latent variable training, so maybe the RSSM with continuous latent variable could outperform the discrete latent version.\n- In BYOL loss equation in section 4.2, should $q(\\hat{y}_{t+k})$ be $q(\\hat{e}_{t+k})$?\n- For Algorithm 1, why did you use the Pseudo code block? It is just a single equation.\n- For the goal-oriented RL loss, please introduce it roughly in the main paper even though the details are in Appendix.\n    - In A.7, there are typo $\\tilde{e}_t+1$.\n- > Similar to the BYOL model, the Barlow model can also be improved with inverse dynamics modeling, while the addition of goal loss has a slight negative impact.\n    - It is interesting. Could you analyze this?\n\n[1] Schwarzer, Max, et al. \"Pretraining representations for data-efficient reinforcement learning.\" Advances in Neural Information Processing Systems 34 (2021): 12686-12699.\n\n[2] Dittadi, Andrea, et al. \"The role of pretrained representations for the ood generalization of rl agents.\" arXiv preprint arXiv:2107.05686 (2021).\n\n[3] Hafner, Danijar, et al. \"Dream to control: Learning behaviors by latent imagination.\" arXiv preprint arXiv:1912.01603 (2019).\n\n[4] Hafner, Danijar, et al. \"Mastering atari with discrete world models.\" arXiv preprint arXiv:2010.02193 (2020).",
            "clarity,_quality,_novelty_and_reproducibility": "Their motivation, model design to evaluate, and evaluation are clearly written except for some minor things, such as the equation typos. Their investigation is novel and looks reproducible through the hyperparameters shared in Appendix.",
            "summary_of_the_review": "This paper investigates the linear probing to the action, and reward could be an indicator of RL performance. Mainly they evaluated the SSL methods, BYOL, and Barlow Twins for 9 Atari tasks. It is interesting and could be helpful for others because applying the pretrained encoder to RL tasks is one of the ways to improve the sample efficiency on RL, but the evaluation is very expensive. However, I felt their writing is rushed, so I think minor revision is necessary. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1254/Reviewer_vEah"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1254/Reviewer_vEah"
        ]
    },
    {
        "id": "FGDHxiYmX3x",
        "original": null,
        "number": 2,
        "cdate": 1666625080078,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625080078,
        "tmdate": 1666625080078,
        "tddate": null,
        "forum": "dfPuLye6RvY",
        "replyto": "dfPuLye6RvY",
        "invitation": "ICLR.cc/2023/Conference/Paper1254/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper attempts to propose an evaluation protocol for lightweight probing of unsupervised representations and investigates the correlation between RL performance and linear probing from a pretrained representation. Authors are testing this on a very specific class of self-predictive (recurrent) representation models that are being trained with SSL objectives. The authors extensively analyse several design choices of their studied model regarding performance on the two probing tasks of predicting reward or action from a held-out labelled train set. Finally, the paper aims to present some correlation between probing and RL performance on 9 Atari games.\n",
            "strength_and_weaknesses": "Strengths\n\n1. Identification of an important problem setup in RL, that is how to assess which representations and pretraining data is best suited for improving downstream RL performance.\n2. Extensive analysis of various design choices of the model architecture studied\n\nWeaknesses\n1. It is not very clear what the goal or objective of the paper is. Authors say they propose an evaluation protocol for unsupervised RL representations that saves up to 600x computation cost. However, this computational cost saving and protocol seems to be not addressed or described in detail in the main paper and doesn't appear to be the main focus of the paper. Instead, the authors analyse a very specific class of self-predictive (recurrent) representation models that are being trained with SSL objectives. I would have expected to see a more thorough coverage of different unsupervised representation learning methods and more empirical analysis to support these claims (see additional comments in point 3)\n2. It is not very clear what is actually novel about the proposed model and what is based on prior work. The listed contributions are very vague. I would hope the authors can clarify what exactly their contributions are. Also, there exist additional prior works that very extensively studied various light-weight probing tasks on unsupervised representations and how their performance relates to RL performances [1,2]\n3. I am a bit confused by the experiments, especially the correlative analysis and it is not very clear to me if this holds beyond the very particular method and environments. In particular Figure 3 is very confusing and I do not fully understand what the 7 representative setups are supposed to be; how this relates to the 9 rows/models presented in Figure 3 (where do they come from?); and how this relates to the 7 Atari games studied in Figure 1. Without this additional information, it is not clear to me if it is sound to draw a general conclusion about the correlation between probing task performance and RL performance or if there are any other confounders. Also, can authors provide error bars for results in Table 1-5?\n4. Minor: There appears to be a slight mismatch between the title/abstract and the presented experiments and contributions of the main paper. The authors study a very particular narrow setting but the title suggests a generally applicable evaluation protocol for different unsupervised representation learning methods. \n5. Regarding the two probing tasks I wonder how generally applicable they really are. E.g. the reward task seems to be constrained to have labelled data from a very early point in RL training (i.e. rather random policy), whereas the action prediction is limited to labelled data from close to expert trajectories. To be generally applicable I would have hoped to see both probes being trained on the same labelled dataset to compare apples to apples. \n\n[1] Dittadi, Andrea et al. \u201cThe Role of Pretrained Representations for the OOD Generalization of Reinforcement Learning Agents\u201d, ICLR 2022.\n\n[2] Higgins, Irina, et al. \"Darla: Improving zero-shot transfer in reinforcement learning.\" International Conference on Machine Learning. PMLR, 2017.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper could benefit a lot from improving clarity and presentation. I would especially suggest that authors more explicitly specify the novel contributions and the scope of the work. Maybe the authors could provide more detail in section 5.4. It is possible that I missed parts when reading the manuscript but I believe novelty and originality are limited in light of my comments above.\n",
            "summary_of_the_review": "While this paper attempts to address an important challenge in RL and aims to propose a generally applicable evaluation protocol for unsupervised pretrained RL representations I am not fully convinced that the paper holds up to these promises and several claims in the paper. I am willing to change my opinion in case I missed central parts of the paper but want to encourage the authors to further improve the manuscript based on some suggestions and comments above and recommend rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1254/Reviewer_etov"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1254/Reviewer_etov"
        ]
    },
    {
        "id": "G3paIwoJ4X",
        "original": null,
        "number": 3,
        "cdate": 1666664388955,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664388955,
        "tmdate": 1666664388955,
        "tddate": null,
        "forum": "dfPuLye6RvY",
        "replyto": "dfPuLye6RvY",
        "invitation": "ICLR.cc/2023/Conference/Paper1254/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper develops an evaluation protocol for unsupervised visual pretraining. They learn linear probes to predict expert agent actions and rewards from encoded states. These probes provide a more cost-efficient way of comparing visual representation learning methods for RL. The evaluation protocol is tested on a handful of Atari tasks and the authors show that the performance of networks on the linear probes well correlates with RL performance.",
            "strength_and_weaknesses": "This paper introduces an interesting idea for the important problem of cost-effective evaluations of visual representations. Currently results are limited to Atari and the evaluation is only performed for a small number of models + self-supervised losses. Because the main goal of this paper is to provide an evaluation protocol that can be used in place of downstream RL performance, it would be helpful to see a much broader range of losses and models as well as more difficult control tasks. It's also unclear how well these evaluation protocols predict downstream performance in the presence of task transfer: e.g., one goal in developing visual representation pre-training methods is to get a good generalizable encoder. Because the evaluation leverages reward information and information about the optimal policy, it doesn't seem like it would predict the fitness of an encoder for new tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall I thought the presentation was straightforward save a few minor confusions:\nIs section 4.1 a contribution of this paper? From the introduction and abstract, I assumed that the paper's main contribution was the evaluation protocol, but it was unclear if the architecture in Figure 2 was adapted from past work or newly introduced for this task. I think this would be helpful to clarify because it would be useful to see the evaluation protocol on multiple kinds of models or on models developed in past work.",
            "summary_of_the_review": "The paper tackles an interesting problem, but I think it should include evaluation over a larger set of ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1254/Reviewer_LabE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1254/Reviewer_LabE"
        ]
    },
    {
        "id": "0t8sTNccIZ",
        "original": null,
        "number": 4,
        "cdate": 1666677810215,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677810215,
        "tmdate": 1666677810215,
        "tddate": null,
        "forum": "dfPuLye6RvY",
        "replyto": "dfPuLye6RvY",
        "invitation": "ICLR.cc/2023/Conference/Paper1254/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for evaluating unsupervised representation learning in reinforcement learning.  Using a linear probe on top of frozen, pretrained representations, the paper suggests learning to predict reward values from various states in downstream tasks.  Additionally, the paper uses a linear probe to predict expert actions from learned representations.  They authors show evidence that, for a selection of representation learning approaches, the F1 score of the linear probe correlates strongly with full reinforcement learning on the downstream task.",
            "strength_and_weaknesses": "The paper tackles a very difficult and relevant problem, that of evaluating self-supervised representations.  The paper shows evidence that linear probing can give strong indications of eventual RL training performance, which promises to shorten evaluation time and could be impactful in the representation learning for reinforcement learning field.  My main concern with the paper is the lack of diversity in methods used to assess the correlation between linear probes and RL training performance.  All methods compared are ablations of the self-predictive representation approach described in the paper.  While these are important and elucidating experiments, I would like to see a broader set of methods compared, like augmentation-based representations (DrQ or CURL).  Do these correlations hold in these cases as well?  Also, I'm curious about the noise in the linear probe F1 score.  Do the numbers reported in the tables stay the same regardless of random seed?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, the experiments are carefully done and interesting ablations are conducted.  Although linear probing is common in computer vision representation evaluation, the generalization to RL and reward prediction is novel as far as I am aware.",
            "summary_of_the_review": "The paper presents an interesting approach to an interesting problem, with the promise of helping evaluate representations much more quickly.  Although the ablation studies are thorough, there could be a broader comparison to other styles of representation learning, which would significantly strengthen the claims that linear probing for reward prediction correlates well with downstream RL training.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1254/Reviewer_Vgoa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1254/Reviewer_Vgoa"
        ]
    }
]