[
    {
        "id": "Wys50JR_ay",
        "original": null,
        "number": 1,
        "cdate": 1666748077117,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666748077117,
        "tmdate": 1666748077117,
        "tddate": null,
        "forum": "6H_uOfcwiVh",
        "replyto": "6H_uOfcwiVh",
        "invitation": "ICLR.cc/2023/Conference/Paper6089/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies how skip connections affect CNNs in the sense of learning theory. The authors investigate this by approximating CNN with kernels based on NTK and the gaussian process. They derive the analytic forms of the kernel and analyze their eigenvalues. ",
            "strength_and_weaknesses": "Strengths\n- The effect of skip connection is rigorously analyzed in the language of kernels. \n- The eigenvalues of the kernels are evaluated.\n\nWeaknesses\n- There's no clear outcome for practitioners. It's not straightforward to utilize the theoretical results for new methods/algorithms/architectures.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mathematically clear. \n\nIn terms of quality, the paper contains significant theoretical results. \n\nThe topic that the paper addresses is a bit narrow. They only focus on the skip connections, which I feel somehow limits the novelty. \n\nReproducibility is OK (There's a minimum experiment). ",
            "summary_of_the_review": "The paper provides a solid analysis of the role of skip connections in CNNs. The impact and the novelty, however, are limited. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6089/Reviewer_zQoE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6089/Reviewer_zQoE"
        ]
    },
    {
        "id": "Kw5lTHjjDN",
        "original": null,
        "number": 2,
        "cdate": 1666817992392,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666817992392,
        "tmdate": 1666817992392,
        "tddate": null,
        "forum": "6H_uOfcwiVh",
        "replyto": "6H_uOfcwiVh",
        "invitation": "ICLR.cc/2023/Conference/Paper6089/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies kernels (GP and NTK) arising from convolutional networks with skip connections, and provides a comparison with those without skip connections. Based on spectral decompositions on products of spheres, the authors make a few findings: (i) the spectral decays are the same as without skip connections, (ii) the skip connections promote a bias towards more localized functions, since the earlier layers play a more prominent role, (iii) when the depth grows, certain parameterizations should be preferred ($\\alpha=1$), and the condition number of kernel matrices may be better for residual networks. The results are accompanied by numerical illustrations.",
            "strength_and_weaknesses": "Understanding skip connections in CNNs is an important question in the foundations of deep learning, and the present paper provides a comprehensive picture of what role they may play in the context of kernels. This is thus a significant contribution, and I support acceptance.\n\nThere are nonetheless a few questions which could deserve more discussion:\n\n* Some of the bounds derived on eigenvalues are not tight, e.g., only lower bounds in Thm 5.2. While the experiments seem to confirm the findings, and suggest the bounds are relevant, is there a way to show upper bounds that also include the $c_k$ quantities?\n\n* In practice, residual networks often involve pooling/downsampling operations in intermediate layers as well, which seem important for good performance (this is also true in convolutional kernels, e.g. in [these](https://arxiv.org/abs/2003.02237) [papers](https://arxiv.org/abs/2102.10032)). Do you have a sense of how these would affect the locality bias in the present paper? Should I interpret your analysis as focusing on a fixed intermediate residual block at a fixed resolution?\n\n* Regarding condition numbers in Section 6.2, this seems to suggest that eigenvalues decay more slowly in the residual case, at least non-asymptotically. Do you have a sense of what could be causing this, and which eigenfunctions might be dominating the spectrum in the residual vs non-residual case? These questions seem important to discuss, since they would give insight on the kinds of target functions for which we can hope that \"smaller condition number\" => \"faster learning\" holds.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. Many of the proof techniques are similar to Geifman et al (2022), but all the findings about residual connections seem novel.",
            "summary_of_the_review": "Good paper, interesting and significant contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6089/Reviewer_TKdt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6089/Reviewer_TKdt"
        ]
    },
    {
        "id": "xY4O7ugd_w",
        "original": null,
        "number": 3,
        "cdate": 1666881109963,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666881109963,
        "tmdate": 1666881109963,
        "tddate": null,
        "forum": "6H_uOfcwiVh",
        "replyto": "6H_uOfcwiVh",
        "invitation": "ICLR.cc/2023/Conference/Paper6089/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper derives explicit formulas for kernels of ResNets' Gaussian Process and Neural Tangent kernels, and provide bounds on their implied condition numbers. \n\nThe main results include\n1) with ReLU activation, the eigenvalues of these residual kernels decay polynomially at a similar rate compared to the same kernel when skip connections are not used.\n2) residual kernels are more locally biased.\n3) the matrices obtained by the residual kernels have better condition numbers than the counterpart of without the skip connections, enabling therefore faster convergence of training.\n4) A theoretical justification for the result that over-parameterized ResNets act like a weighted ensemble of CNNs of various depths.",
            "strength_and_weaknesses": "Strength:\n- An asymptotic bound on the eigenvalues of the kernels of ResNets is given for the first time, which generalized the existing similar bound for that of convolutional networks.\n- A new lower bound for the eigenvalues of the kernels of ResNets is given, which implies that the receptive field of ResCNTK is smaller than that of CNTK (without skip connections), thus the ResNet is supposed to be more local biased.\n- The author justifies why ResNets converges faster than the normal CNN by utilizing the NTK technology.\n- The overall structure and logistics of the paper are good. The formulation and writing of the theorem statements and proofs are well structured and organized.\n- The results of experiments match the conclusions derived from theoretical analysis exactly.\n\nWeakness or Questions:\n- In Theorem 5.1, the author states that the bound matches the counterpart of the case without skip connections up to constants, and the experiments show that the eigenvalues of ResCGPK are quite close to that of CGPK for the same $k$. The given comparison is between CGPK and ResCGPK, I wonder whether there is any comparison between the eigenvalues of CNTK and ResCNTK ?  Also, this result is verified for some specific function, will the phenomenon that the ratio constants are close to 1 holds true for general functions?\n\n- Theorem 5.2 provides a lower bound of the eigenvalues of ResCNTK. But it seems that there is no strict argument for $\\lambda_k$ of ResCNTK being larger than that of CNTK because there is no upper bound for $\\lambda_k$ of CNTK. I wonder that which factors would affect the order relationship between them. The author only posted one figure of experiments to qualitatively demonstrate this fact instead of comprehensive quantitive comparison either in numerical or theoretical way.",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality and clarity of the paper is good. The author proposed several spectrum bounds for the ResCGPK and ResCNTK and made a comprehensive comparison between them and the kernels of plain CNN. The technical writing for the theorems and proofs is good and well structured to follow. \n\nThe paper devotes to justify some phenomenons regarding ResNet that are well known empirically but lack of theoretical explanations. This work makes effort to fill this gap using the NTK method, and the obtained some significant results based on spectrum analysis.",
            "summary_of_the_review": "The theorems proposed in this paper offer several new bounds for the spectrum of ResNets that is helpful for us to understand the practical behavior of ResNets. It is technically innovative to analyze the ResNet with NTK technology and derive some properties of ResNets like frequency-related behavior and locality bias.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6089/Reviewer_RWs1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6089/Reviewer_RWs1"
        ]
    },
    {
        "id": "_vRxoSv5Ztz",
        "original": null,
        "number": 4,
        "cdate": 1667502613950,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667502613950,
        "tmdate": 1670330594768,
        "tddate": null,
        "forum": "6H_uOfcwiVh",
        "replyto": "6H_uOfcwiVh",
        "invitation": "ICLR.cc/2023/Conference/Paper6089/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors provide explicit formulas or residual convolutional Gaussian Process Kernels (GPK) and Neural Tangent Kernels (NTKs), bounds on their eigenvalues, and their condition numbers.\nAs a highlight, they support an empirical observation by Veit et al. (2016) (i.e., that over-parameterized ResNets act like a weighted ensemble of CNNs of various depths). Furthermore, they claim to be the first to establish a relationship between skip connections and the condition number of the kernel matrix. Both derivations suggest possible advantages of residual architectures over vanilla convolutional layers without skip connections.",
            "strength_and_weaknesses": "Strengths:\n+ Explicit formulas or residual convolutional Gaussian Process Kernels (GPK) and Neural Tangent Kernels (NTKs) are provided.\n+ The main insight is provided by Thm. 5.2 and the comparison with the respective convolutional kernels. With their lower bound on the eigenvalues, the authors explain an empirical observation by Veit\net al. (2016): over-parameterized ResNets act like a weighted ensemble of CNNs of various depths.\n+ The experiments support the theory and provide some quantitative insights. \n+ The importance of the scaling parameter $\\alpha$ for large depth and some corrections to typical assumptions in the literature are discussed. (The authors make a case for $\\alpha=1$ for convolutional kernels.)\n+ Bounds on condition numbers: It is shown that the lower bound for ResCGPK matrices is lower than that of CGPK matrices.\n+ The authors claim that they are the first to establish a relationship between skip connections and the condition number of the kernel matrix.\n\nWeaknesses and open questions:\n- The authors only study the NTK regime and thus no feature learning. This limits the practical relevance of their insights. However, this is a common challenge with the analytic approach, which still can provide an intuition for some observed phenomena in practice. \n- What is the additional challenge of considering convolutional residual kernels in comparison with the derivation of fully-connected ones?\n- What is the exact additional challenge in the derivation of the bounds on the eigenvalues in comparison with convolutional kernels?\n-> It sounds like a minor technical contribution to simply combine derivations for fully-connected ResNet kernels and convolutional kernels.\n- What is the rough dependence of $c_1$ and $c_2$ on the depth in Thm. 5.1? This seems to be one of the most relevant questions. If this is not easily derived theoretically, also empirical results could shed light on that issue. (This question is only partially addressed by Thm. 5.2.)\n- How sharp is the bound in Thm. 5.2?\n- The experiments could be extended in support of Thm 5.2. An analysis of the dependence on the depth $L$ is missing.\n\nPoints of minor critique and open questions:\n- The need/advantage of considering three different heads could be better motivated on page 4.\n- How is the set $\\mathcal{R}$ defined in Theorem 5.1?\n- How is $C_0$ defined on page 6? Is it also just an existence statement? \n- How is the number of paths $p_i$ scaled in the infinite width limit? Is this quantity not diverging?\n- It could strengthen the last result on the condition number to discuss briefly why a slower diverging condition number could help in practice.",
            "clarity,_quality,_novelty_and_reproducibility": "The exposition is quite clear overall. In some minor cases, the reader could use some more or repeated definitions of stated variables.\nThe theoretical support of the empirically observed phenomenon that over-parameterized ResNets act like a weighted ensemble of CNNs of various depths seems to be novel and also does not seem to follow trivially from past derivations of residual fully-connected or vanilla convolutional kernels.\nHowever, the authors should make the technical challenges of their derivations more precise. Currently, it sounds like a minor technical contribution to combine ideas for convolutional and residual fully-connected kernels.",
            "summary_of_the_review": "I find the theoretical insights interesting, as they explain empirically observed phenomena. \nThey are of relevance for the ICLR community that tries to understand possible advantages of residual structures (in comparison with architectures without skip connections).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no ethical concerns.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6089/Reviewer_ciS9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6089/Reviewer_ciS9"
        ]
    }
]