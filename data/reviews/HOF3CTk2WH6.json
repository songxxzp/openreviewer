[
    {
        "id": "EkMk-bnG7M",
        "original": null,
        "number": 1,
        "cdate": 1666489189842,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666489189842,
        "tmdate": 1666489189842,
        "tddate": null,
        "forum": "HOF3CTk2WH6",
        "replyto": "HOF3CTk2WH6",
        "invitation": "ICLR.cc/2023/Conference/Paper1925/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes approaches to identify the optimal frame rate for audio classification tasks. Specifically, they propose a series of strategies to identify the optimal rate by inserting the frame-rate identification module between the traditional feature extraction step and the final classifier and casting the problem as a joint optimization problem. Experiments on several classification tasks show that the proposed approach improves over recent approaches. ",
            "strength_and_weaknesses": "Strengths\n1. A novel approach to identify the frame rate that is optimal for different audio classification tasks. \n2. Code is available online.\n3. Experimental details are parameters are clearly stated. \n4. Results on several tasks show that the proposed approach results in higher classification accuracy compared to other recent models.\n\nGrowth Opportunities\n1. The paper makes sweeping novelty claims and misses key references. e.g. Kekre, H. B. et al. \u201cSpeaker Identification using Spectrograms of Varying Frame Sizes.\u201d International Journal of Computer Applications 50 (2012): 27-33. and Huzaifah, Muhammad. \"Comparison of time-frequency representations for environmental sound classification using convolutional neural networks.\" arXiv preprint arXiv:1706.07156 (2017).\n2. Section 2.2.1 indicates a frame-weighting approach. This is similar to the sample weighting approaches common in ML tasks. Temporal frame warping in 2.2.2 is similar to the use of derived features. These similarities are worth pointing to the reader.\n4. The importance of guide loss is understated. It is only in Figure 8 that the large contribution becomes clear. Incorporating that in the abstract, introduction, and conclusion prominently will result in a more faithful representation of the novelty.\n5. Ablation study on other neural and event non-neural classification architecture will also be helpful to understand the generalizartion ability of the proposed feature-optimization approach.\n6. A lot of experimental tactics, e.g., section 3.1, seem like good engineering recipes. Probably this work is a better fit for the ICASSP or Interspeech community.\n7. Legend in table 3 is missing. The difference between the last 2 columns is unclear. \n8. Why is the computational cost of LEAF so much higher? An explanation will be helpful.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is both clear and hard to read. The details of the various steps are clear. However, there are so many engineering tactics that contribute to the overall result that it a challenge to retain focus on the main novelty.  The experimental details, including the listing of the various parameter, along with the availability of the code greatly contributes to reproducibility.",
            "summary_of_the_review": "The paper proposes an end-to-end framework to optimize the time resolution and classification accuracy in audio classification tasks. For ICLR, the generality of the approach is not quite established. There are a lot of good engineering tactics though that would be of interest to the audio classification community. I recommend instead audio/speech-based venues such as ICASSP, Interspeech, and MLSP.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1925/Reviewer_Uoii"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1925/Reviewer_Uoii"
        ]
    },
    {
        "id": "vHk41eaauM",
        "original": null,
        "number": 2,
        "cdate": 1666623904641,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623904641,
        "tmdate": 1666624108062,
        "tddate": null,
        "forum": "HOF3CTk2WH6",
        "replyto": "HOF3CTk2WH6",
        "invitation": "ICLR.cc/2023/Conference/Paper1925/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a drop-in differentiable module (DiffRes) that automatically adjusts temporal resolutions of spectrogram input for audio classification.  The DiffRes module computes frame-level importance score and calculate a warp matrix to dynamically scale temporal resolution. The guide loss encourages empty frames to have a low importance score. The DiffRes module was validated with various audio classification tasks including audio tagging, speech command recognition, and musical instrument classification. The experimental results show the effectiveness of the proposed module. In addition, it was compared to other learnable front-end models, showing superior results. The empirical analysis validates the efficiency in computational cost and the effectiveness of the resolution encoding and guide loss. \n\n  ",
            "strength_and_weaknesses": "Strengths\n- The propose module was well motivated in the introduction section with appropriate references\n- The technical description is delivered well with the illustrated figure of the computational procedure, visual examples, and detailed algorithm formulation in the appendix section. \n- The experiment is comprehensive and rigorous, covering various audio classification tasks and experimental settings  \n- The ablation study is very convincing and explains the effectiveness of the proposed ideas  \n- The visual animation in the demo link is very impressive  \n\nWeaknesses\n- There are some unclear part in writing (See below) \n- The performance increment is notable for environmental sounds, whereas it is marginal in speech command and music instrument. This may indicate that DiffRes is more effective when the audio examples has a lot of silent parts. This result could limit the use of DiffRes.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n- This paper is easy to follow. But, there are some parts which were not clear to me. \n- (page 5) The guide loss is not straightforward to understand. Why the negative sign was used in front? The epsilon as a hyper-parameter seems to be important as it determines the number of empty frames. I wonder how it affects the overall performance. \n- (page 5) The random spec-augmentation was applied directly on mel spectrogram before the diffRes is used. \n\nQuality \n- The motivation is clear and the related work is summarized well.\n- The technical description is very clear\n- The experimental results are convincing \n\nNovelty\n- This is the first work that learns temporal resolution on audio spectrogram\n- The mathematical formulation is neat \n\nReproducibility \n- The supplementary material includes the source code\n- The papers has training details.  \n- The algorithm is described clearly for implementation \n",
            "summary_of_the_review": "This paper is a great contribution to audio classification tasks and has a potential to be used for any high-dimensional time series data such as video as the authors suggested in the conclusion. \n\nThe proposed DiffRes module is neat and handy. It improves not only the classification accuracy but also reduces the computational cost.  \n\nMy only concern is whether DiffRes works well only when the audio signals have sufficient empty frames (e.g. environmental sounds) or it would be also effective when the audio signals have consistent energy (e.g. music tracks). I also wonder if DiffRes would be useful for automatic speech recognition. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1925/Reviewer_wxoQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1925/Reviewer_wxoQ"
        ]
    },
    {
        "id": "vHYEbfUTY_l",
        "original": null,
        "number": 3,
        "cdate": 1666627101347,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627101347,
        "tmdate": 1666636170790,
        "tddate": null,
        "forum": "HOF3CTk2WH6",
        "replyto": "HOF3CTk2WH6",
        "invitation": "ICLR.cc/2023/Conference/Paper1925/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a \"drop-in\" module and a loss function to compress the information codified in spectrograms for Audio classification tasks. The proposed module is based on standard 1D convolution layers, batch normalization, and residual connections. It can be used as an intermediate module between the input (Spectrogram) and a particular architecture for audio classification. As the authors highlighted, It could also be used with other speech representations either following a feature learning or a feature engineering fashion. The output of the module is a vector containing some frame importance indexes (s_i). Since the proposed module must act as a bottleneck to compress the original information in fewer frames, the proposed loss function promotes the scenarios where the \"mass of importance\" is distributed in fewer s_i.\n\n",
            "strength_and_weaknesses": "The paper is interesting and addresses a relevant problem since the information contained in spectrograms is mostly sparse, so data compression can lead to a reduction in the computational load of audio classification systems, such as the authors showed. The authors performed a large enough battery of experiments to show the effects of their proposal on different audio classification tasks and regarding also different hyper-parameters involved in the process.\n\nThe main drawback of the proposed approach is the introduction of several free parameters (hyper-parameters) that make the proper training of the module a problematic task. In this sense, the proposed method has a significant limitation regarding the definition of the dimension reduction rate. Such a parameter is similar to any feature reduction rate that must be adjusted following a validation methodology, which goes against the authors' claim that the proposed approach is an end-to-end approach. Following this idea, I also find the title of the paper problematic; the authors claim that the proposed method \"learns the temporal resolution,\" but it is not changing the temporal resolution of the spectrogram; it is applying a compression algorithm to identify the frames with less or not none information at all, in order to reduce the dimensions of the audio representation processed for further modules. The experiments and results presented in section 3 show that temporal resolution must be changed manually, so there is no any gain in this respect by using the proposed module. \n\nOn the other hand, the proposed loss function to adjust the s coefficients is quite arbitrary as it is the selection of its parameters. The authors should elaborate more about why it is a good choice for the frame compression task. They should also present results and discussion about the sensitivity of the model to delta, epsilon, and lambda parameters. ",
            "clarity,_quality,_novelty_and_reproducibility": "I find the paper clear and self-contained. The proposed approach is reproducible, and the authors shared the code in an open repository. The novelty of the work is limited, and the proposed approach has substantial limitations. Some of the authors' claims do not agree with the proposed approach, especially concerning temporal resolution learning. The data compression module for spectrogram representations provided by the proposed approach is an interesting idea, but its dependency on several free parameters casts doubts about its generalizability. ",
            "summary_of_the_review": "The paper proposes a module for spectrogram compression in audio classification tasks. Even though the authors assert that the module learns the temporal resolution, it is not clear how the proposed approach can do that. The proposed formulation lets us see that the module compresses the information of a given Spectrogram, but the temporal resolution is defined during the Spectrogram estimation, so the proposed module cannot do anything about it. Either way, the idea of applying a compression module to Spectrograms is interesting since the information contained in spectrograms is mostly sparse so data compression can lead to a reduction in the computational load of audio classification systems. The estimation of a vector of importance indices is not new since it is the base of the self-attention mechanism. Still, the way such a vector is used to build the matrix W and the corresponding bottleneck representation O is interesting as it follows a less data-driven (more heuristic) but coherent approach. A significant limitation of the proposed approach is the inclusion of many hyper-parameters that could significantly affect the performance of the proposed module.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1925/Reviewer_qgYk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1925/Reviewer_qgYk"
        ]
    }
]