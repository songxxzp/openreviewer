[
    {
        "id": "jlYKT_88yFk",
        "original": null,
        "number": 1,
        "cdate": 1666597890168,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597890168,
        "tmdate": 1666597890168,
        "tddate": null,
        "forum": "gx2yJS-ENqI",
        "replyto": "gx2yJS-ENqI",
        "invitation": "ICLR.cc/2023/Conference/Paper787/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the novel view synthesis of street view and contributes a new NeRF design (named S-NeRF). Since the conventional nerf solutions could easily fail in street view scenes, the authors use a new scene parameterization function and camera poses for learning better neural representations from street views. The experiments demonstrate the proposed method outperforms state-of-the-art works on general self-driving datasets.  ",
            "strength_and_weaknesses": "+ Strengths\uff1a \n\u25cf The paper is well-written and easy to follow. \n\u25cf The depth supervision design is novel and technically sound. The ablation study verifies its contribution as well. More importantly, such a strategy allows us to use low-cost spare Lidar data instead of the dense one in urban-nerf. \n\u25cf The experiments show the proposed method outperforms SOTA works in both foreground vehicles and background scenes NVS. \n\n- Weaknesses\uff1a \n\u25cf The virtual digital camera transformation process appreciably depends on the quality of 3D detection results. This possible threat should lead to an inconsistent coordinate system transformation.  \n\u25cf Ablation study:  Based on the ablation study in tab 3 and fig 6, the depth confidence surely helps the proposed model. However, the mechanism behind L_depth is still blurry. It is valuable and theoretically interesting to conduct a more detailed ablation study on confidences of rgb, ssim, vgg, depth and flow and show what technology is critical to this work.\n\u25cf Discussion about deeper principles: It is not clear why the proposed method gives significantly better results than other methods for background composition using only RGB images. We also need more analysis on the limitations of the proposed method. It would be good if the authors provide some failure case  evaluation to better understand the limitation of the proposed method.\n\u25cf Make the streets more real: Dynamic objects such as pedestrians, bicycles, and trucks are very common on the streets. Although it is too demanding to reconstruct dynamic objects, it is interesting to study the effect of these dynamic objects on the reconstruction of the target.",
            "clarity,_quality,_novelty_and_reproducibility": "Building on existing approaches, the framework proposed in this paper addresses a research question with an efficient approach. This paper draws on so many existing techniques such as all kinds of NeRF, 3-D object detection and common depth estimation, that it's more like a high-level combination. So I think the innovative newness of this paper still needs to be strengthened and can be followed up with more in-depth work.",
            "summary_of_the_review": "Overall, this paper presents a novel Nerf framework for self-driving applications. But I still think the author could have done more in-depth research and tweaking. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper787/Reviewer_TJeu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper787/Reviewer_TJeu"
        ]
    },
    {
        "id": "bLaNdgBYVW",
        "original": null,
        "number": 2,
        "cdate": 1666659012626,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659012626,
        "tmdate": 1666659012626,
        "tddate": null,
        "forum": "gx2yJS-ENqI",
        "replyto": "gx2yJS-ENqI",
        "invitation": "ICLR.cc/2023/Conference/Paper787/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the problem of neural radiance fields for street views. Considering the low overlap among street view images and the unbounded nature of street scenes, this paper improves the scene coordinate parameterization of existing work and proposes a new method for using sparse and noisy lidar depth as supervision. Experimental results show the performance of the improved method is significantly superior to previous methods. ",
            "strength_and_weaknesses": "Strength:\n\n+ Although the idea of using sparse and noisy Lidar depth as supervision is not new in the community, the proposed method of using confidence scores and the learnable weights to balance the different items to generate confidence scores is interesting and smart. \n\n+ Experimental results seem to be very promising. \n\nWeakness:\n\nThe overall contribution of this paper is a little insignificant. For example, \n(1) one of the claimed contributions, \" improve the scene parameterization\", is just a small modification on the scene coordinates representation strategy, shown in Eq. (2); \n(2) The \"improved camera poses\" illustrated in Sec 3.3 are just some engineering processing tricks. \n\nOne of the unique challenges in the street-view neural radiance field is that it contains static background and dynamic objects (vehicles, pedestrians). I was expecting a novel neural radiance field method that could distinguish the two kinds of scenes automatically. However, this paper adopts an existing method to first detect the dynamic object (vehicle) and estimates its pose, and then apply a NeRF-type method to generate the neural radiance field. This is less attractive. \n\nOther comments:\n(1) The paper below also addresses the neural radiance field of unbounded scenes. It may be worth a discussion in the related work. \n\nLi et al. Neural plenoptic sampling: capturing light field from imaginary eyes. 2021\n\n(2) The second row in Sec 4.5, \"out\"--> \"our\"?",
            "clarity,_quality,_novelty_and_reproducibility": "The wring of this paper is good and easy to follow. This paper clearly states which parts are from other papers and which parts are proposed by themselves. The originality of the work is good. ",
            "summary_of_the_review": "Although I have concerns about the insignificance of some of the contributions, the idea of how to use sparse and noisy lidar points as supervision is interesting and should benefit related tasks where sparse and noisy lidar points are available. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper787/Reviewer_SFqp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper787/Reviewer_SFqp"
        ]
    },
    {
        "id": "Vjg9LqmYRT",
        "original": null,
        "number": 3,
        "cdate": 1667116446201,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667116446201,
        "tmdate": 1667116446201,
        "tddate": null,
        "forum": "gx2yJS-ENqI",
        "replyto": "gx2yJS-ENqI",
        "invitation": "ICLR.cc/2023/Conference/Paper787/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method for novel view rendering using deep learning based on NERF model where the background and the foreground are synthesized jointly. The NERF model is given extra supervision via LIDAR depths and different confidence maps based on LIDAR depth. The confidence maps help in incorporating standard street view datasets which come with noisy LIDAR depths. A number of existing work is used as building blocks in the paper.",
            "strength_and_weaknesses": "Strength:\n* The results are good and show that their method works well.\n\nWeakness:\n* Sec 3.3: One can use the 3D....the ego car. How is the relative calibration computed. Some more details would be good for clarity.\n* Eq.4 is not correct. The correct equation should be (Pi * Pb^-1)^-1 = Pb * Pi^-1\n* Sec 3.4 requires some figures and images to clearly explain the different confidence measures and how they are computed.\n* In \"LiDAR depth completion\", when the LIDAR depths are accumulated, they are from different time instants. So, how are they combined and brought to the same coordinate system. For static scenes, the extrinsics between the car can be used to fuse the depths, but for moving objects in the scene like cars this will not work and they will always end up as outlier depths. This whole paragraph is not clearly explained as it appears to be one of the contributions of this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, although the flow of the paper could be improved. The different blocks of their system pipeline are explained but it is challenging to relate the different blocks. This makes reading the paper a bit difficult requiring going back and forth to understand the blocks. Also, the number of figures is very limited to understand the various equations. The authors should have put more effort in putting more figures to geometrically explain their error functions. The paper also uses a number of existing deep learning work e.g. densifying sparse point cloud, camera pose improvement to name a few as their building blocks. Their is no clarity of explanation on what happens when these blocks do not perform well. From the results, it appears that the results are better than existing methods for street view scenes. This may imply that when the system works, it works very good but would have helped to have some analysis of cases where it failed and why. So, reproducibility may be a challenging factor with this paper. ",
            "summary_of_the_review": "In summary, the paper has some novel contributions in the way they incorporate LIDAR data and confidence measures which enable use of existing standard datasets for training. The writing could have been improved as the flow of the paper is not smooth. More figures should have been put for better explanation of the different blocks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper787/Reviewer_k6S3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper787/Reviewer_k6S3"
        ]
    },
    {
        "id": "wbZt93GwCF",
        "original": null,
        "number": 4,
        "cdate": 1667158562238,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667158562238,
        "tmdate": 1672214066819,
        "tddate": null,
        "forum": "gx2yJS-ENqI",
        "replyto": "gx2yJS-ENqI",
        "invitation": "ICLR.cc/2023/Conference/Paper787/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper (S-NeRF) aims to train a NeRF using images from driving datasets (e.g., nuScenes and Waymo) with a limited number of views (2-6 images). The improvement over Urban-NeRF is that it densifies sparse LiDAR depth for depth supervision, through a reprojection-based confidence map approach. The main claimed advantage is robustness to noise in the depth point cloud. The experiments show sharper, more recognizable novel view synthesis output, over Urban-NeRF and Mip-NeRF.",
            "strength_and_weaknesses": "Strengths\n\n- Achieving robustness to noisy depth in this setting is a practical goal.\n- Using depth completion priors and multi-view reprojection is a reasonable approach.\n\nWeaknesses\n\n- I suspect the improvement is due to the separate evaluation of static background and moving foreground objects (how was this done for the baselines?), on top of the heavy usage of the pre-trained NLSPN depth completion network.",
            "clarity,_quality,_novelty_and_reproducibility": "(minor concerns) My impression is that the paper is hard to understand and not polished enough.\n\n- Writing quality seems to drop noticeably starting around section 3.2.\n- No system figure, no illustration of the contribution.\n- What are we supposed to learn from this paper, what questions does it answer?\n\nIn particular, I had a hard time understanding the \"virtual camera transformation\" part. It seemed related to the separation of the moving objects, but it was not clear to me otherwise.",
            "summary_of_the_review": "The paper lacks technical novelty, despite the increased complexity (see weaknesses). If I understand correctly, the main contribution of the paper is showing that novel view synthesis of static and dynamic objects can benefit from depth densification and multi-camera reprojection. In my view, the impact of this finding alone is below threshold for publication at ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper787/Reviewer_u7q4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper787/Reviewer_u7q4"
        ]
    }
]