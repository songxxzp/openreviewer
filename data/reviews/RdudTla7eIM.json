[
    {
        "id": "eELMvL5cQDU",
        "original": null,
        "number": 1,
        "cdate": 1665874299824,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665874299824,
        "tmdate": 1665874299824,
        "tddate": null,
        "forum": "RdudTla7eIM",
        "replyto": "RdudTla7eIM",
        "invitation": "ICLR.cc/2023/Conference/Paper3564/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper examines behaviors of large language models in causal and moral judgment tasks drawn from cognitive science literature studying human judgments in these domains. They first test on simple cloze tasks using stories and prompts that human participants responded to, and identify model preferences for continuations of \"Yes\" or \"No\". They find low (near-chance) levels of alignment with human judgments. However, when they simplify to giving the LMs abstract descriptions capturing high-level properties of the stories, alignment improves a bit. They then do a factor inference task, where the models are supposed to complete a prompt with a factor label from the cognitive science literature. Lastly they combine the latter two experiments, getting tags from GPT-3 and using these to along with an abstract prompt. This last experiment shows a small amount of improvement in the largest GPT-3 model. ",
            "strength_and_weaknesses": "Strengths: I'm happy to see engagement with cognitive science literature, and this paper deals with the important topic of models' ability to do humanlike causal and moral reasoning. The experiments are interesting creative. \n\nWeaknesses: \n\nThere are issues with methodological clarity, and I'm skeptical about interpretations of a number of the results. \n\nFor R2, it seems likely that simplifying to the abstract stories creates simpler lexical cues that the models are able to latch onto to show the bump in performance that is observed. There may also be biases in terms of frequency of mentions, e.g. \"A\" is mentioned more times in Fig 2a than \"B\" is. Additionally, it seems worth considering the possibility that humans actually wouldn't show the same patterns with these abstract versions of the stories as with the original stories, since these seem like potentially less intuitive inputs to reason about.\n\nFor R3, the authors initially describe the tests as zero-shot, but based on the subsequent description and Appendix examples, it seems what they are doing is few-shot. So this is an issue with methodological clarity, and it makes me concerned about my interpretation of other experiments as well -- as far as I know, the other experiments are truly zero-shot, but I'm not sure if this is actually correct. If that interpretation is correct, however, then the next concern: why is R3 the only experiment where the model is given examples in the prompt? My immediate concern is that the models could identify simple cues in the prompt examples that predict the factor tag (e.g. \"both\" --> conjunctive). The authors acknowledge something like this concern, and argue that because their prompt examples are higher-level than the final item the model needs to respond to, this concern can be dismissed. However, this really does not exclude the possibility that there are overlapping cues between the prompt examples and the real examples. Additionally, given that R3 is (I think) the only few-shot experiment, and is also the only experiment in which the models show notably higher performance than any of the other experiments, this raises further suspicion that the strong performance is attributable to simpler solutions based on the few-shot examples. \n\nAs for R4, I'm having trouble understanding what we stand to gain/learn from this experiment, and how we should interpret improvements relative to R1/R2, since it seems that the R4 is simply adding less accurate factor labels to what is otherwise the R2 setup. I'm (again) not completely clear on the method here, but as far as I can tell R4 is just changing the list of labels prepended to the abstract story from R2, and making them the GPT-3 predicted labels, which are decent but worse. What do we stand to learn from this? Why is it useful to check how the models perform if the labels become the GPT-3 labels? The results table only compares to R1, so I'm having to scroll back and forth to try to check the (seemingly more direct) comparison with R2 -- it looks like it's often worse, but sometimes on par/better.  So is the conclusion that if we do a noisier version of R2, performance is often worse but sometimes better? Why would that be? Something else that is unclear: the description of the \"Expert Reasoning Engine\" comes out of the blue and is not enough for me to be clear what was done (and is also confusing because it uses the phrase \"one can imagine\" multiple times, suggesting this is merely a hypothetical, but then it turns out to be in the reported results). Did the authors train a separate classifier mapping between GPT-3's noisy labels and the \"Yes/No\" labels? Is this no longer a cloze task but a classification? There really needs to be a lot more methodological clarity here. \n\nFinally, zooming out a bit more: the authors are claiming that cognitive science-based scaffolding is helpful for the models' performance, but I'm not sure in what way this is shown. It seems like it's mostly the labeling of factors that comes from the cognitive science literature, but it's not clear that listing those factors along with the story (which I think, but am not sure, is what's being done) is actually making much of a contribution to models being able to make humanlike judgments on the stories. The other candidate for \"cognitive scaffolding\" is the translation of the stories into abstract versions -- this does seem to help the models, but it's not clear that this is a contribution from cognitive science, and I also had concerns about lexical shortcuts as outlined above.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I've outlined issues with clarity above, which I think are non-trivial, and also affect the reproducibility. I think the quality of the paper can be improved by addressing the various concerns I've raised about interpretation of the results as well as validity of the claims about contribution of insights from cognitive science. ",
            "summary_of_the_review": "I am happy to see this engagement with cognitive science, and I think this is an interesting and useful topic -- but I have numerous non-trivial concerns about methodological clarity and validity of conclusions, and I'm skeptical that the paper has given a strong demonstration of usefulness of insights from cognitive science, as it claims. I think these concerns should be addressed before publication, but I am optimistic that the paper can be much improved and make an interesting contribution.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3564/Reviewer_XfnA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3564/Reviewer_XfnA"
        ]
    },
    {
        "id": "aQNToUp_Sq",
        "original": null,
        "number": 2,
        "cdate": 1666629698967,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629698967,
        "tmdate": 1666635402515,
        "tddate": null,
        "forum": "RdudTla7eIM",
        "replyto": "RdudTla7eIM",
        "invitation": "ICLR.cc/2023/Conference/Paper3564/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies how well-correlated human judgments and LLM predictions are for causal and moral reasoning tasks. They collect a dataset of human judgments for a number of text problems used in cognitive science literature for probing human judgments, including expert judgments for relevant factors in the judgment process (e.g., norms, inevitability). They perform a few experiments that find that (1) correlation between human and LLM predictions is low out-of-the-box, but that (2) explicitly providing factors only, or combining the two methods, can yield better performance.",
            "strength_and_weaknesses": "Strengths:\n- The description of the task and crowdsourcing (especially expert judgment) annotation is quite thorough.\n- Experiments are easy to understand.\n- Evaluation of the LLM by disentangling text complexity with reasoning is interesting (Section 4.2).\n\nWeaknesses:\n- There are many missing details about the crowdworkers involved in the judgment tasks. Who was recruited to participate? Where were they located? \n- There is also missing detail about the dataset itself. Perhaps the most interesting part of this dataset might be the actual distributions (between yes/no) of user annotations on the stories, but this is not provided in the paper or materials as far as I can tell. Instead, it appears that stories are bucketed into \"yes\", \"no\", and \"ambiguous\" buckets (when agreement is too low). \n- There is missing discussion on the motivations and framing of these tasks at a meta-level. \"when something bad happened, we naturally ask: who did what, and why?\", \"people's moral knowledge helps them tell apart good from bad\" -- these claims are from a particular philosophical perspective, e.g. where \"good\" and \"bad\" are standalone values that can be placed on an action or event in absence of enough context.\n- Similarly, framing the task of causality as placing blame on an agent rather than on the structural context is a specific choice. I understand this study is coming from the perspective that cogsci has created on causality, but this framing shapes event understanding in a particular way (i.e., that structural context, e.g., that Billy/Suzy's computer has this particular flaw, is not the focus of blame). I also understand that the goal is to study how predictions correlate with human judgments, but it also seems that the questions asked of annotators (\"Did Billy cause the emails to be deleted?\"), rather than asking open-ended questions, may result in a bias towards agents becoming the sole focus of blame in LLMs (if we want to assume that LLMs will or should adopt the moral judgments of humans as studied in tasks like these).\n- The metrics themselves are not well-motivated. Accuracy with discretized categories of judgments (yes, no, ambiguous) seems like an arbitrary choice. Did the authors consider directly evaluating the distribution similarity between the LLM's prediction and the distributions among 25 human raters?\n- It appears there isn't human evaluation for the generated T3 stories, which would make the comparison more accurate in Table 3.\n- I don't think the claim \"we ... only test if LLMs can have any implicit or innate ability to combine these factors and produce the same judgment a human would\" is accurate. It is still possible that the authors' choice in generating the T3 versions of stories influences the model's predictions (e.g., with a spurious bias towards \"yes\" across the stories) simply in how they were worded, so although the details of stories are removed, the surface form itself is still not being controlled for.\n- I'd be interested in seeing \"error analysis\" of the predictions in Table 4, as well as precision/recall of factor prediction rather than plain accuracy.\n- There is an implication in Section 5 that \"aligning\" our LLMs to perform well in these causal/moral judgment tasks will result in better text generation in general (please correct if I am wrong about this implication), yet there isn't really substantial evidence of these LLMs (1) being \"misaligned\" with human judgments in general, free-form text generation, or that (2) \"aligning\" them with the proposed method will improve \"alignment\" in free-form text generation.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- There are a number of typos in the dataset, e.g., \"colleccted\", \"theoriezed\", \"severval\", \"inter-rator\"; also in the \"Dataset\" paragraph of 3.1, there seems to be a partial sentence cut off.\n- Citations for COPA and SuperGLUE appear to be transposed\n\nQuality:\n- The experiments are thorough.\n- The dataset itself could be valuable, especially for cogsci researchers, but there should be more details provided about its collection and the distribution of annotations in each story. The framing of questions and stories could also affect the moral judgments in a way that results in biases in human decision-making.\n\nNovelty:\n- The dataset itself appears to be new, and relatively thorough (reaching back to decades of cogsci research to acquire more high-quality scenarios).",
            "summary_of_the_review": "This paper is thorough, and the paper is clearly written. However, I am a bit wary of underlying assumptions made in the design of the paper, which makes me lean towards rejection:\n\n- That moral and causal judgments can be bucketed into \"yes\", \"no\", and \"ambiguous\".\n- That reasoning about causality is equivalent to answering the question of which agent is to blame under a particular structural context, rather than placing blame on the structural context of the event itself.\n- That we should prioritize \"aligning\" LLMs with some measure (in this paper, majority vote) of human judgments in terms of morality and causality as it's studied from the perspective of cogsci researchers, even though human judgments, reasoning, and perspectives about morality and causality vary significantly across personal experience, cultures, etc. In particular, this paper is focusing on a set of crowdworkers to provide judgments (while providing no details about how those workers were selected).",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns",
                "Yes, Potentially harmful insights, methodologies and applications",
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "Please see above (summary of the review) for a description of several assumptions that I feel this paper is making about judgments of causality and morality. \n\nI worry that any systems which we try to \"align\" to normative human judgments (including judgments about causality and morality) have no reason to behave ethically, fairly, etc. (partially because we know, e.g., from large datasets of text and images, that \"normative\" data replicates biases). ",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3564/Reviewer_ikDj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3564/Reviewer_ikDj"
        ]
    },
    {
        "id": "o3MCY8SdTq",
        "original": null,
        "number": 3,
        "cdate": 1666991288973,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666991288973,
        "tmdate": 1670731093652,
        "tddate": null,
        "forum": "RdudTla7eIM",
        "replyto": "RdudTla7eIM",
        "invitation": "ICLR.cc/2023/Conference/Paper3564/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates whether large language models can make causal judgments and reason about moral permissibility in text scenarios. Two new datasets are collected by aggregating stories from 24 cognitive science papers and standardizing the human annotations from the datasets accompanying these papers into ML-ready datasets (additional expert annotation is performed as well). The authors find that LLMs perform poorly on both datasets. To improve performance, they propose three methods: cognitive scaffolding (predicting useful intermediate attributes), thought-as-text translation (abstracting away irrelevant details), and expert reasoning engine (predicting label directly from the intermediate attributes with a shallow network). In experiments, these additions greatly improve performance.",
            "strength_and_weaknesses": "Strengths:\n- The problem area is important.\n- The proposed methods are intuitively reasonable, and they seem to work well.\n\nWeaknesses:\n- This paper has a lot to take in. I found myself needing to jump back and forth between R1, R3, etc.\n\nQuestions:\n- Could the proposed methods for improving performance on these tasks also be used to improve performance on existing datasets, e.g., ETHICS? This would greatly strengthen the paper.\n\n\nMinor points:\n- Typo: \"With the crowd sourced votes, we can make our Since each of the story is designed to verify a scientific\"\n- Typo: \"The inter-rator agreement\"",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper could be improved. This is the main weakness of the paper right now. It's just a lot to take in, and I suspect readers will have trouble skimming it. There is plenty of novelty, and there are enough details for reproducibility.",
            "summary_of_the_review": "I think this paper could generate good discussion, and the new datasets seem like useful tests for LLMs. I vote to accept.\n\n-----------------------\nUpdate after reading rebuttal and other reviews:\n\nThe other reviewers raise some interesting points. In particular, Reviewer XfnA's point about simplifying stories and lexical cues may need to be further addressed. (Note: I'm not convinced that the paper has serious ethical concerns.) Overall, I'm still convinced that this paper is valuable. This is mainly because the only real avenue people have considered until now for increasing performance on ethics / morality benchmarks is using bigger LLMs. While this does work, it's very valuable to explore methods that differentially improve performance on these tasks using domain knowledge. This work accomplishes precisely that, so I think it provides value to the community. Consequently, I'm keeping the same recommendation of acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3564/Reviewer_FUkn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3564/Reviewer_FUkn"
        ]
    }
]