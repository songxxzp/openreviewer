[
    {
        "id": "kH8-hUMGlfJ",
        "original": null,
        "number": 1,
        "cdate": 1666569300601,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666569300601,
        "tmdate": 1666569300601,
        "tddate": null,
        "forum": "hdghx6wbGuD",
        "replyto": "hdghx6wbGuD",
        "invitation": "ICLR.cc/2023/Conference/Paper363/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submission proposes a potential improvement over the popular and effective outlier exposure (OE) method of Hendrycks et al. (2019). While existing works have explored improving the diversity of surrogate outliers in the input space (by synthesizing artificial examples/applying MixUp/resampling) and in the feature space (by perturbing feature representations of surrogate outliers), the proposed method involves perturbing the weights of the neural network instead to generate features corresponding to a new synthesized data distribution of surrogate outliers. The perturbation at each iteration of training is computed by gradient ascent to maximize a gradient-norm penalty, inspired from Arjovsky et al. (2019). \n\nExperiments suggest that the proposed method outperforms other OE-based approaches.",
            "strength_and_weaknesses": "The key idea in the submission, that of using parameter-perturbation to simulate data-perturbation in the context of outlier-synthesis, is novel (to my knowledge). The results indicate effectiveness and potential superiority over alternatives. The method seems well-designed and implemented overall.\n\nThere are a few things that aren\u2019t clear to me, and I hope the authors can clarify these:\n\n  1. From Algorithm 1 and the description in Section 5, it appears that the method is strictly applied in a fine-tuning sense, i.e. once a network is trained with the classification loss, further fine-tuning training is performed with the DOE loss. It is unclear to me if the OE, MixOE, VOS baselines are also trained similarly. In the original papers, they do not appear to require a fine-tuning setup (except when pre-training with ImageNet-classification is performed, which does not seem the case for CIFAR in this submission). \n\n  2. It is not obvious to me how the regret formulation connects precisely to Eq. 7. My understanding is that the gradient-norm approximation is used whenever the regret-formulation is referred to in experiments. Without a clear relation between WOR and the gradient-norm variant, Table 4 is hard to interpret: if DOE-regret doesn\u2019t actually clearly correspond to the gradient-norm approximation, the comparison between gradient-norm and DOE-risk might suggest something else.\n\n  3. In Appendix A.1, in Eq. 13, it looks like the inequality det(A+B) >= det(A) + det(B) is being used. This is generally not true, and only holds with conditions upon A and B. The statement of Minkowski\u2019s inequality suggests that this is true when A and B are both positive semi-definite. (Potential typos: (a) After Eq. 14, it says v, which should probably be z. (b) In A.3, some of the (L+1)s should probably be L.)\n\nOther comments:\n\n  \u2014 Figure 1 contrasts DRO and DOE as performing interpolation and interpolation+extrapolation. REx (Krueger et al., 2021) shows how DRO might be modified to change constraints for coefficients lying in [0, 1] to lying anywhere, but summing to 1, which enable an extrapolation-variant for DRO.\n\n  \u2014 The writing has plenty of grammatical issues, which can be fixed by getting the draft proof-read, or using a grammar-fixing software (there are free alternatives available).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Generally clear, with some questions remaining (see above).\n\nQuality: Generally high-quality, but with a lot of grammatical issues (which I believe are fixable with any grammar-correcting software, or a proof-reader).\n\nNovelty: To my knowledge, using parameter-perturbation to synthesize surrogate outliers is a novel idea, and the overall algorithm also has smaller elements of novelty in it.\n\nReproducibility: There appears to be sufficient details to reproduce the main algorithm.",
            "summary_of_the_review": "While the theoretical discussion involves significant departure from practical implementation (many assumptions that are going to be un-met in practice), and there is some lack of clarity about how fairly the method is empirically compared to baselines, the general idea is intuitive, well-executed, and appears to work well enough that my overall recommendation at this time is to Accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper363/Reviewer_r8Mp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper363/Reviewer_r8Mp"
        ]
    },
    {
        "id": "9Pq95VZpIpv",
        "original": null,
        "number": 2,
        "cdate": 1666667333661,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667333661,
        "tmdate": 1666667333661,
        "tddate": null,
        "forum": "hdghx6wbGuD",
        "replyto": "hdghx6wbGuD",
        "invitation": "ICLR.cc/2023/Conference/Paper363/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper introduces an outlier exposure(OE)-enhanced method for OOD detection, which searches for model perturbation options to transform a set of given outlier data to its transformed version. It shows through experiments on commonly used benchmarks that this approach is better than OE and a number of other similar approaches.",
            "strength_and_weaknesses": "The strengths of the work include:\n- The key idea of implicit outlier transformation is interesting. Synthesizing model perturbation and the outlier exposure to create a set of outliers of different distributions is new. It is similar to a combination of deep ensemble and OE, but it has an explicit searching for model parameters more suitable for the OOD detection task.\n- Theoretical results are given to support that the resulting transformed outliers has a different distribution from its original distribution.\n- The method is extensively evaluated against both fine-tuning and post-hoc approaches on commonly used benchmarks.\n- The paper is well written and easy to follow.\n\nHowever, the paper also has a number of limitations:\n- The proposed method significantly complicate the OE method. The effective performance of the method relies heavily on a large set of parameter searching and training (pre-training) tricks. This largely limits its real-life applications.\n- Compared to OE, the proposed method uses a different OOD scoring function, MaxLogit vs MSP in OE. As mentioned in the paper, MaxLogit is much better than MSP, so it is not clear the actual improvement of the proposed implicit OE method gained over the OE method. The paper shows a small set of results of comparing OE and the proposed method using the same scoring function MaxLogit and MSP, but the experiments are limited to only a few experiment cases there. It would be more straightforward by directly using MaxLogit in the OE baseline on all the datasets. This concern substantially weakens my confidence on the proposed method.\n- The presented theoretical results have some points there, but they are not that relevant in the sense that although the method can produce a different distribution from the original distribution, it is not clear whether the resulting distribution is closer to the true distribution of OOD data than the original distribution. in other words, both of the original distribution and the transformed distribution play a similar role wrt ID data; they are both surrogate OOD data only: OOD distribution vs. its shifted version.\n- The statement \"It means that for target data of our interest, we can implicitly get them access via finding the corresponding model perturbation. Then, by updating the detection model thereafter, it can learn from the transformed data (i.e., the target ones) instead of original inputs.\" is an unjustified over-claim. As discussed in the above point, the transformed data is still a set of outliers, some shifted version of the original outliers, which are not the target OOD data in the test data. \n- I wonder whether this would affect the classification accuracy. The empirical results show that the method does not. It would be good to elaborate the reasons in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. The originality is good. The reproducibility seems to be negative due to the huge amount of parameter searching efforts and training (pre-training) tricks required.",
            "summary_of_the_review": "The overall idea is interesting, but its theoretical and empirical support have some major issues. Some key statements are unjustified. Overall, the paper may benefit from another round of major revision.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper363/Reviewer_tVEd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper363/Reviewer_tVEd"
        ]
    },
    {
        "id": "c5qi15_hIuI",
        "original": null,
        "number": 3,
        "cdate": 1666691600159,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691600159,
        "tmdate": 1666691600159,
        "tddate": null,
        "forum": "hdghx6wbGuD",
        "replyto": "hdghx6wbGuD",
        "invitation": "ICLR.cc/2023/Conference/Paper363/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a solution to the OOD detection problem. Starting from approaches that train the model on auxiliary data, the authors suggest that the distribution of auxiliary data used for training might differ from that to be encountered in practice limiting the detection capabilities. As such the authors proposed to diverse the ood data using implicit transformation via perturbing the model parameters. \nBased on ReLU networks, the method uses the perturbation with to the worse regret, gradient approximation is used to limit the search space. Theoretical aspects are well supported. The method shows significant improvements over existing works on several OOD benchmarks.\n ",
            "strength_and_weaknesses": "Strengths:\n1) A solution tackling the distribution gap between used auxiliary ood data and that encountered in practice. \n2)The method is supported with theoretical evidence.  \n3) Good results. \nWeakness:\n1) Some closely related works are missing e.g., [1], [2] & [3].\n2) Since the proposed perturbation are parameters based, it is not well discussed to which layers are the perturbations applied or the role of the used architecture. Not sure why Wide ResNet is used for Cifar experiments while ResNet 50 for ImageNet.\n\n\n\n\n[1] Chen, Jiefeng, et al. \"Atom: Robustifying out-of-distribution detection using outlier mining.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2021.\n[2] Ming, Yifei, Ying Fan, and Yixuan Li. \"Poem: Out-of-distribution detection with posterior sampling.\" International Conference on Machine Learning. PMLR, 2022.\n[3] Bitterwolf, Julian, et al. \"Breaking Down Out-of-Distribution Detection: Many Methods Based on OOD Training Data Estimate a Combination of the Same Core Quantities.\" International Conference on Machine Learning. PMLR, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. However, with no code available it is untrivial to implement the proposed methods. \nHyper-parameters are not discussed on how they were chosen.",
            "summary_of_the_review": "The paper proposed an important solution to the OOD detection tackling the distribution gap between auxiliary ood data and real one. \nThe method is supported with theoretical and empirical evidence. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper363/Reviewer_UUPv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper363/Reviewer_UUPv"
        ]
    }
]