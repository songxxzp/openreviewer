[
    {
        "id": "itkSSaeHDy",
        "original": null,
        "number": 1,
        "cdate": 1666665776166,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665776166,
        "tmdate": 1666665776166,
        "tddate": null,
        "forum": "erHaiO9gz3m",
        "replyto": "erHaiO9gz3m",
        "invitation": "ICLR.cc/2023/Conference/Paper5322/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies behavior of NTK on fine-tuning MLM models. Supposedly, pre-trained model is no longer iid random in parameters but still it can achieve good performance on CV tasks per previous literature. In this paper, authors extended the studies to NLP models with both regular fine-tuning and prompt-based learning cases. Authors also provide a modified eNTK version when using ADAM but not SGD. Last, authors tried to propose some hypothesis on why such model on eNTK behaves so.",
            "strength_and_weaknesses": "Strength:\n\n- An interesting observation and continuation of previous work.\n- Provide studies over various related topics with mixed empirical and theoretical analysis.\n\nWeakness:\n\nMain contribution seems to just be combining two major previous results and somehow it's not providing enough insight.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not well organized. Claim many things but the results are spread out here and there. It's very hard to understand the contribution before linking them. In terms of novelty, I think the angle to approach the underlying NLP phenomena is interesting but no real technical difficulties or insightful theoretical results given.\n",
            "summary_of_the_review": "Overall, I like the angles and the topic of this paper. But somehow I think it's a bit disappointing after reading it as it's not providing much insight. Rather, it's more of a mixed of empirical evidence and some not that useful theoretical results. I do believe we should encourage this type of work so overall I am more positive, but there are some confusions and I hope authors could explain it.\n\nOverall, I think the gist of the paper is a bit unclear. Mentioned too many points but I can't find the direct link between the claim and analysis. You listed many contributions in bullet points so I guess it's easier to go from there.\n\n- Does the eNTK method directly work for NLP tasks? In Section 5, we show that eNTK does not\nwork well for standard FT but it achieves performance comparable to prompt-based FT (Schick\n& Schutze, 2021; Gao et al., 2021) on a majority of downstream tasks that we evaluate. \u00a8\n\n- So I think this is just an empirical analysis. Somehow just an extension of domain from wei el. al. \nDo you agree with this? If so, can you highlight this is more of an empirical results. in addition,  can\nyou also greatly re-organize your paper to make contribution of empirical results and theoretical analysis clear?\n\n\u2022 The NTK theory was developed for gradient descent, whereas LM fine-tuning often uses\nAdam (Kingma & Ba, 2014) as a standard practice (Devlin et al., 2019). Can the above method\nbe extended to give insight into the effect of different optimization algorithms? In Section 4,\nwe derive a new asymmetric \u201ckernel\u201d formula (Definition 4.2) that describes the dynamics of\nshort-term training with Adam and shows that the corresponding eNTK achieves comparable\nperformance as FT using Adam (Table 1).\n\n- Here you use yet another approximation of Adam, so I don't see the purpose of it. Why does that \nmatter to analyze yet another version of eNTK  just due to a different gradient method? \nIn particular, Adam results in Table 1 doesn't really outperform SGD version eNTK musch. Especially,\nfor TREC ADAM result is much worse than eNTK one. What's the explanation? \n\n\u2022 Is the eNTK regression method simply an alternate model that also happens to have good\nperformance on the classification task, or is it actually a near-equivalent description of how\nparameters of the original model evolve during fine-tuning? Section 5.3 shows that for tasks that\nthe eNTK can solve, the fine-tuning of the pre-trained model does exhibit behavior consistent\nwith the dynamics of training kernel classifiers. See Definition 3.1.\n\n- I think this could be a real contribution. But somehow I feel not consistent as fine-tuning + eNTK \nseems not to work but here you showed its empirical performance justified the assumptions/desired\nbehavior of NTK in 3.1 ? It's kind of contradicting. \n\n\u2022 If the eNTK can solve NLP tasks, then does the eNTK give insight into phenomena such as why\nprompting can improve performance and how parameter-efficient fine-tuning methods affect\noptimization? Figure 1 demonstrates that adding a prompt is necessary for the eNTK to solve the\ntask, and in Section 6, we apply the kernel lens to provide a possible explanation for the empirical\nsuccess of subspace-based fine-tuning methods (Hu et al., 2021; Aghajanyan et al., 2021).\n\n- Sorry I don't quite get why prompting can improve performance is related to subspace-based fine-tuning methods?\nI think the connection is not very obvious to me. I was feeling strange when I read your section 6. I am not sure\nwhy you want to analyze it. And I felt the connection to other part of paper is too weak such that it's another paper.\n\nWe conclude by proposing a rigorous mechanism through which fine-tuning of complex architectures\n(e.g., Transformers (Vaswani et al., 2017)) with prompts can exhibit kernel behavior. This is done\nin context of networks whose width goes to infinity, but unlike standard infinite-width NTK theory\nit allows a non-random initialization that is the result of pretraining. This result uses the Tensor\nPrograms framework (Yang, 2019; 2020a;b; Yang & Littwin, 2021; Yang & Hu, 2021).\n\n- I actually feel this part of analysis is the most pertinent part regarding your motivations. Why don't you put it in the front? But I am not sure the correctness and framework as I didn't know Tensor Programs before.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5322/Reviewer_NNgP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5322/Reviewer_NNgP"
        ]
    },
    {
        "id": "LssJwCXqcFO",
        "original": null,
        "number": 2,
        "cdate": 1666916128162,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666916128162,
        "tmdate": 1666916128162,
        "tddate": null,
        "forum": "erHaiO9gz3m",
        "replyto": "erHaiO9gz3m",
        "invitation": "ICLR.cc/2023/Conference/Paper5322/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper suggests that the success of language model fine-tuning (or with prompt) can be explained by neural tangent kernel (NTK), a tool that describes optimization behavior of infinitely wide neural networks. To bridge NTK with LM fine-tuning, the paper tries to justify that using NTK after pre-training phase (instead of initializing from lazy regime) is reasonable, and also derived empirical NTK for adam optimizer. The paper found empirical evidence that NTK behaves similarly to the original optimization methods, and also provided a theorem that shows prompt-based fine-tuning can exhibit kernel behavior",
            "strength_and_weaknesses": "Strength: \nThe paper tries to explain language model fine-tuning using NTK and cleverly uses the fact that fine-tuning takes few samples (therefore one can compute eNTK in a feasible way). \n\nWeaknesses:\nOverall, the paper is trying to bridge theory and practice. But there are a few concerns:\n1. While some of the empirical results are good, some observations are quite mixed. For example, RTE linearization does not perform well for k=512 while eNTK still performs ok -- this seems confusing to me. \n2. Section 4 is unclear to me, the paper mentioned \"we model Adam updates with SignGD\", why can you replace Adam with SignGD and is the derived kernel exactly following Adam or is just an approximation of Adam (if later is the case, why not just approximate Adam with SGD)?\n3. The theory section (Section 7): I don't see how it is related to prompt-based fine-tuning but not standard fine-tuning. The paper mentioned \"The theory focuses on the prompt-based setting since we did not find empirical evidence of kernel behavior in the standard setting\" (I hope theory can advance empirical work but maybe now the paradigm is shifted), this is fine, but which assumption in Theorem 7.3 is specific to prompt such that the final conclusion is \"prompt-based FT of f will exhibit kernel behavior\"?\n4. The bigger question is whether NTK is THE theory or just A theory. Obviously a lot of practices in LM training or FT do not match with NTK behavior (e.g., learning rate warmup), therefore assuming NTK is THE theory and develop reasoning that tries to explain FT is a bit insufficient, so I think this is a weakness of the paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity & Quality: The paper is overall clear, I'd be happy to get clarified on Section 4 & 7 a bit more. In terms of writing, I do think there is a better way to re-organize and better separate the assumption & theory with empirical results. Currently, empirical evidence and theory are interleaving and a bit hard to get the full picture. \n\nNovelty & Reproducibility: Overall relatively novel and should be easy to reproduce. ",
            "summary_of_the_review": "My feeling is a bit mixed for this paper. My current rating would be marginally below the threshold given that there are several issues worth clarifying. Regarding Weakness #4, I think different people may have different feeling on how important this is. I am relatively close to the median of that spectrum, but I can definitely see researchers having strong opinions from both sides. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5322/Reviewer_rpG6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5322/Reviewer_rpG6"
        ]
    },
    {
        "id": "H5FQeoSh0PV",
        "original": null,
        "number": 3,
        "cdate": 1667472558924,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667472558924,
        "tmdate": 1669796871500,
        "tddate": null,
        "forum": "erHaiO9gz3m",
        "replyto": "erHaiO9gz3m",
        "invitation": "ICLR.cc/2023/Conference/Paper5322/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the underlying reason behind why fine tuning pre-trained language models works well. The paper shows that in some cases, those fine tuned models can be described using neural tangent kernel. In those cases, the kernel view provide explanation of the fine-tuning success. \n",
            "strength_and_weaknesses": "Strength:\nThe paper tackle one of the most important questions in machine learning: understanding the reason behind the the performance of fine-tuned models, and in particular, when it should works.\n\nWeakness\nI found it hard to follow the paper's arguments. see section below for more details. ",
            "clarity,_quality,_novelty_and_reproducibility": "I found the paper unclear and hard to follow, in a way that is hard to appropriately appreciate the results. In particular I found the following points unclear:\n- Framing: The paper aims to further an empirical and theoretical understanding of pre-training and fine-tuning paradigm for NLP tasks, but this goal is too broad. As a result, the paper highlight too many points, and does not focus on none of them, especially with the the page limit. Creating a coherent story by refining a single take-away message from all of the described results, will help readers to follow the paper.\n- Mathematical motivation: The mathematical definitions appear without a proper motivation. Although the paper claim that some of the definitions are analogues to definitions in other papers, I believe the paper should be self-contained, and provide the intuition in high level.\n- Self contains claims: All mathematical claims should be self contained within the body of the paper, where the appendix can be used for proof, and for a formal version of the claims. Most claims of the paper are indeed presented in a self-contained manner, except Theorem 7.3, in which the conditions themselves appear in the appendix, without any informal phrasing of them.\n- references: it is very helpful that the paper have hyperlink to the arXiv, but it shouldn't replace mentioning the peer-review venue in which the papers appeared in. In the current format it is hard to validate that the paper is based on peered-review works.\n- Acronyms: please define all acronyms, for examples SGD.\n\n",
            "summary_of_the_review": "The paper presents several theoretical and experimental results, that I don't feel creates a coherent bottom line. In addition, It is hard to evaluate the quality of those results, without reading the appendix (which I didn't read). Thus, I cannot recommend accepting the paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5322/Reviewer_Ftva"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5322/Reviewer_Ftva"
        ]
    }
]