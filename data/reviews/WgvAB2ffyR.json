[
    {
        "id": "Ch8ZJbhixN",
        "original": null,
        "number": 1,
        "cdate": 1666625730447,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625730447,
        "tmdate": 1666625730447,
        "tddate": null,
        "forum": "WgvAB2ffyR",
        "replyto": "WgvAB2ffyR",
        "invitation": "ICLR.cc/2023/Conference/Paper2628/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "As claimed by this work, the authors present a framework for designing the best surrogate gradients of SNNs with LIF neurons. \n\nUnfortunately, this paper is hard to follow due to the unclear organization and non-standard presentation. The claimed contributions can be roughly divided into two terms, i.e., summarizing the observed results from experiments and presenting the theoretical understanding of SG design. I cannot demonstrate the correctness of the observed results since there lacks apposite definitions or measures of many terminologies, such as low dampening, high sharpness, and low tail-fatness.\n\nMore importantly, this paper provides a theoretical method for SG choice to improve the experimental performance. What are the assumptions and conclusions of this theoretical investigation?\n\nBesides, I would appreciate to have a clear clarification on the experimental details before showing the experimental results, such as shown in Subsection 3.5.5. ",
            "strength_and_weaknesses": "Weaknesses:\nThe weaknesses are detailed above, including:\n1. This paper is hard to follow due to the unclear organization and non-standard presentation.\n2. Low quality. There lacks apposite definitions or measures of many terminologies, such as low dampening, high sharpness, and low tail-fatness.\n3. The assumptions and conclusions of the theoretical investigation is far from clear.",
            "clarity,_quality,_novelty_and_reproducibility": "nothing to mentioned",
            "summary_of_the_review": "Overall, I believe that the presence of this paper is poor, and thus, it is difficult to verify the quality of its work, which would move the paper away from the accepted standard.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2628/Reviewer_fzV9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2628/Reviewer_fzV9"
        ]
    },
    {
        "id": "Bqt-oj-WQN",
        "original": null,
        "number": 2,
        "cdate": 1666627961739,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627961739,
        "tmdate": 1666627961739,
        "tddate": null,
        "forum": "WgvAB2ffyR",
        "replyto": "WgvAB2ffyR",
        "invitation": "ICLR.cc/2023/Conference/Paper2628/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers different spiking neuron models and explores hyperparameters on multiple tasks in order to facilitate efficient training via the surrogate gradient method. The study focuses primarily on the shape of the surrogate gradient function. They find that low dampening, high sharpness and low tail-flatness generally lead to better performance. More precisely, the fast-sigmoid derivative appears as the best choice of surrogate gradient function. Secondly, they perform a study of different weight initialisations, which reveals that \u201cHe\u201d gives the best outliers, \u201corthogonal normal\u201d the best mean accuracy and \u201cbigamma\u201d the lowest variance. Thirdly, they demonstrate that using high initial firing rates combined with a sparsity encouraging loss term induces better generalisation. Finally, they investigate various conditions to ensure stable gradients and show that these can also lead to better performances.",
            "strength_and_weaknesses": "Strengths\n\nThe paper aims to facilitate the training of spiking neural networks via surrogate gradients, which remains more challenging and less documented compared to more popular, non-spiking artificial networks.\nSNNs are applied to rather diversified tasks involving image recognition, spoken digit recognition and language modelling.\nSome findings can lead to helpful, practical solutions for SNN training.\n\nWeaknesses\n\nThe overall narrative of the paper needs substantial improvements. A fair number of (sub)sections do not lead smoothly into the next ones. The number of components tested in the study is rather ambitious, which can be okay, but the different findings often feel disconnected, which makes the general direction hard to follow. \nFor instance, the authors define a spiking version of the LSTM, but the results with it are only shown to be inferior to those obtained with simpler, more commonly used and more biologically plausible LIF and ALIF models. Even if interesting, the inclusion of this spiking LSTM does not seem justified and feels out of the way in terms of the general objectives of the paper.\nThe use of the different models and data sets does not feel coherent. Some hypotheses are tested on all tasks and models, whereas others for instance only use the LIF on a single task.\nThe paper feels more like a report than an actual article.\n\nRemarks\n\nThe SHD data set only seems to have training and validation splits in its default version. It would be worth explaining what test split you are using.\nThe spiking LSTM, which is said to be defined in Appendix G, is missing (maybe appendixes are simply not included for this review process, in which case, this remark can be discarded).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not very clearly written.  It is probably reproducible, but the novelty is not so high.",
            "summary_of_the_review": "The authors test a variety of techniques, albeit somewhat inconsistently.  As a review the paper could be useful, but overall it comes across as rather rushed and incomplete.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2628/Reviewer_xUW9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2628/Reviewer_xUW9"
        ]
    },
    {
        "id": "RgS2qyHUx4",
        "original": null,
        "number": 3,
        "cdate": 1666667844048,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667844048,
        "tmdate": 1666667844048,
        "tddate": null,
        "forum": "WgvAB2ffyR",
        "replyto": "WgvAB2ffyR",
        "invitation": "ICLR.cc/2023/Conference/Paper2628/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This article aims to experimentally and theoretically define the best SG across different stress tests to reduce the future need for grid search and proposes techniques to improve the performance of SNN training from multiple perspectives, such as the dampening of the SG and the normalization method.",
            "strength_and_weaknesses": "Strengths:\n1.\tThe authors provide empirical results on how to select SGs and initial methods to enhance the final performance.\n2.\tThe authors designed a new kind of SG that is suitable for all the datasets they used.\nWeaknesses:\n1.\tLack of comparison with other existing work.\n2.\tThis paper only provides experiments on very simple datasets and network structures. More complex cases need to be included.\n3.\tThe logic of this paper is confusing, e.g. many important figures are placed in the appendix, making it difficult for the readers to read.\n4.\tTheir main findings are obtained through some experiments with grid search while lacking logic proof. So the article's novelty and contribution are modest.\n\nQuestions:\n1. In Sec 3.1.2, what is the complete model structure for testing different SGs?\n2. 1.The datasets used in the experiments seem to be relatively small datasets. How does the experiment perform on larger datasets? I think the baseline of this work should be compared with the existing work.\n3. 1.The SG used doesn't guarantee $\\int_{-\\infty }^{+\\infty }f(x) = 1$. Maybe that's why the other SGs don't perform well?\n4. 1.In sec 3.4.1.Is there a missing equation before \"where p is the firing rate\"?\n5. 1.Fig 5. If the baseline is replaced with other initialization methods, will the results change?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper seem to be a rush re-wrap from its long preprint. Thus the writing and readability are not sufficiently good. ",
            "summary_of_the_review": "Overall, it adds an interesting paragraph to the training of SNN with LIF but the performance is not convincing. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2628/Reviewer_HAej"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2628/Reviewer_HAej"
        ]
    },
    {
        "id": "DLzVSWiSIp",
        "original": null,
        "number": 4,
        "cdate": 1666957699837,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666957699837,
        "tmdate": 1666957727534,
        "tddate": null,
        "forum": "WgvAB2ffyR",
        "replyto": "WgvAB2ffyR",
        "invitation": "ICLR.cc/2023/Conference/Paper2628/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work finds it unclear to determine which surrogate gradient to apply in different tasks or networks in SNN. Seeking to solve this problem, the authors have done some experiments with different surrogate gradients across tasks and networks, and come to the conclusion that the derivative of fast sigmoid outperforms other chosen surrogate gradients. Besides that, the authors have also done research on how different characteristics of surrogate gradients and initialization affect surrogate gradient learning and found that low dampening, high sharpness, low tail-fatness, orthogonal initialization, and high initial firing rate could help improve the network\u2019s performance. Lastly, the authors provide a theoretical solution based on bounding representations to find a surrogate gradient and initialization that could improve accuracy.",
            "strength_and_weaknesses": "Strength: 1. Training details are explicit, including network architecture and hyper-parameters. 2. Conclusion from the experiments is helpful in choosing surrogate gradient.\n\nWeaknesses: It doesn\u2019t seem reasonable to make the neuron as sensitive to the network history as to new input in section 3.5.2, since the network history is an accumulation of knowlegde learnt from past so it contains more knowledge than new input.\n\nMinor typos:\n\nSec2, line 3 \"y_L the network output\" -> \"y_L is the network output\" or \"y_L means the network output\"",
            "clarity,_quality,_novelty_and_reproducibility": "The authors of this work do not promise any reproducibility in text.\n\n",
            "summary_of_the_review": "The discussion on how to choose SG in SNN is meaningful and missed before.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2628/Reviewer_WUfr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2628/Reviewer_WUfr"
        ]
    }
]