[
    {
        "id": "-w8QY7fHYBG",
        "original": null,
        "number": 1,
        "cdate": 1665676652876,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665676652876,
        "tmdate": 1665676652876,
        "tddate": null,
        "forum": "HtAfbHa7LAL",
        "replyto": "HtAfbHa7LAL",
        "invitation": "ICLR.cc/2023/Conference/Paper6153/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Transformers are complex and require several compute-intensive operations. This paper proposes to simplify the execution of the BERT inference by using matrix arithmetic-only operations. The results show that the proposed method achieves considerable inference time reduction with relatively low accuracy loss compared to the baseline.",
            "strength_and_weaknesses": "Strengths:\n1. The tackled problem is relevant to the ICLR community.\n2. The contributions look solid.\n\nWeaknesses:\n1. It is recommended to evaluate the proposed method executed on AI accelerators, rather than only on CPUs.\n2. The reason why the knowledge transfer is needed (instead of training from scratch the MA-BERT) is unclear. Please provide more details on it.\n3. The results do not seem to provide a significant improvement compared to the related works.\n4. It would be useful to provide the source code for reviewers' inspection during the rebuttal.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: 5/10\n\nQuality: 5/10\n\nNovelty: 7/10\n\nReproducibility: 5/10",
            "summary_of_the_review": "Borderline paper where several concerns should be clarified.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6153/Reviewer_RFc3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6153/Reviewer_RFc3"
        ]
    },
    {
        "id": "WjsrOg8MdG",
        "original": null,
        "number": 2,
        "cdate": 1666666428535,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666428535,
        "tmdate": 1666666428535,
        "tddate": null,
        "forum": "HtAfbHa7LAL",
        "replyto": "HtAfbHa7LAL",
        "invitation": "ICLR.cc/2023/Conference/Paper6153/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces 3 approximations to remove expensive non-linearities in transformers using cheaper matrix math and ReLU activation functions. Next, they perform a knowledge transfer between the trained BERT model and the approximated model to bring the accuracy up to standard in MA-BERT. The results show comparable accuracy while being cheaper to implement MA-BERT in hardware.",
            "strength_and_weaknesses": "Strengths:\n\n* The techniques are simple to understand\n\nWeaknesses\n\n* The reasons as to why you can approximate the softmax using a 2-layer NN is unclear. For example, performer had clear justification why the projection was approximating the softmax. The argument in this paper is poor (universal approx. theory)\n* No hardware results, but just software CPU implementation.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty: The techniques seem to be novel. However, I am not an expert in this area.\n\nClarity: The reasons why the approximations work is not clear. At least an intuitive explanation would have been good. Universal approx. theory based explanation is usually too sparse.",
            "summary_of_the_review": "I enjoyed the premise of the paper and Figure 1, which shows how much hardware cycles are spent in the non-linear computations. The techniques seem simple and easy to understand. However, it is unclear why they would work. The authors should explain why softmax can be approximated using a 2-layer FFN with ReLU. It is not clear to me. Citing universal approx. theory is not enough. For inspiration, look at performer or linformer papers.\n\nI did not see ablations on the techniques suggested in the paper. Since, these approximations are not grounded in theory, I expect more ablations to show which technique mattered more and also why. \n\nI am not an expert in this area, however has a working knowledge base in it. The paper is simple, but lacks justifications for the choices made and hence my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6153/Reviewer_K662"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6153/Reviewer_K662"
        ]
    },
    {
        "id": "quTpWN2zhg",
        "original": null,
        "number": 3,
        "cdate": 1666716773571,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666716773571,
        "tmdate": 1670342974267,
        "tddate": null,
        "forum": "HtAfbHa7LAL",
        "replyto": "HtAfbHa7LAL",
        "invitation": "ICLR.cc/2023/Conference/Paper6153/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims at reducing the complexity of BERT models to accelerate their inference by putting together a series of previously-introduced techniques. To this end, the authors propose to approximate softmax with a two-layer neural network, replace GELU with ReLU, fuse normalization layers with adjacent linear layers, and use knowledge distillation. It has been shown that the acceleration of 1.27x can be obtained on CPUs using the aforementioned four techniques while achieving a comparable accuracy performance w.r.t. the baseline model.",
            "strength_and_weaknesses": "Strengths: \n\n-- I believe this work is well-motivated. The paper targets a very important issue (i.e., reducing latency of inference) associated with large models such as BERT. \n\n-- The detection of the source of the latency overhead during inference is insightful as shown in Fig. 1. \n\n-- The amount of speedup during the inference on CPUs is interesting and shows the effectiveness of the proposed techniques.\n\n-- The paper in general is well-written and easy to understand.\n\nWeaknesses:\n-- In general, the contribution of this paper is rather limited since each of those techniques have been previously-proposed.\n\n-- It would have been great if the breakdown of cycles in Fig. 1 was shown for the sequence length of 128 to be compatible with experimental results in Table 2.\n\n-- My major concern about this paper is the lack of comparison with prior works. First, there is no results for inference on GPUs. Second, there is no direct comparison with prior works such as Linformer? Where does the proposed method stand w.r.t. prior works? Finally, what speedup can be obtained when using MA-DistilBERT or MA-DistilRoBERTa?\n\n-- The GELU and softmax functions can be approximated using polynomials similar to I-BERT with no impact on accuracy.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This work puts together a series of previously-proposed works to reduce the complexity of BERT models. The paper is well-written, clear and easy to understand. I believe the results of this paper can be reproduced.",
            "summary_of_the_review": "In general, I believe the contribution of this paper is rather limited since the main method is a combination of previous works. On the other hand, the proposed method is simple and effective (when running on CPUs). My main concern is the lack of comparison with other works. It is not clear where this works stands w.r.t. prior works that also tries to reduce the complexity of BERT models such as Linformer and I-BERT. There is also no evaluation on GPUs while it has been mentioned as one of the motivation of this work in the introduction. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6153/Reviewer_9tBM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6153/Reviewer_9tBM"
        ]
    }
]