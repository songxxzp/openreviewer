[
    {
        "id": "bBEGaWxgKmx",
        "original": null,
        "number": 1,
        "cdate": 1666602848014,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666602848014,
        "tmdate": 1666666203147,
        "tddate": null,
        "forum": "mX56bKDybu5",
        "replyto": "mX56bKDybu5",
        "invitation": "ICLR.cc/2023/Conference/Paper1444/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a method to learn a shared set of object representations from paired views captured across 3D scenes, in a self-unsupervised manner. The core difference to existing 3D object-centric learning work is that it learns reoccurring objects across scenes and explicitly localize the learned objects, which are demonstrated in a set of downstream tasks, such as unsupervised image segmenation, object navigation, and depth ordering, that require the understanding of the scene layout and object goemetry and appearance.",
            "strength_and_weaknesses": "Strength:\n\n1. I value the concept of learning of a codebook over 3D scenes, which could benefit many downstream tasks.\n\n2. this paper extends the technical route presented in the MarioNette for unsupervised dictionary learning on 3D scenes, with some novel technical contributions for working on 3D scenes, i.e., the modules for learning instance variation and the dynamic changing number of objects.\n\n\nWeakness:\n\nMy concerns over this paper are mainly on the evaluation/demonstration of the efficacy of the proposed learning setup.\n\n1. the paper ships itself that it learns reoccurring 3D objects across scenes, and explicitly localize the learned objects, but the demonstration of this is only done in a indirect way. Ideally, a successful model should be able to segment the 3D scene into meaningful 3D units, thus the evaluation of the learning could have focused on the 3D, i.e., measuring the cluster of predicted instances/objects against the GT cluster, a similar way as done for the unsupervised image segmenation as show in the paper, but with an emphasis in 3D. Moreover, more visual results of how 3D scenes are decomposed into meaningful units should be presented for qualitative examing the performance.\n\n2. the evaluation are mostly done on synthetic data. While this is not a fatal flaw, there is indeed no enough evidence to support that the proposed method can be applicable on more real scenarios. ScanNet and Matterport3D for example contain 3D scenes collected from the real world, with estimated camera poses available. I am cuorious about the performance on these kinds of real-world data, where the noisy camera estimates, difficulty in finding corresponding frames, and the imaging quality differ significantly from synthetic data.\n\n3. for the object navigation task, again, I think the evaluation can be conducted in a bit more realistic setting. What about evaluating on Gibson and Matterport3D scenes, where we can have more realistic scenes with camera poses available.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the technical originality is moderate, it extends the idea in MarioNette, with additional new modules. And I can follow the paper successfully, but the exposition could be improved for clarity.\n\n1. Figure 2 is not self-contained and is not consistent with the textual description of the technical part. For example, what is $F$ in the figure, where are the module $H_{\\theta}, G_{\\theta}$ in the figure.\n\n2. In Figure 1, how does the label of the catogery objects come, say, Couch, Floor, and Fridge? If I follow the paper correctly, the learned dictionary does not provide catogerical labels.\n\n3. In Table 2, what is the ImageNet Pretraining at the second row. Is it a segmentation module pretrained on ImageNet? If yes, it may be unfair to compare on synthetic data.\n\n4. Although the submission includes source code of some experiments, it would be appreciated much that the textual implementation details can be included first in the supplementary, with source code as the strong complementary.\n\n\nA few spots I found that need correction:\n1. In Section 2, \"NERF\" -> \"NeRF\"\n\n2. Section 4.1, \"between train, valiation, and test\" ...\n\n\n",
            "summary_of_the_review": "While I recognize the value of learning a codebook of 3D units over 3D scenes, my concerns expressed above make me leaning towards negative on this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1444/Reviewer_GeqQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1444/Reviewer_GeqQ"
        ]
    },
    {
        "id": "-B7zePaveS",
        "original": null,
        "number": 2,
        "cdate": 1666625332715,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625332715,
        "tmdate": 1666625754561,
        "tddate": null,
        "forum": "mX56bKDybu5",
        "replyto": "mX56bKDybu5",
        "invitation": "ICLR.cc/2023/Conference/Paper1444/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper target the learning of an object-centric scene representation through NeRF-wise volume rendering. The key component is inspired by Marionette (Smirnov et al., 2021) to utilize a dictionary of object codes. The modifications include a gated function that allows dynamic addition of new categorical codes, category-wise code to instance-wise code through a variation module. Experiments indicate that the learned representation can be applied to tasks such as unsupervised segmentation, object navigation, and depth ordering.",
            "strength_and_weaknesses": " Pros: \n+ The major differences between the proposed method and others are clearly presented and easy to follow.\n+ The paper tries to address a critical problem of learning object-centric neural scene representation\n\nCons:\n- Several key elements of the proposed method are missing. For instance, how the codebook is learned should be clarified even if it is similar to Marionette. Otherwise, it is hard to understand how the codebook in Fig. 2 is constructed during training. Besides, how the learned object-centric representation is applied to the three tasks should be detailed with illustrations. \n- The experimental results provide limited information regarding the performance of the proposed method. Without qualitative results of object navigation (video for instance) and depth ordering tasks, readers cannot intuitively understand how the proposed method performs. The analysis for results on view synthesis, unsupervised segmentation, object navigation, and depth ordering is also insufficient. \n\nMinors:\n1. The floor in Fig. 1 is placed in the middle row instead of the \"bottom row\"\n2. \"Additionally, we learn can model ...\" (page 3)\n3. The reused annotations lead to confusions: i \\in n indicates the index of scenes, while i \\in k indicates the index of codes; (x,y) indicates the pixel position while (x,y,z) indicates the world coordinate.  (page 3)\n4. The symbols such as O_h, f_h^{x,y}\uff0c G_\\theta, H_\\theta are encouraged to be added to Fig. 2. It is unclear where the concatenation of the relative position and the absolute position occur in Fig. 2.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The method is not clearly demonstrated as several critical parts are missing. The novelty is limited as the dictionary learning part is mainly borrowed from Marionette. The experimental results are not insufficient to verify the efficacy of the learned scene representation.",
            "summary_of_the_review": " Due to the issues mentioned in the Weakness section, I am leaning toward rejection currently. Though the experimental results in Tab. 4 indicate that the modification of the proposed method does bring benefits, major revision is required to make the method and the corresponding applications clear and intuitive. I would like to see how the authors respond and the opinions from other reviewers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1444/Reviewer_BHLx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1444/Reviewer_BHLx"
        ]
    },
    {
        "id": "wXzV1i_YlYq",
        "original": null,
        "number": 3,
        "cdate": 1666642885912,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642885912,
        "tmdate": 1668759150024,
        "tddate": null,
        "forum": "mX56bKDybu5",
        "replyto": "mX56bKDybu5",
        "invitation": "ICLR.cc/2023/Conference/Paper1444/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to marry the power of semantic codebooks with NeRFs, by conditioning the MLP on a fixed codebook (fixed after training, of course).\n\nWe are given two views - \"input view\" A and \"novel view\" B, as well as a shared codebook\nThe first stage is to run A through an encoder network, obtaining the (2D!) feature map F. \nNext, each 3D query point needed to render B is projected into A's 2D coordinates, essentially retrieving the relevant \"pixel feature\" f from F. We find the nearest neighbor atom m in the dictionary of the aforementioned \"pixel feature\" f, and add a small perturbation to account for intra class variability. This perturbation is generated by feeding both m and f into a small MLP.\nThe final value (atom+pertubation) is concatenated to the standard NeRF inputs, that is the view direction and the aforementioned 3D query point (transformed into positional encoding).\n\nThe total number of atoms is also learned with a special penalty which is added to the NeRF recunstruction loss\n\nThe method is tested on two synthetic and one real datasets, and present competitive resutls\nThe authors provide an ablation study to justify the two main modules, namely the dictionary size penalty and the perturbation MLP",
            "strength_and_weaknesses": "### Strength \nThe manuscript is generally well written, and the method seems rather straightforward.\nThe experimental section presents impressive transfer from synthetic to real images.\n\n### Weaknesses\nThere are issues I hope the authors can clarify:\n\n1. It seems that the input, as in PixelNeRF, images are needed in inference time, otherwise - where is the feature map obtained from to query the dictionary?\nIf this is so - this is a major caveat compared to other NeRF methods that can render an image using only the MLP.\n\n2. Another possible discrepancy is the fact that the atom is generated from 2D coordinates of a view different from the novel one - but unlike \"PixelNeRF\", if an object is occluded in the \"input view\" we will get **a wrong atom** and hence the wrong class for the segmentation map - is this resolved by a low density for the rendering process as well?\n\n3. How exactly is a new atom added to the dictionary? Specifically, when the parameter 's' \"signifies when to add a new code\" - what value is the atom given?\n\nThere are some places in the text that can be made clearer:\n- I would recommend using a different notation for the two input images, rather than h and h'.\n- Figure 2 - It is not clear which part of the pipeline that are included in the MLP training, are not included in inference. For example:\n   - Where is the feature map F generated from?\n   - What feature values (or atoms) will we get for queries of regions out of the \"input view\"?\n- In addition, it was hard to follow how eq2 is incorporated into the flow - are discarded atoms kept as zero? \n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nMost of the paper was easy to follow, with the exception I mention above.\n\n---\n\n### Quality\n\nI raised 3 questions above regarding some gaps in the method.\n\n---\n\n### Novelty\n\nThere are some partially similar works, but this is a field with a **very** high publication rate, so this is not a reason to reject.\nSpecifically, here are 3 similar works, that can be mentioned in the text:\n1. [In-Place Scene Labelling and Understanding with Implicit Scene Representation](https://shuaifengzhi.com/Semantic-NeRF/) [ICCV 2021]\n2. [NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes](https://zhiwenfan.github.io/NeRF-SOS/) [TMLR 2022]\n3. [NeSF: Neural Semantic Fields for Generalizable Semantic Segmentation of 3D Scenes](https://nesf3d.github.io/) [arXiv]\n\n---\n\n### Reproducibility\n\nThe method itself is quite straightforward and should be possible to implement over an existing version of NeRF.",
            "summary_of_the_review": "This seems like a promising method. If my concerns are addressed, I see no reason to accept this paper.\n\n---\n\nThe authors have addressed my concerns, and I think this paper is clearer now. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1444/Reviewer_77Ao"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1444/Reviewer_77Ao"
        ]
    },
    {
        "id": "ZLs-s2855lL",
        "original": null,
        "number": 4,
        "cdate": 1666887813651,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666887813651,
        "tmdate": 1666976399141,
        "tddate": null,
        "forum": "mX56bKDybu5",
        "replyto": "mX56bKDybu5",
        "invitation": "ICLR.cc/2023/Conference/Paper1444/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper decouples the scene representation into a set of learned codes. The codebook is object-level. The feature volume of a scene is encoded using the codebook before rendering with an MLP (NeRF). Experiments show the effectiveness of the proposed representation on various downstream tasks. The representation is evaluated using three tasks, object navigation in THOR, unsupervised segmentation in various datasets, and depth ordering estimation. These tasks demonstrate the effectiveness of the learned codebook.",
            "strength_and_weaknesses": "Strength:\n- Using an object-level codebook to represent a scene is not a new idea (MarioNette). But this paper makes the learned representation more effective with various techniques.\n- The experiment is through by comparing many related methods on three distinct tasks. The authors also show the method is not only useful for synthetic data but also for more complex real data.\n\nWeakness:\n- The majority of the experiment is done using synthetic datasets like THOR. The scene is relatively simple and has less clutter and occlusion than typical real-world scenarios. It is unclear how much the degradation will be for a more complex scenario.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear and easy to understand. It might be better if the authors could summarize the contribution of this method so that readers and easily get the key information.\nThe method is simple and clearly described. It should not be too hard to implement it. The authors did not mention whether the code will be open source.\nThe idea of using a codebook to represent a scene is not novel, but using volume rendering and NeRF to learn the object-level codebook is novel.",
            "summary_of_the_review": "This paper presents an effective way to learn object-level codebooks through volume rendering. It shows the effectiveness of this codebook with various experiments. It is unclear whether this method is robust to cluttered scenarios, with more occlusions. I think overall it is a nice paper and should be valuable for the community to read.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1444/Reviewer_PdhP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1444/Reviewer_PdhP"
        ]
    }
]