[
    {
        "id": "d9wlLaRBeMj",
        "original": null,
        "number": 1,
        "cdate": 1666581195395,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666581195395,
        "tmdate": 1666581195395,
        "tddate": null,
        "forum": "Z8qk2iM5uLI",
        "replyto": "Z8qk2iM5uLI",
        "invitation": "ICLR.cc/2023/Conference/Paper3357/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "A new vector quantization method is proposed, named Vector Quantized Wasserstein Auto-Encoder (VQ-WAE). The presented VQ-WAE employs the Wasserstein (WS) distance in both the observation x space and the latent z space to encourage matching, mimicking the existing Wasserstein Auto-Encoder. Experiments on MNIST, CIFAR10, SVHN, and CelebA datasets are conducted to demonstrate the superiority of the VQ-WAE to the existing VQ-VAE and SQ-VAE methods.",
            "strength_and_weaknesses": "Strength:\n\n(1) The paper is well-written in general.\n\n(2) The empirical experimental results seem to be convincing. \n\nWeaknesses:\n\n(1) The theoretical derivations are not sound, because, in both the VQ-VAE and the presented VQ-WAE, many (like 8 $\\times$ 8) latent codes correspond to one input image $x$, whereas the presented theorems are derived based on one latent code per x. In other words, there is a mismatch between theory and implementation.\n\n(2) No explicit demonstrations like the reconstructed images are shown, which makes it difficult to evaluate the quality of the vector quantization. For example, is the reconstruction quality better than that of the VQ-GAN?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written in general. The novelty is kind of incremental, even if the above-mentioned weakness (1) is corrected. The reproducibility is satisfactory as the code is available.",
            "summary_of_the_review": "The main weakness of the proposed VQ-WAE lies in its mismatch between theory and implementation. Besides, it's not clear if the presented VQ-WAE works indeed better than existing VQ methods like the VQ-GAN.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3357/Reviewer_7LM4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3357/Reviewer_7LM4"
        ]
    },
    {
        "id": "nlGcvQ_AYFp",
        "original": null,
        "number": 2,
        "cdate": 1666627623863,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627623863,
        "tmdate": 1666627623863,
        "tddate": null,
        "forum": "Z8qk2iM5uLI",
        "replyto": "Z8qk2iM5uLI",
        "invitation": "ICLR.cc/2023/Conference/Paper3357/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a vector quantized autoencoder that minimizes the Wasserstein distance between the encoded samples and the code word. In short, the loss function is simply the quantized autoencoding loss plus a penalty term that measures the Wasserstein distance between the empirical distribution of the encoded samples and the empirical distribution of the code words. The authors then evaluate their approach on the generative modeling for CIFAR10, MNIST, SVHN, and CelebA datasets showing competitive performance compared to VQ-VAE and SQ-VAEs. ",
            "strength_and_weaknesses": "### Strengths\n\n* The idea of using Wasserstein distance for vector quantization is interesting. \n\n### Weaknesses\n\n* The paper is not well-written, and the flow of the paper can significantly benefit from a significant rewriting. \n\n* The paper's novelty is limited to applying the Wasserstein distance in the latent space of an autoencoder with a moving sparse target distribution (i.e., the codeword distribution). \n\n* Given the deterministic nature of both the encoder and decoder and applying the Wasserstein distance in the embedding space, the paper is more closely related to the Sliced-Wasserstein Auto-Encoders (SWAE), Kolouri, et al. 2018, as opposed to the WAE that uses probabilistic encoder/decoders. \n\n* The choice of uniform $\\pi$s is not well justified in the paper. Also, the authors may want to use a different symbol for $\\pi$, because $\\pi$ is often used to denote the `transport plan' in the optimal transport-related literature and in calculation of the Wasserstein distance. \n\n* The stop gradient used in VQ-VAE could also be used in your formulation. In particular, if you freeze the encoder, the optimal codeword could be directly obtained from the barycentric projection of the transportation plan. Some discussions on these aspects of the training could improve the quality of the paper. \n\n* Eq (3) is missing a $f_d$ for the inner minimization. \n\n* The paragraphs following Eq (3) state $\\bar{f}_e$ is deterministic repeatedly. There are several instances of repeated reworded statements throughout the paper, which flags that the paper could use significant polishing to improve the flow and communicate the concepts more concisely. \n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper could be reorganized and rewritten to increase clarity and conciseness. \n\n* The proposed approach lacks novelty and originality. Moreover, the various choices, like dropping the stop gradient or assuming that the codewords have uniform weights, are not very well justified in the paper. \n\n\n\n",
            "summary_of_the_review": "While the paper has exciting aspects, I think its current state lacks sufficient quality for ICLR. In particular, the paper can benefit from reorganization. Also, the authors could provide a finer prior work section that better places their work among the existing approaches. Finally, detailed discussions on various choices throughout the paper could benefit this paper. To summarize, I think this paper has potential, however, it is not ready for publication yet. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3357/Reviewer_zbTA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3357/Reviewer_zbTA"
        ]
    },
    {
        "id": "ebpNfodLk0",
        "original": null,
        "number": 3,
        "cdate": 1666633486726,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633486726,
        "tmdate": 1666633486726,
        "tddate": null,
        "forum": "Z8qk2iM5uLI",
        "replyto": "Z8qk2iM5uLI",
        "invitation": "ICLR.cc/2023/Conference/Paper3357/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper extends the Wasserstein autoencoder (WAE) framework due to Tolstikhin et al. (2017) to incorporate discrete representations as considered by the vector quantized variational autoencoder (VQ-VAE) due to van den Oord et al. (2017). It begins with a theoretical development leading to the optimization problem (4) and then propose an approximate solution procedure in Algorithm 1. The performance of the proposed algorithm is tested on the standard benchmark, namely CIFAR10, MNIST, SVHN, and CelebA. Comparisons are made with VQ-VAE and SQ-VAE (Takida et al., 2022), a stochastic variant of the former. While for the traditional pixel- and patch-level metrics SQ-VAE exhibits better performances, VQ-WAE has merits in dataset-level metrics such as rFID, and Shannon entropy of the latent codeword distribution. The latter phenomenon can be understood as that VQ-WAE yields better codebook utilization and thus prevents codebook collapse problem.",
            "strength_and_weaknesses": "## Strengths\n\nApplication of the WAE framework to discrete representation learning is reasonably demonstrated with various evaluation metrics. The derivation of objective function (4) from the Wasserstein distance minimization problem (1) is natural at least intuitively.\n\n\n## Weaknesses\n\n1. The major weakness is the gap between the theory and algorithm. The optimization problems (1), (2) and (4), which are claimed to be equivalent to each other in Theorem 3.2, are basically about fitting a $K$-support discrete latent distribution $\\sum_{k=1}^K \\pi_k \\delta_{c_k}$ to the data distribution by minimizing the Wasserstein distance between them. In particular, the mass $\\pi_k$ assigned to cordwood $c_k$ is an optimization variable to be fitted. However, in the actual algorithm (Algorithm 1), it is fixed as $1/K$ and only $c_k$'s are fitted. So Algorithm 1 in fact solves a different problem from (4). This distinction is important since the so-called \"codebook collapse\" occurs precisely because some $\\pi_k$ is zero but is destined to be prevented if the proportion of codewords is forced to be uniform. In fact, that the optimal transport problem (1) is equivalent to the $K$-means clustering has been known at least since 1982 (Pollard, D., 1982. Quantization and the method of k-means. IEEE Transactions on Information theory, 28(2), pp.199-205). In light of this result, the modified problem that Algorithm 1 solves is the $K$-means clustering problem under the constraint that each (latent) cluster contains the same number of sample (latent) points. I do not think that this modified problem is necessarily better or more desirable than the original $K$-means problem (1). Suppose (1) is solved exactly and there is a codebook collapse. Then this means that strictly less than $K$ codewords best represent the data. Is this bad? So the case that codebook collapse becomes problematic is when the algorithm is trapped in a bad local minimum. This is a problem of the algorithm, rather than the sin of codebook underutilization. In this sense, the role of the added constraint is to *regularize* the algorithm to avoid bad local minima. On the other hand, Takida et al. (2022) avoids the same problem by using stochastic quantization but not constraining cluster sizes.\n\n2. The proof of Theorem 3.2 is incomplete. The proof only states that the (3) $\\leq$ (4). The possibility of strict inequality cannot be ruled out because the constructed variables $(C^{*2}, \\pi^{*2}, f_d^{*2}, \\bar{f}_e^{*2})$ are only feasible for (3) but not necessarily optimal. Further, measurability of $\\bar{f}_e^{*2}$ needs to be proved.\n\n3. The objective of Algorithm 1 is not differentiable because of the $\\arg\\min$ operator in Line 5 and the Wasserstein distance penalty (6).  How this is put to the gradient learning pipeline is not stated. Is the  gradient passing trick due to van den Oord et al. (2017) used to deal with the $\\arg\\min$ operator? Or the Gumbel-softmax trick? is (6) smoothed with entropy regularization? If the answer to the first and the third questions are yes (which I presume), then the real contribution of this paper appears to be replacing the last two terms in the VQ-VAE objective with (a smoothed version of) (6); see Sect. B.1. \n\n4. Finally, I wonder what are the authors' opinion about some peaks of codewords in Fig. 2a when $|C|$ is large in the MNIST dataset.\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n\nAs mentioned, it is unclear how Algorithm 1 is implemented with backpropergation. \n\n## Quality\n\nThis paper is written in plain Ensligh and easy to follow most of the time. In Table 2, MSE is mentioned in the title but not included in the table. \n\n## Novelty\n\nTheorem 3.1 is a classical result in $K$-means clustering and vector quantization. Theorem 3.2 is incorrect. \n\n## Reproducible\n\nCode is attached for reproducibility.\n",
            "summary_of_the_review": "There are gap between the theory developed and the actual algorithm, both of which have unresolved flaws.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3357/Reviewer_9V99"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3357/Reviewer_9V99"
        ]
    }
]