[
    {
        "id": "95TEP8hxaj",
        "original": null,
        "number": 1,
        "cdate": 1666585635068,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666585635068,
        "tmdate": 1666585635068,
        "tddate": null,
        "forum": "XrMWUuEevr",
        "replyto": "XrMWUuEevr",
        "invitation": "ICLR.cc/2023/Conference/Paper2627/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "To improve upon the current best performance of models that predict molecular properties in typical drug discovery scenarios (where the number of molecules with known properties are very small), the authors propose MHNfs, an embedding-based few-shot learner using Modern Hopfield Networks to provide learned associative memory for enriching a given molecular representation with those of reference molecules, in order that the properties of the given molecule are more accurately predicted. The authors show that 1) MHNfs outperform a variety of other few-shot learning methods on the activity prediction tasks in the FS-MOL dataset, and 2) that contextual enrichment is responsible for the improvement in performance over the next-best method. They also demonstrate that their method generalizes better to non-drug-like molecules than the next-best method.",
            "strength_and_weaknesses": "Strengths:\n- Proposes a novel few-shot mechanism based on comparisons to knowns molecules, which is loosely analogous to how medicinal chemists typically explore chemical space.\n- Demonstrates that the novel mechanism outperforms all models to which it was compared.\n- Shows that a very simple predictor based on average activity performs better than most previous few-shot learning methods.\n\nWeaknesses:\n- The performance of MHNfs on FS-MOL tasks and when generalizing to Tox21 is within the standard error of the next-best model. Moreover, MHNfs perform worse on non-kinase-related tasks. The authors should point these facts out, perhaps addressing the contributions of different factors to the standard errors.\n- No other comparable memory-based model is included for comparison. Comparison to a simple memory-based baseline would help understand the role of memory in the reported performance improvement.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This work is novel and appears to be of high quality. In particular, a large number of methods are included for comparison, including new rational baselines for the FS-MOL tasks, and descriptions of each method and how they were trained are provided, though no code is provided. The authors include additional robustness results for MHNfs in the appendix.\n\nThe paper is written very clearly, and the work is original due to the use of associated memory (MHNs) for contextual enrichment.",
            "summary_of_the_review": "The use of associative memory for enriching molecular representations is novel, and the improvement of MHNfs on FS-MOL tasks over other methods appears significant, despite being within the standard error of the next-best method. The establishment of simple, rational baselines for the FS-MOL tasks is also an important contribution to understanding that benchmark and the source of performance gains (or losses).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2627/Reviewer_kPjE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2627/Reviewer_kPjE"
        ]
    },
    {
        "id": "S9WscW8o35l",
        "original": null,
        "number": 2,
        "cdate": 1666626129786,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626129786,
        "tmdate": 1666626129786,
        "tddate": null,
        "forum": "XrMWUuEevr",
        "replyto": "XrMWUuEevr",
        "invitation": "ICLR.cc/2023/Conference/Paper2627/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses molecular property prediction in the few-shot setting by suggesting a novel Modern Hopfield Network architecture and testing it on the molecule-specific few-shot dataset FS-Mol. The novel architecture seeks to make use of the context available in the wider molecular training set through means other than a pretraining step. ",
            "strength_and_weaknesses": "Strengths\n* The paper is very clearly written and thorough; the model is transparently described. While a relatively simple idea, the execution is excellent and baseline comparisons thorough. \n* The ablation studies are informative. \n* The novel method proposed works well: the performance gains are significant in a challenging setting \n* The paper also addresses in detail a number of state-of-the-art baselines, and importantly brings to the attention of the machine learning community the importance of the Frequent Hitters naive baseline.\n* The appendices are similarly thorough.\n\nWeaknesses\n* It would be somewhat valuable to have more discussion around why the method is regularizing the covariance as the authors discuss and how this compares to/ why this is outperforming a pretraining or meta-learning strategy using the entire training set of tasks available in FS-Mol. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe work is very clear and well-written. \nQuality: \nThe work is of clearly high standard and appropriately addresses all baselines thoroughly. \nNovelty:\nWhile many of the components have previously been applied in few-shot learning, and MHNs are of course not new, this combination of components and insight into its value in the few-shot molecular property prediction are novel and useful. ",
            "summary_of_the_review": "This is a simple but effective idea that beats previously suggested methods on this task. While the idea is simple, the work is well executed, described, and thoroughly explored, and thus makes a valuable contribution to this particular body of work. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2627/Reviewer_XyoZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2627/Reviewer_XyoZ"
        ]
    },
    {
        "id": "JHR0CrrveYj",
        "original": null,
        "number": 3,
        "cdate": 1666665980014,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665980014,
        "tmdate": 1666665980014,
        "tddate": null,
        "forum": "XrMWUuEevr",
        "replyto": "XrMWUuEevr",
        "invitation": "ICLR.cc/2023/Conference/Paper2627/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors develop a deep learning method on molecules to better leverage training data and contextual information to make property predictions.",
            "strength_and_weaknesses": "I really appreciate the implementation of the baselines, as well as the authors\u2019 interpretation of them (Frequent Hitters and Similarity Search). Upon release of code, it is useful to see those implementations as well.\n\nIn Section 5.3 - A domain shift experiment is performed. There 8 positive and 8 negative were randomly selected. How different are they? Can better sets of divergent molecules be made by clustering based on molecular fingerprints and/or Tanimoto distances?\n\nSection 5.3 - Are there better datasets than Tox21 for this analysis? It is difficult to really see your model outperform others in this challenge. Generally, I feel that this community is hitting diminishing returns with Tox21, so understanding if algorithmic improvements actually improve performance is challenging.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Section 2 - I found the problem setting to be very clear.\n\nThe standard errors of models in Table 1 are overlapping. Is it possible to chose a single best performing model if these standard errors overlap? To what confidence does a given model/architecture perform over others?\n\nEquations: X, X\u2019, X\u2019\u2019, etc: Would it make more sense to have them as Z, since they are some embedding? It is unclear to me as a reader as how to track x, X, as well as any \u2018 of X, when some are molecules, and others are embeddings or representations.",
            "summary_of_the_review": "I found the work to be interesting and somewhat complex (albeit convoluted). Though the work was well presented and in a generally interesting research area, it feels like this work, as demonstrated in the paper, is hitting diminishing returns, and could find additional evidence to help bolster and argument for the utility and necessity of this approach.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2627/Reviewer_WgGe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2627/Reviewer_WgGe"
        ]
    },
    {
        "id": "XzAvtuGLmI",
        "original": null,
        "number": 4,
        "cdate": 1666715535083,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666715535083,
        "tmdate": 1666715535083,
        "tddate": null,
        "forum": "XrMWUuEevr",
        "replyto": "XrMWUuEevr",
        "invitation": "ICLR.cc/2023/Conference/Paper2627/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a novel few-shot learning approach (Hopfield-based molecular context enrichment for few-shot drug discovery or MHNfs) that exploits a context of other compounds outside the support and query set to improve the performance of the model. The model relies on the proposed context-module, made up of Hopfield layers, to associate the representations of the query and support set samples with other samples in the training data to enrich the representations. Afterwards, a cross-attention module is used to associate the representations within the query and support set. Experiments are conducted on FS-MOL and Tox21 datasets to show the proposed MHNfs is competitive versus state-of-the-art baselines and ablations experiments are conducted to show how the two modules improve the model performance.",
            "strength_and_weaknesses": "Strengths:\nCompetitive few-shot learning performances\nThe approach to use the context of other training samples is novel and interesting.\n\nWeaknesses:\nThe paper could benefit from studies to show how context benefits the model\u2019s performance. The current approach of sampling 5% of the training data does not give much intuition of how the proposed helps improve the performance.\nProposed approach relies on a specific model architecture, limiting its applications to other model architectures. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Questions:\nAre the context molecules for the Tox21 experiment sampled from the Tox21 or FS-MOL pretraining dataset and how many were used?\nHow much is the performance benefit dependent on the size of the context, in the FS-MOL experiments, 5% of training samples were randomly sampled. Have other % attempted?\nWhat is the extent of computational burden from the context on the inference?\nCould the proposed approach be applied to other few-shot learning domains outside molecular learning?\n",
            "summary_of_the_review": "The paper proposed several novel contributions to the molecular few-shot learning literature, especially its context retrieval module. Though there are still some questions about the proposed context retrieval module such as its applications to other model architecture, scalability and intuition behind its benefit, the experimental results are convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2627/Reviewer_b2MR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2627/Reviewer_b2MR"
        ]
    }
]