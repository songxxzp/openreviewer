[
    {
        "id": "mfH_ryGGFXm",
        "original": null,
        "number": 1,
        "cdate": 1666569003666,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666569003666,
        "tmdate": 1666569003666,
        "tddate": null,
        "forum": "1KtU2ya2zh5",
        "replyto": "1KtU2ya2zh5",
        "invitation": "ICLR.cc/2023/Conference/Paper4933/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a few improved variants of STORM+ that generalizes to a broader class of functions with weaker assumptions and better theoretical guarantees. STORM+ is a VR algorithm for non-convex stochastic optimization problems. However, prior methods suffer from assumption like boundedness of function value and the theoretical convergence depends on such parameters. In this work, the authors propose an adaptive framework for STORM+ that removes this dependence and relaxes a few other assumptions. They also corroborated their new algorithms with empirical experiments.",
            "strength_and_weaknesses": "The paper is well-motivated and structured in a way to help authors understand the technical contributions in proving the results. I think the writing and exposition may be improved from the following few aspects, and would be happy to re-evaluate the paper once these are addressed properly:\n\n1) I think the technical section 3.1 and 3.2 could be written slightly more clearly to highlight which parts of the analysis are inherited from prior work, and which are new contributions. The current way written without reading the previous papers it is not immediate to me which part helps remove the additional / stronger assumptions in prior work. Also is there any difference in the adaptivity design part compared with prior work?\n\n2) The final complexity result stated in theorems doesn't agree on units (i.e. see definition of kappa, Q_1 in the paper etc.). When scaling the function by an additional factor L, the convergence result doesn't seem scale-invariant, which is a bit counter-intuitive to me. Would there be a proper analysis that could make this scale-invariant?\n\n3) The final high-probability bounds it may be helpful to directly state it using log(1/delta), and save the reasoning in the proof sketch. This makes the results more citable for future work.\n\n4) In the experiments, the figure could be improved a little bit to make each line of comparison look clearer (i.e. the color of STORM+ and another META-STORM-SG(H) looks quite confusing). Also, some parts of the comparison doesn't feel very fair since you are comparing META-STORM with heuristic to STORM+ without. It may be beneficial to use same heuristic on both methods and then compare.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has a clear exposition and there seems to be some novelty in obtaining the results. I think the authors could make it clearer by comparing with prior work more and stating the concrete novel elements in the new proof. The experiment section could be written in a clear way (see last point in previous comment).",
            "summary_of_the_review": "I think the paper is studying an interesting problem and has some nice idea in it. I think both the theory and experiment part could be improved more to make the contributions and supporting arguments clearer. Due to these reasons I gave my recommendation of the paper as below but am open to changing once my concerns are properly addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4933/Reviewer_HMzj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4933/Reviewer_HMzj"
        ]
    },
    {
        "id": "9WNA6LWclqp",
        "original": null,
        "number": 2,
        "cdate": 1666577932384,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577932384,
        "tmdate": 1666577932384,
        "tddate": null,
        "forum": "1KtU2ya2zh5",
        "replyto": "1KtU2ya2zh5",
        "invitation": "ICLR.cc/2023/Conference/Paper4933/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies variance reduced methods for nonconvex optimization. In particular, variants of STORM are proposed to remove the unreasonable assumptions in STORM+. Numerical results are then presented to demonstrate the proposed algorithms.",
            "strength_and_weaknesses": "**Strength:** \nOverall, the strength of this work lies in its theoretical part. What interests me the most is the theoretical frame work of updating $a_t$ in META-STORM. This novel design helps to remove unnecessary assumptions in previous works. \n\n**Weakness:**\n\nThe numerical experiments are relatively weak compared to its theoretical part. \n\n- (W1) Numerical experiments on Cifar10 is not optimal, for example I assume that there is no learning rate decay by examining the loss curve of SGD. And the final test accuracy seems not matching the best that one can achieve (93 to 94%).\n\n- (W2) The paper is somehow contradicted: it motivates fully adaptively to parameters when choosing learning rates, yet has to tune through grid search for best numerical performance in numerical tests.  \n\n- (W3) It is not clear what are the specific heuristics used for in META-STORM (H). Moreover, can the authors explain the reasons for coordinate-wise learning rate in META-STORM(H)? It looks to me that the b_t update is a normalized adaptive learning rate, which is a simplified version of coordinate-wise learning rate. It is hence less intuitive for me to make adaptivity adaptive.\n\n- (W4) How does an epoch counts in the experiments? Since the proposed algorithms compute two gradients while SGD and Adam only needs one.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The theory is clear but some of detailed implementation is not. Theory is novel.\n",
            "summary_of_the_review": "This works falls more into theoretical part. And it indeed removes unreasonable assumptions in previous works. The numerical results do not fully support the theoretical findings due to the suboptimal implementation and benchmarks. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4933/Reviewer_caqd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4933/Reviewer_caqd"
        ]
    },
    {
        "id": "XSR8iF6kgA0",
        "original": null,
        "number": 3,
        "cdate": 1666704083404,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666704083404,
        "tmdate": 1666705096366,
        "tddate": null,
        "forum": "1KtU2ya2zh5",
        "replyto": "1KtU2ya2zh5",
        "invitation": "ICLR.cc/2023/Conference/Paper4933/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the variance reduction technique for nonconvex optimization, along the research line of STORM and STORM+. The proposed META-STORM extends STORM+ by relaxing the assumption of bounded function value STORM+ while achieving the optimal convergence rate.",
            "strength_and_weaknesses": "Strengths:\n\n1: This paper is somewhat novel.\n\n2: The theoretical analysis is technically sound.\n\n3: This paper is well-written and easy to follow. \n\n\nWeaknesses:\n\n1: The proposed META-STORM introduces an additional learning rate that needs to be tuned. (Note STORM and STORM+ do not require this additional hyperparameter.) Thus, META-STORM is not a fully adaptive algorithm. The authors need to explain this.\n\n2: How will relaxing the assumption of bounded function value STORM+ affect the optimization process in practice? Can the authors provide some examples with unbound function values that STORM+ would fail? Or this assumption is only required for the analysis of STORM+? The authors may need to provide more explanations here. ",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty: This paper is somewhat novel. The improvement is limited and not very interesting.\n\nQuality: The theoretical analysis is technically sound with excellent analysis. Extensive experimental validation is provided.\n\nClarity: The paper is clearly written and easy to follow.\n\nReproducibility: The code needed to reproduce the experimental results is not provided.",
            "summary_of_the_review": "This paper extends STORM+ by relaxing the assumption of bounded function value STORM+ while achieving the optimal convergence rate. The improvement is somewhat novel, but not very interesting.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4933/Reviewer_KkGZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4933/Reviewer_KkGZ"
        ]
    },
    {
        "id": "nVzE_DBiib",
        "original": null,
        "number": 4,
        "cdate": 1666845641353,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666845641353,
        "tmdate": 1666845671771,
        "tddate": null,
        "forum": "1KtU2ya2zh5",
        "replyto": "1KtU2ya2zh5",
        "invitation": "ICLR.cc/2023/Conference/Paper4933/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a variance reduction method based on the STORM algorithm. Not like STORM, their algorithm does not require access to specific problem parameters such as the Lipschitz constant and the gradient bound. They prove that this algorithm will converge at the rate of $O(T^{-1/3})$ under slightly weaker assumptions than STORM.",
            "strength_and_weaknesses": "Strength:\n- Based on STORM\u2019s framework, the paper designs an algorithm that does not require the knowledge of the Lipschitz constant or the variance bound constant, which is practical.\n- The algorithm converges under the same iteration complexity of $O(T^{-1/3})$ as the pioneering work STORM+ without the assumption that the function value is bounded. Their assumption is also slightly weaker than that of the original STORM.\n- The algorithms seem to have more stable convergence behavior than Adam and STORM+, and the heuristic version has a competitive convergence speed compared with Adam.\n\nWeakness:\n- The choice of parameter $p$ seems to have a large effect on the algorithm\u2019s convergence.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well organized and easy to follow. The analysis (that removes the previous bounded function assumption) is interesting. ",
            "summary_of_the_review": "The paper proposes a fully adaptive optimization method based on STORM and proves its convergence under mild assumptions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4933/Reviewer_XfXs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4933/Reviewer_XfXs"
        ]
    }
]