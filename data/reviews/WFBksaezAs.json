[
    {
        "id": "XpFy-3Bcn1",
        "original": null,
        "number": 1,
        "cdate": 1666529448243,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666529448243,
        "tmdate": 1666529448243,
        "tddate": null,
        "forum": "WFBksaezAs",
        "replyto": "WFBksaezAs",
        "invitation": "ICLR.cc/2023/Conference/Paper2433/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses the multi-source UDA problems by proposing the method Multi-Prompt Alignment (MPA). MPA is built on the pre-trained CLIP which can effectively encode images and texts. Compared to other existing approaches to mutli-source UDA, MPA only needs to train a small number of parameters by prompt learning.\n",
            "strength_and_weaknesses": "\n++ The proposed method based on CLIP can address the multi-source UDA problems by prompt learning which is more efficient during model training.\n++ The proposed method can generalise to unseen domains and the parameters to learn can be further reduced.\n++ The experimental results on three benchmark datasets show competitive performance compared with others. The ablation study also validates the effectiveness of different components of the proposed approach.\n\n-- It seems the threshold for pseudo-label selection is a very important hyper-parameter; the author should discuss how the value affects the performance.\n-- The proposed method uses pre-trained CLIP as the backbone. Since CLIP is pre-trained on a different dataset from other comparative models pre-trained on ImageNet. How to justify the performance gain is not from a more powerful pre-trained model when compared with other multi-source UDA approaches? A simple baseline can be \"source combine\" + \"simple prompt learning\".\n-- The authors claim the proposed approach can generalise to unseen domains by the introduced Latent Subspace Tuning (LST) strategy. Does it mean it can solve the domain generalisation problem? It seems pseudo labels are required in Eq.(10) which means the \"unseen\" domain is actually seen during training, please clarify this.\n-- Although the proposed method aims for multi-source UDA, it seems that it can also solve single-source UDA problems. How it performs when compared with SOTA UDA approaches?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written well; the presentation is clear and easy to understand; the novelty is a little bit weak; the experimental settings are presented and the results may be reproduced based on the details given in the manuscript.",
            "summary_of_the_review": "The proposed approach is based on the pre-trained CLIP and utilises the prompt learning strategy to learn useful prompts for mutli-source UDA. However, the main concern is that it is not justified where the performance gains mainly come from, the proposed approach MPA or the pre-trained CLIP?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2433/Reviewer_iJo5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2433/Reviewer_iJo5"
        ]
    },
    {
        "id": "UFD1FcAVixE",
        "original": null,
        "number": 2,
        "cdate": 1666632710083,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632710083,
        "tmdate": 1666632710083,
        "tddate": null,
        "forum": "WFBksaezAs",
        "replyto": "WFBksaezAs",
        "invitation": "ICLR.cc/2023/Conference/Paper2433/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper explores how CLIP can be used effectively for multi-source domain adaptation. In this problem setting there is a single labeled source domain dataset and multiple target domain datasets that are not labeled. Several methods have been proposed in past to address this problem, but they all have limited accuracy owing the limited abilities of ImageNet trained model (in comparison to CLIP dataset). Authors propose a simple idea to learn prompts using pseudo-labels from CLIP and propose an auto-encoder network to learn a latent space that generalizes to new domains. While not surprising, the proposed CLIP based method outperforms the existing methods. It is interesting to note that ResNet models initialized from the CLIP's image encoder do not perform as well.",
            "strength_and_weaknesses": "**Strengths**\n\n* The paper demonstrates how prompt engineering/learning can be performed in context of multi-source domain adaptation.\n* The paper shows that the the text encoder plays an important role in obtaining higher accuracy.\n* The results on the challenging domain-net dataset is impressive. The proposed method achieves state-of-the-art results with fewer trainable parameters.\n\n**Weaknesses**\n\n* While the paper includes ablation studies on other methods with image encoder, it is not clear (unfair comparison) if this enough to benchmark the proposed methods against the previous method. \n* It is not clear how to go about deciding the hyperparams $M_1$ and $M_2$. 16 seems a little too big for the proposed method.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and is novel in context of multi-source domain adaptation. The paper contains enough details for reproduction.",
            "summary_of_the_review": "Based on the strengths and weaknesses of the paper, I vote to accept the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2433/Reviewer_rNJs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2433/Reviewer_rNJs"
        ]
    },
    {
        "id": "l0L8koVnkKh",
        "original": null,
        "number": 3,
        "cdate": 1666641021031,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641021031,
        "tmdate": 1667691408281,
        "tddate": null,
        "forum": "WFBksaezAs",
        "replyto": "WFBksaezAs",
        "invitation": "ICLR.cc/2023/Conference/Paper2433/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a method based on prompt learning for multi-source domain adaptation. By adopting a pre-trained CLIP based text and image encoders, the authors design prompts that are learnable to adapt them for source and target domains. Specifically, there are two stages. First, prompts are designed with class-specific and domain-specific ones, and are learned for each source and target pairs individually via a contrastive loss. Then, to further align between target prompts, the authors apply the autoencoder-based method for prompt reconstruction, with a loss function to ensure output predictions are similar from different prompts. In addition, the authors also show a finetuning scheme that can adapt to new domains with small finetuning parameters. Experiments are conducted on three benchmark settings, including ImageCLEF, Office-Home, and DomainNet.",
            "strength_and_weaknesses": "**Strength**\n\n* The idea of using prompt learning for domain adaptation is interesting, as prompt learning has the potential for transfer learning in the new domain under a cheaper cost\n* Experimental results show improvements over the CLIP baseline\n* Ablation study shows some gains for the designed modules\n\n**Weakness**\n\nTechnical motivation and novelty\n* Although applying prompt learning for domain adaptation is interesting, the motivation is not very intuitive as prompt learning is based on a highly generalized CLIP model, which already suffers less in the domain adaptation setting. This may raise two questions: 1) whether the experimental comparisons are fair (see comments below), and 2) whether the alignment really happens as CLIP already performs very well in target domains (see comments below).\n\n* The proposed techniques in this paper are not new, e.g., extending the prompt design to multi-domains is straightforward and the loss functions (e.g., contrastive loss) are also not new. More importantly, some designs are not well motivated and lacks experimental validations (discussed below).\n\n* For multi-prompt alignment, it's not clear how the autoencoder is needed to achieve alignment between prompts. In experimental results (Table 5 (a)), performance gain is also marginal.\n\n* It seems the performance of using prompt learning would highly depends on the CLIP model (e.g., Qdr results in Table 2). This should be discussed in the paper.\n\nTechnical clarity\n* For eq(8), it's not clear which prompts are aligned, e.g., between P_i and P_j\n* d_I appears many times in the paper but the definitions are not clear, e.g., eq(6), dimensions in v_tune and d_tune.\n\nExperimental results\n* Since CLIP already has a strong generalization ability, e.g., many results in Table 1 and 2 (Zero-Shot) are already better than other existing DA methods, it's not clear whether the comparisons are fair, as CLIP has been trained with millions of images. While I understand it's not easy to perform experiments using the CLIP backbone with existing methods, the proposed training scheme also cannot validate it's DA ability but only show the transfer learning ability.\n* Prompts are designed with class-specific and domain-specific ones. However, there are no experiments to validate whether it is necessary.\n* Lots of sensitivity experiments are not provided, e.g., length of prompts, thresholds for pseudo-labels, /alpha in eq(9)\n* The explanation of using LST for experiments is not clear (paragraph above Table 3)\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Implementations are not provided\n- Some technical details are not very clear for reproducibility (see above comments)",
            "summary_of_the_review": "Overall, using prompt learning is an interesting direction for domain adaptation. However, there can be issues by using pre-trained CLIP that already has strong generalization ability for DA. In addition, some design choices in the proposed method are not well motivated nor well validated. The authors should consider the above comments and address them carefully in the rebuttal.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2433/Reviewer_xddt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2433/Reviewer_xddt"
        ]
    },
    {
        "id": "sknscmsW5j",
        "original": null,
        "number": 4,
        "cdate": 1666674311955,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674311955,
        "tmdate": 1671013584293,
        "tddate": null,
        "forum": "WFBksaezAs",
        "replyto": "WFBksaezAs",
        "invitation": "ICLR.cc/2023/Conference/Paper2433/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a multi-prompt alignment method for multi-source unsupervised domain adaptation. The main idea is to first learn an individual prompt for each source-target domain pair and then mine the relationships among learned prompts through deriving a shared embedding space. The resulting embedding is expected to be domain-invariant and can be generalize to unseen domains. ",
            "strength_and_weaknesses": "- Strength\n    1. The introduction of prompt learning into multi-source domain adaptation is interesting. \n    2. The performance of the proposed method on MSDA is good compared to previous methods.\n- Weaknesses\n    1. The idea of prompt learning for domain adaptation is not new, and the prompt design in this paper is highly based on the method proposed by Ge et al., 2022. So the main contribution is to extend the prompt learning for domain adaptation from the single source domain to multiple source domains. The novelty is limited.\n    2. How to ensure the two-stage training will achieve the desired solutions? Since the individual prompts are learned to reduce domain shift between each individual source and the target domain while the multi-prompt alignment stage aims to align multiple source domains. The learning objective is changing in the two stages. Is it possible to combine the two stages into a single one to achieve the alignment of all the domains simultaneously?\n    3. Why is the auto-encoder necessary? If the two stages of prompt learning and alignment strategy can be combined into a single stage with multiple objectives for alignment and consistent prediction, is it still require the auto-encoder for reconstruction?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written in general. However, the originality of the work is limited.",
            "summary_of_the_review": "In summary, the overall novelty is limited. Moreover, the design of the model is not very well justified, which requires further clarification.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2433/Reviewer_Zi5U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2433/Reviewer_Zi5U"
        ]
    },
    {
        "id": "4TsJaSoogV",
        "original": null,
        "number": 5,
        "cdate": 1666766098730,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666766098730,
        "tmdate": 1666766098730,
        "tddate": null,
        "forum": "WFBksaezAs",
        "replyto": "WFBksaezAs",
        "invitation": "ICLR.cc/2023/Conference/Paper2433/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces prompt learning to multi-source UDA. A simple two-stage framework is proposed. In the first stage, individual prompts for each source and target pair are learned. In the second stage, Multi-Prompt Alignment (MPA) is proposed to align the learned prompts. The experimental results show the effectiveness of the proposed framework.",
            "strength_and_weaknesses": "Strengths:\n1.\tThis paper is first to apply prompt learning to multi-source UDA problem.\n2.\tThe proposed framework is simple and achieves state-of-the-art results on multi-source UDA benchmark.\n3.\tThe paper is well written, easy to understand.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- What is the difference between stage one and DAPL? Stage one (learning individual prompts) seems to be similar to DAPL. \n\n- The effectiveness of L_CLS loss in stage 2. Table.5 (a) shows the effectiveness of objective function Equation 9. However, the results of L_CLS loss are missed. \n\n- What does \u201cZero\u201d mean in Table.5 (b)? \u201cZero auto-encoders means we completely discarded the auto-encoder structure\u201d is not clear to me. If the autoencoder structure is discarded, what structure is adopted?\n\n- The similarity between the reconstructed prompts. In Table.4, the reconstructed prompts of different domains achieve almost the same results on the target domain. Does this mean that these reconstructed prompts are the same?",
            "summary_of_the_review": "Overall, this is a good submission with strong results. I hope the author could provide more detailed explanations. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2433/Reviewer_iJht"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2433/Reviewer_iJht"
        ]
    }
]