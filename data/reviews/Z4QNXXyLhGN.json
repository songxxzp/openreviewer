[
    {
        "id": "uwW1JzsgN9F",
        "original": null,
        "number": 1,
        "cdate": 1666647473980,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647473980,
        "tmdate": 1670639314118,
        "tddate": null,
        "forum": "Z4QNXXyLhGN",
        "replyto": "Z4QNXXyLhGN",
        "invitation": "ICLR.cc/2023/Conference/Paper2046/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes FedDA, which is a federated learning algorithm that uses adaptive gradient and local steps. Different from the previous work, this paper supports the clients to run the adaptive gradient locally and update the global model using dual averaging - average the gradient information in the dual space. Theoretical analysis shows the effectiveness of the method, and numerical experiments are presented to corroborate the theoretical findings.",
            "strength_and_weaknesses": "Strength:\n\n1. Introduce a new federated learning algorithm that combines adaptive gradient and local steps\n2. The proposed algorithm supports the clients to use adaptive gradient because it averages the gradient information in the dual space\n3. The theory looks good, since it has the $1/T$ communication round.\n4. Numerical experiments corroborate the theoretical findings.\n\nWeakness:\n\n1. For theory, it needs the bounded dissimilarity assumption, which is a huge drawback for federated learning scenarios but it is acceptable for the analysis of adaptive gradient methods.\n2. In the numerical experiment, current discussion cannot conclude that FEMNIST experiment is heterogeneous. It is not clear how you split the data to client: if you have 500 clients and each client has the data of a single person, then maybe you can say it is heterogeneous. If you only has 10 clients and each client random get 50 people's data, then maybe it is not heterogeneous enough.\n3. To me, more ablation studies/numerical experiments on the effectiveness of the local steps are needed, e.g., compare the difference between $I=1$ and $I=5$ and $I=50$ etc.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nFrom my perspective, it is a little bit hard to understand the details of FedDA, especially because there is a difference between the global update (Line 9) and the equation (2): the $\\eta$ and $v$ disappear and are included in the dual difference $z$. I suggest the author show an example that sets the number of local steps to $I=1$ and reduces FedDA to an adaptive algorithm w/o local step.\n\nAll other parts are clear and well-written.\n\nQuality: Good\n\nNovelty: Neutral to Good\n\nReproducibility: Good",
            "summary_of_the_review": "Although there are some weaknesses, I still think that the contribution is enough and I would like to weakly accept this paper.\n\nAfter rebuttal:\nI still vote for weak accept of this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2046/Reviewer_M1w3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2046/Reviewer_M1w3"
        ]
    },
    {
        "id": "ms6emNrNUR",
        "original": null,
        "number": 2,
        "cdate": 1666736873534,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666736873534,
        "tmdate": 1668789222244,
        "tddate": null,
        "forum": "Z4QNXXyLhGN",
        "replyto": "Z4QNXXyLhGN",
        "invitation": "ICLR.cc/2023/Conference/Paper2046/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This papers proposes a meta algorithm for the federated learning optimization problem with heterogeneous data. This algorithm uses adaptive learning rates. The authors view the adaptive methods from the point of view of mirror descent: the proposed algorithm ressembles a Federated mirror descent algorithm where the clients perform several steps and the central server aggregates the dual vectors. The proposed algorithm has some flexibility in the way the gradients are computed. When the gradient are variance reduced and some momentum is introduced (algo FedDA-MVR), the authors show a gradient complexity 1/eps^1.5 and communication complexity 1/eps for the resulting adaptive method to find an eps stationary point.",
            "strength_and_weaknesses": "Strength\n\n- While this is a theory paper, the authors provide numerical evidence for the effectiveness of FedDA-MVR in a ML context\n\n-The paper is clearly written and fits with ICLR in terms of topic (adaptive gradient methods for FL) and format (one algo, one clear theorem with a clear contribution and some numerical exp)\n\n-I didn't check the proofs, but the main theorem and its Lyapunov approach seem reasonable.\n\nWeaknesses\n\n-Why do we need to assume the variance of the gradient bounded if we use variance reduction later on?\n\n-It seems that the theory only cover the case of minimization over a compact set. \n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality:\n\nThe paper is well written. There are (too) detailed explanations of the meta algorithm and explanations of the FedDA-MVR. It was a bit difficult to understand the relationship between mirror descent and adaptive method in the context of FL, but that minor. Maybe the authors could give the algo first and then explain that taking a particular matrix H recovers adaptive gradient methods.\n\nQ: What do you mean by constraint problem? The fact that the optimization problem is over a compact set?\n\n\nNovelty:\n\nThe paper provides SOTA complexity result for adaptive gradient methods in the context of FL. I think that this significant and somewhat new (some adaptive methods for FL were proposed see Table 1). \n\nReproducibility: No issue",
            "summary_of_the_review": "Good theory contribution which fits well with ICLR and the practice of Federated Learning",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2046/Reviewer_pitV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2046/Reviewer_pitV"
        ]
    },
    {
        "id": "gsPwKBpKQl",
        "original": null,
        "number": 3,
        "cdate": 1666820180129,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666820180129,
        "tmdate": 1666820422071,
        "tddate": null,
        "forum": "Z4QNXXyLhGN",
        "replyto": "Z4QNXXyLhGN",
        "invitation": "ICLR.cc/2023/Conference/Paper2046/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel framework of adaptive gradient methods for Federated Learning. By incorporating momentum-based variance reduction techniques, the proposed method is able to achieve acceleration in computation complexity $O(\\epsilon^{-1.5})$ and communication complexity $O(\\epsilon^{-1})$. Experiments are provided to demonstrate the acceleration in practice. ",
            "strength_and_weaknesses": "Strength: \n- Theoretical strong\n\nWeakness \n- Lack of explanation on theoretical analysis \n- The experimental section can be improved, for example, include non iid experiments ",
            "clarity,_quality,_novelty_and_reproducibility": "The literature are throughly covered, the writing is technical which makes reader harder to digest the result. ",
            "summary_of_the_review": "The paper proposes a novel framework of adaptive gradient methods for Federated Learning. The framework is base on a dual averaging step along with Bregman proximal gradient step. By setting the Bregman distance with a quadratic mirror map, the proposed framework enables adaptive stepsizes where AdaGrad and Adam can be retrieved. The main contribution of the paper is to combine a momentum based variance reduction step in the local update to achieve faster computational and communication complexity. \n\n**My main concern is how exactly is the theoretical acceleration achieved? As far as I understand, the adaptiveness does not contribute to the theoretical acceleration. Moreover, I don't think the algorithm has the \"momentum-based variance reduction\" component, the equation 5 and 6 are simple average, which is neither extrapolation not variance reduction. Hence I am confused how the acceleration is achieved.**\n\nFirst, from Theorem 5.1, the analysis only requires the adaptive matrix is larger than $\\rho Id$. In other words, the acceleration does not come from the adaptive step. This is also commonly known in standard stochastic optimization literature where adaptive method has better practical performance but no theoretical improvement. Hence the theoretical acceleration is not coming from the adaptiveness. \n\nSecond, usually acceleration is achieved by extrapolation. In equation 5 and 6 where the paper claims as momentum-based variance\nreduction update, it is a simple weighted average, it is neither variance reduction nor momentum-based, see for example [1] for an example algorithm with variance reduction and dual averaging. Hence I don't fully understand how the acceleration is achieved. Please clarify this point, if possible provide some details within the simplest unconstraint case with adaptive matrix being constant i.e. $\\rho Id$. \n\n[1] Song et al, Variance Reduction via Accelerated Dual Averaging for Finite-Sum Optimization\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2046/Reviewer_u8LZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2046/Reviewer_u8LZ"
        ]
    },
    {
        "id": "9rNGrPnPHzL",
        "original": null,
        "number": 4,
        "cdate": 1667519985172,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667519985172,
        "tmdate": 1667519985172,
        "tddate": null,
        "forum": "Z4QNXXyLhGN",
        "replyto": "Z4QNXXyLhGN",
        "invitation": "ICLR.cc/2023/Conference/Paper2046/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies how to apply adaptive gradient methods into federated learning. Although there are many existing works in this direction, this paper brings up a very novel view. A key problem in adaptive gradients in FL is that there are too many ways to combine the model parameters and local optimizer states at different clients. The authors mentioned that most previous works do not make sense or have obvious drawbacks if we view adaptive methods as a mirror gradient descent algorithm. Instead, in this paper, the authors proposed a more theoretically meaningful framework to aggregate primal (i.e. model parameter) and dual states (i.e. gradients) separately. Under this framework, they provide convergence analysis to an instantiation with momentum-based variance reduction. Experiments on some classification datasets validate the performance of the proposed algorithm.",
            "strength_and_weaknesses": "Strength\n1. I really enjoy reading the introduction. I think this paper brings a very novel and important view to adaptive federated learning optimization algorithms. This provide meaningful guidance on how to combine different local optimizer states.\n2. The theoretical results are solid. It is not easy to derive the convergence guarantee for adaptive methods with momentum-based variance reduction. I appreciate the authors' efforts.\n\nWeakness\n1. While I acknowledge the theoretical contributions of this paper, I'm not very convinced that the proposed method is better than existing much simpler ones.\n    - First of all, the comparisons in figure 2 are not fair. In FedDA, clients use local momentum based variance reduction. But for other methods, clients just perform local SGD (even without local momentum for FedAdam). So it is really unclear whether the performance improvements came from momentum-based variance reduction or the new adaptive optimization framework. Although the authors presented MIME-MVR, I think this is not enough. A more convincing way is to just remove the VR part at clients and compare it with all other methods. I found these results in Appendix and actually if we compare figure 5 option 2-1/2 with other methods in figure 2, the improvements due to a better adaptive optimization framework is very marginal and even worse than FedCM. Also, this new framework comes with additional price. Compared to FedAdam, the clients need to download 3x more parameters and upload 2x more. This additional communication overhead does not appear in the experiments. Given the marginal accuracy improvement and much more additional communication, I'm not convinced that this is a better algorithm.\n    - Similar to the above point, if you want to show this is a better adaptive optimization framework, then you need to analyze it without any VR and compare it with FedAvg. Analysis of FedMVR is valuable, but it should be treated as an extension.\n    - Also, the authors missed a very simple baseline [1], where clients just run any adaptive optimizer from scratch at each round, and the server averages accumulated model changes to uses it in a server-side adaptive optimizer. This simple solution does not introduce any additional communication costs and can work with any optimizer (although not theoretically). But it is worth to discuss and compare with it to see whether the proposed method is unnecessarily complicated.\n\n[1] Wang et al. ICML 2021 workshop. \"Local Adaptivity in Federated Learning: Convergence and Consistency\"",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, especially the introduction. The theoretical results are solid.",
            "summary_of_the_review": "I acknowledge that this paper makes some contribution in getting a better understanding on the adaptive federated optimization methods. But I am not convinced that the proposed framework is better than previous works, given the fact that without MVR, it only provides marginal improvements but costs 2-3x more communication. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2046/Reviewer_7JLR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2046/Reviewer_7JLR"
        ]
    },
    {
        "id": "jxqNXCvvsEN",
        "original": null,
        "number": 5,
        "cdate": 1667677022754,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667677022754,
        "tmdate": 1669608565456,
        "tddate": null,
        "forum": "Z4QNXXyLhGN",
        "replyto": "Z4QNXXyLhGN",
        "invitation": "ICLR.cc/2023/Conference/Paper2046/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors have considered the standard problem of federated learning (FL) and proposed an adaptive gradient method to solve the problem. The paper is written and theoretical convergence analysis is provided for the proposed approach. The experimental results are also mentioned to further support the claims. My main comments are provided below. ",
            "strength_and_weaknesses": "*Strengths*\n\nThe paper is well written. The theoretical analysis is well done and experimental results are detailed. \n\n*Weakness*\n\n- My main concern is regarding the motivation. If the eventual convergence rate is similar to other wxisting works, then what is the additional benefit to consider this approach?  The experimental gain alone is also not significant enough. And that too coming at the cost of added computatinal complexity at the device to perform argmin operation. \n\n- Also, it seems the comparison to FedAvg is also not fair because the proposed algorithm requires to share some gradient information with the server, which is not needed in FedAvg. How to incorporate that fact during the comparisons? Also, isn't that against the spirit of federated learning in general to share the gradient information with the server? \n\n- The dual state is not defined before it's use in Line 4 in the description of Algorithm 4. \n\n- Is it true that via $\\nu$, the information of global gradient is getting used for the local updates at each client? If yes, this needs to be discussed in detail. \n\n- In the proposed algorithm, because it is assumed that we can share the estimated gradients with the server, then what if we evaluate the average of these local gradients at the server, and then run the update at the server.  What is the additional benefit of running local gradient updates, except for the linear speedup?\n\n- ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. ",
            "summary_of_the_review": "Please refer to the weakness mentioned above. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2046/Reviewer_rANd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2046/Reviewer_rANd"
        ]
    }
]