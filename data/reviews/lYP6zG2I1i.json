[
    {
        "id": "y_-LGzhbOfw",
        "original": null,
        "number": 1,
        "cdate": 1666639320320,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639320320,
        "tmdate": 1666639320320,
        "tddate": null,
        "forum": "lYP6zG2I1i",
        "replyto": "lYP6zG2I1i",
        "invitation": "ICLR.cc/2023/Conference/Paper4866/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a Crossword Puzzle engine for searching the optimal pruning metrics automatically. It first gives three properties of a good metric/coordinate: centralized distribution,  retraining recovers centralized distribution and central collapse. Then, it introduces a metrics called LKL-CD  for measuring the centralization degree. The paper finally presents their coordinate obtained from the engine and empirically evaluates the effectiveness of the coordinate system for pruning.",
            "strength_and_weaknesses": "Strengths\n\n\u2022\tThe paper gives a comprehensive introduction to previous work on pruning metrics categorized in 3 different groups, magnitude, impact, and distribution-based methods. These build the metrics blocks for search.\n\u2022\tThe paper introduces the Fabulous Coordinate with three properties, a new perspective of understanding the model redundancies.\n\nWeaknesses\n\n\u2022\tThere is little explanation on why the proposed LKL-CD is better than other metrics. The paper attempts to explain it through Table 1, the four layers in ResNet-50. It might not be statistically enough to claim the advantage of it using one type of network. More validation should be done either empirically or theoretically. It would be better to see that comparisons between them are included in the experiment section.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper needs clarifications in a number of places. \n\n\u2022\tHow should one understand the second requirements of the Fabulous Coordinates, that retraining recovers centralized distribution? This seems to contradict to the third requirement (central collapse). In the Figure 7 pipeline, a coordinate goes through retraining recovery verification first, then to central collapse inspector. These two procedures seem to do similar things, i.e., pruning, retraining, and then test distribution. How can a coordinate gives recovered center in the first procedure, while gives collapsed center in the second procedure?\n\n\u2022\t In Section 3.1, the paper proposes the LKL-CD measurements. Table 1 highlights the value of it. Why does a lower value indicate a better measurement?\n\n\u2022\tIn Figure 4, how can one conclude that the degradation of centralization is more apparent for the proposed pruning method? From the figure, the LKL-CD value of the method is higher than the base method.\n\n\u2022\tThere is no information on how the pruning metric blocks are build. For example, in Section 4, the search engine can jump out of the given intuitive rule when trigged by the search guider, while it is unclear how the search guider behaves. The only description for it is the acceptance probability.\n\n\u2022\tFollowing the previous question, the description for other parts of the pipeline is also ambiguous. It is strongly suggested that the paper should include more details of them. The implementation code is not included, which makes the reviewer harder to understand each part.\n",
            "summary_of_the_review": "The paper proposes novel perspective of pruning. Intensive experiment is done to search better coordinate systems to guide the pruning procedure. However, some key points need further and clearer explanation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4866/Reviewer_bFZJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4866/Reviewer_bFZJ"
        ]
    },
    {
        "id": "Mki5xzfO0p",
        "original": null,
        "number": 2,
        "cdate": 1666665044064,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665044064,
        "tmdate": 1666665044064,
        "tddate": null,
        "forum": "lYP6zG2I1i",
        "replyto": "lYP6zG2I1i",
        "invitation": "ICLR.cc/2023/Conference/Paper4866/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Existing pruning approaches rely on intuition- or experience-based measures to determine the constraint based on which to prune neural networks. Instead, the current work proposes a framework, Crossword Puzzle, to guide the search of a pruning criterion given basic building blocks and evaluate the quality of the pruning criterion when applied on networks. Specifically, the proposed framework is based on the Fabulous coordinates, which indicate that when the distribution from a provided model match the constraints set for these coordinates, they are highly qualified to provide good pruning outcomes.",
            "strength_and_weaknesses": "Strengths\n- The proposed work offers an interesting idea on measuring the quality of a proposed pruning metric.\n- Combined with an almost meta-learning-like framework on identifying a high quality pruning metric, automation of this domain is certainly important and brings pruning a step closer to being part of the development cycle of DNNs.\n\nWeaknesses\n- Could the authors provide more quantitative bounds when mentioning \"relatively-high pruning rate\" in Pg. 1, footnote.\n- When highlighting the improvement in acceleration, could the authors clarify whether the \"3x\" improvement is in theoretical FLOPs or real-time inference latency?\n- I would encourage the authors to look closely into the literature discussing distribution-based pruning. Apart from the papers mentioned in the current manuscript, there are broad swathes of work on probabilistic pruning that can and should be referenced.\n- Could the authors provide an intuition for the formulation of the LKL-CD, including how they solve for the final values?\n- Across multiple instances there are mix ups between the terminology of SFN and FSN (E.g., Fig. 2, Fig. 3 and others). Please correct them.\n- The LKL-CD of Raw Weights in Table 1 provide a much better approximation than the chosen SFN formulation, assuming lower values are better. Could the authors justify the selection of SFN?\n- Could the authors clarify the meaning of the notation $C_W$?\n- I encourage the authors to increase the font of legends and Axes ticks for readability.\n- Quantifiably, could the authors provide metrics used to measure the collapse as well as highlight how exactly they were applied to measure the values provided in Fig. 5?\n- In addition, could the authors provide more comparisons on the recovery process of the central hole across the various conv layers?\n- From Tables 2 and 3,\n     - Across VGG16 and Table 3, SFN provides better accuracy at a lower % of FLOPs removed when compared to previous approaches.\n     - Could the authors clarify their choice in prior art and comparisons? Since there are a number of previous works with higher performances that haven't been included.\n     - In addition, could the authors include the decrease in number of parameters as well, since it provides a slightly different perspective on the degree to which a network can be pruned.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\nThe explanation of the method and important observations are good. However, certain portions of the measures and evaluation scheme is slightly obscure.\n \nQuality and Novelty\nThe measure and overall search framework offer a requisite level of novelty and quality to the proposed work. However, a deeper evaluation is necessary to highlight these ideas.",
            "summary_of_the_review": "While the core ideas are relatively straightforward, the evaluation and explanation of certain aspects of the work are obscure. Addressing the weaknesses mentioned above will bring more clarity to the results and content of the proposed work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4866/Reviewer_TNSn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4866/Reviewer_TNSn"
        ]
    },
    {
        "id": "0r_S4F-16Eg",
        "original": null,
        "number": 3,
        "cdate": 1666670906862,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670906862,
        "tmdate": 1666670906862,
        "tddate": null,
        "forum": "lYP6zG2I1i",
        "replyto": "lYP6zG2I1i",
        "invitation": "ICLR.cc/2023/Conference/Paper4866/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors proposed a so-called Crossword Puzzle method to find the optimal criteria for network pruning. The key idea is to find a so-called Fabulous Coordinate which satisfies three key properties for pruning. The authors validated their method on ImageNet and show that they can compress ResNet family by 50% without accuracy degradation.",
            "strength_and_weaknesses": "[Weakness]\n\nThe paper is poorly written. It looks like the paper is generated by some Google translation engine with lots of meaningless words. I tried my best but honestly speaking, I cannot understnad what the authors are writing about. In Section 3, the authors throwed up lots of formulars and figures without sufficient explanation. I have no clew what these numbers or equations mean.",
            "clarity,_quality,_novelty_and_reproducibility": "* Poor clarity, unable to read\n* Not sure about novelty because it is difficult to get useful information from the paper.",
            "summary_of_the_review": "Strong reject.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4866/Reviewer_jFro"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4866/Reviewer_jFro"
        ]
    },
    {
        "id": "e9yXrq60bCy",
        "original": null,
        "number": 4,
        "cdate": 1666676104968,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676104968,
        "tmdate": 1666676104968,
        "tddate": null,
        "forum": "lYP6zG2I1i",
        "replyto": "lYP6zG2I1i",
        "invitation": "ICLR.cc/2023/Conference/Paper4866/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes the Cross Puzzle to find something called the Fabulous Coordinate for pruning neural networks. ",
            "strength_and_weaknesses": "The paper is very difficult to read. Though experiments are performed, the experiments are not up to the quality expected. For example, there is no direct comparison with The Lottery Ticket Hypothesis paper (https://arxiv.org/pdf/1803.03635.pdf), which can reduce network size by over 90%. \n\nTerms are not clearly defined. I cannot find clear definitions of the Cross Puzzle, the Fabulous Coordinate or the Fabulous Distribution. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very difficult to read. \n\nA partial list of edits: \nedits: \np. 2:\n\"cutting off by pruning\" --> \"cut off by pruning\"\n\"nearly none redundancy \" --> \"nearly no redundancy\" \n\"intrigues lots of follow-up works\" --> \"inspired many follow-up works\"\n\"amount of efforts\" --> \"amount of effort\"\n\"We refer impact-based pruning to methods for ... .\" --> \"We refer to methods for ... as impact-based pruning.\"\n\"coarse-estimated second-order derivative\" --> \"a coarsely-estimated second-order derivative\"?\np. 3:\n\" leverages empirical fisher matrix\" --> \" leverages the empirical Fisher matrix\"\n\"constrains\" --> \"constraints\"\n\", and etc\" --> delete this.\n\"aware of/\" --> delete this.\n\"should obeys Gaussian Distribution\" --> unclear. Maybe: \"should approximate a Gaussian Distribution\"?\n\"fast narrow the neural network\" --> unclear. Maybe \"first narrow the neural network\"?\n\"in Bayesian DNN \" --> \"in a Bayesian DNN \"\n\"two-folded\" --> \"two-fold\"\n\"as handstuned issues\" --> unclear. Maybe \"for hand-tuning\".\n\"introduces new observation point\" --> \"introduces a new observation point\"\n\"distribution itself is not the protagonist\" --> unclear\n\"\u200b\u200bto parameter distribution\" --> \"to parameter distributions\"?\np. 4: \n\"weights in the form Laplace \" --> unclear. Is the suggestion that the weights are distributed approximately as a Laplace distribution? \n\"neuron as\" --> \"neurons as\"?\n \"represent each others\" --> \"represent each others\"\n\"coordinate psi\" --> Should this be \"parameter psi\"?\np. 5: \n\"don\u2019t fit our requirement\" --> it is unclear to me what the requirement is. \nFigure 1 and 2: vertical axis would be better labeled \"Count\" than \"Counting\"\n\n\"Crossword Puzzle search engine\" --> unclear to me what this is\n",
            "summary_of_the_review": "Overall, this paper id not ready for publication.\nThe key idea is not clearly communicated, the terms are not well defined, and there are not experiments that support the key idea. \n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4866/Reviewer_bVnh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4866/Reviewer_bVnh"
        ]
    },
    {
        "id": "Da78YfRQB-Z",
        "original": null,
        "number": 5,
        "cdate": 1666903539477,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666903539477,
        "tmdate": 1668849448126,
        "tddate": null,
        "forum": "lYP6zG2I1i",
        "replyto": "lYP6zG2I1i",
        "invitation": "ICLR.cc/2023/Conference/Paper4866/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new method called  \"The Cross(word) Puzzle\" that helps in getting better pruning metrics to further improve pruned neural networks in accuracy for the same compute footprint. This is informed by what the authors call Fabulous Coordinates (the choice of pruning criteria) and Fabulous distribution (the target distribution of weights after pruning). \n\nThe paper starts of by motivating what the useful properties of a good pruning function should look like and leverages it to build a search pipeline for semi-automating it. They evaluate the method with a couple of experiments on ImageNet and CIFAR-10\n\n\nThe brevity of the review is due to the fact that the paper is extremely incoherent and it took me over 5 hrs to digest the information. The paper is extremely poorly written, with no solid definitions, inconsistent naming (SFN vs FSN vs SNF -- what is this?), poor naming (Fabulous?), lack of details of the methods, no signs of reproducibility, and inconsistent experimentation and design motivations. \n\n\n",
            "strength_and_weaknesses": "Strength:\n1) The problem paper tries to solve is real and has real-world utility.\n\nWeakness:\n1) The abstract is extremely uninformative, so is the introduction.\n2) There are terms used without any explanation or pointers to -- what is a coordinate? is something it too me a long to process\n3) What is fabulous about the distribution?\n4) The motivation for the design principles come from unstructured pruning and the paper experiments on structured pruning, I am not sure how that works. I know the connections between both styles exist, but connecting Han et al 2015 to the current paper does not gel well. \n5) I have no idea how the candidates for coordinates are even generated for search, the entire pipeline is so under-explained at required places and over-exposited at places with little impact on the entire method. \n6) Table 1 is extremely hard to comprehend. \n7) definitions of equations follow the same suit.\n8) Figure 5 shows your method is more inaccurate than Han et al., I do not understand. \n9) The experiments are performed on limited evaluations and at 50% structured sparsity which is not even considered to be sparse because of the known redundancies. The usual comparisons happen at over 70% for ImageNet. \n10) FLOPS is not directly proportional to sparsity due to non-uniformity across layers, so that metric needs to be fixed. \n11) While the figure are slightly accessible, they are difficult to read due to poor placement and design. ",
            "clarity,_quality,_novelty_and_reproducibility": "Poor presentation and quality. While novelty exists in the motivation, it is executed in an unscientific fashion. It seems like a paper thrown all together at the last moment and I do not recommend publishing of the paper even after significant registrations.",
            "summary_of_the_review": "Poor presentation and quality. While novelty exists in the motivation, it is executed in an unscientific fashion. It seems like a paper thrown all together at the last moment and I do not recommend publishing of the paper even after significant registrations.\n\n--------------------\nPost Rebuttal: Given the need for a significant revision, I recommend rejecting the paper from ICLR 2023 but encourage the authors to make the material more accessible to readers in the next iteration.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4866/Reviewer_uiqw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4866/Reviewer_uiqw"
        ]
    }
]