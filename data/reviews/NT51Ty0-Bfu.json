[
    {
        "id": "FeHlNpJy9bn",
        "original": null,
        "number": 1,
        "cdate": 1666214022051,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666214022051,
        "tmdate": 1670336957357,
        "tddate": null,
        "forum": "NT51Ty0-Bfu",
        "replyto": "NT51Ty0-Bfu",
        "invitation": "ICLR.cc/2023/Conference/Paper2096/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes offline RL algorithms with DP guarantees for tabular case and linear MDP. The key components are the private estimates of the visitation counts/conditional variance based on the Gaussian Mechanism, with some modifications to allow the use of existing pessimism-based value iteration algorithms. The paper provides privacy guarantees and utility guarantees for the proposed algorithms, and an empirical study to support the theoretical findings. ",
            "strength_and_weaknesses": "**Strength**\n- The paper proposes a new offline RL algorithm with DP, which is claimed to be the first DP RL algorithm in the offline setting. I think the paper can have significant contribution to the offline RL community. \n- The paper in general is clearly-written and easy to follow. \n- The theoretical analysis is sound (although I acknowledge that I only check the proof for tabular case, and didn\u2019t check the proof for linear MDP carefully).   \n\n**Weaknesses**\n- The definition of DP for the RL setting used in the paper is not well-motivated (see comments below).\n- The presentation of the key components in Algorithm 1 and Algorithm 2 is not very clear to me (see the next part on quality and clarity).\n- Major improvement is required for the empirical study.\n\n**DP Definition**  \nThe paper defines neighboring datasets as two datasets that differ in one trajectory. However, I can think of other definitions in the RL setting. For example, two datasets that differ in one transition, or differ in one observed reward (if we want to preserve the privacy for reward only). The paper should provide more motivation for this particular definition used here, and discuss how existing work define DP in the RL setting. \n\n**Empirical study**  \nI think the empirical study requires some improvements. The empirical study is supposed to support the theocratical findings, but I think the current results raise more concerns.  \n\nFirst of all, the results do not have error bars, so I don\u2019t know whether the difference are statistically significant. Please include the error bar in the results. Moreover, the paper mentions\n> as the size of dataset goes larger, the performance of DP-VAPVI will converge to that of VAPVI\n\nI don\u2019t see the performance of DP-VAPVI converges to the non-private one in Figure 1. Maybe try to run the experiments with more episodes. Otherwise, I think this observation is not true. \n\nFinally, please explain how significant the difference in performance between the private and non-private ones are. For example, non-private gets a suboptimality gap ~= 1, while the private one with $\\rho = 5$ gets a gap ~= 2. Is the difference (i.e., 1) significant? What is the range here? What value of $\\rho$ is commonly used in practice?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is clearly-written, but some improvements are needed. \n\nThe presentation of the key components can be improved. For example, it takes me some time to understand the optimization problem in (3), so I think the paper would be better if some descriptions are provided to explain what is happening in (3) and how is $E_P$ chosen (a short sentence suffices). I think this is the non-trivial part to combine common DP mechanisms in pessimism-based VI algorithm, so it is definitely worth spending some space to explain it more clearly. \n \nI have some other questions:\n> Our algorithms involve substantial technical innovation over previous works on online DP-RL with joint DP guarantee. \n\nFor the tabular case, is the main novelty how you obtain private visitation counts that satisfies $n\u2019_{s,a} = \\sum_{s\u2019} n\u2019_{s,a,s\u2019}$? For the linear MDP case, the proposed algorithm seems like a straightforward application of Gaussian mechanism in the plug-in method. Am I missing something here? \n\n> Assumption 2.1 is the weakest assumption needed for accurately learning the optimal value.\n\n\"weakest\" in what sense? Do you have any citation for that? \n\nFor Algorithm 2, you add a noise with variance of the order of $H^4$. I am wondering if this can be an issue in environments with a higher horizon (e.g., 100 or 1000). Also where does the quadruple dependence come from? Is it improvable? \n",
            "summary_of_the_review": "In summary, I am giving a weak accept recommendation, conditional on the empirical section can be improved as suggested. I think the paper can have a significant contribution to the offline RL community as a first step towards more practical DP offline RL algorithms, however, some improvements are required as mentioned in my comments on weakness and clarity. I would be happy to raise my score if the concerns are addressed. \n\n------Post-rebuttal update------  \nAfter the author response and internal discussions with other reviewers and AC, I think my concerns about the presentation clarity and the empirical study have not been fully addressed. I think these issues are not difficult to address, however, the authors did not upload a revision. Therefore, I think the paper requires some improvements before being published, and I slightly lowered my score to 5. However, I encourage the authors to keep improving the paper and re-submit to a future conference. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2096/Reviewer_MbLZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2096/Reviewer_MbLZ"
        ]
    },
    {
        "id": "QSZL9UQo_WN",
        "original": null,
        "number": 2,
        "cdate": 1666466844732,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666466844732,
        "tmdate": 1669250336139,
        "tddate": null,
        "forum": "NT51Ty0-Bfu",
        "replyto": "NT51Ty0-Bfu",
        "invitation": "ICLR.cc/2023/Conference/Paper2096/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes two privacy-preserved (i.e., DP Guaranteed) versions of existing offline RL algorithms (i.e., APVI and VAPVI) proposed in previous works . DP-APVI  is for tabular settings and  DP-VAPVI  for the case with linear function approximation (under linear MDP assumption). The authors provide theoretical analysis and also simulation results to confirm their theoretical analysis and show that their DP Guaranteed algorithms come with little drop in utility. ",
            "strength_and_weaknesses": "Strength:\n* Addresses an important problem in offline RL\n* The paper is well organized, theoretically grounded\n\nWeakness:\n* Lack of empirical evaluation such as experimentations on benchmark tasks such as D4RL. Additionally is there any reason that there is no simulation results for DP-APVI algorithm ? \n* Incremental Novelty. Would be nice to distinguish more clearly the novelty of this paper from previous offline works (i.e., VAPVI).\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Quality: In the interest of time, I did not check the proof. But, according to a sketch of the analysis, it sounds correct.\n* Clarity: Yes, it is clear\n* Novelty: Incremental novelty over previous offline works by adding DP. It adds  to some extent  to the knowledge of the community.\n* Reproducibility: Not clear to me, since source code and dataset were not available in supplementary materials. ",
            "summary_of_the_review": "Overall, the paper is well motivated, aiming to solve an important problem in the area of offline reinforcement learning. The work is theoretically well grounded, and well organized. As said the novelty is low, and I think it lacks enough experimental evaluation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2096/Reviewer_txBL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2096/Reviewer_txBL"
        ]
    },
    {
        "id": "Y615QHk8ED9",
        "original": null,
        "number": 3,
        "cdate": 1667203277149,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667203277149,
        "tmdate": 1667203277149,
        "tddate": null,
        "forum": "NT51Ty0-Bfu",
        "replyto": "NT51Ty0-Bfu",
        "invitation": "ICLR.cc/2023/Conference/Paper2096/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper provides an algorithm for Differentially-Private offline reinforcement learning, for the tabular and linear MDP settings.",
            "strength_and_weaknesses": "Strengths\n- the problem is interesting and has good motivation:  protection of sensitive information in the trajectories saved in offline reinforcement learning, and this is especially important in healthcare and financial contexts.\n\nWeaknesses\n1.  The paper is not clearly written.  Objects and variables are used without proper notation or introduction.  For instance I'm assuming $n$ is the number of trajectories in the dataset, but this doesn't seem to be stated.  What is $\\phi(s,a)$, used in algorithm 2?\nAlso, the theorems can be stated more clearly.  For instance Theorem 3.1 has parentheses with phrases that distract from the reading of the theorem - this should be re-written.\n2.  It's not obvious how the noise value $\\sigma^2$ is derived for Algorithm 1.  Shouldn't the l2 sensitivity of the counts be $Hn\\sqrt{n}$, and so $\\sigma^2= H^2n^3/(2\\rho)$ ?\n3.  Too much information is relegated to the appendix, to the extent that it isn't easy to the read the paper without reading the appendix.",
            "clarity,_quality,_novelty_and_reproducibility": "Originality:  the work is original in that this particular setting for differential privacy has not been considered before.  The algorithms themselves are heavily based on previous work of Yin and Want 2021b.  \nClarity:  As mentioned above the paper lacks in clarity.",
            "summary_of_the_review": "While the topic of the paper is important, has good motivation, and is novel, the paper is very difficult to read and may contain errors.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2096/Reviewer_g4wC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2096/Reviewer_g4wC"
        ]
    }
]