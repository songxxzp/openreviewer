[
    {
        "id": "81mlr2DW08T",
        "original": null,
        "number": 1,
        "cdate": 1666519830037,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666519830037,
        "tmdate": 1666593165145,
        "tddate": null,
        "forum": "c0U6KmokuFK",
        "replyto": "c0U6KmokuFK",
        "invitation": "ICLR.cc/2023/Conference/Paper622/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a probabilistic graph generator that models the distribution over good architectures using GNNs. This converts the neural architectures to a learnable computational graph. Extensive experiments indicate the effectiveness and efficiency of the proposed method. This paper is generally well-written, and the idea is interesting. ",
            "strength_and_weaknesses": "Strength:\n1. The idea of considering the NAS as GNN is interesting.\n2. The paper is well-organized and easy to follow. \n\n\nWeaknesses:\n1. The experiment does not show the efficiency of the method. Would you consider adding computational complexity analysis or time consumption?\n2. I don't quite understand why this method is less effective than other methods when the number of architectures evaluation is small. It suddenly got better as the number increased \uff08Figure 2). Can you explain?\n3. In addition, the authors do not conduct experiments on the most common DARTS space, making it hard to conduct comparisons with most existing SOTA NAS methods. I understand that the DARTS space is an old benchmark and not easy to get significant improvement, while I am still interested in the performance of the proposed method in this space.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper proposed a clear framework, and the idea is easy to follow. The extensive experimental results on several benchmark datasets demonstrated the effectiveness of the proposed method. However, the lacking of open-source codes may limit reproducibility. ",
            "summary_of_the_review": "This paper is generally well-written, and the idea is novel. I'd like to increase the score if the authors can resolve the above concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper622/Reviewer_QTZh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper622/Reviewer_QTZh"
        ]
    },
    {
        "id": "bBZ1om23I7Y",
        "original": null,
        "number": 2,
        "cdate": 1666542380877,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666542380877,
        "tmdate": 1666542380877,
        "tddate": null,
        "forum": "c0U6KmokuFK",
        "replyto": "c0U6KmokuFK",
        "invitation": "ICLR.cc/2023/Conference/Paper622/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed to use GNN as the architecture controller in reinforcement learning based NAS.",
            "strength_and_weaknesses": "Strength:\n1. Some novelty as the first paper to propose using GNN-based generative modeling for generating architectures in NAS.\n2. Improved results compared to an RNN-based generator and several baselines that are quite old though.\n\nWeakness:\n1. I feel the cited papers and compared baselines are quite out-of-date. Cited works are mostly before 2020. While I am not well literatured in NAS, I do know there were multiple NAS methods proposed in 2021/2022. Maybe most of them are differentiable NAS methods but some comparisons should be made.\n2. Can evaluation be performed on the larger ImageNet and the newer NAS-bench-201 datasets? I feel results on the smaller and older datasets, without many more recent baselines, are less convincing.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. Not reproducible as code is not provided and it is not trivial to implement the proposed method.",
            "summary_of_the_review": "An interesting paper that first proposed using GNN as architecture controller, but need to be benchmarked with more recent methods and on more datasets.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper622/Reviewer_kb9c"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper622/Reviewer_kb9c"
        ]
    },
    {
        "id": "FXs1n9qlImd",
        "original": null,
        "number": 3,
        "cdate": 1666575433376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666575433376,
        "tmdate": 1666575433376,
        "tddate": null,
        "forum": "c0U6KmokuFK",
        "replyto": "c0U6KmokuFK",
        "invitation": "ICLR.cc/2023/Conference/Paper622/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a GNN-based neural architecture method that can model the distribution of well-performing architectures. With a reinforcement learning formulation, the paper claims that the learned GNN-generator is more flexible and efficient than existing work.",
            "strength_and_weaknesses": "### Strengths\n1. The paper proposes a GNN-based graph generator for NAS, which is an interesting exploration.\n2. The paper applies the method to extensive experiments and shows state-of-the-art results on multiple benchmarks.\n3. The paper also introduces a larger search space and proves the efficiency of the method against the existing work.\n\n\n### Weaknesses\n1. More discussions of how the neural architecture is encoded in a GNN model would be appreciated. For example, figure 4 in the appendix should be in the main paper and illustrated.\n2. The evaluation of the method is applied to smaller datasets (Tiny-Imagenet, which is not common in the previous work. It would be more interesting to see how the method performs on large-scale real applications.\n3. It would be interesting to see different constraints for the NAS method, such as latency constraints of the neural architecture.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear, the quality is good, and it is an interesting application of GNN.",
            "summary_of_the_review": "Please refer to above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper622/Reviewer_DyTs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper622/Reviewer_DyTs"
        ]
    },
    {
        "id": "-vN_qQoXYA",
        "original": null,
        "number": 4,
        "cdate": 1666681918065,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681918065,
        "tmdate": 1666681918065,
        "tddate": null,
        "forum": "c0U6KmokuFK",
        "replyto": "c0U6KmokuFK",
        "invitation": "ICLR.cc/2023/Conference/Paper622/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper propose a NAS algorithm named GraphPNAS, which uses a graph generator instead of classical RNN generator to sample architectures from the space. RL algorithm is used to update the generator. Experiments on several different search space show the effectiveness of the method.",
            "strength_and_weaknesses": "Pros:\n1.\tThe paper is clear written and easy to follow.\n2.\tThe Figure3 show the architecture space model by the proposed generator is better than classical RNN controller.\n3.\tThe authors use several different search spaces on different datasets to demonstrate model effectiveness.\n\nCons:\n1.\tThe literature review is not sufficient. There are lots of papers use graph generators for NAS[1~3]. So this point is not new. The authors should explain how their model is more brilliant than these methods.\n2.\tThe graph generator model designs are borrowed from previous works. There are few contributions in this part.\n3.\tMore NAS baselines should be compared in the experiment part. The authors should also show why the architecture space model by the proposed graph generator is better than previous works.\n\n\n[1]Zhang, Miao, et al. \"Differentiable neural architecture search in equivalent space with exploration enhancement.\" Advances in Neural Information Processing Systems 33 (2020): 13341-13351.\n[2]Shi, Han, et al. \"Bridging the gap between sample-based and one-shot neural architecture search with bonas.\" Advances in Neural Information Processing Systems 33 (2020): 1808-1819.\n[3]Ru, Robin, Pedro Esperanca, and Fabio Maria Carlucci. \"Neural architecture generator optimization.\" Advances in Neural Information Processing Systems 33 (2020): 12057-12069.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: fair\nNovelty: low\nReproducibility: good\n",
            "summary_of_the_review": "This paper propose a NAS algorithm named GraphPNAS, which uses a graph generator instead of classical RNN generator to sample architectures from the space. However, the contributions are limited on model design. Furthermore, extend explanations and experiments are needed to support the method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper622/Reviewer_8CC8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper622/Reviewer_8CC8"
        ]
    }
]