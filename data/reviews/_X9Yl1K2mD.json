[
    {
        "id": "8For8AHl2-",
        "original": null,
        "number": 1,
        "cdate": 1666387973586,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666387973586,
        "tmdate": 1666387973586,
        "tddate": null,
        "forum": "_X9Yl1K2mD",
        "replyto": "_X9Yl1K2mD",
        "invitation": "ICLR.cc/2023/Conference/Paper274/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors proposed a new method (Rotamer Density Estimator): a flow-based generative model to estimate the probability distribution of conformation. The proposed Rotamer Density Estimator was evaluated on mutational effects for ddG (by using entropy of the probability distribution as the measure of flexibility). ",
            "strength_and_weaknesses": "Strengths:\n+ Interesting approach for modeling the probability distribution of conformations with flow models. \n+ The problem of modeling the distribution of conformation itself is of wide interest. (I would encourage the author to also explore more applications.)\n+ Empirical results are based on a common benchmark dataset (SKEMPI).\n+ Relatively clear presentation of the method and the results.\n\nWeaknesses:\n+ On the mutation effect prediction task evaluated in the paper, several machine learning baselines are missing (e.g. GVP, ESM-IF1).\n+ Misleading bolding in the tables (when the proposed method is not the best, the best method is not bolded)\n+ No confidence intervals for Table 1 and Table 2\n+ Does the training data (crystallography) faithfully capture the distribution of conformations? Empirical proof or theoretical reasoning would be helpful.\n+ Is there any rigorous biophysical justification for the entropy correlating with ddG? ",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: Comparable (but not significantly improved?) empirical results to the state of the art for mutation effect prediction. The SKEMPI ddG prediction results lack some machine learning baselines (e.g. GVP, ESM-IF1).\n\nClarity: Two lingering questions: 1) Is there any rigorous biophysical justification for the entropy correlating with ddG? 2) Does the training data (crystallography) faithfully capture the distribution of conformations?\n\nOriginality: New approach to do density estimation for protein conformations.\n\n",
            "summary_of_the_review": "Original approach for density estimation of protein conformations with limited improvement in empirical results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper274/Reviewer_NQyq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper274/Reviewer_NQyq"
        ]
    },
    {
        "id": "sX2B9SS0Ub",
        "original": null,
        "number": 2,
        "cdate": 1666653145214,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653145214,
        "tmdate": 1670554961067,
        "tddate": null,
        "forum": "_X9Yl1K2mD",
        "replyto": "_X9Yl1K2mD",
        "invitation": "ICLR.cc/2023/Conference/Paper274/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on quantifying the effects of mutations on protein-protein interactions. It does so by first building a (conditional) normalizing flow model to estimate the probability of sidechain conformations (rotamers) conditioning on the type, position, orientation, and prior rotamer of itself and other amino acids. Building on the intuition that higher binding affinity between two proteins implies higher rigidity (lower entropy), it then estimates the change in binding free energy due to the mutation as the change in delta entropy (bound - unbound) between mutated sequence and wild type. Experiments on the SKEMPI2 dataset illustrate the strengths of the method over current baselines.",
            "strength_and_weaknesses": "**Strengths**\n- Very clear write up, with a well-thought-through experimental design\n- From a methodology standpoint, the unsupervised modeling approach via RDE avoids potential issues due to label scarcity / bias. Furthermore, the choice of normalizing flow allows efficient sampling of rotamers when estimating the different entropy terms since it enables exact likelihood estimation\n- The method achieves very strong performance relative to other baselines, without relying on structure data as an input\n\n**Weaknesses**\n- While it seems intuitively sensible, the claim that sequence-based models fail to predict \u2206\u2206G for protein-protein binding does not seem particularly well backed-up experimentally. To achieve good performance with ESM-1v, one must both: a) use an ensemble of 5 ESM1v transformers b) fine-tune each model on a set of homologous sequences. When both conditions are not met (and it seems it is not the case here), a single non fine-tuned ESM-1v performs worse than a site independent model (see Tables 1 and 2 of [1], or table 2 of [2]). More performing baselines include Tranception [2], EVE [3] or MSA Transformer [4].\n- Table 1: it seems that DDGPred does best in terms of both RMSE and MAE (bolding issue?) \n\n----------------------------------------------------------------------------------------\n[1] Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.\n\n[2] Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.\n\n[3] Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.\n\n[4] Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n- The paper is well written and structured.\n- Related work is adequately referenced, and proper background is provided to the reader to understand the work\n\n**Quality**\n- Sound experiment design: comprehensive set of performance metrics; limited overlap between folds to mitigate data leakage between train and test sets\n- Strong empirical results relative to baselines\n- Some claims not fully substantiated (see above)\n\n**Novelty**\n- The use of normalizing flow for exact rotamer density estimates is novel  \n\n**Reproducibility**\n- Code not provided at submission. Authors confirmed in appendix A.4 that all data and code will be made available once the paper is made public\n",
            "summary_of_the_review": "A compelling approach to quantify the effects of mutations in PPI, which does not rely on labels for rotamer density estimates and does not need structure input to make predictions. The approach achieves strong empirical performance relative to baselines. I would be willing to recommend acceptance more enthusiastically if the concerns discussed above are addressed during rebuttal. \n\n------------------------------------------------------------------------------------------------------------\n[UPDATE POST REBUTTAL]\nAuthors' responses adequately addressed my feedback during rebuttal. Updating my score accordingly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper274/Reviewer_GvK9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper274/Reviewer_GvK9"
        ]
    },
    {
        "id": "8lqJPZD69x-",
        "original": null,
        "number": 3,
        "cdate": 1667317586069,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667317586069,
        "tmdate": 1667317586069,
        "tddate": null,
        "forum": "_X9Yl1K2mD",
        "replyto": "_X9Yl1K2mD",
        "invitation": "ICLR.cc/2023/Conference/Paper274/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper develops a flow-based generative approach to model the probability distribution of side-chain conformations (called RDE) in protein-protein interactions and then estimates changes in entropy by measuring the changes in side-chain flexibility. The paper uses a conditional generative model built upon normalizing flows for estimating densities. It uses a neural network to extract delta delta G from the RDE representation.",
            "strength_and_weaknesses": "Strength.\n1- The paper tackles an important problem. Predicting protein-protein interactions in a low-data regime is a very important problem.\n\n2- No direct homology and evolutionary information is required in the model and thus the method is applicable to find the effect of mutations on protein-protein interactions with a lack of homology information. \n\n3- Empirical results in Table 1 are convincing (although I have some questions about the baselines).\n\n4- The flow-based generative model works on exact likelihood.\n\n5- Using side-chain conformational change as a proxy for delta delta G seems to be novel.\n\nWeakness.\n1- The approach only takes into account flexibility in the side chains and not the backbone.\n\n2- If mutations cause change beyond side-chain conformational variations it is not clear how the method can handle those mutations.\n\n3- The significance of the contribution to the ML community is limited. The work tackles a specific problem in protein-protein interactions and mostly uses a combination of standard DL modules (flow based generative models, MLP, attention with rotation/translation invariance) as building blocks. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper start with a very clear introduction but then since the method has multiple building blocks it becomes harder to follow. \n\nI have some questions for the authors: \n\n1- Are there any protein overlaps between X-ray structures in PDB used for training the rotamer density estimator and SKEMPI2 database? How can we make sure there is no data leakage?\n\n2- Are the ESM-1v and other baselines fine tuned on the same training data used in RDE?\n\n3- Can the authors elaborate on the two mutations in Table 2 that do not rank above top-10%? Why does the algorithm fail in those cases?\n\n4- Is there a regime where DDGPred is more favorable compared to RDE?\n\n5- Results in Table. 3 show statistical significance however the correlation coefficients are still not high between RDE and entropy. Is rank correlation a better measure?\n",
            "summary_of_the_review": "Overall the paper tackles an important problem and some of the empirical results are very string however in terms of methodology it is not a significant contribution to machine learning.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper274/Reviewer_R6Cf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper274/Reviewer_R6Cf"
        ]
    }
]