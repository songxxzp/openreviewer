[
    {
        "id": "XVFf3rXbweW",
        "original": null,
        "number": 1,
        "cdate": 1665782280486,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665782280486,
        "tmdate": 1665782280486,
        "tddate": null,
        "forum": "8tYRqb05pVn",
        "replyto": "8tYRqb05pVn",
        "invitation": "ICLR.cc/2023/Conference/Paper5799/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies to what extent models trained on image-caption-only data can be frozen, passed through a linear projection, and then provided to a generative LM to generate captions for images. This builds off some earlier models/ideas like Frozen (Tsimpoukelli et al 2021) and Magma (Eichenberg et al 2021),  except here, only one linear projection is learned -- mapping an image encoder to the vocabulary space of a language model. In contrast, Frozen finetunes the whole image encoder, and Magma uses adapter layers to minimally finetune the image encoder.\n\nThe paper studies how well various image encoder learning paradigms can be used here -- BEIT (which was trained only on image patches), imagenet supervised training, and CLIP. When CLIP is plugged in here, CLIP performs well across the board on VQA and image captioning; even outperforming the authors' tuned implementation of Magma in a 0shot setting on VQA.\n\nThe paper dives further and probes how well concepts transfer (e.g. nouns, modifier, relations on COCO; perceptual concepts from Animals with Attributes). ",
            "strength_and_weaknesses": "Strengths:\n* This paper makes a simple yet important extension to the Magma / Frozen works -- adding the simplest possible adaption layer from an image-only encoder to a language model. The simplicity also means that there are plenty of opportunities to probe what the underlying image encoder learns during pretraining. For that reason, I think this paper could be informative to researchers who work in a variety of different communities. The model also could be a great baseline for further vision-language models.\n* The analysis about what fine-grained concepts are learned by the various models seems very detailed and interesting to this reviewer. The methodology could be useful for other works that want to probe these models, and to perhaps make them better.\n\nWeaknesses:\n* To this reviewer, the paper could be improved if it considered different model sizes (e.g. larger or smaller CLIP models or supervised resnets or BEITs). An interesting question is whether the probing results get better or plateau with model size (of both the encoder and perhaps the decoder; which here is GPT-J (6B parameters).\n\nMiscellaneous: I think there is something weird with the font in this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Seems clear and novel to this reviewer.",
            "summary_of_the_review": "Overall, to this reviewer, this paper seems impactful and I'd vote to accept it -- would still be curious as to how well models of different sizes do under this evaluation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5799/Reviewer_N4fG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5799/Reviewer_N4fG"
        ]
    },
    {
        "id": "qTY5u4kDXy",
        "original": null,
        "number": 2,
        "cdate": 1666607657178,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666607657178,
        "tmdate": 1666607657178,
        "tddate": null,
        "forum": "8tYRqb05pVn",
        "replyto": "8tYRqb05pVn",
        "invitation": "ICLR.cc/2023/Conference/Paper5799/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a simple idea to test a clearly-defined hypothesis: whether representations learned by a vision model (trained on images only vs. trained on images including limited degrees of linguistic supervision) are functionally equivalent to those learned by a language model (up to a linear transformation). The authors report on different experiments and show that the degree of linguistic supervision used in the (frozen) image encoder matters, and the more linguistic supervision leads to better results on captioning and VQA. In turn, this is interpreted as the extent to which visual representations are functionally equivalent to textual representations (up to the linear transformation).",
            "strength_and_weaknesses": "Strengths: \n- Paper has a clear hypothesis and story.\n- Paper's results are compelling and substantiated by well-thought experiments, which cover the important sources of variation that affect the hypothesis laid out.\n- Good balance between empiricial decisions and theories to support those decisions.\n\n\nWeaknesses:\n- None that I can think of.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and clear, and I find the experiments to be appropriate to answer the hypothesis. Reproducibility is addressed by sharing code to reproduce experiments, and in terms of novelty the paper contributes a small data point to a series of works in frozen / soft prompts in vision and language models. Even though it is a small data point, I still find it valuable.",
            "summary_of_the_review": "Comments and questions:\n\n- Figure 4 has no number.\n\n- Why did you not have a BEIT-Large model random initialised as a baseline? That would have added much information to the NFRN50 random baseline, since it would allow you to compare both BEIT-Large and BEIT-Large random in a similar fashion to what you did with NFRN50.\n\n- I suggest clearly defining what is meant by conceptual/visual properties vs. lexical categories already at the introduction. You state early on in the paper that you find that all three models distinguish the former, but the more textual supervision models have during pretraining, the better models distinguish the latter. It would be clarifying to have one- or two-sentence explanations of what is meant by these properties/categories, perhaps including one or more examples of each.\n\n- Page 9: \"By looking at failure cases for each model, we can establish whether each model is predicting the presence of a similar animal or not. In Figure 4a, we show that when captions generated from each model mistake one animal for another, the mistaken animals are highly similar to the ground truth animal when measuring both\nWu-Palmer similarity (Averages: BEIT: 0.8, NFRN50: 0.81, CLIP: 0.8) and overlap of AWA properties (Averages: BEIT: 0.62, NFRN50: 0.68, CLIP: 0.59)\" -> There is an implicit 'if' statement in this part of the text that troubles me. You say that \"when captions mistake one animal for another\" something happens. But how often do models make a mistake that is of a different nature, e.g., generate an spurious caption? Does BEIT do that more often than NFRN50 and CLIP? I would like to see these numbers.\n\nTypos and presentation:\n\n- \"k is determined by the architecture of [the] E\"\n\n- In the Appendix, some of your Tables should be resized.\n\n- Page 19: \"All probes are [trianed]\"\n\n- The last Figure in your Appendix has no number.\n\n- In your appendix, you include many figures. Perhaps it may be a good idea to try to be more concise?\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5799/Reviewer_kxWt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5799/Reviewer_kxWt"
        ]
    },
    {
        "id": "GSeuAkMH9h",
        "original": null,
        "number": 3,
        "cdate": 1667117947356,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667117947356,
        "tmdate": 1671735936072,
        "tddate": null,
        "forum": "8tYRqb05pVn",
        "replyto": "8tYRqb05pVn",
        "invitation": "ICLR.cc/2023/Conference/Paper5799/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies whether learning a linear layer is good enough to map image features to text space. Three different image encoders are tested and it's found CLIP performs best on tasks requiring fine-grained category visual information.",
            "strength_and_weaknesses": "Strength:\n1. How to bridge pre-trained image models and pre-trained LM is quite an interesting and intriguing problem. And it can enable desirable downstream tasks such as few/zero-shot VQA utilizing stored knowledge from both worlds. \n2. This paper verified a strong hypothesis: training only a linear layer is enough for mapping visual pre-trained knowledge to text space.\n3. In experiments, this paper goes through recent popular visual pre-trained models and brings a relatively comprehensive analysis.\n \nWeakness:\n1. A major concern is the novelty of the paper. As admitted in this paper, Frozen also adds and tunes a linear mapping layer to process image encoder output features for LM input. The only technical difference against Frozen is that this paper doesn't update visual encoders. It's a good finding but not enough for publication at ICLR.\n2. Performance vs MAGMA. In Tab1, when reporting image captioning results, only MAGMA(released) and MAGMA(ours) are compared while MAGMA(reported) is missing. If compared with MAGMA(reported), the proposed method in this paper would perform worse in NoCaps on four metrics. On the other hand, for VQA, the best result of MAGMA is from RN50x4. But in this paper, author(s) compares results on RN50x16 and the proposed method still underperforms on 1-2-4 shot VQA. \n3. It's already found in other works that CLIP is better than other non-text-guided visual pre-trained models in language/multimodal understanding [1]. What this paper found echoes similar findings before.\n\nRef:\n[1] Shen, Sheng, et al. \"How Much Can CLIP Benefit Vision-and-Language Tasks?.\" ICLR 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is good. The code should be easy to reproduce and the analysis/experiments are solid. But the novelty is limited based on the weaknesses I mentioned above.",
            "summary_of_the_review": "Overall, I lean toward rejection because of limited novelty and low performance.\n\n`After Rebuttal:`\n\nI agree that it's an interesting finding that people should know. But it doesn't surprise me because as I said, many papers have explored this direction, and this finding shares similar intuition as [Shen, et al.]. Overall, I would raise my score to weak reject but I am also okay with accepting it.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5799/Reviewer_s1Pu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5799/Reviewer_s1Pu"
        ]
    },
    {
        "id": "pCDOik936v2",
        "original": null,
        "number": 4,
        "cdate": 1667192149139,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667192149139,
        "tmdate": 1667192149139,
        "tddate": null,
        "forum": "8tYRqb05pVn",
        "replyto": "8tYRqb05pVn",
        "invitation": "ICLR.cc/2023/Conference/Paper5799/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors proposed to learn a linear mapping from vision embedding space to text embedding space of a pretrained language model. The authors provided in-depth analysis and comparison between vision-only pretrained models and (text-)supervised vision pretrained models. The effectiveness is shown through baseline comparisons.",
            "strength_and_weaknesses": "Pros:\n1. The idea of linear transforming is interesting.\n2. The comparisons between BEIT and other (text-)supervised models are interesting.\nCons:\n1. Significantly lack of discussion with relevant work. These works are highly relevant and even somewhat similar to the proposed method. Therefore, it is necessary to provide more comprehensive literature review to clarify the distinction with these work.\na. [r1,r2,r3,r4] reports a line of alternative method of leveraging pretrained (text-)supervised models for visual recognition: directly using text words to represent the information in visual input, which is a highly relevant baseline to the proposed method: they all use very few parameters or even no new parameters to bridge vision and text.\nb. [r5] reports the possibility of reading out the visual information from any pretrained vision only models by tuning a text encoder with the image model fixed through contrastive learning.\n\n2. The core assumption \"structural similarity between the two spaces\" is not directly tested. This could be checked by simply compare the vision similarity graph of objects from a few categories and the text similarity graph of the text embeddings of those categories.\n\n3. The authors assumed the linearity of the transformation from vision space to textual space, which is certainly not the case in the real world as the reviewer believe simple contradictions could be found: especially when there are non-visual correlation between two words, the distance between vision space could be non-correlated with the distance in text space. For example, blue and a depressed person. \nTherefore, the review takes the proposed idea as an approximation of the real world. But the author didn't provide any proper-upper bound for this.\na. For example, the linear transforming could be progressively added to a MLP. \nb. Tuned MAGMA is not considered as a proper upper-bound because this is essentially a trade-off between non-linearity and the number of examples that could be used to train the non-linear mapping.\nc. MAGMA also reports results on more datasets, which should be ideally also provided for the proposed method.\n\n\n\nMinor:\nFigure 5, the font size and font seem to be inconsistent.\n\n[r1] VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs, CVPR 2021\n\n[r2] Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language\n\n[r3] Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning\n\n[r4] Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners\n\n[r5] LiT : Zero-Shot Transfer with Locked-image text Tuning",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, quality and reproducibility look overall good. The novelty is rather limited.",
            "summary_of_the_review": "Overall, the reviewer thinks the analysis provided in this paper could be possibly helpful for the community but currently several cons need to be resolved towards a publication at ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5799/Reviewer_Sxvp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5799/Reviewer_Sxvp"
        ]
    }
]