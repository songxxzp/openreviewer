[
    {
        "id": "UbxbTCYcII",
        "original": null,
        "number": 1,
        "cdate": 1666589325535,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589325535,
        "tmdate": 1670578664110,
        "tddate": null,
        "forum": "8XfHh4XSQ0Q",
        "replyto": "8XfHh4XSQ0Q",
        "invitation": "ICLR.cc/2023/Conference/Paper6490/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper attempts to improve knowledge distillation (KD) research from the perspective of allocating adaptive weighting factors to update layer-wise learnable parameters of the student model during back-propagation. The authors conjecture that the knowledge from the teacher  should be different to the student layers from shallow to deep. Based on this, they propose a new KD method called adaptive block-wise learning (ABL), which uses auxiliary networks for learning to dynamically generate meta weighting factors for gradients propagation at different blocks of the student network. In designs, these auxiliary networks are added to intermediate student layers, and leverage the local error signals to approximate the global error signals on student objectives. In implementation, ABL uses a greedy bi-level (two-stage) optimization. The effectiveness of ABL is validated on image classification datasets CIFAR100 and ImageNet.     ",
            "strength_and_weaknesses": "Strengths.\n\n+ Making the knowledge from teacher to layers of the student network to be different is interesting.\n\n+ The proposed ABL sometimes shows improvements (but actually are not fair) to different KD methods.  \n\n+ Experimental comparisons are conducted on both CIFAR100 and ImageNet datasets with different teacher-student network pairs.\n\nWeaknesses.\n\n- The method.\n\nThe core ideas of the proposed ABL are in two aspects: 1) adding auxiliary networks to some intermediate student layers to ease the training; 2) in optimization, auxiliary networks are used to approximate the global error signals from the pre-trained teacher, learning to dynamically generate meta weighting factors for gradients propagation at different blocks of the student network. However, there already exist many KD works that explore the use of auxiliary networks to improve KD process. Some representative works are DKS [1], BYOT [2], DCM [3] , MetaDistiller [4], to name a few. Besides, MetaDistiller and some other works also explore the use of meta learning. Unfortunately, these works are completely missed by the authors. A comprehensive comparison of ABL with them, both in methodology and performance is necessary. \n\n[1] Deeply-Supervised Knowledge Synergy, CVPR 2019.\n\n[2] Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation, ICCV 2019.\n\n[3] Knowledge Transfer via Dense Cross-layer Mutual-distillation, ECCV 2020. \n\n[4] MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation, ECCV 2020.\n\n- The motivation.\n\nIn the paper, the authors argue that different layers of the student network have different emphases on the knowledge learned through the one-hot labels and the knowledge distilled by the teacher. Furthermore, the authors claim a new contribution that the deep and abstract representation inclines to learn from student knowledge, while the shallow and less abstract representation tends to be guided by teacher knowledge. However, the analysis and experiments provided in the paper are not convincing enough. Could the authors provide more evidence or richer experiments to support these claims? E.g., how about the role of auxiliary networks? how to design them? the effects of  different auxiliary classifiers? how about the performance of adding low-quality/shallow auxiliary networks? what will happen if smoothing the predication of the pre-trained teacher model?\n\n- The experiments.\n\nExperimental comparisons are problematic and misleading: (1) For experimental comparisons (Table 1 and Table 2) on CIFAR100, it seems that all results for counterpart baseline methods (denoted as 'Stan.') are directly copied from the paper of CRD (mostly), and the paper of DKD. As a result, the authors merely tested the combination of ABL and each of them, but not tested each corresponding counterpart with the same training machine and training code settings. As a result, all reported \\delta values are totally misleading; (2) For experimental comparisons (Table 3 and Table 4) on ImageNet, the authors also use such an unfair comparison; (3) This even applies to some ablations. In a nutshell, the authors did not actually run any experiments for counterpart baseline methods (denoted as 'Stan.') at all. Therefore, main experiments need to re-design and re-implement. Even w.r.t. the current results, the improvement from ABL is mostly marginal.\n\nA comprehensive comparison of ABL with closely related methods such as DKS [1], BYOT [2], DCM [3] and MetaDistiller [4] is necessary. \n\nAs auxiliary classifiers play a key role in ABL, a deep analysis of them is also necessary, please see my comments in 'The motivation' for details. \n\nHow about the training cost of ABL compared to counterpart baselines methods?\n\n**----Update----**\n\nI keep my original score as my major concerns are not well addressed.",
            "clarity,_quality,_novelty_and_reproducibility": "The basic ideas of this paper are easy to understand, but the presentation is not good enough. This paper has serious issues in novelty, experiments and claims. Code is not provided.\n\nPlease refer to my comments in 'Strength And Weaknesses' for details.",
            "summary_of_the_review": "This paper is below the acceptance bar of ICLR.\n\nPlease refer to my comments in 'Strength And Weaknesses' for details.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6490/Reviewer_Tp5S"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6490/Reviewer_Tp5S"
        ]
    },
    {
        "id": "-_k8rkUhkj",
        "original": null,
        "number": 2,
        "cdate": 1666665318087,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665318087,
        "tmdate": 1666665318087,
        "tddate": null,
        "forum": "8XfHh4XSQ0Q",
        "replyto": "8XfHh4XSQ0Q",
        "invitation": "ICLR.cc/2023/Conference/Paper6490/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes Adaptive Block-wise Learning, a method that adapts the amount of knowledge distillation from a teacher to a student at each layer of the network. A set of auxiliary networks is used for this purpose. The method uses local error signals to control how much teacher or student knowledge should each layer use. This is accompanied by a set of metavariables to control these contributions, which are optimized using bilevel optimization. The authors report experimental results on several SOTA knowledge distillation methods, showing that this more selective knowledge distillation strategy can benefit both homogeneous and heterogeneous knowledge distillation problems.",
            "strength_and_weaknesses": "Strengths:\n- The paper presents a novel idea of using local errors to determine the contribution of teacher and student networks during knowledge distillation.\n- The paper is well written and well organised. \n- The paper reports relevant findings for the knowledge distillation area, in particular that deep blocks of the network benefit more from student knowledge, while shallow blocks of the network benefit from more teacher knowledge.\n\nWeaknesses:\n- Some terms are used that are never defined. This makes the paper difficult to understand for a researcher not in this area. For example, what are homogeneous and heterogeneous knowledge distillation? Please provide a definition or cite a relevant definition.\n- In the experiments, it is never mentioned how big is the teacher and how big is the student for each of the datasets. Furthermore, how relevant is this for the proposed method? \n- I would like to see how the findings regarding the contribution of teacher and student at different layers in the knowledge distillation setting are connected to previous similar findings in transfer learning in general (see for example [1])\n\nReferences: \n[1] Neyshabur, B., Sedghi, H., & Zhang, C. (2020). What is being transferred in transfer learning?. Advances in neural information processing systems, 33, 512-523.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and appears technically sound. The quality of the proposed method and the experimental results is high. Although local error signals have been studied in the past, the proposed approach seems novel, although I am not fully familiar with knowledge distillation literature. The reproducibility is enough for a researcher in the area.",
            "summary_of_the_review": "The paper presents a method for knowledge distillation that considers the contribution of teacher and students distinctly at different layers of the network. This approach seems novel and useful for the area. The conclusions around the different degrees of contribution of teacher and students at different depths of the network seem to be an important contribution (although I am not fully aware of the knowledge distillation literature, so I may be convinced otherwise based on other reviews). Based on this, my recommendation at this stage is for acceptance of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6490/Reviewer_XFGM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6490/Reviewer_XFGM"
        ]
    },
    {
        "id": "yRNWIiAtvki",
        "original": null,
        "number": 3,
        "cdate": 1667051613785,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667051613785,
        "tmdate": 1669336363916,
        "tddate": null,
        "forum": "8XfHh4XSQ0Q",
        "replyto": "8XfHh4XSQ0Q",
        "invitation": "ICLR.cc/2023/Conference/Paper6490/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors argue that contributions of knowledge from the teacher to the student network should be layer dependent.  Adaptive block-wise learning automatically balances the contribution of knowledge between the student and teacher for each block.\n",
            "strength_and_weaknesses": "The paper talks in generalities and employs a general abstract notation. There is no experimental specificity.\n",
            "clarity,_quality,_novelty_and_reproducibility": "There is minimal novelty and no technical depth to the paper.  \n\n+ There is no description of the teacher, student or auxiliary network. \n+ It would have been nice if the loss functions $\\mathcal L_S$ and ${\\mathcal L}_{KD} $ could have been defined. \n+ There is no description of how the data was divided between training and validation. \n+ What were the $\\gamma_1^{(l)}$ and $\\gamma_2^{(l)}$ set to in the experiments? \n\n+ There was no analysis of the experimental improvements.  Are the improvements statistically significant?\n",
            "summary_of_the_review": "The paper is incrementally novel, but it has extensive experiments.\nThe paper talks in generalities without any technical or experimental details.  A below average ML paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6490/Reviewer_Jk2C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6490/Reviewer_Jk2C"
        ]
    },
    {
        "id": "VsfhfQTzCG",
        "original": null,
        "number": 4,
        "cdate": 1667230673655,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667230673655,
        "tmdate": 1667230673655,
        "tddate": null,
        "forum": "8XfHh4XSQ0Q",
        "replyto": "8XfHh4XSQ0Q",
        "invitation": "ICLR.cc/2023/Conference/Paper6490/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The manuscript observes the problem of fixed contributions of ground truth knowledge and teacher knowledge at different blocks of the student networks during knowledge distillation training. The author proposes a bi-level optimization scheme to balance the knowledge on the lower level and update the network based on optimized balance at the higher level.",
            "strength_and_weaknesses": "Strength:\n+ Paper is well written\n+ Experiments on common benchmark datasets for KD\n+ A number of KD schemes are tested with the proposed scheme in the experimental results\n\nWeaknesses:\n1. The problem might not be well justified. The main argument of the paper is to use the performance of the proposed scheme to claim the solving of the fixed contributions of the two types of knowledge in different blocks. It would be more interesting to see what part of the network goes wrong with the fixed contributions scheme. \n\n2. Whether the improvement of the network comes from the auxiliary network? Please consider adding an experiment by removing the bi-level optimization but still keeping all the auxiliary networks. Consider tuning the fixed parameters between ground truth knowledge and teacher knowledge in this scheme. \n\n3. The increase in performance might be the result of solving the gradient vanishing not solving the balance between ground truth information and the teacher at the different blocks. The addition of an auxiliary could also be interpreted as a shortcut to the last layer. Consider checking the gradient vanishing between the standard KD scheme and the proposed KD scheme. Or further, can we just add a shortcut from each layer to the final layer and have different losses corresponding to each shortcut?\n\n4. For some datasets, the utilization of teacher knowledge only could also achieve comparable or even better results compared to the utilization of both teacher knowledge and ground truth knowledge. The authors should consider adding a KD scheme with only teacher knowledge guidance as one of the baselines. So that the scheme of balancing between teacher knowledge and ground truth knowledge would be more meaningful.\n\n5. How does the proposed scheme training time increase compared to the standard KD schemes? The main concern about the bi-level optimization is that it always takes too much time, while the achieved improvement in the manuscript seems to be not too significant (only around 0.5%, and poorer in some cases). This might be the strongest challenge to apply the proposed scheme to practical applications.\n\n6. In table 3, Is that normal where the standard KD performs poorer than supervised training?\n\n7. The authors should details out how each parameter is fixed in standard KD schemes in all experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: good\nNovelty: good",
            "summary_of_the_review": "The paper might be helpful to the research community. However, the concern about training time might reduce the chances of its practical applications. While considering about the fundamental contributions, the authors should justify the problem clearly as well as put more analyses on the proposed schemes. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6490/Reviewer_BeZK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6490/Reviewer_BeZK"
        ]
    }
]