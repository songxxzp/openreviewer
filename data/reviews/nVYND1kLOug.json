[
    {
        "id": "Mt0ZvRP26O0",
        "original": null,
        "number": 1,
        "cdate": 1665947755793,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665947755793,
        "tmdate": 1665947755793,
        "tddate": null,
        "forum": "nVYND1kLOug",
        "replyto": "nVYND1kLOug",
        "invitation": "ICLR.cc/2023/Conference/Paper77/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors introduced Dual Gradient Field (Dual GF) as an offline learning example-based planning. The two gradients are coming from the target task and the support task. The former encourages the movement towards accomplishing the goal, while the latter enforces safety (e.g. avoiding collisions). Offline, two networks are trained to estimate each gradient using collected samples. During runtime, each gradients are calculated and mixed. The result is passed as an input to a heuristic controller that generates the movement. Authors evaluated their approach across three environments comparing with seven learning, planning, and oracle based techniques. ",
            "strength_and_weaknesses": "Strengths \n+ The idea of separating the task from the world limitations is great. The latter can be generalized across various tasks.\n+ The whole idea is simple and easy to implement, although the assumption of having access to the P_tar may not be easy for all cases.\n+ The approach is overall explained well.\n\nWeaknesses\n- Experimentations was the main low light of the paper. Both the methodology and the clarity needs more attention. Regarding the methodology, as part of solution, it is okay to bring the world constraints as soft penalties, yet it is not okay to change the problem based on your solution. If the world does not allow collisions, then merging the quality of colliding and non-colliding trajectories muddies the results, which in turn makes it difficult to judge the effectiveness of your method compared to others. For example, if your path collided in the beginning, it is not meaningful to discuss how close the end of that path was to the target distribution. A better metric to your ACN, is path feasibility: what percentage of generated paths were collision free. Concretely, I recommend focusing on TR (main) and CS (secondary) metrics for collision free paths. If an algorithm fails to find paths it should reflect it in the TR. \n\n- Regarding limitations, while authors discussed the assumption of a simple controller that can move object(s) from s to s', they did not discuss about the situations were the combined gradient may not work well. For example, what if the two gradients cancel each other out and the agent freezes?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was mostly clear up until the experimentation section. This section felt rushed with several unclear points (see details below). I think the idea of extracting gradient for both task and the environment limitations is novel and interesting. \n\nDetails:\n- How did you \"manually designing the reward function\": Can you provide more discussion of why this is hard. For your domains, they did not seem to be difficult. I also could not find the exact reward values for all cases.\n- \"free space, i.e.states without collision\" => Add space after dot\n- \"\u03c0 : S \u2192 S\", Shouldn't the policy output an action rather than the next state? I understand your are using a simple controller but the definition this way hides this assumption.\n- \"Online interactions are costly\": But all of your environments are simulated so simulations are free.\n- \"Futher\" => Further\n- r_t reward and r_{i,j} for collision is confusing\n- \"Heuristic-based Baseline: Oracle: We perturb the target examples via a small Gaussian noise to obtain the terminal states at an oracle level.\" Not clear what you mean.\n- \"Task Reward\" : You meant Task return as it is the cumulative values of rewards. \n- Averaged Collision Number: They definition is confusing, but if I understood it correctly, it means the average number of objects colliding on each step of the plan. To me this is a hard constraint and if violated the rest of plan does not matter. You should filter the results that do not respect this and provide insight into the remaining valid paths based on other metrics. Also r_i,j should have t subscript as well as object are moving through time.\n- In Figure 3, can you keep the X and Y axis values the same across figures (i.e. same algorithm order on X axis and same scale used for Y axis)? It will help readability immensely.\n- Bring images later in the paper. Currently it is distracting that they are all presented before the main text corresponding to them\n- Figure 5, PL row, Clustering: What does it mean to have likelihood numbers more than 1?\n- Why using various metrics? PL specifically is not a great metric, as almost getting to the goal is not the same as getting to the goal. ACN should be zero for good paths. Once you found good paths then you can look at their average return and coverage score.\n- Not sure why you needed ablation studies to remove support gradient as it captures your environment limitation.",
            "summary_of_the_review": "Overall, I think the paper is going in a great direction, but I believe authors should take a step back for their experimentations in order to turn this into an amazing publication. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper77/Reviewer_kjqZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper77/Reviewer_kjqZ"
        ]
    },
    {
        "id": "in3x7Ewkke",
        "original": null,
        "number": 2,
        "cdate": 1666583096294,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583096294,
        "tmdate": 1666584542536,
        "tddate": null,
        "forum": "nVYND1kLOug",
        "replyto": "nVYND1kLOug",
        "invitation": "ICLR.cc/2023/Conference/Paper77/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel path planning approach that learns two gradient fields from two sets of task and support examples, as opposed to learning from whole trajectories or (inverse) reinforcement learning. These gradient fields are mixed at runtime using a heuristic velocity-based controller to generate feasible paths that lead the agent to the goal states in several simulated test scenarios. It is shown that the method outperforms other learning based path planners and is more efficient in finding solutions compared to sampling-based planners.",
            "strength_and_weaknesses": "* The strength of the paper is in showing how the proposed simple solution works across multiple environments, when evaluated with multiple metrics and compared to multiple alternative approaches. Overall the experiments are extensive and ablation studies are included.\n\n* The paper is well written and the approach is introduced clearly. \n\n* As for weaknesses, I was missing some more details on the tasks and the environment details. Some example successful paths generated by the agent could have been included. Likewise, we would have profited from including some failure example paths, in order to appreciate better what the agent can achieve and when, in what circumstances. \n\n* On a related note, we do not know when the introduced can fail from the paper. When it fails, does it fail slightly or terribly? The ablation studies show the effect of initial lambda as one potential source of issues, from which the lambda update seem to recover. \n\n* We know that Lagrangian relaxation used here to solve the problems is not fool-proof at all. Some details on the potential issues would have been more illuminating.\n\n* I did not read the appendix - if some of these 'improvements' of the main text are mentioned there then I'd suggest putting pointers to the relevant sections in the main text.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, clear and seems to be novel with a minor contribution. See above for more details on how to improve the quality. I have also noted below some typos and minor unclear points:\n\n=== MINOR POINTS ===\n\n* remain to require -> require\n* design -> designed in page 1 and 2\n* remove two () in page 2\n* success -> successful in Related Work section (page 3)\n* how do you determine \\epsilon in eq. 1 ?\n* why is log p_sup^{\\sigma} - c is hard to estimate directly?\n* any drawbacks of approximating log p_sup^{\\sigma} with eq. (8). ?\n* Section 5.2, = is extra it seems.\n* \"TarGF is model-based framework\" -> what is the model like for the introduced tasks?\n* Why does PRM need to search a path again when the agent reaches the goal?\n* pg. 9 Ball Rearrangement: single mode? what are the modes like?\n* pg. 9 Ours -> DualGF?\n* Limitations and Future Work: extending the framework for 'bision-based control and the embodied robot\" seems too general.",
            "summary_of_the_review": "Overall I recommend the acceptance of this paper, it is well written and it introduces a path planning approach that seem to improve over several learning-based alternatives introduced recently in the literature. Note the weaknesses mentioned, although the work is evaluated extensively, we're missing the details of failure cases and why the method works well (across multiple environments).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper77/Reviewer_QU9s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper77/Reviewer_QU9s"
        ]
    },
    {
        "id": "OWT-WCT9Hwf",
        "original": null,
        "number": 3,
        "cdate": 1666674687454,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674687454,
        "tmdate": 1667372972538,
        "tddate": null,
        "forum": "nVYND1kLOug",
        "replyto": "nVYND1kLOug",
        "invitation": "ICLR.cc/2023/Conference/Paper77/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an offline learning approach for path planning. The proposed Dual Gradient Fields (DualGF) model the probability distributions of (1) target states (e.g. goal states) and (2) free space. Then, the problem of finding a path toward a target state while avoiding collision becomes simply following the gradients of the mixture of two probability distributions. The experiments show that the proposed method can find a path without access to online interactions and the ground truth model.\n",
            "strength_and_weaknesses": "### Strengths\n\n* The proposed method is simple but effective at finding a collision-free path.\n\n* The experiments show better goal-reaching performance as well as fewer collisions in various 2D environments.\n\n* The experiments show that the proposed method scales well with the number of manipulatable objects or agents.\n\n### Weaknesses\n\n* The core of the proposed method is learning a good target gradient field and a support gradient field. However, learning the dual gradient fields purely relies on the target examples and a dataset of randomly sampled states from the free space. The paper claims that the support examples are easy to collect by randomly initializing objects; however, collecting such data is challenging in more complex environments, such as the real world.\n\n* The proposed method simply infers the target gradient field from a set of target states. The experiments are done with linear state spaces (e.g. x, y coordinates), where estimating the target gradient field from a few target examples is straightforward. However, when the state space is complex and nonlinear, such as pixels and non-smooth state changes, the learned target gradient field may not provide meaningful gradients for path planning. \n\n* Moreover, the experiments are done in toyish environments with relatively large datasets for target states (100,000 states). Although the size of the support example data is not mentioned in the paper, to learn a reasonable support gradient field, it must need data that covers most of the states along potential solution paths. This can limit the scalability of the proposed method to general path planning problems.",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is easy to follow but there are some typos and blanks.\n\n* It would be great if the order of baselines in Figure 5 is consistent across CS and ACN.\n\n* The proposed approach introduces learning target and support gradient fields from examples and finding a path following the mixture of gradients, which is simple and novel.\n\n* As the state and action spaces are complicated, it would be great to provide code for reproducing the results.",
            "summary_of_the_review": "Overall, the proposed method is novel, simple, and effective. However, the experiments are done in path planning in 2D actions, which does not resolve the concerns about learning reasonable support gradient fields and path planning with non-linear state and action spaces. Adding experiments on a complex, non-linear environment would make this paper strong. But, as of now, I would recommend weak rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper77/Reviewer_vd31"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper77/Reviewer_vd31"
        ]
    },
    {
        "id": "8Id7WO9e56",
        "original": null,
        "number": 4,
        "cdate": 1666974504035,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666974504035,
        "tmdate": 1666974504035,
        "tddate": null,
        "forum": "nVYND1kLOug",
        "replyto": "nVYND1kLOug",
        "invitation": "ICLR.cc/2023/Conference/Paper77/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an approach to learn target (goal) and support (obstacle) gradient fields for navigation planning from a set of example configurations. As opposed to many other learning approaches for navigation planning, it does not rely on demonstration trajectories but only on sets of successful target configurations and of freespace configurations. Based on these gradient fields, trained in the form of score networks, a Lagrangian formulation is used to derive a planning algorithm that dynamically mixes the gradient fields to successfully navigate to a target in a new configuration. Studies show that the system can perform well in terms of task success with relatively low computational cost compared to traditional randomized path planners and can address complex re-arranging tasks.\nThe main contribution of the paper lies in presenting an approach that can build a path planner for a task from just a set of example configurations. The studies performed show that this simple approach can be effective at planning for a range of tasks with relatively limited overhead.",
            "strength_and_weaknesses": "The paper presents a new approach to learn path planning from simple examples. A strength here is that as the examples are not trajectories but rather just target and freespace states, the freespace examples often transfer across tasks and can thus be shared for different task objectives in the same types of environments. While this does not apply to the target samples, which need to be specific to the task, they should still be easier to provide to the system than entire trajectories. This reduced effort in providing examples makes the approach interesting for a range of tasks where task objectives can easily be translated into target state examples.\n\nWeaknesses of the paper lie mainly in the lack of discussion of some of the underlying assumptions in the approach and some missing discussions related to relations to other navigation planning work.\nOne implicit assumption that seems to be made (and is not discussed) is that the planning is applied to a holonomic system (i.e. a system with no inherent movement constraints. The reason for this is that in the problem statement section it seems too be assumed that any point in an epsilon ball around the current state is reachable by the system. This assumption makes section 4.3 (the integration with low-level control) problematic as it is not clear that a low-level control for a system with dynamic constraints exists for a step generated by the higher-level planner. It would be important to discuss this.\nA second point that would warrant discussion is the seeming reliance on the right amount of noise in the DSM approach to make the target and support gradient fields work. In the technical description, the target gradient field is described as the gradient of the target state distribution. This, however seems to be very problematic since this gradient would be 0 anywhere outside the target region and thus not produce a gradient towards the target region. The only way to bridge this problem is the gaussian noise which effectively props up the value of the target potential (and thus the target gradient) for non-target points based on there proximity to the target region, basically creating a gradient towards the target region. It would be very useful for the authors to include some discussion regarding the way target gradients in non-target areas are derived (and how the same is achieved for non-freespace regions in the support gradient field). This effect also suggests that the choice of sigma in the DSM algorithm might have significant effect on the performance and a study of the effect of changing sigma would be very useful.\n\nOverall, the approach seems to be similar in character to the mixture of goal and obstacle potential approaches to path planning just that the potentials are learned from samples and that the mixture is dynamically adjusted using the optimization of the Lagrangian formulation. Some discussion of this relation, and in particular a comparison with mixture potential path planning as an additional baseline would be very beneficial. In this context it would also be useful if the authors could compare navigation tasks in more complex environments (multi-room or maze type environments) to study to what degree their methodology to adjust mixture weights (lambda) dynamically overcomes the local minima problem of traditional mixture potential approaches, and to what degree it introduces collision problems due to target gradients overwhelming support gradients near obstacles due to lambda being too small. In this context it might also be beneficial to motivate why only a single step of lambda adjustment is made for each navigation planning step.\n ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well written and clear to understand. However, the paper lacks discussion of some important aspects, including its relation to mixture potential path planning, the effect of noise in the DSM calculation to make the approach work, and the conceptual relation between lambda and collision avoidance abilities of the approach (see more details in the Strength and Weaknesses section).\nThe approach has novelty as it allows learning from just state examples without the need for state samples (without the need for trajectory samples). However, it would be important to put this novelty in the context of the related traditional frameworks (see above).\n\nIn the experimental section, the authors do not describe the environmental settings (which is left to the appendix. In particular in the navigation example, however, it would be important to include some description of the environment complexity in the main paper to allow readers to better assess the significance of the performance results. Another item that is lacking in the experiment section is any description of the size and form of the example set for both the target and support fields. How those examples are derived and how many are sued would be very important information to assess to what degree the system was able to generalize. Similarly, knowing the structure of the score matching network used would be useful to assess generalizing capabilities vs potential overfitting. In a similar way, the similarity measure used for the epsilon ball neighborhood (in particular in terms of the conditional state features - obstacle states) would be useful as this would significantly influence the tradeoff between target and support gradients).\nWhile the paper presents all the technical approach, lack of the parameters makes reproduction of the results difficult to achieve.\n\nThe paper also contains a few grammatical issues:\n* First paragraph of the Introduction: \"...are hard to design the objectives/reward with human priors.\" needs to be rewritten.\n* Page 2, second paragraph: \"..., which largely alleviate.\" is incomplete.\n* In equation 5, there should be a separation between max and min.\n* Page 7, CS section \"... terminal states S_T=.\" seems to have some missing description.\n* In the caption for Figure 6, \"...Clusting...\" should be \"...Clustering...\"\n",
            "summary_of_the_review": "The paper presents an interesting approach with one novel that attempts to learn a gradient based planning system from state examples without the need for rewards or trajectories. The usability of the approach is demonstrated in a number of examples with tasks of different complexity.\nThe paper, however, lacks discussion of some implicit assumptions and of its relation with traditional mixture potential path planning approaches (this seems important as it seems to be largely an adaptive mixture potential approach with dynamic mixture weight adjustment).\nWhile the experiments cover interesting domains, their detailed description is completely left to the appendix with not even a short description of the obstacle complexity I the paper, making it hard to assess the results. In addition, it seems that a study of the influence of the noise parameter sigma would be of high importance as it seems to drive the propagation of the target gradient field away form the target/goal region.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper77/Reviewer_Msq6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper77/Reviewer_Msq6"
        ]
    }
]