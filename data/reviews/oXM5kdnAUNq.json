[
    {
        "id": "bhSu-nYbfhn",
        "original": null,
        "number": 1,
        "cdate": 1666281596265,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666281596265,
        "tmdate": 1666281596265,
        "tddate": null,
        "forum": "oXM5kdnAUNq",
        "replyto": "oXM5kdnAUNq",
        "invitation": "ICLR.cc/2023/Conference/Paper1245/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the problem of learning (neural network) closed-loop controllers using supervised learning based on data that is generated by trajectory optimization.\nTo mitigate the well-known problem of distribution mismatch, the paper proposes an iterative procedure. The paper assumes that a few time steps $t_i$ are specified to split the trajectory into different segments. In the first iteration, trajectory optimization is used to generate trajectories starting at time step $t_0=0$, which form the training data for generate the neural network controller. In the second iteration the training data from [t_0, t_1] are kept, but the data from [t1, T] are replaced by optimal trajectories that start at t_1 from states reached by the NN-controller (which can be found by approximately solving the corresponding initial value problem). This process is repeated until each of the pre-specified time-steps had served as initial time-step.\n\nThis procedure of generating training data for the closed-loop controller seems novel, although similar methods have been proposed in the past. Furthermore, the paper provides a theoretical analysis on a specific LQR problem showing that the iterative data collection leads to smaller state-error. The method is evaluated for learning controllers for landing a quadrotor and for performing a reaching motions for a robotic arm (both in simulation).",
            "strength_and_weaknesses": "Strength\n----------\n- The paper is well-written and clear. The presentation is good.\n- The claims seem correct.\n\nWeaknesses\n-----------------\n- The novelty is rather limited (details below).\n- The theoretical results are weak (details below).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n---------\nThe presentation of the approach is clear.  However, I am missing some details in the experimental section:\nHow is the neural network controller optimized? In particular: how long was it trained? Was a validation set used for early stopping? If not, how was it ensured, that the improved performance was due to the change in the training set, and not simply due to longer training?\nFurthermore, I was surprised that no state-trajectories where shown for the quadrotor experiment. I think it would be important to show that the data generated by the early controllers is still relevant at later iterations.\n\nQuality\n---------\n- The theoretical analysis is rather weak, as it only applies for a simplified LQR  assuming that the state error of the closed-loop controller is zero mean. While I see the merits of using LQR for tractable analysis, the assumption of zero mean error seems too strong and unrealistic. \n- It does not seem principled to throw away optimal labels that were quite costly to obtain. Why are parts of the training data replaced by the newly collected data, instead of augmenting the data set? The paper does not discuss this option at all, nor does it show experimental comparisons. In general, the paper does not provide motivation for the *specific* way the additional data is considered, and what are the benefits compared to alternate methods. \n\nNovelty\n---------\n- The overall procedure of iteratively selecting initial states for trajectory optimization in order to obtain data to improve the closed-loop controller is not novel (the paper already contains several references). So the only algorithmic contribution relates to the specific way these points are chosen.  While the paper discusses the forward training algorithm, it does not discuss its follow up DAGGER (Ross et al., 2011) which might be even more related. The analogous method of DAGGER in continuous time, would proceed as follow. \n1. solve optimal control from the initial states, and train close-loop controller (same as the proposed method)\n2. solve IVP to obtain the states at each temporal grid point $t_i$\n3. solve optimal control for each of these states and augment dataset, retrain close-loop controller\n4. Goto 2, if necessary\n\nI would argue, that such approach would not be that different from the proposed method. The difference would be that for every solved IVP, it would need to solve $K$ trajectory optimization problems (one per grid point). Performing a single iteration (i.e. skipping step 4) would require as many calls to the trajectory optimizer as the proposed method, but fewer calls to the IVP-solver. If the IVP can not be solved analytically, but requires numerical integration, I could imagine that the DAGGER approach can be more cost-effective.\n\nReproducibility\n--------------------\nWithout source code it is not possible to reproduce the results, as details---in particular related to training the NN controller---are missing in the paper.\n\n\nReferences\n---------------\nRoss, St\u00e9phane, Geoffrey Gordon, and Drew Bagnell. \"A reduction of imitation learning and structured prediction to no-regret online learning.\" Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011.",
            "summary_of_the_review": "The paper is clear and seems technically correct. However, the contributions seem rather small, as the provided theoretical, empirical and intuitive justifications are rather weak. The paper could be improved by evaluating the effect of keeping the data of previous iterations, and by evaluating the DAGGER-style procedure (comparisons should be over computational cost/time). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1245/Reviewer_gCS8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1245/Reviewer_gCS8"
        ]
    },
    {
        "id": "EntIC5kSwQ",
        "original": null,
        "number": 2,
        "cdate": 1666509576244,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666509576244,
        "tmdate": 1666509576244,
        "tddate": null,
        "forum": "oXM5kdnAUNq",
        "replyto": "oXM5kdnAUNq",
        "invitation": "ICLR.cc/2023/Conference/Paper1245/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new method for learning deep closed-loop control policies from training on data generated by open-loop optimal control solvers. The paper proposes a resampling procedure that iteratively retrains the controller in states that it may not have seen in the initial training data. The approach was tested empirically in two problem domains and was shown to achieve good performance and to be somewhat robust to dynamics noise.",
            "strength_and_weaknesses": "Strengths:\n- Points out a weakness in previous works and proposes a novel solution to it.\n- Writing is intuitive, and a good mix of theoretical and empirical results\n\nWeaknesses:\n- Main concern is the reliance on determining the sampling intervals when training a model. This seems like a critical hyperparameter and I don't have a sense for how easy it is to tune, and hence I'm not sure how easy it would be to extend the approach to other problem domains.",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned above, I have some concerns about reproducibility given the reliance on determining the sampling intervals. However, the others have acknowledged this in the conclusion so I think it's reasonable to not have a clearer process for this yet. Beyond this I thought the writing was clear and of high quality. \n\nIn terms of novelty, I am not too familiar with the related work, so I can't evaluate this too well. The problem of distribution mismatch seems somewhat obvious to me since it plagues many ML/RL-related problems, but as far as I can tell this is a novel solution to that issue.",
            "summary_of_the_review": "This paper identifies a problem in an existing line of work, proposes a solution to that problem, and demonstrates the value of that solution theoretically and empirically. Taken together, I think that makes this a worthwhile read and worthy of acceptance, although I will admit that this specific area of learning closed-loop controllers from open-loop data is new to me, so I would defer to others with more knowledge of the related work for their judgments.\n\nSome comments/questions:\n- The problem setup here involves applying control to a system over some finite interval [0, T]. How would this approach extend to a system that needs to operate continuously (e.g. a robot in a factory) if the premise here is that eventually control quality begins to degrade? Would you need to break up tasks into sub-tasks?\n- The problem formulation here assumes that we have knowledge of the system dynamics. What if those models are inaccurate? I would think that you would end up with a similar problem to the one you used to motivate this work, i.e. that the controls you apply bring you to states you haven't seen in your training data and that degrades performance more and more over time. Given that, do you think this approach could be extended to problems where dynamics models are inaccurate? I feel like this theme was somewhat explored in the experiments where noise was added to the dynamics, but it wasn't explicitly framed in this way\n- In the iterative training procedure do you use the same network throughout and finetune that network during each training stage or do you train a new network during each stage. It may have said this somewhere in the paper and I missed it, but it might be nice to state this more clearly e.g. within Algorithm 1.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1245/Reviewer_s6p1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1245/Reviewer_s6p1"
        ]
    },
    {
        "id": "l5-NaaQCLQF",
        "original": null,
        "number": 3,
        "cdate": 1667319589646,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667319589646,
        "tmdate": 1669478545255,
        "tddate": null,
        "forum": "oXM5kdnAUNq",
        "replyto": "oXM5kdnAUNq",
        "invitation": "ICLR.cc/2023/Conference/Paper1245/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes the initial value problem enhanced sampling method to mitigate the distribution shift issue in applying supervised learning to solve the Hamilton-Jacobi-Bellman equation. The authors theoretically prove that the proposed sampling strategy improves over the vanilla strategy on the LQR problem by a factor proportional to the total time duration. Then they further numerically demonstrate that the proposed sampling strategy significantly improves the vanilla strategy on the optimal landing problem of a quadrotor and the optimal reaching problem of a 7 DoF manipulator.\n",
            "strength_and_weaknesses": "Strength:\n1) The proposed approach seems to improve significantly over the vanilla approach on the two examples presented in this paper.\n2) The proof for the theoretical results on LQR seems to be correct.\n\n\nWeaknesses:\n1) The numerical study is not sufficient to demonstrate the competitiveness of the proposed approach. How does the proposed approach compare with policy gradient (TRPO, PPO, SAC) on those problems?\n\n 2) The theory is only provided for the simple LQR problem, and does not provide too much insights for nonlinear control problems, which are the main focus of this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly. The readability is good. However, the LQR setting for the theory study and the two examples for numerical study are relatively simple. It is quite difficult to see what is the unique novelty in the proposed approach. It is also unclear how to compare the proposed approach with policy-based approaches. \n",
            "summary_of_the_review": "I cannot recommend acceptance for this paper since the novelty is not very clear. The LQR setting for the theory study and the two examples for numerical study are relatively simple. It is also unclear how to compare the proposed approach with policy-based approaches. \n\n========================\n\n**Post rebuttal**\n\nThanks for addressing my comments. I increase my score to 6. However, I still think the theory in this paper is relatively weak. The paper will be much more insightful if the theory part can be strengthened.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1245/Reviewer_9LKe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1245/Reviewer_9LKe"
        ]
    }
]