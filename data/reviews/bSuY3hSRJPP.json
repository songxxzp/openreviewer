[
    {
        "id": "p_TCKn_s39",
        "original": null,
        "number": 1,
        "cdate": 1666143934292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666143934292,
        "tmdate": 1666143934292,
        "tddate": null,
        "forum": "bSuY3hSRJPP",
        "replyto": "bSuY3hSRJPP",
        "invitation": "ICLR.cc/2023/Conference/Paper2684/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "To tackle the distribution shift and missing data problem of time series, this paper proposes a latent space spectral decomposition method for simultaneous time series forecasting and imputation. The latent vector is optimized individually for unseen data and thus can generalize well to unseen data distribution. ",
            "strength_and_weaknesses": "Pros:\n1) The idea to optimize a latent vector for different data is interesting and this is the first time this idea is applied on time series. Previously this idea has been used to images [2]. \n2) Experiments result are better than several baselines\n\nCons: \n1) The idea to combine several basis functions to reconstruct the input is not new. Previously, [1,3] used this idea to reconstruct music signals. Also, I am not sure that only using sin, cos, and poly functions are enough for all types of time series datasets. Some datasets may have strange shapes that can not be reconstructed well.\n2) For optimization, I am not sure about using the inference step and the learning step can lead to a globally optimal solution. It would be great if the authors can provide some explanation here. \n\n[1] DDSP: Differentiable Digital Signal Processing\n[2] Optimizing the Latent Space of Generative Networks \n[3] Differentiable Wavetable Synthesis",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good\nQuality: Good\nNovelty: Good\nReproducibility:Good",
            "summary_of_the_review": "Despite the drawbacks that I mentioned, I think this is an interesting paper overall.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2684/Reviewer_r8BM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2684/Reviewer_r8BM"
        ]
    },
    {
        "id": "bZZK01Vv5h",
        "original": null,
        "number": 2,
        "cdate": 1666548919107,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666548919107,
        "tmdate": 1666560561039,
        "tddate": null,
        "forum": "bSuY3hSRJPP",
        "replyto": "bSuY3hSRJPP",
        "invitation": "ICLR.cc/2023/Conference/Paper2684/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, authors proposed a multivariate time-series forecasting model (SpectraNet) that unified the forecasting and interpolation problem. Specifically, the model first infers the optimal latent vector on the reference window by minimizing the reconstruction error, then the model generates the full predictions via a latent space spectral decomposition module. The proposed model achieves the best performance compared with other existing methods.\n",
            "strength_and_weaknesses": "**Strengths**:  \n1. Well-organized and clearly written.  \n2. Promising results.  \n\n\n**Weaknesses**:  \n1.  In Sec. 3.5, the distribution shift is defined as the difference of missing data regime between training set and test set. However, in my opinion, this kind of difference is not the distribution shift, since the training and test sets both come from the **same** dataset in a random split (or not) manner, the **data distribution** should be the same in the training and test sets, even though the total numbers or locations of missing values are different. Therefore, I would say there is no distribution shift problem.\n\n2. Some key details are missing.   \n(a) In Sec. 3.2, how many temporal bases $B$ are used? What's the insights or reasons?   \n(b) In Sec. 3.3, there is no clarification of the ConvTranspose1d, more details should be included. Why do the first two layers learn a common representation and the second layer refine the temporal resolution? There is no such experiment or explanation of this claim. In addition, the *causality* is not clear either, \"the latent vector $z^{*}$ is only inferred with information on the reference window\" is not the reason of  *causality*.  \n(c) In Sec. 4.3, it says \"all models are trained with the training loss only on available values\", how about the missing points? I think the ground truth of missing points is accessible during training, if the model doesn't use the missing points during training, what's the meaning of interpolation?  \n\n\n\n3. Experiments are not sufficient. There is no ablation study to demonstrate the effectiveness of some designs in the model, i.e., temporal basis, the inference step, etc.\n\n\n4. Minor mistakes:  \n(1) In Sec. 3.1, the first sentence is incoherent, \"of the\", \"instead of\".  \n(2) In Fig. 2, the legend blocks the first curve $\\hat{y_{1}}$,  even though it looks the same as $\\hat{y_{10}}$ . Almost the same problem in all the figures.  \n(3) Eq. (8), not aligned.  \n(4) In Fig.3, the characters are too small.  \n(5) In References, the names of conferences or journals, i.e. AAAI or AAAI Press , are not consistent.  \nI highly encourage authors to revise the submission to avoid such small mistakes, as well as some typos.  \n\n\n5. Some missing references[1,2] when addressing distribution shift problem in time-series.  \n[1] Adaptive Trajectory Prediction via Transferable GNN. CVPR2022  \n[2] Expanding the Deployment Envelope of Behavior Prediction via Adaptive Meta-Learning. arXiv 2022\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Details are in [Weaknesses]\n\n**Clarity**:  \nThe clarity is good but with some minor mistakes.\n\n**Quality**:  \nClear-written and well-organized.\n\n**Novelty**:  \nMotivation is good but the contributions are not significant.\n\n**Reproducibility**:  \nThe training procedure is clear in Appendix 5,  and authors promise to release the code after paper acceptance. \n",
            "summary_of_the_review": "I list my concerns in [Weaknesses], I am happy to discuss and increase the rating if my concerns are addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2684/Reviewer_HyvP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2684/Reviewer_HyvP"
        ]
    },
    {
        "id": "0RBhCAwhyLa",
        "original": null,
        "number": 3,
        "cdate": 1666668613171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668613171,
        "tmdate": 1666668613171,
        "tddate": null,
        "forum": "bSuY3hSRJPP",
        "replyto": "bSuY3hSRJPP",
        "invitation": "ICLR.cc/2023/Conference/Paper2684/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In the paper, the authors proposed a time-series forecasting framework to overcome the distribution shifts and tackle missing values simultaneously. ",
            "strength_and_weaknesses": "Strengths\n1. An important problem is studied\nWeaknesses:\n1. Weak motivation for proposed model components\n2. Paper presentation and organization need to be improved",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is difficult to follow due to the unclear presentation, especially in the model part. Further improvement is needed to improve the clarity. ",
            "summary_of_the_review": "In this paper, the authors proposed a novel time series forecasting framework that handles missing data and distribution drift. However, the paper is not clearly presented, and the following issues should be addressed: \n\n1. The presentation and organization of this paper need to be improved. There are many typos and incomplete sentences. For example, on page 2, \"practice, imputation models are first ...\". \n\n2. What's the advantage of inferring the latent variable separately instead of using an encoder? The motivation is not strong here, and the authors didn't provide enough details. Besides, a point-wise distance-based loss function (e.g., MSE) may not be enough to capture the shape correlation between two-time series. Can the authors provide more information and analysis regarding the design of latent vector inference?\n\n3. What's the meaning of s_w / 2 in Figure 1 and equation 7? What is p in equation 7? The authors should clearly describe and explain the equations used in the paper. \n\n4. How does the imputation work in the inference of latent vectors? Can the authors provide more details?\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2684/Reviewer_Mu1M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2684/Reviewer_Mu1M"
        ]
    }
]