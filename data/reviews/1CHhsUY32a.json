[
    {
        "id": "yntP_LO3Y6",
        "original": null,
        "number": 1,
        "cdate": 1666678697247,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678697247,
        "tmdate": 1671176196090,
        "tddate": null,
        "forum": "1CHhsUY32a",
        "replyto": "1CHhsUY32a",
        "invitation": "ICLR.cc/2023/Conference/Paper1116/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method to generate images from a collage of patches representing objects. Each patch is fed to a feature encoder and the resulting features are spatially concatenated such that their layout corresponds that of a collage where each object occupies a specific bounding box. The aggregated feature map is then fed to a decoder that is trained adversarially such that both the full image and each bounding box corresponding to the object are realistic.",
            "strength_and_weaknesses": "\nOverall, the idea is nice and is very practical in terms of applications in the sense that it seems more intuitive to copy/paste objects in a scene to represent the target image that you had in mind rather than describing it with a long text (that can be cumbersome and not precise enough) or drawing a segmentation mask. The idea stems from image editing, but here the goal is different as you certainly do not want the resulting image to be a smooth blending of the input collage which would not be diverse enough as a creation tool.\n\nThere are weaknesses in the experimental setup and in the overall positionning. For the experiments, the results are not extremely convincing due to the lack of comparision with methods that can do the same. Only IC-GAN is tested, but there are many more GAN inversion methods that could take the image collage as input encode it to the GAN latent space and then decode it, like:\n- Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage compositionality in gans. arXiv:2103.10426, 2021.\n- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.\n- Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.\n- Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In European conference on computer vision, 2020.\n- Thibaut Issenhuth, Ugo Tanielian, J\u00e9r\u00e9mie Mary, David Picard. EdiBERT, a generative model for image editing. arXiv:2111.15264, 2021\n- Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang. DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models. arXiv:2210.08573, 2022. (this one came out after the submission deadline, so of course it cannot be compared to)\n\nAdditionally, the experiment of fitting the training set is not convincing at all. Memorizing the training set is essentially what it takes to obtain a good score in this test and it does not tell anything about generalization capabilities. Why not just reserve a validation set and perform this metric on anything else than training data?\nOverall, the visual results are not super impressive. Maybe it is because the task is very hard, but comparing with editing methods (even back to blending methods that do not use deep learning), would have shown the difficulty of trade-off between fidelity and diversity.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear enough.",
            "summary_of_the_review": "It's a good idea but the experimental validation is a bit lacking.\n\n---\n The authors made efforts to address my concerns",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1116/Reviewer_NK57"
        ]
    },
    {
        "id": "bxDOwZxlhZl",
        "original": null,
        "number": 2,
        "cdate": 1666844248830,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666844248830,
        "tmdate": 1666844248830,
        "tddate": null,
        "forum": "1CHhsUY32a",
        "replyto": "1CHhsUY32a",
        "invitation": "ICLR.cc/2023/Conference/Paper1116/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for controllable generation of images conditioned on image collages as input. The proposed approach, called \u201cmixing and matching scenes\u201d (M&Ms), is an adversarially trained generative image model conditioned on appearance features and spatial positions of collage elements.  M&M combines these components into a coherent image, using a model based on Instance-conditioned GAN (IC-GAN, Casanov et al. 2021), but using two discriminators; a global discriminator and an object discriminator. The global discriminator encourages the generator to produce plausible and appealing images, while the object discriminator ensures  visual quality and collage-consistency of objects. In practice, the architecture follow the BigGAN generator and discriminator architectures. For experiments, M&M is trained on the 1.7 million OpenImages dataset and validated  in both in-distribution and out-of-distribution scenarios using collages derived from OpenImages and in a zero-shot dataset scenario using collages derived from MS-COCO. Performance is evaluated using image- and object-FID scores. On OpenImages, M&Ms significantly outperforms IC-GAN on object-FID, while maintaining the image-FID scores in both in-distribution and out-of-distribution scenarios. On zero-shot transfer to MS-COCO, the model also obtains low FID scores. \n",
            "strength_and_weaknesses": "Strengths:\n1. The paper is clearly written. The proposed approach is described well and looks sensible and correct. While the proposed method is an extension of the IC-GAN framework, the methodological improvement of adding object discriminators and incorporating spatial layout information leads to improvement in generation quality. \n2. The model obtains significant improvement over the comparable baseline method (IC-GAN) in both in-distribution and out-of-distribution scenarios in generating objects\n\nComments:\n1. While the model seems to obtain good FID score on the MS-COCO, I am not convinced by the comparison with text-to-image generation models like DALL-E. These are completely different models with different approaches for image generation (autoregressive, diffusion), which enable completely different image generation capability. So while its interesting to know that the FID obtained by M&Ms is better to DALL-E (for instance), it\u2019s an apples-to-oranges comparison. \n2. The examples for MS-COCO generation (in Figure 5) look qualitatively worse than those obtained by M&M on OpenImages, perhaps because these represent more cluttered scenes. Can the authors comment on how the results on MS-COCO differ from those on OpenImages?\n3. How can the approach be extended to image inpainting?\n4. How do you expect the method to improve with access to larger training datasets?\n",
            "clarity,_quality,_novelty_and_reproducibility": "See above. ",
            "summary_of_the_review": "The paper proposes an approach for image collage-conditioned image generation using a GAN framework. Experimental results show qualitative and quantitative improvement over the baseline in generating objects in the resulting images for both in-distribution and out-of-distribution scenarios. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1116/Reviewer_abbj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1116/Reviewer_abbj"
        ]
    },
    {
        "id": "gcMoCxmPcKz",
        "original": null,
        "number": 3,
        "cdate": 1667640368703,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667640368703,
        "tmdate": 1667640368703,
        "tddate": null,
        "forum": "1CHhsUY32a",
        "replyto": "1CHhsUY32a",
        "invitation": "ICLR.cc/2023/Conference/Paper1116/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper introduces M&Ms framework that aims to generate controllable scene generation with given visual descriptions. The proposed framework takes the background scene and set of objects along with the object location and size as conditions and synthesis the photorealistic collages. In addition to realistic image generation, it is capable to control the generated collage by changing the conditions.",
            "strength_and_weaknesses": "Strength:\n\n1 The model has significant control over the generation and it is clearly justified with qualitative results. \n\n2 The generated collages are photo-realistic which is shown with particularly quantitative results.\n\n3 Ethical concerns are addressed.\n\n4 Limitations (pre-trained feature extraction) and failure cases (overlapping BB) are stated in the supplementary material.\n\n\nWeakness: \n\n1 The generated collages are sometimes blurry. For example, generated human is blurry in Collages 3 and 4 in Figure 2. \n\n2 Figure 3 is misleading in that it seems that the model uses more than one object discriminator. Please redraw the figure.\n\n3 Although the paper claims the model enables control over the position and size of the objects. It is unclear how M&Ms handles the location and the size of the objects (For example, does the model preserve the ratio of the height and width). Please elaborate on it and show more examples of object generation in different locations and with different sizes as it would be good to see how the model controls them. So, please add some results of collage generation with the same object with different locations and sizes (from small scale to big scale or maybe with the change over the ratio of height and width).\n\n4 In Table 1, M&Ms' object FID scores are better and scene FID scores are roughly the same. Why? Does it mean that although M&Ms can generate objects with better fidelity, it is not capable to put them together? As well as, isn't it expected as IC-GAN is conditioned on only global scene features? Please add results of an ablation model which is M&Ms conditioned on only global features like IC-GAN.\n\n5 The model of M&Ms is designed by leveraging IC-GAN, so comparison on Table 1 is like comparing with the older version M&Ms. So please include other GANs to this table.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and easy to follow.\n\nThe overall quality is good with high-quality figures, especially and it can be improved with elaboration on equations.\n\nThe idea and the model are incremental somehow.\n\nThe model can be reproduced with the help of supplementary material.\n\n",
            "summary_of_the_review": "Although I like the idea and the model, the paper has some weaknesses related to the justification of fulling objectives (control over size and location) and the performance (Table 1). That's why my recommendation is \"5: marginally below the acceptance threshold\".",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1116/Reviewer_LMNa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1116/Reviewer_LMNa"
        ]
    },
    {
        "id": "yM22A3fWHS",
        "original": null,
        "number": 4,
        "cdate": 1667709206851,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667709206851,
        "tmdate": 1667709206851,
        "tddate": null,
        "forum": "1CHhsUY32a",
        "replyto": "1CHhsUY32a",
        "invitation": "ICLR.cc/2023/Conference/Paper1116/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a fine-grained scene image mixing generation method using visual descriptions in image collages. The M&Ms framework is optimized at the image and object level, following the generative adversarial training manner. ",
            "strength_and_weaknesses": "# Strength: \n\n1. This paper is well-organized and easy to follow.\n2. Some quantitative experiments demonstrate the advantages of the proposed method to IC-GAN.\n\n\n# Weakness:\n\n1. The idea is incremental to IC-GAN, but the improvement is limited, especially for the qualitative evaluation. \n2. The quantitative evaluation metrics are not enough, the author could provide more evaluations on the generated objects, such as classification errors.\n3. More visual comparisons with other methods should be provided?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-organized and easy to follow. However, the novelty is limited, compared with IC-GAN.",
            "summary_of_the_review": "While this paper extends the IC-GAN to blend multiple localized distributions, the novelty is incremental, as extracting a collage representation. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1116/Reviewer_ovRC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1116/Reviewer_ovRC"
        ]
    }
]