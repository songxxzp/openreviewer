[
    {
        "id": "fnSgJC1OyAG",
        "original": null,
        "number": 1,
        "cdate": 1666170127118,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666170127118,
        "tmdate": 1668772266582,
        "tddate": null,
        "forum": "hp_RwhKDJ5",
        "replyto": "hp_RwhKDJ5",
        "invitation": "ICLR.cc/2023/Conference/Paper4484/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides an algorithm for learning the structure of a directed causal model from a combination of observational and interventional data, where interventions consist of setting one node to a specific value.\n",
            "strength_and_weaknesses": "UPDATE: The reviewers have answered my concerns, so I'm upping my score to 8.\n\n----\n\nThe empirical results look impressive, and it seems to beat all existing methods at this task.  \n\nThe main weakness is that there is no discussion of how much training is required for CSIvA to obtain the results it does.  How much synthetic data must be generated, and how long does the computation take?  How does this compare to the other methods you consider?\n\nAnother weakness is that the method does not enforce acyclicity of the resulting graph, though the authors report that empirically this is never a problem.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is fairly clear, though as a non-expert in neural networks some aspects seemed slightly vague to me.  Again, as a non-expert the work seems novel.  Reproducibility is harder to gauge, since no code is provided to replicate the results. \n",
            "summary_of_the_review": "UPDATE: My score has been increased to 8.\n\n----\n\nThe simulations are thorough and the results do seem promising, but I would have to hear a convincing answer to the question about runtimes, and have a link to the authors' code before I were to revise my score upwards.\n\n### Minor Comments\n\n- Figure 4 doesn't seem well presented.  It would seem more sensible (to me) to group the graphs by ER-1 vs ER-2, and then have separate line plots over the increasing number of nodes.  This would make it easier to visualize the best methods.\n- Table 1 seems limited in terms of the methods listed.  What about the others you use in your comparisons?  (i.e. unsupervised approaches)\n- page 8: \"BnLearn\" should be \"bnlearn\"\n- Your spelling is inconsistent.  For example, on page 3 in the top paragraph you use both \"modeling\" and \"modelling\", and then on page 6 you use both \"modelling\" (not North America) and \"analyzing\" (only North America).  \n- The capitalization of \"Hamming\" is also not consistent, even within A.5.  I would suggest always capitalizing, as it is someone's name.\n- Please ensure that the capitalization of your references is correct (use {} around such words in article titles in BibTeX).  e.g. \"dag\", \"markov\", \"bayesian\", \"kaggle\", \"Chalearn\", \"Innovations in machine learning\" etc.\n- Caption for Table 11: \"<=\" should be \"\\leq\", and the opening quote on 'Abs' is not inverted.\n- Tables 5-8 and 12 - again, it would seem better to group the ER=1 and ER=2 results together.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4484/Reviewer_s1nT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4484/Reviewer_s1nT"
        ]
    },
    {
        "id": "K56WIbJz6_0",
        "original": null,
        "number": 2,
        "cdate": 1666518147590,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666518147590,
        "tmdate": 1668942069004,
        "tddate": null,
        "forum": "hp_RwhKDJ5",
        "replyto": "hp_RwhKDJ5",
        "invitation": "ICLR.cc/2023/Conference/Paper4484/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a transformer based neural network architecture to learn the mapping from both observational and interventional data to graph structures in a supervised manner. The learned model is shown to be able to generalize to new synthetic graphs and is robust to some train-test distribution shifts. \n\n",
            "strength_and_weaknesses": "Pros:\n\n- a new design of transformer model to enable supervised learning for structure learning\n- extensive empirical results\n- writing is good and clear.\n\nCons:\n\n- It is not clear how one can trust the transferability result. That is, if we can give a new setting, how accurate can we say about the output? Unlike NLP or CV tasks, I don't see what could be transferred across causal discovery tasks.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper writing is really good, and I only have a few questions.\n\n\n\n- The only major concern of mine is regarding the transferability across different causal discovery tasks: they may have different graph types, different causal mechanisms, different data types, etc. This is unlike NLP or CV big models. In fact, authors of [1] also use pretraining transformer based models but the output of pretrained models is only used a good initialization. \n\n- In the experiment, the training dataset consists of ER graphs. For out-of-distribution test, there are no results on scale-free graph (as far as I checked). Can you add experiments on this part?\n\n- Suggestion: Transformer based NN model for causal discovery has been used in existing works in [1,2]. While presenting the used architecture in this paper, I think it beneficial to clearly state the novel part compared with [1,2].\n\n  \n\n[1] Ordering-based causal discovery with reinforcement learning.\n\n[2] Causal discovery with reinforcement learning",
            "summary_of_the_review": "I am not convinced that there is a transferrable quantity across various causal discovery tasks. I look forward to author response on this point and would like to change score if this concern is addressed.\n\n==== after reading author response====\n\nI am accepting the transferable part. Based on my understanding of the transferability and the listed examples, I feel that the application scope is weakened. For instance, what if we are given a totally new dataset that we do not have such prior knowledge, or that the associated prior knowledge is far different from the training datasets? I suggest authors to add more examples where the proposed method could be applied.\n\nDue to the above point, I decide to increase my score to WA, but I also feel OK if the paper is not accepted. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4484/Reviewer_iFSS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4484/Reviewer_iFSS"
        ]
    },
    {
        "id": "XiLo7H91e8",
        "original": null,
        "number": 3,
        "cdate": 1666583752842,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583752842,
        "tmdate": 1666587295332,
        "tddate": null,
        "forum": "hp_RwhKDJ5",
        "replyto": "hp_RwhKDJ5",
        "invitation": "ICLR.cc/2023/Conference/Paper4484/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a novel deep-learning model capable of inferring the underlying causal structure. The method does this by treating the process as a black-box function which is estimated through a network that maps observational and interventional data to a causal diagram (the output). They demonstrate their approach in a number of experimental settings.",
            "strength_and_weaknesses": "This is the main review. It is organised according to the headings of your paper.\n\n# Abstract\n\n- First sentence, perhaps we can be a bit less specific and say that graph learning is _one_ fundamental problem in causal inference (there are many).\n- First question; if yours is a supervised approach, what happens when you have a setting for which you have no data? Learning directed acyclic graphs (DAGs) from data is an NP-hard problem after all.\n- It would help if you in the appendix stated if your method assumed causal sufficiency or not.\n\n# Introduction\n\n- Excellent first paragraph.\n- You say: \"Our approach can be viewed as a form of meta-learning, as the model learns about the relationship between datasets and structures underlying them\" - what precisely is \"meta\" about this?\n- What is a \"naturalistic CBNs\"? A commonly occurring CBN, something we would find in nature?\n- Please make the subfigures larger in figure 1; even with an exceptionally large screen, it is difficult to read the text in your images.\n- I am curious: what do you do about data scarcity and imbalance (the question relates even more to the latter)? There is very little interventional data knocking about, and when there is, there is usually not very much of it compared to the amount of observational data in the same setting which would suggest that your approach has to deal with the data imbalance problem too. How do you do that?\n- Again... what is a \"naturalistic\" CBN?\n\n# BACKGROUND\n\n- If you are tight on space, you can reduce both of these sections and just link to the relevant literature. You've given the main sources for CBNs and a few sentences are enough IF you are short on space. If not, leave as is, it is good.\n\n# CAUSAL STRUCTURE INDUCTION VIA ATTENTION (CSIVA)\n\n- The last paragraph on page three could do with a few equation blocks; it is currently very dense and hard to follow.\n- I cannot help but think that section 3.2 could have been considerably shorter by simply employing a depiction (image) of the encoder and decoder and simple explanations to go alongside it. This long and verbose text to explain a model seems far too verbose when there is no novelty in the individual parts, simply just how they are put together - which could be more easily shown with an image.\n\n# SYNTHETIC DATA\n\n- Careful not to confuse your reader when you discuss identifiability, as it has two meanings within the field of causal inference (and I have not seen it used the way you are using it). See Pearl, 2009 for a formal take - essentially we say that an intervention is 'identifiable' if it can be composed of observational distributions i.e. $p(y| do(x)) = p(x|y)p(x)$. The way Eberhardt et al. use the word is not common and not standard and clashes with a much more central domain within causal inference as noted.\n\n# EXPERIMENTS\n\n- Again, I think you are being far too verbose in your writing. Figure 3 clearly tells us what we need to know regarding the in-distribution experiments, you do not need separate paragraphs for each type of data.\n- It would be helpful if you had included some graph topologies which CSiVA actually learned compared to the other methods. \n- When did you stop learning? Table 3 only tells us the hyperparameters but not when you consider a graph 'learned' from CSiVA's perspective.\n\n# DISCUSSION\n\n- It would have been more useful if you had shown that this method is useful in an RL setting in which an RL agent interacts with the environment via observing low-level pixel data.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity\n\nThe paper is well-written and easy to follow.\n\n# Quality\n\nThe figures could be far better; they are hard to read (even with a very large screen), are clearly not vector based (have a look at IPEDraw) and are in general of sub-par quality compared to the rest of the graphs.\n\n# Novelty\n\nThe novelty is low. The authors have combined known parts into a new network which has some interesting properties but no theoretical results which show that this is a general method which would work in all settings. Now, this is not to absolve their competitor methods of this same analysis, they are under the same critique, but this paper is the one under scrutiny at the moment.\n\n# Reproducibility\n\nI imagine the authors will release their model in due course hence there are not concerns in this area.",
            "summary_of_the_review": "The authors have contributed a novel deep-learning model which has some interesting properties when it comes to causal discovery. They demonstrate impressive empirical results on a range of datasets but ultimately contribute very little methodological or theoretical novelty. Their contribution is empirical which is examined in detail in the review.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4484/Reviewer_uyeG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4484/Reviewer_uyeG"
        ]
    },
    {
        "id": "T0sDB99h7P4",
        "original": null,
        "number": 4,
        "cdate": 1666945622259,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666945622259,
        "tmdate": 1666945622259,
        "tddate": null,
        "forum": "hp_RwhKDJ5",
        "replyto": "hp_RwhKDJ5",
        "invitation": "ICLR.cc/2023/Conference/Paper4484/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an interesting supervised learning setting for causal discovery. Firstly, it creates synthetic training data ( observational and international data ) with various simulators and settings. And then it designs a neural network that outputs the causal graphs. Based on such a supervised learning manner, it claims that the method can recover the ground-truth graph (which lacks analysis and proofs, see details in the following comments) and can handle the distribution-shift / out-of-distribution problem (which can be over-claiming the contribution, see details in the following comments).",
            "strength_and_weaknesses": "__Strength__\n\nIt deserves to encourage different thoughts and problem formulations for causal discovery. The idea of leveraging synthetic data,  supervised learning, and meta-learning for solving causal discovery problems is interesting and new. If there are related works with similar ideas that I may not realize, then the novelty should be deducted; otherwise, I would like to support such new ideas in causal discovery. However, the work still requires more rigorous considerations. \n\n\n__Weakness__\n\nThe work proposes a framework for causal discovery. However, besides the accuracy, causal discovery also cares about what are the learned results, what are their properties, and what guarantee we have. For example, we know that supervised learning can suffer from \n covariate shift, poor transfer learning performance, the lack of uncertainty, overfitting, etc al. Then how would they bother the proposed method? Maybe the paper could discuss such points. It mentions OOD performance of which I will provide detailed comments in the clarification part. \n\nMoreover, some other aspects can be interesting for causal discovery is that \n1. Can you show the identifiability and consistency of the method? (I will further explain this point in the clarification part.)\n2. Are the results in Markov equivalent class of the underlying causal graph? \n3. How are the results related to the existing results about causal discovery, e.g., the identifiability of different functional causal models (ANMs, PNLs, etc al.)?\n4. Try to explain how the discrepancy between synthetic/simulation and real-world data is or is not a problem for the method. And what can be the problem? Because it is well-known that the evaluation of causal discovery on synthetic data is too simple compared with real-world data. Then why such a gap between synthetic data and real-world data is (or not) a problem for the proposed method?\n\n__Some minor concerns:__\n\nIn Sec, 1, it mentions that the work is related to meta-learning. However, the method doesn't seem to have any meta-learning module to deal with the distribution shift or OOD problem.\nIn Sec. 3.1\n1. Why distributions are denoted by t, not the convention p? \n2. Why there are $N^2$ numbers of $A_l$, for DAGs should it be less ?\n",
            "clarity,_quality,_novelty_and_reproducibility": "## The proof of consistency and identifiability for the proposed method\n\nSec. 3 mentions consistency, but please elaborate on it.\n\"The data-sampling distribution t(G, D) and the MLE objective uniquely determine the target distribution learned by the model.\" \n\nSec. 4 mentions the identifiability, please elaborate on the proposed method.\n\"As discussed in Eberhardt et al. (2006), in the limit of an infinite amount of such single-node interventional data samples, Gi is identifiable.\"\n\nPlease elaborate and prove \n1. what is the target distribution?\n2. why the target distribution can be uniquely determined?\n3. the consistency of the results. \n4. the identifiability of the proposed method \n\n\n\n## About In and out of distribution\nI have a question about the experiment setting. As for causal discovery, there is only one graph per application for testing. So what does it mean in or out of distribution? Is it all about creating training data sets such that ( for example ) the in-distribution setting requires that the training data cover the parameters for generating the data of the underlying causal graph in an application? \n\nFurthermore, if the ID and OOD settings only differ according to covering the statistical parameter values for generating testing data or not (like the graph density and the parameter values of Dirichlet prior), then this can over-simplify the OOD case. Because as long as creating the training data in a proper way, even though the parameters are not exactly the same as the testing case, by interpolation or extrapolation, it should easily recover the test graph. Besides, in most cases, the mechanisms which shift away from ID and training data are unknown. \n\n\nTherefore, if this is the case, the paper may over-claim its contribution regarding this point.\n\n",
            "summary_of_the_review": "The new and interesting idea of causal discovery deserves support and encouragement, but the paper requires more consideration towards causal discovery. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4484/Reviewer_REpm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4484/Reviewer_REpm"
        ]
    },
    {
        "id": "mPs42ooB2Z",
        "original": null,
        "number": 5,
        "cdate": 1667484835062,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667484835062,
        "tmdate": 1667484835062,
        "tddate": null,
        "forum": "hp_RwhKDJ5",
        "replyto": "hp_RwhKDJ5",
        "invitation": "ICLR.cc/2023/Conference/Paper4484/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a method for learning the structure of a directed graphical model using supervised learning from synthetically generated data. They perform extensive evaluation of the proposed method, with impressive results.",
            "strength_and_weaknesses": "\u2013 Of the four contributions listed at the end of Section 1, only the last two are contributions. Introducing approaches and architectures is easy, but showing whether, how, and why they work is difficult.\n\n\u2013 The authors use the term \"causal Bayesian network\". While the terminology of causal graphical models is hardly unified, a more widely used and accurate term would be either \"causal graphical model,\" \"structural causal model,\" or \"directed graphical model.\" The models targeted in this paper are not necessarily Bayesian, so \"causal Bayesian network\" isn't really accurate.\n\n\u2013 The current version of the experiments uses structural Hamming distance as a measure of accuracy. The authors should strongly consider using more useful measures of structural accuracy.  Structural Hamming distance has been shown to be a poor measure of model quality. Instead, consider SID (Peters & B\u00fchlmann 2015) or BSF (Constantinou et al. 2021). \n\n+ Even given the use of SHD, the experimental methodology and results are impressive. The authors perform experiments under a wide range of circumstances, far wider than the typical paper. The use of a variety of methods (ablation studies, visualization, effect of interventional data, etc.) is particularly impressive. In all cases, the proposed method performs much better than existing baselines.\n\n\u2013 Even given the impressive results, readers would benefit from a more through analysis showing what sorts of errors were corrected in CSIvA\u2019s results and explaining why this happened. Were the edges (that were correctly added or omitted) weak or strong? Was it primarily about edge orientation or edge existence? Was it primarily about adding missing edges or omitting incorrect edges?\n\nReferences\n\nPeters, J., & B\u00fchlmann, P. (2015). Structural intervention distance for evaluating causal graphs. Neural computation, 27(3), 771-799.\n\nConstantinou, A. C., Liu, Y., Chobtham, K., Guo, Z., & Kitson, N. K. (2021). Large-scale empirical validation of Bayesian Network structure learning algorithms with noisy data. International Journal of Approximate Reasoning, 131, 151-188.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, novelty and reproducibility are all well above threshold.",
            "summary_of_the_review": "The empirical results from applying the proposed method are very impressive and should not be ignored. Clearly, there is more work to do, but the existing work presented here should be published so that others can build on these results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4484/Reviewer_UgAy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4484/Reviewer_UgAy"
        ]
    }
]