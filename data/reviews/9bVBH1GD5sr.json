[
    {
        "id": "p_v8j_vQrBi",
        "original": null,
        "number": 1,
        "cdate": 1666303821360,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666303821360,
        "tmdate": 1666303821360,
        "tddate": null,
        "forum": "9bVBH1GD5sr",
        "replyto": "9bVBH1GD5sr",
        "invitation": "ICLR.cc/2023/Conference/Paper3222/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies fairness in federated learning where clients have heterogeneous local data. In particular, the paper introduces a new fairness definition/measure to leverage the excess risk at each local client. Subsequently, the paper proposes an algorithm that provably outperforms FedAvg in terms of this new fairness definition under certain conditions. Empirical results on several real datasets against some baselines are provided.",
            "strength_and_weaknesses": "### Strengths\n\n- The exposition is generally clear in terms of setting, algorithm, theoretical results.\n\n- Theoretical guarantees (both convergence and fairness analysis) are provided.\n\n- Several real datasets are adopted in experiments.\n\n\n### Weaknesses\n- The motivation for fairness in federated learning can be strengthened. To elaborate, the abstract does not seem to motivate why fairness is important or necessary in FL. Similarly, the introduction does not seem to clearly motivate why fairness is important in FL.\n\n- The specific design choice for fairness via agent-awareness (i.e., the equity among the difference between their excess losses) can be motivated better. To elaborate, why is equity in any form a good notion of fairness? What are the motivating use cases for this notion of fairness? \n\n- Related to the above point, in introduction it says\n    > \"we aim to define and enhance fairness by explicitly considering different contributions of heterogeneous agents\" \n    \n    But it seems the heterogeneity is indirectly defined through the distributions of datasets instead of _explicitly_ via some concrete quantity to measure the heterogeneity. Moreover, it does not seem to provide a clear/explicit definition for the contributions. Following this, how does this fairness relate different levels of contributions of the clients?\n\n- The usage of Bayes optimal error can be elaborated better. For instance, how does Bayes optimal error affect the analysis, especially for regression-based analysis? What would happen if Bayes optimal error is not used? A subsequent question, how are high-quality and low-quality data defined?\n\n- The formalization of the heterogeneity of data can be justified better. In particular, the features of the datasets are assumed to be generated from Gaussian distributions with different means but identical and isotropic covariances. And each dataset has a true underlying mean vector. Is there justification/motivation for this setting (e.g., real use-cases where data are generated in this way, or approximately)? Then, is clustering performed w.r.t. the full model parameters (treated as a high-dimensional vector)? If so, how will the clustering affect the FL performance in such high-dimensional space? \n\n- It seems some other related works (listed below) might also be relevant and can be empirically compared.\n\n#### References\n\nTian Li et al, 2021, ICML, Ditto: Fair and Robust Federated Learning Through Personalization.\n\nXinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao, Chuan Sheng Foo, and Bryan Kian Hsiang Low. Gradient driven rewards to guarantee fairness in collaborative machine learning. Advances in Neural Information Processing Systems, 34:16104\u201316117, 2021. \n\nI believe the second reference is included in the paper",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity and Quality\n\nThe paper is generally clearly and well-written.\n\n### Novelty\n\nThere is some novelty in the new definition of the fairness.\n\n### Reproducibility\n\nThe reproducibility is good.",
            "summary_of_the_review": "The motivation for the problem (i.e., fairness in FL) studied by this paper can be strengthened. There seems to be some mismatch between what is mentioned in the introduction and the main paper (e.g., contributions of agents are not explicitly/clearly defined). The approach (via Bayes optimal error) and setting (data heterogeneity) can be justified better. Overall, there is certainly merit to this paper, but in its current form, it is not ready to be published.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3222/Reviewer_MUHj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3222/Reviewer_MUHj"
        ]
    },
    {
        "id": "O1FTN6gOCGC",
        "original": null,
        "number": 2,
        "cdate": 1666604863087,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604863087,
        "tmdate": 1670196480499,
        "tddate": null,
        "forum": "9bVBH1GD5sr",
        "replyto": "9bVBH1GD5sr",
        "invitation": "ICLR.cc/2023/Conference/Paper3222/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of training fair models in federated learning with heterogeneous agents. The authors proposed a new fairness metric which is the maximal difference of excess risks for any pair of agents. The authors then proposed an algorithm which is based on the Expectation-Maximization algorithm, that clusters the similar agents into groups and minimize the fairness-violation loss.\n\nTheoretically the convergence guarantee for the proposed algorithm was provided for linear agents, and non-linear agents with smooth and strongly convex loss functions. Empirically the authors compared the proposed algorithm with FedAvg, q-FFL (Li et al., 2020b) and AFL (Mohri et al., 2019), showing that the proposed algorithm performs better under the proposed fairness criteria.",
            "strength_and_weaknesses": "Strength:\n- Learning fair models in federated setting with heterogeneous agents is an important research problem. The proposed fairness criteria minimizes the maximal difference of excess risks for any pair of agents which is a natural criteria to use.\n- The FOCUS is based on the Expectation-Maximization algorithm which is very intuitive.\n- Both the theoretical convergence guarantees and experimental comparisons are studied carefully, and the experimental results demonstrate the effectiveness of the FOCUS algorithm in minimizing the proposed fairness loss.\n\nWeakness:\n- First concern is about the scalability of the proposed fairness criteria and algorithm. The proposed metric minimizes the maximal difference of excess risks for any pair of agents, whose evaluation requires looking at all the pairs of agents --which grows exponentially with the number of agents. It seems that even evaluating such a fairness criteria is computationally-expensive with a large number of agents.\n- Following the scalability concern, the EM algorithm also tends to have a slow convergence.\n- The experimental comparisons are comparing the proposed algorithm ---which is designed to minimize the proposed fairness loss ---to other methods which are not designed for that. I think a fair set of comparisons should include the performance of FedAvg if that is also using the clusters that FOCUS ends up using; and evaluating the results of FOCUS on the fairness criterion proposed in other works, e.g. the agnostic loss in AFL (Mohri et al., 2019). Moreover the runtime and computational costs should be compared as well. From the current evaluations, it is hard to conclude that FOCUS out-performs the existing methods. E.g. FedAvg is not using any clustering and outputs one single model; but FOCUS is allowed to return M models to the agents.\n\nTypos:\n- Equation (2), $\\Epsilon$ missing the subscript $e_1$",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is written nicely and the structure was clear to follow. There was some concern on the comparisons and evaluations w.r.t. other baselines in the prior works. The implementation details and code are provided.",
            "summary_of_the_review": "Overall this paper studies an important problem ---FL with heterogeneous agents. The proposed fairness metric and algorithm are quite intuitive. The experimental comparisons are missing some necessary evaluations. I also have some concern on the scalability of the proposed approach.\n\n\n---------\nPost-rebuttal: I thank the authors for the detailed response to my questions and concerns! The additional clarifications and comparisons addressed my concerns on the scalability and computation cost of the proposed methods. Therefore I'd like to improve my score based on the author feedback.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3222/Reviewer_wM5N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3222/Reviewer_wM5N"
        ]
    },
    {
        "id": "Au1WBTyshmX",
        "original": null,
        "number": 3,
        "cdate": 1666908033333,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666908033333,
        "tmdate": 1666910710718,
        "tddate": null,
        "forum": "9bVBH1GD5sr",
        "replyto": "9bVBH1GD5sr",
        "invitation": "ICLR.cc/2023/Conference/Paper3222/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers a federated learning setting. The idea and novelty of the paper consist in taking fairness considerations into account. Namely, agents that contribute a lot of/little data or lower/higher quality data should not be treated the same way. ",
            "strength_and_weaknesses": "Strength: \n- the motivation of the paper is clear, and important. It is unfair to give the same outcomes and benefits to agents that barely contribute to the model learned by the federated learning algorithm and simply free-ride. Further, the presence of agents with low-quality data can further compromise the performance of agents with high-quality data. \n- the formal definition of fairness is an interesting proxy to address this issue, by looking at the excess risk of the model seen by population e compared to the best model population e could train on their own data. In turn, if this excess risk is low, it guarantees that a population with low-quality data sees a model $\\theta_e$ that cannot be (much) better than the low-quality model they could have trained themselves. \n- the techniques are interesting, with a 2-step approach: i) clustering agents with similar data distribution together (so that agents with accurate/high-quality data are for example clustered together), then ii) give different weights to different agents as a function of their cluster in the loss minimization problem\n- The authors provide theoretical statement showing that their algorithm performs well for a reasonable/common class of losses (linear + smooth and strongly convex)\n- The authors provide convincing experiments showing that their approach leads to better fairness, but also overall model performance, than previous work. The results are especially good when the data is synthetically generated according to the assumptions of the paper, but the experiments also show that their approach does better than FedAvg and previous work even on real data. \n\nWeaknesses:\n- I found the notations of the main theorem a bit confusing, with K being the number of rounds (which is what I think would typically be called T), and T the number of updates in each round. \n- Theorem 4 and the fairness analysis is a bit hard to understand in the case of smooth + strongly convex losses. It is also a bit weak in that i only looks at 2 clusters. However, this weakness is partially made up for through the experiments. \n- It is possible I missed this, but the authors seem to assume that M is known in advance, since it is provided as an input to the algorithm. How can one estimate/upper-bound the number of clusters? This seems that it would require to run the clustering first, before actually knowing M, and evaluating how close the data distributions are in each cluster. ",
            "clarity,_quality,_novelty_and_reproducibility": "The work and definition of fairness are original. Results are supported by theorem and proofs as well as experiments. ",
            "summary_of_the_review": "I think this is a good paper that studies an important problem and provides a comprehensive (both theoretically and practically) solution to said problem, and as such recommend acceptance into the program",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3222/Reviewer_p3Qb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3222/Reviewer_p3Qb"
        ]
    }
]