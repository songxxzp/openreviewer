[
    {
        "id": "-xYm8_ygwGg",
        "original": null,
        "number": 1,
        "cdate": 1666258914273,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666258914273,
        "tmdate": 1666258914273,
        "tddate": null,
        "forum": "UAmH4nDH4l",
        "replyto": "UAmH4nDH4l",
        "invitation": "ICLR.cc/2023/Conference/Paper5197/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a contrastive vision transformer for self-supervised OOD detection. An ensemble module is developed to build representative ensemble features for achieving the balance between semantic and covariate OOD detection. The experimental results show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "## Strength\n### An ensemble module is developed to build representative ensemble features for achieving the balance between semantic and covariate OOD detection.\n\n## Weaknesses\n(1) The architectures and the modules in the proposed method are not novel. The four extra modules (except for the ensemble module) are mostly existing modules from previous methods, such as MOCO, MOCO v3, BYOL, et. al.\n(2) The contributions of this paper are not significant. The major contribution is apply the contrastive learning architecture for OOD detection.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\n\nQuality: fair\n\nNovelty: poor",
            "summary_of_the_review": "The novelty and the contributions of this paper are limited as indicated in the weaknesses. Thus, I tend to give a negative rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5197/Reviewer_RtUc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5197/Reviewer_RtUc"
        ]
    },
    {
        "id": "3ct4veX2mWi",
        "original": null,
        "number": 2,
        "cdate": 1666662225511,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666662225511,
        "tmdate": 1666662225511,
        "tddate": null,
        "forum": "UAmH4nDH4l",
        "replyto": "UAmH4nDH4l",
        "invitation": "ICLR.cc/2023/Conference/Paper5197/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an attention-based contrastive learning model (CVT)  for OOD detection in image classification tasks based on self-supervised methods by using a ViT as a feature extractor.  The proposed model outperforms several SOTA methods based on CIFAR-10/100.",
            "strength_and_weaknesses": "Strengths:\nIdentifying OOD data is an important topic.  The ability to identify OOD data using the strengths of SSL is a valuable endeavor.\n\nWeaknesses:\nI think it would be valuable to see how the choice/amount of augmentation impacts OOD score.  For example, if a sample is augmented with new/significant noise, will it be declared out of domain?\n\nIt's not clear to me from 3.3 and 4.1/2 how the other models in Table 1 were trained.  If they were just trained to predict ID vs. OOD, this doesn't seem like a realistic task since that will never be known in the real world.  Or were the models trained to carry out the normal prediction task and only if the overall class likelihood was near chance, was it declared OOD?   And in contrast is the proposed model scored using the new metric defined in (4)?\n\nThe authors reference VAEs as another common approach which has been successful in the past, but that is not explored here.  Why?\n\nCIFAR is a small dataset (i.e. the images are small) so the queue size can be quite large on a \"standard\" machine.  If the batch size has to be reduced because \"normal\" sized images are used (512x512 or much larger), how will this impact performance and/or training/inference feasibility?\n\nWhy is the approach of Fort et al. not shown in Table 1? \n\nThe authors claim better than SOTA, but this isn't quite true.  The binary task isn't really \"fair\" at some level because ID vs. OOD is not known in advance.  The best comparison seems to be with Fort et al., but all related work and discussion around this is missing.  This omission feels intentional, but the authors need to discuss the strengths/weaknesses to each.\n\nComments:\nThe authors state a number of factors generating outliers.  In the context of self supervision, in particular, often \"outliers\" arise because no massive dataset exists for the task of interest.  The pretraining dataset may be a large public dataset like imagenet.  The learned representation is then transferred or fine-tuned to the domain/task of interest.  Can such an approach still be used in those scenarios.\n\nWhat impact does augmentation procedure have on the ability to identify OOD samples?\n\nHow dependent is this approach on the specific constrastive learning approach selected?\n\nThe authors state three metrics are used in 3.3, but tables 1 and 2 only focus on AUROC.  What do the other metrics reveal?\n\nFigure 4 seems confusing.  For as clean as the embedding clusters are on the right, there is still fairly significant overlap of the distributions on the left.",
            "clarity,_quality,_novelty_and_reproducibility": "It's not clear to me how the other models in Table 1 were trained.  Were these models just trained as binary classifiers to predict ID vs. OOD?  How the metrics were defined for both the alternate and proposed approach in this table is unclear.\n\nFort et al. is shown to be significantly better than the proposed approach, but this approach is never discussed anywhere- attention needs to be given to this in the related works.",
            "summary_of_the_review": "The authors propose a clear, relatively straightforward approach to identifying OOD data using a supervised framework.  Overall the writing is clear, but a few improvements could be made.  The impact is limited by the focus on small toy datasets (CIFAR 10/100, SVHN) as opposed to more real world datasets where such approaches would be needed. \n\n The results when compared against a binary ID/OOD task are much better, but this doesn't feel like a fair comparison since such information would never be known in advance; when comparing to the more standard task (Table 2) the results are still quite good, but not better than Fort et al.  Additional discussion needs to be provided to this key work.  The paper can still be accepted even if it does not beat Fort et al., but it can't be overlooked as it is currently since this is SOTA (not the approaches in Table 1, as the authors claim).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5197/Reviewer_nmrp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5197/Reviewer_nmrp"
        ]
    },
    {
        "id": "kqW8bGUgn77",
        "original": null,
        "number": 3,
        "cdate": 1666737634123,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666737634123,
        "tmdate": 1666737634123,
        "tddate": null,
        "forum": "UAmH4nDH4l",
        "replyto": "UAmH4nDH4l",
        "invitation": "ICLR.cc/2023/Conference/Paper5197/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes to use a contrastive method that uses ViT models as encoders for out-of-distribution detection. Different from previous works, this paper uses contrastive methods with Vision Transformers encoders instead of convolutional ones. Different representations obtained by the model seem to be more appropriate to different settings (near vs far OOD) and they are combined. The resulting method obtains good results on OOD detection datasets.",
            "strength_and_weaknesses": "Strong Points:\n\nHaving a strong encoder is very relevant for OOD detection, thus having a contrastive method that works with ViT model is beneficial.\n\nWeak Points:\n\nSince the contribution of this paper is empirical, coming from replacing a convolutional model with a stronger, Transformer model, the evaluation should be more rigorous. At the moment only CIFAR10-100 and SVHN are used to form IN / OOD pairs. The setups from CSI and [A] should also be evaluated.\n\nThe claim that Encoder features and Predictor features are more appropriate in different settings is based on the experiments in Table 3. But these are not conclusive, as Encoder features are better in 3 of the 4 cases so it cannot be said that they are better for semantic or covariate shifts. \n\nMultiple seeds should be used when computing the results and confidence intervals should be given.\n\n[A] Sun et al. \u201cOut-of-Distribution Detection with Deep Nearest Neighbors\u201d ICML 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper can be reproduced. The originality of the work is low. Given that the paper repurpose existing methods with different encoders, the quality should come from rigorous evaluations, but the paper still needs to improve in this regard.",
            "summary_of_the_review": "The paper proposes to use a contrastive learning based on ViT models for OOD detection. It results in a good method, but it needs to be evaluated more rigorously. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5197/Reviewer_YUrT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5197/Reviewer_YUrT"
        ]
    }
]