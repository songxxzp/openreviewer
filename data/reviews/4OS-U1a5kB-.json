[
    {
        "id": "rHCAZCm4-_",
        "original": null,
        "number": 1,
        "cdate": 1665636765210,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665636765210,
        "tmdate": 1665636765210,
        "tddate": null,
        "forum": "4OS-U1a5kB-",
        "replyto": "4OS-U1a5kB-",
        "invitation": "ICLR.cc/2023/Conference/Paper3574/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a contrastive-learning inspired objective for shaping the reward function for RL agents to prevent encountering predicted, future \u201cunsafe\u201d states while training. They validate their claims on standard Mujoco environments.",
            "strength_and_weaknesses": "For full disclosure, I have already reviewed a previously submitted (and rejected) version of this paper. My review is based on criticisms/strengths from the old review and old meta-review, along with the changes that have been made to the paper in this new submission. Overall, this paper is much improved from the original version I reviewed.\n\n## Strengths\n\n**Clarity:** The paper is generally clear and readable.\n\n**Mathematical Rigorousness:** \n\n- The contrastive classifier $F_\\theta$\u2019s intuition isn\u2019t described very well, but other than that Section 4.1 describing the objective is intuitive and grounded\n- The proof of Theorem 1 is well-presented and presents a mostly-grounded way of selecting the dual variable $\\lambda$ rather than just treating it as a hyperparameter like in other works.\n\n**Experiments/Results:**\n\n- Experiments and ablations are sensible. The results presented hold up the claim that this works as well as SMBPO while being a model-free method, which is valuable due to fewer assumptions made by model-free algorithms about the environment.\n\n## Weaknesses\n\n**Clarity:** \n\n- Figure 1 is not very useful for demonstrating how the method works. There\u2019s no indication of some risk prediction threshold ($\\eta$) nor anything to indicate that actions are actually modified based on this threshold ($a_t, a_t\u2019$ aren\u2019t differentiated).\n- The contrastive classifier $F_\\theta$\u2019s intuition isn\u2019t described very well, and Eq. 2 just appears without explaining the terms. I understand that this derivation of $F_\\theta$ makes Eq 3 convenient (and maybe the authors figured out Eq 3 before figuring out Eq 2), but this could be made more clear.\n\n**Experiments:** \n\n- Assumption 1 is presented without empirical evidence it exists. Can the authors experiment and demonstrate how true Assumption 1 is?\n- It\u2019s hard to say that RPT+DA is better, as the authors claim, due to the overlapping standard deviations.\n\n**Method and Framing:** \n\n- Intro: \u201cbenefitting from the generalizability of risk prediction, the proposed approach can avoid safety violations much earlier in the training process\u201d \u2192 It\u2019s not clear intuitively how this risk prediction differs from the aforementioned prior work at this point in the intro, and therefore how it is more generalizable than prior work.\n- Method: \u201cWith safety constraints and risk predictions, the proposed approach (to be\nelaborated below) is expected to incorporate the strengths of both categories of safe RL techniques.\u201d \u2192 by this point, it\u2019s still not clear why this is better. **The authors should make more clear, in the intro and method introduction, that their contrastive method is able to reason about *future risk*, while many current methods either only implicitly do (through reward modifications and TD-learning) or do not at all (binary classification with model-based RL).** This would make the paper's contributions clearer, and make it better in my opinion.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is pretty clear and easy to read.\n\nThe paper's objective and Theorem 1 is original/novel.\n\nThe paper is of above average quality, but figure 1 should be improved.\n\nThe paper seems reproducible from the paper descriptions alone.",
            "summary_of_the_review": "I recommend a weak accept. While I still have some problems with this paper, especially regarding how it's framed, I think the paper is much improved over the previous submission that I reviewed and makes valuable contributions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3574/Reviewer_mSBr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3574/Reviewer_mSBr"
        ]
    },
    {
        "id": "ja2h0ASNOB2",
        "original": null,
        "number": 2,
        "cdate": 1666894824636,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666894824636,
        "tmdate": 1666894824636,
        "tddate": null,
        "forum": "4OS-U1a5kB-",
        "replyto": "4OS-U1a5kB-",
        "invitation": "ICLR.cc/2023/Conference/Paper3574/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies safety in reinforcement learning. The goal is to maximize return while keeping the cost of visiting unsafe state-action pairs below a threshold. The paper proposes learning a classifier $F_\\theta(s, a)$ that predicts whether a given state action $(s, a)$ will lead to eventually visiting an unsafe state-action pair. This is then used to update the policy and the $F_\\theta$ is then relearned on the fly.  ",
            "strength_and_weaknesses": "Strength:\n\n1. Paper is on safe RL which is an important topic\n\nWeakness:\n\nThis paper suffers from three concerns.\n\n1. *Broad Concerns:* The paper makes assumptions that will not apply to applications that the paper uses for motivation in the beginning. To begin with, the paper assumes a deterministic setting that holds in very simple cases. Secondly, a real robotic experiment can immediately fail if the robot hits a wall and breaks down. Therefore, encountering unsafe states even once may not even be an option in such cases. Lastly, $p(y = 1\\mid, s, a)$ is a non-stationary distribution that evolves as the policy evolves. This seems undesirable from a learning point of view, as the model has to constantly adjust to the changes in the policy. As the policy optimizes for state-action pairs that are safe/unsafe based on value of $p(y=1 \\mid s, a)$ and not directly the value of $c$, therefore, the policy may start taking certain actions or avoid actions, in a given state, as the training progresses. A more desirable strategy would be that $(s, a)$ pairs that are marked as unsafe are never revisited. \n\n2. *Technical Concerns:* Several statements in this paper seem either wrong or unjustified to me. It is my understanding that the classifier $F_\\theta(s, a)$ is trained on random $(s, a)$ pairs that are treated as safe $(y=0)$, and a set of unsafe pairs $(y=1)$ that were selected during the previous exploration. If yes, then the Bayes classifier for this problem will be given by $p(y=1 \\mid s, a)$ where\n\n$p(y=1 \\mid s, a) = \\frac{p(s, a \\mid y=1) p(y=1)}{p(s, a)} = \\frac{p(s, a \\mid y=1) p(y=1)}{ p(s, a \\mid y=1) p(y=1) + p(s, a \\mid y=0) p(y=0)}$.\n\nHere $p(s, a \\mid y=0)$ denotes the probability over random state-action pairs in the dataset, as that is the noise distribution for contrastive learning. Now if $F_\\theta$ is an expressive class that contains $p(y=1 \\mid s, a)$ then we have asymptotically $F_\\theta(s, a) \\rightarrow p(y = 1 \\mid s, a)$ on $(s, a)$ pairs that lies in support of the distribution on which it was trained.  \n\nThe paper equates $p(y =1 \\mid s, a) = \\frac{F_\\theta(s, a)}{1 - F_\\theta(s, a)}$ which does not make sense to me.  Note that in the above version, no correction is required and all probability values are in [0, 1]. \n\nBecause of this error, or confusion, I am unable to fully check Lemma 1 and Theorem 1. \n\nIn another example, assumption 1 seems unclear to me. Does the increase have to be strict? If not, this already rules out deterministic policies since a deterministic policy in a deterministic setting (which is what this paper assumes) will result in a straight path and if this policy visits an unsafe trajectory, then $p(y=1 \\mid s, a) = 1$ for all state-action pairs along the way.  This assumption needs to be made clearer to understand it clearly, and an example can help. \n\n3. *Writing Concerns:* The paper is not well-written. I give numerous examples below in the clarity section.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity/Writing Quality\n\n1. The first point at the end of the introduction, is not a contribution but a description of what is being done. \n2. Definition of $\\tau$ uses $|\\tau|$ which leads to a circular definition. You can define $|\\tau|$ as the number of actions in the trajectory in a second line.\n3. Definition 1 is also partially an assumption since it is assumed that $c$ is binary-valued. \n4. Definition 2 is likewise not a definition. It is an algorithmic choice. The true safety is decided by $c$ and not $\\eta$ and $p(y=1 \\mid s, a)$.\n5. Unsafe state set $S_U$ is actually a state-action set\n6. What is $\\pi$ on line 8? Does it mean $\\pi_\\phi$\n7. What do lines 14 and line 15 do in the Algorithm? Are they being used in optimizing $F_\\theta$. \n8. $F_\\theta$ is the learned model and not the Bayes' classifier. It could be good to separate these two notions. \n\nSuggestions:\n1. Consider using $\\pi^\\star$ as opposed to $\\pi^*$ \n\nQuestions:\n1. Can definition 1 be generalized to handle stochastic transitions?",
            "summary_of_the_review": "Because of concerns with methodology, writing, and technical statements, I am recommending rejection. My score can change greatly if the authors clarify any misunderstanding. In particular, I would like to know if technical statements concerning the relation between the Bayes' classifier $p(y=1 \\mid s, a)$ and the learned classifier $F_\\theta$ is true. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3574/Reviewer_aGao"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3574/Reviewer_aGao"
        ]
    },
    {
        "id": "beKD-ozaYqb",
        "original": null,
        "number": 3,
        "cdate": 1666961951131,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666961951131,
        "tmdate": 1667134517993,
        "tddate": null,
        "forum": "4OS-U1a5kB-",
        "replyto": "4OS-U1a5kB-",
        "invitation": "ICLR.cc/2023/Conference/Paper3574/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors of this study provide a risk-preventive training strategy for safe RL that trains a statistical contrastive classifier to forecast the likelihood that a state action combination would result in unsafe states. They can gather risk prevention trajectories and remodel the reward function with risk penalties based on the expected risk probability to promote safe RL policies. In robotic simulation environments, the authors run experiments. The outcomes demonstrate that the suggested methodology outperforms traditional model-free safe RL approaches and is on par with cutting-edge model-based methods in terms of performance.",
            "strength_and_weaknesses": "**Strengths:**\n- The motivation is clear and promising.\n- The prior art is discussed concisely.\n\n**Weaknesses:**\n- I couldn't understand some statements in the paper as they seem unjustified. For instance, the classifier $F_{\\theta}$ is trained on randomly selected state-action pairs that are treated as \"safe\" and some pairs that are \"unsafe\", which are selected in the previous exploration. According to this, the Bayes classifier for such a problem is given by $p(y= 1|s, a)$, which is defined as:\n$p(y = 1|s, a) = \\frac{p(s, a|y=1)p(y=1)}{p(s, a)} = \\frac{p(s,a|y=1)p(y=1)}{p(s, a|y=1)p(y=1) + p(s,a|y=0)p(y=0)}$,\n\nwhere $p(s,a|y=0)$ is the probability over random state-action pairs in the dataset, corresponding to the noise distribution for contrastive learning. If we consider that $F_{\\theta}$ is an expressive class containing $p(y=1|s,a)$, then we asymptotically have $F_{\\theta} \\rightarrow p(y=1|s,a)$ on $(s,a)$ pairs that lie in the distribution on which the classifier is trained. The paper, however, equates $p(y=1|s,a)=\\frac{F_{\\theta}(s,a)}{1 - F_{\\theta}(s,a)}$, which is left unjustified and I couldn't understand it. Due to such an unjustified claim, Lemma 1 and Theorem 1 become inconsistent to me.\n\n- Why didn't the authors consider giving a continuous \"safeness\" value between 0 and 1? In my opinion, using a binary representation for the safety of a state-action pair is taking the easy way out. Considering a continuous value, on the other hand, would increase the novelty.\n- The comparative evaluations are insufficient as the authors only consider Ant, HalfCheetah, Humanoid, and  Hopper. While these can be considered challenging tasks, I expect the results for BipedalWalker (from BOX2D), Walker2d, and Swimmer. Although the latter has a low state-action dimensionality, it is a very challenging task in which many SOTA methods exhibit poor performance. \n- Although it is indicated for the HalfCheetah environment that the robot violates the safety constraint when it falls over, I don't believe it is a credible judgment. Because the Cheetah environment is \"stable\", a trajectory doesn't terminate unless a pre-specified number of time steps is reached, i.e., 1000. While it'd good to examine the proposed method's performance in a stable environment, the authors don't provide such a discussion. \n- The authors should assess a method's performance on a distinct evaluation environment per a pre-specified number of time steps, as it is a common practice in deep RL literature. For example, every 1000 steps, the learning is stopped, and the agent is evaluated in a distinct environment where no exploration or learning is performed for 5-10 episodes. However, the authors used the training rewards as far as I understood. \n- The number of random seeds used to produce the reported results is highly insufficient. At least ten random seeds should be used [per the deep RL benchmarking standards](https://ojs.aaai.org/index.php/AAAI/article/view/11694).\n\n**Detailed Comments:**\n- An RL agent should be \"an RL..\" not \"a RL\".\n- Replace the reward function in the MDP definition with $\\mathcal{R}$ instead of $r$ as it can confuse readers.\n- Punctuation should be put after each equation as they are used in conjunction with the text.\n- It looks ugly when a reference is used, such as: \"Similar to (Hans et al., 2008)...\". Either refer to the natbib package or use it as: \"Similar to the work (Hans et al., 2008)...\".",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** The authors are clear throughout the paper in expressing their motivation and describing their methodology (except for unjustified claims and math in the paper). \n\n**Quality:** The study is a bit behind the quality standard for a venue like ICLR due to my concerns with the credibility of the experiments. See the _Weaknesses_ section.\n\n**Novelty:** In my opinion, the proposed method can be regarded as novel.\n\n**Reproducibility:** I don't believe the reported results are reproducible as the authors did not provide a source code, although they extensively explain the implementation and experimental setup. In fact, the number of seeds used to evaluate the methods' performance is not sufficient.",
            "summary_of_the_review": "While the authors appear clear in their claims and methodology, I don't believe the paper can be accepted to a venue like ICLR. My main concerns with this study are:\n1) The simple binary assumption used to define the cost function $c(s, a)$ and unjustified claims.\n2) The credibility of the experiments:\n- An insufficient number of seeds.\n- Insufficient environments.\n- Reported returns are training rewards, as far as I understood.\n3) Reproducibility as no code or any anonymous repository, which does not reveal the authors' identity, of course, is provided.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3574/Reviewer_jp1h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3574/Reviewer_jp1h"
        ]
    }
]