[
    {
        "id": "5TiZ4Xcjih",
        "original": null,
        "number": 1,
        "cdate": 1666490080628,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666490080628,
        "tmdate": 1666490080628,
        "tddate": null,
        "forum": "KoEa6h1o6D1",
        "replyto": "KoEa6h1o6D1",
        "invitation": "ICLR.cc/2023/Conference/Paper1303/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method \"interventional rationalization (Inter-RAT)\" to discover the causal rationales. Inter-RAT tries to detect spurious correlations between the input and rationales, and between rationales and results, respectively, by analyzing the causalities among them and identifying the confounder in the causalities, and further remove these spurious correlations using a causal intervention method based on the backdoor adjustment. Experimental results on three real-world datasets are used to verify the effectiveness of Inter-RAT.",
            "strength_and_weaknesses": "- Strength\n    - The paper is trying to work on a practical and useful problem regarding spurious correlations between the input and rationales, and between rationales and results, respectively.\n- Weaknesses\n    - The writing and organization of the paper can be much improved. For example, the format of the citations in the main text is incorrect.\n    - Figure 1 seems not an appropriate example to illustrate spurious correlations\n        - Since the label is \u201cManslaughter\u201d, if the generator fails to select the underlined text, the predictor can not give the right prediction. So, the optimization process will force the generator to select the underlined text, right?\n    - Error in the Equation (1) in the Problem Formation of rationalization\n        - It should be $P(Y|X) = \\sum_R P(Y|R)P(R|X)$, instead of $P(Y|X) = P(Y|R)P(R|X)$\n    - Chang et al. (2020) worked on the same problem of spurious correlations and used the same casual inference method closely related to this paper, but the difference between these two papers is not well illustrated with enough details. I suggest the authors to clarify the difference and contributions clearly.\n    - The readability can be improved as well. Although the paper exploits some existing methods of causal inference, such as Structural Causal Model (SCM), it should be more self-contained for better understanding.\n    - The experiments are pretty weak. I do not buy the results of the experiments.\n        - (1) Baselines. It is ok to choose RNP and INVRAT (2020) since they are most related. But some latest or STOA rationalization methods, such as SPECTRA (2020), A2R(2021), are missed in the experiments. To tackle the challenge of spurious correlations, there is also a new method CDA(2021). It is pretty strange that the results in Table 2 are from A2R but A2R is not compared in the experiments.\n            - [SPECTRA (2020)] SPECTRA: Sparse Structured Text Rationalization, EMNLP-2020\n            - [A2R(2021)] Understanding Interlocking Dynamics of Cooperative Rationalization, NeurIPS-2021\n            - [CDA(2021)] Making a (Counterfactual) Difference One Rationale at a Time, NeurIPS-2021\n        - (2) Results. The implementation details are quite vague. How the batch size and learning rate are chosen? Are they the best for all the baselines? The results are sensitive to the sparsity, but from Table 1, I noticed that the paper only reports the preset \\alpha in equation 4 but the real sparsity of selected rationales on the test set is not presented. My previous experiences showed that Equation 4 can only constrain the sparsity softly and the final sparsity may be quite different from \\alpha in various methods. The paper does not report \\lambda_1 and \\lambda_2 in equation 4 as well. For Table 2, the paper mentions that it adopts Bert to replace bi-GRU for both RNP and Inter-RAT. But I noticed that the results of Bert_RNP are the same as the one in A2R implemented by DeYoung et al. (2020), which does not replace GRU with Bert but uses Bert embedding as input to GRU. I suggest author to clarify and fix the issue.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The writing and organization of the paper need to be improved for better Clarity and Quality\n- Novelty and contribution should be further clarified.\n- Based on the experiments reported in the paper,  some issues need to be fixed for reproducibility.\n",
            "summary_of_the_review": "Some major issues in the paper need to clarified and improved.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1303/Reviewer_DUoA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1303/Reviewer_DUoA"
        ]
    },
    {
        "id": "4905KAjOXEt",
        "original": null,
        "number": 2,
        "cdate": 1666622091536,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622091536,
        "tmdate": 1666622091536,
        "tddate": null,
        "forum": "KoEa6h1o6D1",
        "replyto": "KoEa6h1o6D1",
        "invitation": "ICLR.cc/2023/Conference/Paper1303/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to design a rationalization method based on causal inference. To achieve this goal, the authors first analyze the potential confounder underlined the input and output. Such latent confounder may influence the relation between the input and the rational as well as the rational and the output. To alleviate the impact of the confounders, the authors design a method to achieve the real causal relation. In the experiments, the authors have conducted many experiments to demonstrate the effectiveness of the proposed models.\n",
            "strength_and_weaknesses": "\nThe paper is well written, and it is very clear. The motivation is interesting.\nMy major concerns are as follows: the confounder is hard to estimate, using entire label set can be not enough. The authors should better detail why the label set can be used to represent all the latent confounders. In addition, if different labels are correlated, whether we can still have equation 5. It seems that, without the causal concepts, the models can still be easily obtained, why we need causal inference. Basically, if we know that the labels can influence X, R and Y, we can easily build a more advanced neural model without the causal inference notions. The identifiability of (5) can only be proved when C can fully represent the latent confounders.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, but I think the novelty is not enough. It only use very simple causal inference concepts to solve an existing problem.",
            "summary_of_the_review": "See the above comments",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1303/Reviewer_vjRf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1303/Reviewer_vjRf"
        ]
    },
    {
        "id": "qpDe55NtAG",
        "original": null,
        "number": 3,
        "cdate": 1667307528204,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667307528204,
        "tmdate": 1667307528204,
        "tddate": null,
        "forum": "KoEa6h1o6D1",
        "replyto": "KoEa6h1o6D1",
        "invitation": "ICLR.cc/2023/Conference/Paper1303/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new rationalization method, in which the standard conditional probabilities are replaced by interventional probabilities, which in turn are expressed by the back-door formula. The purpose, roughly, is to encourage generating rationales that have sufficient causal influences on the outcome rather than merely contain sufficient information about the outcome, so as to resist the temptation to select rationales based on spurious correlations due to confounding. Experiments on some benchmark datasets are presented to demonstrate the competitive performance of the proposed method. ",
            "strength_and_weaknesses": "Strengths:\n\n1. The task is an important one, and the proposal is sensible, at least in the abstract.\n2. The experimental results seem promising.\n\nWeaknesses:\n\nMy main reservation has to do with the theoretical or conceptual development of the proposed method. As I said, I appreciate the general goal, of distinguishing causal effect from spurious correlations and using the former to select rationales and make predictions. But the exposition of the details of the proposed method is rather hard to follow and some details are particularly puzzling. For example, equation (1) is already confusing; shouldn't R be integrated out? More importantly, I have two main concerns. First, I don't quite follow the explanation in the paper regarding the possible confounding of R and the *prediction* Y. As stressed more than once in the paper, the basic framework is such that in the prediction module, the prediction Y is made solely based on R. Despite the presence of the sparsity and continuity constraints, I fail to see how C can have a causal influence on R that bypasses Y. It is not entirely clear to me, but the proposed method seems to build a predictor not just on R but on a function of R and C (labeled as H). If so, then the proposed method deviates from the standard rationalization framework and the confounding by C is really self-imposed. On the other hand, I am also puzzled why it should be the predictor of the target variable (which is perhaps better denoted as Y_{\\hat}) rather than the target variable itself that is of interest here. Shouldn't we be more interested in which parts of the input text causally influence the true value of the target variable rather than the predicted value of the target variable? In fact, if we are talking about the target variable itself, then I have no trouble understanding the possibility of some confounding between R and Y.  \n\nSecond, as acknowledged in the paper, the confounding is only partially observed. Then why should we regard the back-door adjustment as appropriate for the intended purpose? A related worry is that the proposed method seems to assume a specific causal structure, which licenses back-door adjustment. As I recall, some motivating examples in this literature feature quite different causal structures. I wonder how the proposed method would perform in those cases.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The main proposal is reasonably novel, but as I indicated above, the details of the proposal are not entirely clear and sometimes puzzling. I am not sure how reproducible the empirical results are. The technical quality is overall ok, but there is quite some room for improvement. A minor suggestion is that it is better to call the causal models used in this paper graphical causal models rather than SCMs, as the main machinery of the latter is nowhere mentioned in the paper.",
            "summary_of_the_review": "This is a sensible and promising paper with interesting empirical results but some confusing theoretical details.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1303/Reviewer_qL6H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1303/Reviewer_qL6H"
        ]
    },
    {
        "id": "NwWCCuPTpUL",
        "original": null,
        "number": 4,
        "cdate": 1667438390593,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667438390593,
        "tmdate": 1667438390593,
        "tddate": null,
        "forum": "KoEa6h1o6D1",
        "replyto": "KoEa6h1o6D1",
        "invitation": "ICLR.cc/2023/Conference/Paper1303/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors attempt to improve the accuracy of methods that explain the inferences of natural language processing (NLP) models (particularly neural network models). Specifically, the authors propose to: (1) learn a causal model; (2)use that causal model to identify which factors in the neural network model are causal; and (3) Use those factors only to explain the inference.",
            "strength_and_weaknesses": "\u2013 Figure 2 will be confusing to readers, largely because it lacks a caption. \n\n\u2013 Inferences of natural language processing models are an unlikely target for causal modeling. Much of the actual generative structure of the world that produces natural language is latent, thus the inferences NLP models are likely to be non-causal in nature. That doesn't make their inferences incorrect, but it will make them difficult to explain to human interpreters. Those human interpreters will likely be expecting a causal explanation that better corresponds to their knowledge of the world (which the NLP model does not have).\n\n\u2013 It is unclear how a structural causal model (SCM) is created. Specifically, it is not clear from the paper how to formulate variables, derive the structure of the SCM, or learn the conditional probability distributions of the SCM. If the SCMs discussed in section 3 are purely notional, then this should be clearly specified. Based on my best interpretation of Section 3, it appears that variables correspond to tokens, the structure of the SCM is pre-determined, and that the causal effects are estimated heuristically as given in equations 6 and 7. However, this is not nearly as clear as it should be from the text.\n\n\u2013 The nature of the experiments in Section 4 are also unclear. The goal of Inter-RAT appears to be \"better explanations\". However, the results reported in the tables of Section 4 are about precision, recall, and F1. This implies that Inter-RAT is producing categorical predictions which are either being compared to ground truth or to the predictions of a neural network. Neither of these assess the accuracy of the explanations, on how well Inter-RAT reproduces the predictions of the neural network or the actual class labels. Merely because the Inter-RAT model predicts well doesn't mean that its explanations are good ones.\n\n\\+ That said, it is interesting that the accuracy increases (substantially, in some cases). Inter-RAT does appear to be doing something interesting. What that is, though, is less clear.\n\n\u2013 The authors repeatedly use the term \"real causality\". The term \"causality\" or \"causal dependence\" alone is sufficient. Rather than \"false causality\", the authors appear to be distinguishing between mere association and causality. Those terms should be used in preference to \"real causality\".",
            "clarity,_quality,_novelty_and_reproducibility": "The writing itself is clear, but what the authors choose to explain sometimes misses important topics (see above). The paper does appear to be novel. Reproducibility is impaired due to lack of clarity.",
            "summary_of_the_review": "The paper addresses an interesting and important problem (explanation). However, the idea of explaining NLP inferences seems inherently problematic, given the odd status of causation in natural language (see above). In addition, the experiments, although extensive, are unclear and may not even address the right question.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1303/Reviewer_78ZX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1303/Reviewer_78ZX"
        ]
    }
]