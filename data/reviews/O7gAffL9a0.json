[
    {
        "id": "camFnK8u7gh",
        "original": null,
        "number": 1,
        "cdate": 1666308760772,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666308760772,
        "tmdate": 1666418530011,
        "tddate": null,
        "forum": "O7gAffL9a0",
        "replyto": "O7gAffL9a0",
        "invitation": "ICLR.cc/2023/Conference/Paper133/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors of the paper argue that confidence and dispersity are two important factors regarding the performance on the out-of-distribution datasets. Along this line, the authors propose to use the (normalized) nuclear norm of the prediction matrix as a metric for the generalization capabilities of the models. Various experiments on image datasets show there exists a strong correlation between the accuracy on the out-of-distribution data and the proposed metric.\n",
            "strength_and_weaknesses": "Strength: it is an interesting thought to consider the distribution of the predicted labels and use them as a metric to evaluate the generalization capability for the out-of-distribution data.\n\nWeakness: I do have some serious concerns about the underlying assumptions used in the paper. To me the dispersity assumption may be too strong for a lot of real applications. For those image data sets used in the paper (imgnet, cifar10, CUB-200), those assumptions seem perfect since the class labels on the shifted test data are distributed relatively evenly. However for real world applications, the labels may be distributed very differently. For example some classes are rarely observed, and some classes are observed very frequently. The power law and long tail effects are prevalent. In such a situation, the dispersity assumption used in the paper is no longer valid, and thus the proposed metric may not work. This makes me wonder if the good performance shown in the paper is only an artifact of the \u201clabel-balanced\u201d data sets.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Some typos:\n1. The second line in the Notation of Section 3.1: \\Delta_k was introduced in the where clause without being used before in the sentence\n2. The fourth line in the Notation of Section 3.1: it should be \\Delta_k instead of \\Delta_n\n",
            "summary_of_the_review": "Overall my review for this paper tends to be negative due to the major concerns I stated in the previous comments. If the good empirical performance shown is mostly due to the artifacts of the relatively label-balanced datasets then I feel maybe it is better to introduce more new datasets that have highly unbalanced classes to test the robustness of the models instead of making use of the artifact for a little improved numbers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper133/Reviewer_JXCX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper133/Reviewer_JXCX"
        ]
    },
    {
        "id": "6OHkJ-k1EPd",
        "original": null,
        "number": 2,
        "cdate": 1666542611197,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666542611197,
        "tmdate": 1669105887887,
        "tddate": null,
        "forum": "O7gAffL9a0",
        "replyto": "O7gAffL9a0",
        "invitation": "ICLR.cc/2023/Conference/Paper133/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new heuristic to estimate the accuracy of a given model for OOD test observations without labels. It uses the nuclear norm (i.e., the normalized sum of singular values) of the prediction matrix (i.e., a matrix of softmax outputs for the test data) of a given model. The measure is motivated by results from previous work, which show that the nuclear norm integrates the confidence and dispersity of the model predictions. While the confidence was shown to be predictive for the OOD performance by previous work, the paper demonstrates that the dispersity is also a predictive feature. Empirically, the paper shows a strong correlation between the computed nuclear norm and the OOD classification accuracy on multiple datasets with synthetic and realistic corruptions. The proposed heuristic shows a slightly but consistently stronger association with OOD accuracy compared to existing measures across multiple datasets, models, and types of shifts. However, the nuclear norm performs worse on test datasets with imbalanced classes and seems to overestimate the OOD accuracy on randomly synthesized datasets",
            "strength_and_weaknesses": "**Strengths**\n\nThe paper is quite clear and easy to read. The goal is stated clearly. The empirical results are in line with the claims from the abstract and introduction.\n\nIt's helpful that the paper provides an initial motivation by showing the relationship between dispersity and OOD accuracy.\n\nThe association between nuclear norm and OOD accuracy is very strong and consistent across datasets. The used visualizations show the trend very clearly.\n\nThe paper is transparent about the limitations of the proposed heuristic and points out directions for future work.\n\n\n**Weaknesses**\n\nThe presented setting for unsupervised accuracy estimation seems to be a special case of the unsupervised domain adaptation setup with only one training domain. It is not clear why the problem is not presented from the more general perspective of domain adaptation. To better assess generalization, the evaluation could consider more evaluation metrics, not only accuracy.\n\nThe results suggest that the nuclear norm performs best on datasets with balanced classes. However, on imbalanced test datasets, the predictive value of the nuclear norm starkly decreases, whereas at least one of the competing methods seems to be much more robust. In light of these results, it would be helpful to have more thorough ablations with respect to the performance on imbalanced datasets. For instance: What if both the training and test data were highly imbalanced? How well do the other heuristics (AC, ANE, DoC, MI) perform under imbalance and/or label shift? Can you provide an experiment that demonstrates that the nuclear norm can be adapted to deal with label shift?\n\nThe nuclear norm seems to consistently overestimate the OOD accuracy for randomly synthesized datasets (CIFAR-10-$\\overline{\\text{C}}$-Rand). Is there a particular reason for this? It would be helpful if you would also provide the performance metrics for each subplot in Figure 5 and Figure 7.\n\nFrom the abstract, it sounds like the nuclear norm and its connection to confidence and dispersity would be a core contribution of the paper. Only at the end of the introduction was it indicated, and in Section 3.3 explained, that the nuclear norm has been used for similar applications (domain adaptation) and that the connection to confidence and dispersity was already established by previous work. I think it would be better to state the contribution more clearly/conservatively in the abstract.\n\n\n**Questions and comments**\n\nIn Table 1, the performance improvements compared to existing methods seem relatively small in many cases. Is there a way to compute standard deviations or other uncertainty estimates to quantify the significance of these results?\n\nAt the beginning of Section 3.2, the prediction matrix is defined on the real numbers. Shouldn't it be defined on the interval (0, 1) instead?\n\nIn Section 3.2 it is suggested that classifiers that do not generalize well tend to give degenerate predictions by assigning test samples to some specific categories. Are there any insights into *which* categories typically correspond to degenerate predictions? Or is it quasi-random and depends more on the seed than on the actual categories?\n\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty is somewhat limited because the nuclear norm has been used in the context of domain adaptation and the connection to confidence and dispersity was already established. To me, this was not sufficiently clear from the beginning; it was indicated at the end of the introduction and only became sufficiently clear in Section 3.3.\n\nBoth the clarity and quality of the paper are good. The analysis is mainly empirical but the experiments are comprehensive. However, I think the paper should provide more results for the case of imbalanced datasets and/or label shifts because this is a common issue in practice.\n\nReproducibility should not be a problem, as most of the experiments are based on existing models and datasets. However, it could be improved by providing code for all experiments.",
            "summary_of_the_review": "For the goal of unsupervised accuracy estimation, the paper proposes a simple heuristic that shows a strong association with OOD accuracy across many models, datasets, and types of shifts. It is a solid paper with a thorough empirical evaluation, but the utility for imbalanced datasets is not sufficiently clear. It could be stated more clearly that the idea is heavily based on existing work, and the connection to the more general setup of domain adaptation could be clearer.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper133/Reviewer_UZsk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper133/Reviewer_UZsk"
        ]
    },
    {
        "id": "7dJn6z8fTE8",
        "original": null,
        "number": 3,
        "cdate": 1667169796664,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667169796664,
        "tmdate": 1667169796664,
        "tddate": null,
        "forum": "O7gAffL9a0",
        "replyto": "O7gAffL9a0",
        "invitation": "ICLR.cc/2023/Conference/Paper133/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies accuracy estimation from unlabeled data, in the presence of distribution shifts. They show that \"dispersity\" that captures the marginal distribution of the predicted labels correlates well with the accuracy. The paper assumes that the true label distribution is uniform and proposes a measure of dispersity that measures how much predicted label distribution deviates from uniform. This measure builds off of previous works on nuclear norm. Intuitively, this won't work when there is label shift and the paper shows that nuclear norm based estimation does indeed fail under large label shifts but is able to handle moderate label shifts. ",
            "strength_and_weaknesses": "Strengths: \n(i) The paper studies a very important problem of unsupervised accuracy estimation under distribution shifts. \n(ii) The paper considers an interesting idea and is well motivated. The paper is overall well-written and easy to follow as well. \n(iii) The paper performs very systematic and thorough experiments on a large number of datasets. \n\nWeaknesses:\n(i) The paper can do a better job at synthesizing the results of the experiments, especially because they could reveal interesting insights about deep networks or distribution shifts, beyond just providing a method that improves accuracy estimation. For e.g. why is ObjectNet always off the line---is this because of the label shift?\n(ii) The paper is missing some comparisons to recent related work: Baek et al. 2022 (agreement-on-the-line) and projection norm (Yu et al. 2022). Baek et al. is a recent paper, but projection norm was older. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is quite clear and easy to follow. I would recommend moving some intuition for why nuclear norm captures dispersity to the main paper, even though this is from prior work. It seems crucial to appreciating and understanding the proposed method. \n\nQuality: the experiments are exhaustive and claims are substantiated. The paper also addresses limitations of the method appropriately, and hence I rate the paper as high quality. \n\nOriginality: the paper takes an existing idea of dispersity (and how it can be approximated by nuclear norm of the prediction matrix) and applies it to the problem of unsupervised accuracy estimation. It is a novel application of an existing idea. ",
            "summary_of_the_review": "This paper presents an interesting and intuitive idea of using dispersity in model predictions for estimating accuracy on unlabeled data from a shifted distribution. The results are compelling and I think this would be an interesting contribution to the ICLR community and inspire follow-up work. The paper could do a better job with exploring more interesting takeaways and insights based on the strong correlation between dispersity and performance. The paper could also benefit from exploration of other metrics (not based on nuclear norm) that also try to incorporate the marginal of the predictions to determine OOD performance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper133/Reviewer_eGpU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper133/Reviewer_eGpU"
        ]
    }
]