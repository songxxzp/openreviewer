[
    {
        "id": "u2PmnogB0_",
        "original": null,
        "number": 1,
        "cdate": 1666708186061,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666708186061,
        "tmdate": 1666708186061,
        "tddate": null,
        "forum": "PzBGIu-llo7",
        "replyto": "PzBGIu-llo7",
        "invitation": "ICLR.cc/2023/Conference/Paper1455/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method to learn proximal mappings which have multiple beneficial properties as optimisers, alternative to gradient descent and variants.",
            "strength_and_weaknesses": "Strength: The formulation is neat and clean -- the paper is well written. On top of that, theoretical results are provided which is rare in learning to optimise literature.\n\nWeakness: Would be great if more experiments regarding approximation of true proximal operators were done.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and ideas are novel. I do not have many comments, I think this is a good contribution in general enabling further research.\n\n1) In my opinion, there could be more experiments regarding recovering true proximal operator in different (more challenging settings than $\\ell_1$) and especially to demonstrate whether there is any issue with increasing dimension of the problem.\n\n2) Sampling from $\\text{Unif}(\\mathcal{X})$ does not look like the most efficient way to train the objective proposed in (2) and can certainly be very difficult. Can authors give their opinion about this or whether it makes sense to use another measure or technique to estimate the loss?\n\n3) As the problem gets higher dimensional, how is the number of samples needed to approximate the loss affected?",
            "summary_of_the_review": "The paper reads well and is motivated well. Proximal operators tend to perform much better and more stable than gradient updates, therefore learning prox operators is a viable idea for improving \"learning to optimise\" tasks.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1455/Reviewer_T3qd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1455/Reviewer_T3qd"
        ]
    },
    {
        "id": "s4w0NWuy74e",
        "original": null,
        "number": 2,
        "cdate": 1666721788343,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666721788343,
        "tmdate": 1670348189247,
        "tddate": null,
        "forum": "PzBGIu-llo7",
        "replyto": "PzBGIu-llo7",
        "invitation": "ICLR.cc/2023/Conference/Paper1455/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the general problem of multi-solution optimization (MSO),\nwhere one is interested in obtaining a sampling (perhaps all) of the minimizers\nof each of a family of (nonconvex) functions $\\{ f_{\\tau} : \\tau \\in \\mathcal{T} \\}$\ndefined on a low-dimensional domain $\\mathcal{X}$ (here, $\\mathcal{T}$\nrepresents parameters -- an example given is bounding box detection, where\n$\\mathcal{T}$ is a class of digital images and $\\mathcal{X}$ is the\nfour-dimensional euclidean space of bounding box parameters). The authors\npropose a method for this problem that consists of learning a proximal operator\n(parameterized by a neural network) for the functions $f_{\\tau}$\n(assuming access to ground truth) over a training set of parameters $\\tau$ and\ninputs $x$ (the loss/training setup is similar to what one would formulate for\nmulti-task learning); at test time (given a new $\\tau$), a set of putative\nsolutions is generated by repeatedly iterating the learned operator from\nvarious random initializations (in an attempt to simulate the proximal point\nalgorithm). The authors propose an evaluation metric for these families of\nputative solutions in general MSO problems, given ground truth (it is described\nas a weighted chamfer distance between the putative set and the ground truth\nset, with weights proportional to a certain Voronoi tiling of the space\n$\\mathcal{X}$ -- hence more robust to outliers than standard metrics). They\ncompare their approach to a benchmark of gradient descent from repeated random\ninitialization and an approach that attempts to learn an operator that\napproximates a finite number of gradient descent iterations (trained similarly\nto theirs) on several synthetic and practical MSO tasks, showing that their\napproach performs well.\n\n",
            "strength_and_weaknesses": "## Strengths\n\nThe authors present a method applicable to general MSO problems that strikes a\ngood balanced between computational/optimization principles (beginning with the\nproximal point iteration) and learning-based approaches (learning a proximal\noperator on datasets). This suggests the ability to provide theoretical\nguarantees for the approach.\n\nThe experiments are extensive, and strike a good balance between synthetic\ntasks (which allow specific behaviors of the method to be understood) and\nimportant applications (symmetry detection and object detection on big\ndatasets, comparing to reasonable benchmarks). Ablations and comparisons to\nother general MSO baselines are given in the main text and the extensive\nexperimental appendices. It seems reasonable to imagine that with further\ntuning, the approach could be even more competitive relative to strong\nbaselines.\n\n\n## Weaknesses\n\nReferences to a large body of literature on learning proximal operators for\ninverse problems (esp. in imaging) seem to be omitted. See e.g. [1-3] below --\nand note that in particular the paradigm in many of the most recent works of\nthis line is different from that of LISTA (which is referenced and discussed),\nwhere the optimization problem is used to derive an architecture. These\ncomparisons would be helpful in situating the present work in this broader\ncontext (there may be intuitions developed here that can be applied in those\nsettings, and vice versa).\n\n[1] http://arxiv.org/abs/2102.07944\n\n[2] http://dx.doi.org/10.1109/JSAIT.2020.2991563\n\n[3] https://arxiv.org/abs/2209.04504\n\nThe assumption of a Lipschitz gradient in Theorem 3.1 does not seem to make\nsense to me in the context of multi-solution optimization. For instance,\nconsidering the example of bounding box detection given by the authors in the\nintroduction where $f_{\\tau}(x)$ is the minimum distance of the bounding box\n$x$ to a ground truth box in the image $\\tau$, at any time where there are\nmultiple boxes in the image $\\tau$, there will be boxes $x'$ that are at an\nequal distance from two or more ground truth boxes, implying that $x'$ is a\npoint of discontinuity for $\\nabla f_{\\tau}$ (and therefore the gradient is not\nLipschitz). More generally, it seems like this issue should arise any time\n$f_{\\tau}$ is a metric and there are multiple minima (this feels like a result\nfrom geometry, involving regularity of the distance function at the cut locus).\nThe authors may want to apply a global convergence theorem that does not\nrequire smooth activation functions and Lipschitz gradients, as Kawaguchi and\nHuang's does -- for example, Du et al., or Allen-Zhu et al, which are\nformulated for networks with ReLUs (hence non-Lipschitz gradients are an\ninherent part of the problem). Some discussion of this assumption in section\n3.3 seems warranted (I did not see it discussed anywhere -- the authors discuss\nGD vs. SGD, the network architecture, weak convexity).\n\nFor the experiments on symmetry detection, it would be\ninteresting to see how the approach compares to strong baselines that are\nspecifically tuned for these tasks (rather than just POL vs. GOL). For object\ndetection, the comparisons to Faster R-CNN are very interesting and seem\npromising, but would comparisons to more recent networks like YOLO (etc.) be\npossible also?\n\n## Questions / Minor Points\n\nI am curious whether other baseline approaches were attempted than just the one\nproposed at the bottom of page 5 -- it seems to me like this proposed approach\nmight be more challenging to optimize than the proximal loss (2), which is like\napproximating a single map which is iterated at test time, given that here we\nneed to approximate $Q>1$ iterations of the gradient map in general (which\nseems like it might depend a lot on the architecture of the network $\\Psi$\nbeing suitable). Is it possible to solve this problem for $Q=1$, then iterate\nthe learned mapping to simulate many steps of gradient descent (possibly with\nsome stability-promoting regularization?), or the like? \nI did not understand why the \"particle descent\" approach on the previous page\nis \"not comparable\" -- can't one just run gradient descent from many random\ninitializations given a \"test\" $\\tau$?\n\nFor the sparse recovery experiment, it could be that learning in this setting\nhas an advantage over GD due to the low-dimensionality ($m=4$, $d=8$) of the\nexperimental setup.\n\nWhy are DGCNN and ResNet used in the symmetry detection and object detection\nexperiments as encoders for the raw data? Is there a restriction that not only\n$x$ but also $\\tau$ should be low-dimensional here?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written very clearly, with unambiguous notation and ample detail\n(both mathematical and engineering-type). The contribution of a general\nframework and a few baselines for general MSO problems seems to be novel.\n\n",
            "summary_of_the_review": "The method is clearly presented, and the paper seems to do a useful service\nof organizing lots of thought and baseline approaches to general MSO problems.\nThe approach taken here to learning a proximal operator and then applying it as\nin the proximal point iteration has been studied elsewhere, but this study in\nthe MSO context seems new and useful. The theory presented here seems mostly\n\"plug and play\" with regards to existing results, so it could be changed to a\nMSO-relevant context by applying a different result from the literature.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1455/Reviewer_4Grh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1455/Reviewer_4Grh"
        ]
    },
    {
        "id": "5IpnM0ot34",
        "original": null,
        "number": 3,
        "cdate": 1667594315779,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667594315779,
        "tmdate": 1667595124148,
        "tddate": null,
        "forum": "PzBGIu-llo7",
        "replyto": "PzBGIu-llo7",
        "invitation": "ICLR.cc/2023/Conference/Paper1455/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors propose a new estimator, inspired from proximity operator theory.\nThe proposed estimator is supposed to be able to recover multiple local minima.\nAuthors proposed multiple experiments on toy problems\n",
            "strength_and_weaknesses": "\nMajor concerns:\nI do not know if it is because this paper is too far from my research area, but I really had troubles to understand the idea and the originality of the paper.\n\n- I found the paper very verbose \"the proximal term in our loss promotes convexity of the formulation\". I am not sure to understand how the paper is related to Wasserstein graident flow. Is it only because they also parametrize le fixed-point oerator by a neural network?\n- At the end of section 3.2 authors talk about computational gains, I did not understand were do these gains come from. Would it be possible to write the training algorithm, with the cost of each step in order to understand better? In the same vein, it cold be nice to make the difference with usual L20 method clearer, from myself I did not understand.\n\nExperiments:\nI cannot comment on all the experiments, but it feels like a succession of toy examples, without proving that the method is actually useful.\nI can comment on the sparse recovery benchmark: why authors used an $\\ell$-p norm, which is not alpha semi-convex, despite their exist alpha-semi-convex sparsity promoting norm such as the Minimax Concave Penalty, or SCAD, see for instance [1]. Morover this benchmark is performed in dimesion $d=8$, and $m=4$, which is too small to be considered as a serious experiment. For problem of this scale, it seems that one can directly solve the original $\\ell_0$ constrained problem [2]\n\n[1] Breheny, P. and Huang, J., 2011. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. The annals of applied statistics, 5(1), p.232.\n\n[2] Marmin, A., Castella, M. and Pesquet, J.C., 2019, May. How to globally solve non-convex optimization problems involving an approximate \u2113 0 penalization. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5601-5605). IEEE.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper lacks clarity",
            "summary_of_the_review": "In conclusion I did not fully understand the idea and the contribution of the paper. The theoretical and practical impact of the paper seems limited.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1455/Reviewer_HUdf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1455/Reviewer_HUdf"
        ]
    }
]