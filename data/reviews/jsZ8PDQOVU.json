[
    {
        "id": "4Os-3s7aKs2",
        "original": null,
        "number": 1,
        "cdate": 1666556161546,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556161546,
        "tmdate": 1669740325252,
        "tddate": null,
        "forum": "jsZ8PDQOVU",
        "replyto": "jsZ8PDQOVU",
        "invitation": "ICLR.cc/2023/Conference/Paper3799/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors propose an algorithm for task-agnostic online meta-learning in nonstationary environments.",
            "strength_and_weaknesses": "Strengths:\n  * The insight that the simplicity of the task-adaptation step (one single application of batch gradient) convexifies the problem despite the use of deep architectures, allowing the application of online learning techniques.\n  \nWeaknessess:\n  * Task switch detection heuristic does not correspond to dynamic regret algorithm techniques from the online learning literature.  The paper lacks details on how this heuristic (threshold) is tuned in practice.\n  * The paper emphasizes the setting where task boundaries are not known, but without comment the analysis (e.g., Theorem 1) appears to assume task distributions are switching at a regular cadence.  \n  ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is:\n  * occassionally excellent: this reviewer was initially disinclined to believe that dynamic regret analysis would apply to a deep architecture, but the discussion preceeding Theorem 1 was quite convincing.\n  * occassionally bad: \n    * Algorithm 1 line 6 uses $\\ell$ was doesn't list it as an input or indicate how it is computed.\n\t* what is $g_k$ (\"the prediction model's output?\") mentioned after equation (4)?  \n\t* when describing the experimental setup in section 5, the moderately stationary case has a higher task switch probability than the low stationary case.  Also the phrase \"where $p$ is very small\" makes no sense.\n\t\nNovelty looks good: as previously indicated, the semantics of Theorem 1 were not apriori expected, so this reviewer learned something valuable (reviewing is mostly thankless, so this is a nice upside surprise).",
            "summary_of_the_review": "**Don't panic**: This paper wants to be accepted.  You just need to address my concerns in order for me to raise my score.\n\nMy primary concern is the task switching heuristic.  Both the heuristic and the description of the heuristic are lacking.  Most of the clarity issues I identified were related to this aspect.  Just fix them.\n  * Step 1: better describe what you are actually doing to set the temperature.  \n  * Step 2: provide some experimental commentary on the sensitivity of the technique to the choice of temperature.\n  * Step 3: quantify how homogenous (or not) the task loss distributions are in the setups you are using to give an idea of why the heuristic is working and when it would fail.\n  * Step 4: provide some commentary relative to what dynamic regret algorithms *actually* do to detect environment drift: they do regret bound arithmetic, e.g., [1]\n\t\nThe other concerns are lower in importance but to the extent you address them you'll have a better paper.\n\n**Updated**: 5->6 after author response\n\n\t\n[1] https://arxiv.org/abs/2102.05406",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3799/Reviewer_HHCs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3799/Reviewer_HHCs"
        ]
    },
    {
        "id": "Et7i-oEkPN",
        "original": null,
        "number": 2,
        "cdate": 1666576818139,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576818139,
        "tmdate": 1666576818139,
        "tddate": null,
        "forum": "jsZ8PDQOVU",
        "replyto": "jsZ8PDQOVU",
        "invitation": "ICLR.cc/2023/Conference/Paper3799/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work considers online meta-learning in a non-stationary environment where new tasks might come and task distribution can change. To solve these problems, this work designs two heuristic detectors to detect the task switches and distribution shifts. It also provides a theoretical analysis of the dynamic regret under certain assumptions.",
            "strength_and_weaknesses": "### Strength:\n- This paper considers an interesting but hard problem that the new task/distribution could appear in the online meta-learning process.\n- The paper is clearly structured and well-written.\n- Theoretical analysis of the dynamic regret is provided.\n\n### Weakness:\n- My main concern about the paper is the mismatch between the theoretical analysis and the proposed methods. The authors have proposed two heuristics to detect the shift on tasks and task distribution, which are the main algorithm contributions of this paper. But, the theoretical analysis is irrelevant to the proposed mechanism.\n- About the novelty of the dynamic regret bound. The analysis for the dynamic regret seems a direct application of that for the OGD algorithm [Zinkvich,2003] under the convexity assumption (Assumption 2). In this sense, the theoretical technical novelty of this is somewhat limited to me.\n- About the correctness of the dynamic regret bound. It seems to me that $P_T$ in Theorem 1 should be defined as $P_T = \\sum \\Vert \\theta_d^*-\\theta_{d+1}^*\\Vert$ instead of the second-order formulation. It is unclear to me why the second inequality of Eq. (13) holds.\n- About the claim on the sublinear regret: It seems that the proposed regret bound will suffer from a linear regret when $P_T = O(T)$. Therefore, the claim on the sublinear dynamic regret is somewhat an overclaim to me. \n- About the experiments: To validate the effectiveness of the proposed detection scheme, it would be better to provide a more detailed inspection of the shift detection instead accuracy instead of just providing the comparison on the overall performance.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The proposed method and the theoretical analysis are mismatched. Besides, it seems that the definition of the $P_T$ is not correct to me. \n\nClarity: The paper is overall well-written. But there are still some minor points. \n- I think it would be better to provide more explanation on what task boundaries means in Section 1 for readers who are not familiar with meta-learning.\n- It\u2019s better to distinguish the expected loss and the empirical loss in the paper.\n- Should it be $\\mathcal{L}(\\theta_{t-1};S_t)$ in Line 14 of Algorithm 1?\n- The axis and legend of the figures are not clear enough.\n\nOriginality: The proposed detector for distribution shift detection is novel to me, but they are not brand new since it is established on the OOD detection proposed by previous works. The theoretical analysis is also a standard analysis for the OGD algorithm.",
            "summary_of_the_review": "This paper has studied an interesting but challenging problem. But I am concerned about the mismatch between the proposed algorithm and the theoretical analysis. Besides, though the shift detector is novel to me, the criterion is mainly established by the previous work [Liu et al., 202], and the theoretical analysis is standard. Overall, I think this paper has some interesting parts, but the flaws outweigh its merit. So, I would like to reject this paper.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3799/Reviewer_1HZs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3799/Reviewer_1HZs"
        ]
    },
    {
        "id": "Wn7BTytETM5",
        "original": null,
        "number": 3,
        "cdate": 1666848116257,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666848116257,
        "tmdate": 1666848116257,
        "tddate": null,
        "forum": "jsZ8PDQOVU",
        "replyto": "jsZ8PDQOVU",
        "invitation": "ICLR.cc/2023/Conference/Paper3799/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the problem of online meta-learning and proposes an algorithm to handle the case where the task boundaries are unknown and the the comparator can change over time. They propose detection mechanisms for task switches and a method that adapts the meta-model when the switches occur and the fine-tuned model when they do not. The proposal is supported with theoretical and experimental analysis.",
            "strength_and_weaknesses": "### Strengths:\n1. The problem is well-motivated as being able to adapt quickly in change environments is a core aspect of learning.\n2. The switch detection mechanisms seem interesting, although I do not know if they are novel; are they connected to change-point detection algorithms? A comparison with past work such as CMAML++ would also be useful\n3. Code is provided in the supplementary material.\n\n### Weaknesses:\n1. My main concern with this paper is in the theory, where there are several issues:\n    1. The statement \u201cas shown in Finn et al. (2019), Assumption 2 holds when the function is convex and the adaptation mapping corresponds to the one-step gradient descent\u201d is incorrect; the relevant statement in that paper (Theorem 1) holds only for strongly convex, strongly smooth, and Hessian smooth functions, and it is unclear if the result can be extended to convex functions without making the step-size of the adaptation mapping zero and thus uninteresting in this context.\n    2. It is a bit difficult to understand exactly which algorithm Theorem 1 gives guarantees for, but my understanding is that they are for Algorithm 1 assuming the detection mechanisms are correct every time. What this means is that, while the paper is advertised as not knowing the task boundaries, for the actual guarantee the task boundaries are indeed known. The issue then becomes that the authors do not compare to the trivial baseline, which is restarting gradient descent whenever a new task is detected; in the regime where all tasks have at most K rounds doing this has dynamic regret O(D*sqrt(K)), which is better than the provided guarantee unless the path-length is o(sqrt(D)). If the task-boundaries are known, the setting studied by the authors is in-fact more akin to that of online-within-online meta-learning (Denevi et al., 2019; Khodak et al., 2019), where you have a sequence of tasks with some number of rounds per task; the only difference is here the loss functions are composite (but convex, as in those papers). Notably, Theorem 3.3 of the latter gives a guarantee of O(D^(3/4)*sqrt(K)) as a special case, which is better than the provided guarantee unless the path-length is o(D^(1/4)).\n    3. Even if Theorem 1 were providing guarantee for unknown task-boundaries, work by Jadbabaie et al. (2015) shows a dynamic regret of O(sqrt((P+1)*T) for a similar notion of path-length P (basically without the square on the norm) and to my understanding without needing to know the path-length to set the step-size. Applying their algorithm to the convex composite losses here would yield what I would view as a better result than the proposed approach (and in-fact can adapt to other types of regularity in the sequence).\n2. The discussion of memory-efficiency is a bit overstated; theoretically at least, the O(T) issue in Finn et al. (2019) is the result of the odd choice of using FTL to optimize the sequence of strongly convex functions, when adaptive OGD (Hazan et al., 2007) would have achieved the same regret and with O(1) memory. Methods studying the online-within-online setting (Denevi et al., 2019; Khodak et al., 2019) also use O(1) memory. Notably, memory efficiency is not a \u201ctheoretical advance in online learning\u201d that occurred in 2016.\n3. It is strange to motivate an adversarial theoretical analysis and algorithm with a discussion of distribution shifts, which suggests a stochastic problem.\n\n\n### References:\n- Denevi, Stamos, Ciliberto, Pontil. Online-within-online meta-learning. NeurIPS 2019.\n- Finn, Rajeswaran, Kakade, Levine. Online meta-learning. ICML 2019.\n- Hazan, Rakhlin, Bartlett. Adaptive online gradient descent. NeurIPS 2007.\n- Jadbabaie, Rakhlin, Shahrampour, Sridharan. Online optimization: Competing with dynamic comparators. AISTATS 2015.\n- Khodak, Balcan, Talwalkar. Adaptive gradient-based meta-learning methods. NeurIPS 2019.\n- Mokhtari, Shahrampour, Jadbabaie, Ribeiro. Online optimization in dynamic environments: Improved regret rates for strongly convex problems. CDC 2016",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: aspects of the theory are unclear, such as assumptions on the loss functions and for what precise algorithm the theory holds.  \nQuality: the theoretical component of the work is of poor quality due to incorrect statements and lack of comparisons. The experimental results are interesting.  \nNovelty: the work has some novelty in the task detection algorithm and setting studied.  \nReproducibility: good.",
            "summary_of_the_review": "I do not believe should be accepted in its current form due to the issues with the theoretical results outlined above.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3799/Reviewer_ZoyY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3799/Reviewer_ZoyY"
        ]
    },
    {
        "id": "K8Dw95J-Bg",
        "original": null,
        "number": 4,
        "cdate": 1666971341552,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666971341552,
        "tmdate": 1666971341552,
        "tddate": null,
        "forum": "jsZ8PDQOVU",
        "replyto": "jsZ8PDQOVU",
        "invitation": "ICLR.cc/2023/Conference/Paper3799/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper propose to investigate meta-continual learning scenario where the task boundaries are not known and whether distribution shift occur or not. To solve this challenging problem, they focus on some empirical findings that the loss values or free energies sharply increases when the boundary meets. Thus they introduce two threshold values for detecting task boundaries and distributional shifts, in order to selectively control between task-specific learning and meta-updates. The empirical results demonstrate the efficacy and effectivenss of their methodologies on various learning scenarios.",
            "strength_and_weaknesses": "Strength\n- The target problem is largely important for real-world applications.\n- the paper is well written and method easy to understand.\n- Method performs well on various learning scenarios, outperforming current state-of-the-arts on the given problem.\n- Theoretical results support their claim.\n\nWeaknesses\n- The proposed method seems to largely based on empirical observation that the loss value sharply increases when the task switches. The corresponding thresholding scheme is also very naive, although it seems intuitive and empirically effective. I wonder how they set the threshold value in practice - Have those hyperparameters been found with a holdout meta-validation set? or meta-test set?\n- The quality of the paper is a bit low. \\citep and \\citet are not clearly distinguished. Also, the learning curves are just copy and past of wandb plots, making the quality of the paper looking very low. Some of the baselins have not been fully run (e.g., FOML in Fig 3)\n- I'm not sure whether some of the existing baselines such as FTML by Finn et al. and OSML by Yao et al. are not applicable the setting the authors considered. Could you compare against FTML and OSML as well? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity \n- The paper is easy to understand and mostly clear\n\nQuality\n- The paper needs to improve the quality by a lot\n\nNovelty\n- I'm not very sure about the novelty part (the method seems to be a single thresholding scheme)\n\nReproducibility\n- They provided their code.",
            "summary_of_the_review": "In summary, although there are some concerns regarding the novelty and quality of the submission. I think the strength outweights its weaknesses which I think can be easily addressed. Thus I recommend (weak) acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3799/Reviewer_8xrn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3799/Reviewer_8xrn"
        ]
    },
    {
        "id": "v7I0YDV17-G",
        "original": null,
        "number": 5,
        "cdate": 1667000035497,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667000035497,
        "tmdate": 1667000897863,
        "tddate": null,
        "forum": "jsZ8PDQOVU",
        "replyto": "jsZ8PDQOVU",
        "invitation": "ICLR.cc/2023/Conference/Paper3799/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new algorithm for task-agnostic online meta-learning when data is non-stationary. The algorithm has two components. First, the algorithm can detect task switches. Second, the algorithm can adapt to distribution shifts. The paper provides a regret analysis of the algorithm.",
            "strength_and_weaknesses": "Strength:\n\nThis paper seeks to solve a challenging problem, and provides a nice solution. The solution makes intuitive sense and is supposed by a regret analysis.\n\nWeakness:\n\n1. How is the threshold $\\ell$ in Algorithm 1 chosen? When the algorithm is used online, it seems unclear to users about how to choose an appropriate $\\ell$. Some practical guidance will be very useful. \n\n2. What doesn't the regret bound depend on $\\ell$? It seems if we choose $\\ell = 0$, then the regret should grow linearly in $\\sum_d K_d$.  In the regret definition (6), does $\\sum_d K_d$ equal $T$?\n\n3. Consider two scenarios with two very different tasks (data from each task are drawn from a stationary distribution). The first scenario is that from time 1 to T/2, it is the first task, and from time T/2+1, ..., T, it is the second task. The second scenario is that two tasks frequently switch between each other. Clearly, the proposed algorithm performs well in the first scenario, but not in the second. I don't seem how the regret varies between these two scenarios. Some clarification will be helpful.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper provides a nice solution to the challenging problem in online meta-learning when tasks are agnostic and data is non-stationary. The paper demonstrates the good performance of its proposed method through a number of experiments. The paper is well-written, and I enjoy reading the paper. ",
            "summary_of_the_review": "This paper proposes a new algorithm for online meta-learning to simultaneously address the problem of agnostic tasks and the problem of distribution shift. The paper is well-written. However, I think the paper can benefit from more discussion on the theoretical results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3799/Reviewer_R5Rh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3799/Reviewer_R5Rh"
        ]
    }
]