[
    {
        "id": "GA1nCk4H8b",
        "original": null,
        "number": 1,
        "cdate": 1666584876970,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584876970,
        "tmdate": 1669753037126,
        "tddate": null,
        "forum": "hzjQWjPC04A",
        "replyto": "hzjQWjPC04A",
        "invitation": "ICLR.cc/2023/Conference/Paper3081/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an agent architecture called VIMA (for **Vi**suo**M**otor **A**ttention model), which is a standard transformer encoder-decoder pair. Multimodal inputs are encoded as a single sequence of interleaved text tokens and visual object tokens. Transformer outputs are mapped to actions, as in the Decision Transformer (DT). VIMA is evaluated on a new multi-modal benchmark, VIMA-Bench, consisting of 17 meta-tasks, and thousands of tabletop task instantiations arranged in a 4-level protocol of increasing difficulty. Baseline models (including Gato and DT) are also evaluated on VIMA-Bench. The results show that VIMA outperforms the baselines by consistently large margins.",
            "strength_and_weaknesses": "**Strengths**\n\nVIMA-Bench is a flexible and useful environment for evaluation of multi-modal prompting, scalability, and generalization in a robotic setting. Interleaving language and image tokens in a single input sequence has shown recent promise, as with the Gato agent. \n\n**Weaknesses**\n\nVIMA\u2019s performance is reliant on three critical design features:\n\n- Visual **object tokens** from an off-the-shelf object detector.\n- **Cross-attention** to the prompt sequence.\n- A pre-defined set of **high-level actions**.\n\nThe **object tokens** are a crutch, giving VIMA effective access to the underlying state space, while baseline models are not given similar access. This disparity is more than enough to explain VIMA\u2019s improved results over the baselines, and makes the comparisons unfair. The paper says \u201cWe hypothesize that the data efficiency can be attributed to VIMA\u2019s object-centric representation, which is less prone to overfitting than learning directly from pixels in the low-data regime.\u201d But it\u2019s common knowledge that state representations allow greater sample efficiency than raw image observations. So there is nothing novel or surprising about taking advantage of object-level representations when available.\n\n**Cross-attention** is one standard method of inputting a prompt sequence. Another standard method is to prepend the prompt to the single sequence, and rely on self-attention. Cross-attention is most appropriate when the prompt sequence (as in machine translation) contains a wealth of potentially variable information that strongly determines the correct output sequence. Recent models like Imagen have leveraged cross-attention in this way. Self-attention (over a single sequence) is most appropriate when the prompt sequence is less informative, and serves simply to identify the current task from among a trained set of tasks. Recent models like Gato have leveraged self-attention in this way. By construction, prompts in VIMA-Benchmark are highly variable, and strongly determine the correct outputs, and therefore call for cross-attention. Gato and DT were originally designed for the other type of prompt, and cannot be expected to perform as well on VIMA-Benchmark. But cross-attention could easily be added to Gato or DT. In fact, VIMA\u2019s transformer architecture is equivalent to DT with cross-attention. There is nothing novel or surprising about taking advantage of cross-attention when it is called for.\n\nVIMA\u2019s **high-level actions** are severely limited to a pre-defined set including \u201cpick-and-place\u201d and \u201cwipe\u201d. The baseline agents are given access to the same high-level actions, so fair comparisions are at least possible. But the results are not indicative of how the agents would perform on more common low-level actions, like robotic joint torques. This makes the term \u201cgeneralist robot agent\u201d in the paper\u2019s title especially inappropriate.\n\nVIMA\u2019s performance depends on these three components. The paper itself notes that good results \u201ccan only be achieved with both cross-attention and object token sequence representation\u201d. So given the strong caveats explained above, the experimental results presented in the paper provide no reliable evidence for how VIMA compares to the baseline models. \n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nMuch of the paper is clearly written, though the language can be improved, and a few parts are confusing.\n\nFor instance, in the statement of the second contribution:  \u201ccapable of multi-tasking\u201d. This is abuse of the verb multi-tasking, which actually refers to switching rapidly from one task to another, as humans are able to do. Multi-tasking does not refer to an agent\u2019s ability to learn to perform multiple tasks.\n\nMore detailed explanation is needed on the following topic:  \u201cpassing the resulted token sequence to a pre-trained T5\u201d\n\n**Quality**\n\nVIMA-Bench appears to be a potentially valuable set of tasks. But the paper\u2019s experimental comparisons to baselines are poor, for the reasons cited above.\n\n**Novelty**\n\nVIMA-Bench is somewhat novel, but the VIMA architecture is not, as explained above.\n\n**Reproducibility**\n\nCode is provided.\n",
            "summary_of_the_review": "VIMA-Bench is potentially useful. But because of the flaws in VIMA's comparisons to baseline agents, little can be concluded about VIMA's capabilities.\n\n***** POST-DISCUSSION UPDATE *****\n\nIn their last response to me, the authors said:\n\n**\"Among these variants, we find that the recipe of object token + cross-attention is the best for multimodal-prompted robot agents through extensive experiments.\"**\n\n\nThis is a nice summary of the work\u2019s key experimental finding. My concern is that both parts of this recipe are unremarkable:\n- The object tokens come from a model (Mask R-CNN) trained on this task\u2019s ground-truth state information, making it unlikely to scale well beyond VIMA-Bench. This limitation makes the object tokens a crutch, and readers already know that such crutches boost performance. \n- A long line of prior work, starting with the original transformer paper of 2017, has shown that cross-attention is a powerful method for processing complex input spaces. Since VIMA-Bench\u2019s input space is complex by construction, readers shouldn\u2019t be surprised to see that cross-attention performs well here.\n\nI still see value in VIMA-Bench itself, with its diversity of tasks. In fact, I would argue for acceptance of a *VIMA-Bench* paper submitted to a datasets and benchmarks track, as long as the models evaluated (including VIMA itself) were presented simply as baselines that the community could compare against in further work. In such a light, VIMA\u2019s reliance on an unscalable vision processing pipeline would not pose a problem, and its usage of cross-attention would simply provide experimental verification of performance one might expect. \n\nBut instead of focusing on VIMA-Bench, this paper focuses on VIMA as a model for general robot manipulation, starting with the title itself:  **\u201dVIMA: General Robot Manipulation\u2026\u201d** This is why I have to leave my scored unchanged, despite all the impressive work that has gone into the paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3081/Reviewer_L5UP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3081/Reviewer_L5UP"
        ]
    },
    {
        "id": "VbI54-Uzzv",
        "original": null,
        "number": 2,
        "cdate": 1666594387158,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594387158,
        "tmdate": 1666594517191,
        "tddate": null,
        "forum": "hzjQWjPC04A",
        "replyto": "hzjQWjPC04A",
        "invitation": "ICLR.cc/2023/Conference/Paper3081/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents VIMA, a robot agent that can act conditioned on multimodal prompts consisting of natural language and image objects. The architecture consists of self-attention and cross-attention layers and train the model to minimize behavior cloning loss. The main novelty of the paper is to propose a scheme for multimodal prompting, and the paper shows that the proposed approach can work well for various tasks with a single model.",
            "strength_and_weaknesses": "### Strengths\n- Clear writing with nicely looking illustrations\n- Exhaustive experiments\n- Introducing an interesting benchmark\n\n### Weaknesses\n- Dependency on a separate object detection module. While it's shown to be effective as using oracle information in considered setups, it's not clear whether this can work in real-world scenarios or non-tabletop tasks as the paper promises to be a generalist robot agent. To add on this, I don't see this as a big weakness but it could be an interesting future direction to consider.\n- Interesting results and analysis on a newly introduced benchmark. But comparison with the existing approaches [1,2] on other benchmarks [3] could further strengthen the claims made in the paper.\n- MIssing details on baselines. From only reading the paper, it's a bit difficult to understand what's the main difference of the approach to considered baselines without reading the baseline papers in a detail.\n- on Decision Transformer -- Trajectory Transformer [Janner et al., 2021] significantly differs from Decision Transformer [Chen et al., 2021] but the paper writes this as Decision Transformer (DT) (Chen et al., 2021; Janner et al., 2021). This should be corrected. Moreover, the main component of DT is to condition on desired returns and make the agent select the desired actions -- without this, the method that conditions on multimodal prompt cannot be called DT. It's just a behavior cloning agent conditioned on multimodal prompt with the GPT architecture. \n- on cross-attention -- it's worthwile to cite relevant video prediction literature [4] that also utilizes cross-attention to reduce the compute cost when conditioning on previous frames (it seems that the architecture is almost the same, or very similar).\n\n[1] Dasari, Sudeep, and Abhinav Gupta. \"Transformers for one-shot visual imitation.\" In Conference on Robot Learning, pp. 2071-2084. PMLR, 2021.\n\n[2] Mandi, Zhao, Fangchen Liu, Kimin Lee, and Pieter Abbeel. \"Towards more generalizable one-shot visual imitation learning.\" In 2022 International Conference on Robotics and Automation (ICRA), pp. 2434-2444. IEEE, 2022.\n\n[3] Zhu, Yuke, Josiah Wong, Ajay Mandlekar, and Roberto Mart\u00edn-Mart\u00edn. \"robosuite: A modular simulation framework and benchmark for robot learning.\" arXiv preprint arXiv:2009.12293 (2020).\n\n[4] Yan, Wilson, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. \"Videogpt: Video generation using vq-vae and transformers.\" arXiv preprint arXiv:2104.10157 (2021).",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity and Quality\n- The paper is very easy to read, clearly written.\n\n### Novelty\n- Architecture and training schemes are not novel but prompting scheme and introduced benchmarks seem novel enough\n\n### Reproducibility\n- The authors promised to release everything\n",
            "summary_of_the_review": "The paper is written well, provides a thorough investigation of the proposed architecture in a very wide setups. I don't see major concerns with the paper but I'm reluctant to further increase my score as all the results are based on a newly introduced benchmarks and baselines which are not exactly clear how they are implemented. I'm willing to change my score up or down after the discussion during the rebuttal phase.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3081/Reviewer_nbxK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3081/Reviewer_nbxK"
        ]
    },
    {
        "id": "xeZOheLRFf",
        "original": null,
        "number": 3,
        "cdate": 1666628287127,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628287127,
        "tmdate": 1668865182371,
        "tddate": null,
        "forum": "hzjQWjPC04A",
        "replyto": "hzjQWjPC04A",
        "invitation": "ICLR.cc/2023/Conference/Paper3081/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper describes a technique to create a robot control agent that can control a manipulator to execute a variety of tasks specified in the form of a multi-model prompt, combining text and visual tokens. \n\nThe paper also introduces a large scale benchmark for problems focusing on the properties of the system VIMA-Bench.  \n",
            "strength_and_weaknesses": "+ The paper reports on an impressive amount of work on the development of the agent and the benchmark. The 20+ pages of appendices are an impressive quantity of work. \n- The paper proper only spends about 2/3 of a page (section 5) describing the model. This is unsufficient for understanding what exactly the contribution is. \n- As a robot manipulation problem, the tasks are very simple. These are pick and place tasks, performed on a flat surface, in simulation, with objects that are clearly distinguishable. \n- The paper does not explain why it considers that the multi-model prompt is superior. Are the prompts supposed to be generated by humans? Humans don't insert visual snippets or texture samples in the speech. \n- The tasks in the tasks suite are specified in ways that are much simpler than what is the state of the art in this field. For instance, \"novel councept grounding\" appears to be just attaching a label to a definition. \"one shot video imitiation\" appears to be the exact reproducing of an exact trajectory - a much weaker definition that what is normally meant by imitation learning. ",
            "clarity,_quality,_novelty_and_reproducibility": "+ The paper is clearly written.\n- The novelty of the paper is the multimodal prompt input. However, the paper spends very little space explaining this novelty. ",
            "summary_of_the_review": "The paper describes an impressive project, with a large number of demonstrations. However, it does not clearly focuses on what the novelty of the project is. \n\n----\n\nI have read the responses of the authors. They do not change my rating of the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3081/Reviewer_xPBt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3081/Reviewer_xPBt"
        ]
    },
    {
        "id": "1bCwnlOhLhR",
        "original": null,
        "number": 4,
        "cdate": 1666893554818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666893554818,
        "tmdate": 1666893554818,
        "tddate": null,
        "forum": "hzjQWjPC04A",
        "replyto": "hzjQWjPC04A",
        "invitation": "ICLR.cc/2023/Conference/Paper3081/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This research develops a novel multimodal , multitask architecture for robot manipulation. Also introduces a multimodal and multitask benchmark for robot manipulation. Finally introduce a carefully thought methodology in 4 steps to quantify the capability of the developed architecture.  The results section provide relevant ablation studies that bring insights on this line of research, providing ideas to further improve this research domain.",
            "strength_and_weaknesses": "Strengths:\n1) Novel transformer architecture for robot manipulation.\n2) Novel multimodal interface for robot human interaction.\n3) Novel insight on improved performance on using object centric approaches for this tasks.\n4) New dataset for multimodal and multitask robot manipulation.\n5) Novel methodology to measure generalization in robot manipulation.\n6) Novel human robot interaction modality.\nWeaknesses:\n1) Unclear how this method will generalize to temporal extended actions : i.e, working the dough, beating eggs, writing, folding clothes, etc.\n2) Finteunning Mask-RCNN seems that might not be practical if we want to extend this method to work in-the-wild. What's the accuracy for a non-finetune Mask-RCNN?\n3) What will happen if the user change his mind after saying the command? \n4) How is uncertainty deal in this model? How could be resolved?\n5) What would happen if you have a cluttered scene and the number of object grow and there some that irrelevant to task?\n6) What would happened if there is something incomplete, or wrong in the text command?\n7) It could be interesting if also the model can output text to resolved some of the aforementioned items.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is extremely clear and well written. The sections are well organize. The methodology is well done and the ablation studies well selected. The related work is extensive and completely relevant for this work.\n\nQuality:\nThe paper quality also is superb. The content of the theory, the figures, the explanation and text provide a very high quality paper. The completenees of the work with the available code, the extent of the work on the dataset are very thorough and extensive.\n\nReproducibility:\nThe code is available and seems well done and functional. I have not run it.\n\nNovelty: This work even if aligned to GATO as well describe in the text, is novel as pointed out on the different point on the strengths section before.",
            "summary_of_the_review": "A large effort on developing a novel human robot interaction interface on robotics manipulation tasks that generalize. The work provide a relevant datase, new methodology to measure generalization, and novel models to solved this multitask and multimodal tasks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3081/Reviewer_DxBB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3081/Reviewer_DxBB"
        ]
    }
]