[
    {
        "id": "s3mQWqHtr7o",
        "original": null,
        "number": 1,
        "cdate": 1666565705338,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666565705338,
        "tmdate": 1666585971077,
        "tddate": null,
        "forum": "r0xte-t40I",
        "replyto": "r0xte-t40I",
        "invitation": "ICLR.cc/2023/Conference/Paper1867/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper the authors tackle the problem of case-based decision support. In the problem setting, a ML algorithm assists humans in making decisions on classification tasks by identifying an example from the training set that is similar to the unseen example. In the general case, the similarity between examples is determined by the metric space identified by the ML algorithm while training. However, this metric space can be very different from the space considered by the humans. This can cause the human judge to be misled. To tackle the issue, the authors propose a loss function which combines both MLE and a similarity loss. The similarity loss is derived from a triple learning problem (triplet margin loss). Results on both synthetic data and human experiments show that the proposed method performs much better than existing approaches.",
            "strength_and_weaknesses": "Strengths:\nThe authors tackle an important problem. The proposed approach is simple yet intuitive. Combining two different losses is the correct approach to tackle this issue. Moreover, the experiments are well designed and consider various issues that might pop luck (like data augmentation gain). Finally, the paper is well written and easy to follow.\n\nWeaknesses: \nIn cases where human judgement is necessary, often time the user would be an expert. Their judgment might vary a bit from the regular individuals. In such cases, the gain might be different from what is found in this paper. Furthermore, even though the paper is well written there are still some typos left. I would encourage the authors to fix those. The authors should also evaluate if the results hold for models other than ResNet. Furthermore, the authors can also consider how the approach can be used for other domains that is not image classification. ",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned above the paper is well written and easy to follow. The authors also tackle an important problem the gain in metric is significant attesting to the quality of the paper. As neither the MLE loss or the triplet loss is new, the proposed research is not very novel. The authors provide details regarding how the results can be reproduced.",
            "summary_of_the_review": "The authors tackle an important problem and provide an important solution. Through multiple experiments they show that the proposed method is indeed better. However, the proposed approach is not significantly novel. Furthermore, more experiments are required with humans who are field experts and problems that are not in the image domain. These aspects reduces the impact of this paper a bit.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1867/Reviewer_J1hq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1867/Reviewer_J1hq"
        ]
    },
    {
        "id": "yMDgg_dIjNo",
        "original": null,
        "number": 2,
        "cdate": 1666653156831,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653156831,
        "tmdate": 1670283184781,
        "tddate": null,
        "forum": "r0xte-t40I",
        "replyto": "r0xte-t40I",
        "invitation": "ICLR.cc/2023/Conference/Paper1867/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors combine metric learning with classification to propose a data collection process and loss function for training models that are both accurate classifiers and learn representations that align with human perceptions of similarity in the data. The human-aligned representation can be used for case-based decision support, including justification, where a prediction is given and supported by similar training data with ground-truth labels, and neutral decision support, where no prediction is given, but the most similar labeled training data is given for each label to aid the human decision making process. Experiments on synthetic data with simulated humans show major improvement over a pure classifier for both justification and neutral decision support. Experiments on image classification with human subjects show minor or insignificant improvements in justification, but major and significant improvements in neutral decision support.  ",
            "strength_and_weaknesses": "Strengths\n\nThe presentation is clear and the paper was easy and enjoyable to read. The work addresses an important and challenging problem and shows major improvement over baselines. The discussion of the ethics of decision support strategies and the difference between \"neutral\" and \"persuasive\" models is appreciated, and I get the impression that the work is well thought out and potential concerns are addressed in good faith. The results using human subjects are appreciated and make the claims credible.\n\nWeaknesses (and questions and requests for clarification)\n\nThere is hardly any discussion of context and prior work in decision support. As a result, it is not clear to me how novel the methods are. Additionally, I'm not sure whether there is any previous work aimed at the particular decision support problems addressed in this work that would function as baselines for the proposed methods. If not, this should be explicitly stated, as the baselines used here are naive and fairly weak.\n\nThe purpose of the \"justification\" mode of decision support is not clear, and it should be better motivated. What is the goal? If the model's prediction is given, how would it improve human decision accuracy? Is it meant to earn trust? To be explainable? If the goal is related to trust or explainability, there seems to be an unstated assumption that training data with ground-truth labels can be blindly trusted and/or are themselves useful explanations, which seems insufficient. Whatever this goal is, is it reflected in the H2H score for evaluation?\n\nI have concerns about the neutrality of filtering of class-inconsistent triplets. With respect to neutral decision support, you state that \"the goal is not simply to maximize human decision accuracy, because one may use policies that intentionally show distant examples to nudge or manipulate human towards making a particular decision. Choosing nearest neighbors in each class is thus an attempt to present faithful and neutral evidence from the representation space so that humans can make their own decisions, hence preserving their agency. Therefore, the chosen nearest neighbors should be visually similar to the test instance by human perception [...]\". The acknowledgment of this nuance is appreciated and shows that the ethical aspect of decision support is well considered. However, it seems to me that class-inconsistent triplet filtering is itself a roundabout way to allow the model to be \"persuasive\" rather than \"neutral\" by learning a representation that is only human-aligned when convenient, but not human-aligned when it does not result in a compelling argument for the model's predicted label. This could explain the gains in performance by using filtering for neutral decision support. I think that this should at least be addressed in the paper; ideally, the human subject experiments should also include results without filtering. I understand that this may not be possible in the timeline for revisions, however. Regardless, the neutral decision support results are impressive even with this concern.\n\nThe H2H score does not take into account the accuracy of the model, so it's hard to say that a HC model is useful for justification based on H2H alone. I would at least like to see the classification accuracy of the HC and MLE models. It would be even better if there is some way to consider accuracy as part of the the H2H score itself. For instance, one way might be to define the H2H score as, considering only data where the MLE prediction is correct, the fraction of such data that HC is preferable *and makes the correct prediction* so that cases that are well-justified but incorrect are not counted in its favor.\n\nIt's not clear how to interpret the relationship between task alignment and decision support results in the synthetic experiments.\n\nIt's not clear why we see a large disparity between H2H results for synthetic and human subject experiments. This should be addressed. Maybe it makes a difference whether the same human(s) (or synthetic humans) are used for both training data collection and testing?\n\nMinor nit-picks:\n- End of section 3, under \"Head-to-head comparisons\", the sentence \"In addition to the typical justification for the predicted label, we also examine\nthat for the other class as those examples will be used in decision support\": I cannot tell what the latter half of this statement is trying to say.\n- Table 1, \"Persuasive decision support\", last column: the 1.000 in the middle row should be bold.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the work is easy to follow. Some elements are unclear in the introduction, but they are clarified on further reading.\n\nQuality: the ideas are thoroughly examined and the experimental results are convincing. The human subject studies are appreciated and establish credibility for the proposed methods. However, the baseline is weak, and I wonder if there are other decision support methods that should be considered for comparison. See strengths and weaknesses section for other points.\n\nNovelty: I am not sure. I am not familiar with decision support, and the authors do not provide much context for the work, such as prior methods that aim to address the same problem.\n\nReproducibility: I did not have time to read the appendix in detail, but the information required to reproduce the experiments seems to be present.",
            "summary_of_the_review": "With the disclaimer that I don't have any background knowledge in decision support methods, I think this is a good paper with a few small but important concerns and missing pieces, detailed in the \"Weaknesses\" part of my \"Strengths and Weaknesses\" section. If these can be addressed, I will recommend it for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1867/Reviewer_7Qs2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1867/Reviewer_7Qs2"
        ]
    },
    {
        "id": "U5OS39JrAn",
        "original": null,
        "number": 3,
        "cdate": 1666663906588,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663906588,
        "tmdate": 1666663906588,
        "tddate": null,
        "forum": "r0xte-t40I",
        "replyto": "r0xte-t40I",
        "invitation": "ICLR.cc/2023/Conference/Paper1867/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for jointly learning a case-based predictive algorithm that maximizes accuracy and alignment with human assessments of similarity. The paper provides a loss function for this purpose and describes in detail experiments on synthetic and real-world data annotated by human labelers on Prolific. The real-world datasets describe chest x-rays (CXR) and moth vs. butterfly image classification.  ",
            "strength_and_weaknesses": "+ The paper considers an important and impactful problem: how to build decision support tools that are interpretable in a faithful way to the prediction. The motivation was really strong and well-communicated.\n+ The empirical evaluation is detailed and the results are interesting.\n\n- The discussion on limitations is lacking. What situations may it be insufficient to provide only the most similar examples in each class? The ethics discussion at the end begins to discuss these limitations, but I think it would be useful to elaborate on this and put it some discussion in the main paper since it has implications for effectiveness. It could be illustrative for instance to give two examples: one setting that is well-suited to this method and another that is not. \n- It would also be helpful to provide caveats about the empirical setup. For instance, what limitations result from having Prolific users (who are presumably not experts at reading x-rays) do the chest x-ray similarity assessments?\n\nPlease see next section for more on strengths and weaknesses with respect to novelty and clarity.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the contribution is good. The paper is fairly well-written. It could be improved if the introduction were explicit about the settings in which this method is applicable--for instance, that it requires human labelled data of triplet form. I also found the section on \"machine teaching\" in related work to be confusing. It would be helpful to clarify who is the teacher and the student.\n\nThe methodological novelty appears incremental as the method simply uses a linear combination of cross-entropy loss and a loss from prior work, the triple margin loss, for human compatibility. It would be helpful to clarify if there is more methodological novelty. \n\nThe paper offers novelty in its evaluation and empirical investigation. The paper provides a metric that assesses alignment with human assessment of similarity, the Head-2-Head (H2H) comparison. There's another evaluation metric under \"neutral decision support\" that is confusing since there are no details to explain what \"accuracy of a synthetic human\" refers to (perhaps this is in the appendix?). The H2H comparisons show that their proposed method better aligns with human assessment of similarity than the cross-entropy-loss minimizing model (the MLE). The plots contain error bars which is great. \n\nThe quality is acceptable but could be stronger if the paper provided more discussion and details about some key points:\n- how much does the choice of metric impact the results. The paper states that Euclidean distance was chosen but were alternative metrics considered? It would be helpful to provide discussion of the benefits and limitations.\n- how computationally intensive is the method?\n",
            "summary_of_the_review": "This paper provides a simple solution to the well-posed problem: how to provide case-based decision support that aligns with human assessments of similarity while still being faithful to the predictions? The proposed method combines existing work to answer this, and the authors provide good empirical evaluations. The paper would be stronger if it provided more discussion on limitations and use cases.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1867/Reviewer_BaWn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1867/Reviewer_BaWn"
        ]
    },
    {
        "id": "UqPUW5DEfi4",
        "original": null,
        "number": 4,
        "cdate": 1666688964115,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688964115,
        "tmdate": 1666688964115,
        "tddate": null,
        "forum": "r0xte-t40I",
        "replyto": "r0xte-t40I",
        "invitation": "ICLR.cc/2023/Conference/Paper1867/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on the setting of case-based decision support where, in addition to receiving a model's prediction, a human decision-maker can also observe cases with similar-looking examples from the training set of the same label (justification), or provide similar looking examples with other labels (calibration). Specifically, the authors propose a loss function combining standard cross entropy loss with a triple margin loss that aligns representations closer together based on human judgements of similarity. ",
            "strength_and_weaknesses": "S1: The paper tackles an interesting and well-defined problem of learning more human-compatible representations in the specific context of case-based decision support. \n\nS2: The paper is very clearly written. \n\nS3: The proposed approach, HC, strongly outperforms baselines in two real-world user study experiments. \n\nW1: The details in 4.1 on the simulated human perceptual similarity metrics are very sparse and unclear, and it's not obvious whether this is a justifiable model of human perceptual judgements. \n\nW2: The filtering of class-inconsistent triples is a bit concerning, as ideally the proposed method would be able to leverage information captured by such \"noise\" in annotations. It would be interesting to see how much filtering was performed for the experiments. \n\nQ1: Could the authors clarify whether the model's predictions were shown to uses in the decision-support task and if not, why they were omitted, as case support is typically an augmented task (providing justification for a model's output)? It is not clear from the interface, and I'm not sure how relevant the results are if these outputs are omitted, as that would be a natural thing to include in a real world set-up.   ",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The paper is overall quite clear and easy to follow. However, it might help to provide an early visual example of what case based decision making appears like \n\nNovelty: I think the paper has limited novelty, as effectively learning from triplet comparisons or human feedback is a well-studied problem, so the main contribution is showing the effectiveness of these representations for human-in-the-loop decision making. I think the author could discuss other explanation methods in the Related Works sections as well, as I think the paper's contribution falls more as another form of explanation and should thus be position with respect to those. \n\nReproducibility: Model training and crowdsourcing details are all provided.",
            "summary_of_the_review": "Overall, I think the paper addresses an interesting and relevant problem, and it's main weaknesses are novelty and clarity on certain experiment choices I mentioned in the review, and I'm happy to raise my score if the authors address them. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1867/Reviewer_5Yxd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1867/Reviewer_5Yxd"
        ]
    }
]