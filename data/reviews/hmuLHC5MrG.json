[
    {
        "id": "uSa6ntBSrGE",
        "original": null,
        "number": 1,
        "cdate": 1666350919489,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666350919489,
        "tmdate": 1669109303199,
        "tddate": null,
        "forum": "hmuLHC5MrG",
        "replyto": "hmuLHC5MrG",
        "invitation": "ICLR.cc/2023/Conference/Paper3175/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an optimization method for the Gaussian likelihood. The authors argue that optimization of the Gaussian log-likelihood is prone to instabilities. The method is a combination of two approaches: using structured natural gradients with local parameterizations, and constraining them with trust region layers. The authors provide experimental evidence of the instabilities of the Gaussian NLL optimization and performance of their proposed method. The new method is more stable and performs on par or better than other methods such as ADAM or Pitfalls.",
            "strength_and_weaknesses": "*Strength*. The paper provides analysis of the source of the instability of the Gaussian NLL optimization. The authors did a multitude of experiments to provide evidence of the said instability. They provided necessary background information required to understand their approach. The method provides a more stable and in many cases better-performing optimization routine for Gaussian NLLs.\n\n*Weakness*. The new optimization strategy is a combination of previous work. The new method seems to achieve superior results on synthetic data while on UCI datasets, results are quite close for different optimizers (table 7). Although Trustable does seem to produce a more stable descent (Figure 5).\nLimitations of the method are not discussed clearly.",
            "clarity,_quality,_novelty_and_reproducibility": "* The work is relatively clear. I have only noted a typo on p. 2: \"according some Gaussian distribution\". The limitations should be addressed more clearly.\n* The analysis is sound and backed by empirical evidence. \n* The authors did not include any supplementary material such as the code. However, reproducing the experiments should be doable.\n* The novelty of the approach is somewhat not clear, as the work is a combination of previous research.  I am not familiar with some pieces of related research, such as TRPL. However, TRPL seems general enough to cover natural gradient updates? So the contribution of the authors seems to be in empirically studying the behavior of NG with trust regions.",
            "summary_of_the_review": "The paper provides a new strategy to optimize Gaussian NLL. The approach combines previously known methods. The authors should discuss more clearly the limitations of the method. My initial judgement is the paper is borderline.\n\nEDIT: Given the lack of authors' response and in light of other reviews, I'd like to lower my score to 3: Reject.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3175/Reviewer_gGLg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3175/Reviewer_gGLg"
        ]
    },
    {
        "id": "x6Nt1gthf-",
        "original": null,
        "number": 2,
        "cdate": 1666611160378,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611160378,
        "tmdate": 1666611351859,
        "tddate": null,
        "forum": "hmuLHC5MrG",
        "replyto": "hmuLHC5MrG",
        "invitation": "ICLR.cc/2023/Conference/Paper3175/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes Trustable, a novel optimizer that overcomes instabilities methodically by combining systematic update restrictions in the form of trust regions with structured, tractable natural gradients. It is demonstrated in several challenging experiments that Trustable outperforms current optimizers in regression with neural networks in terms of the NLL, MSE, and further performance metrics.",
            "strength_and_weaknesses": "Strengths:\n1. Well motivated. This is indeed an critical problem for uncertainty learning. \n2. Proposes good methedology from the perspective of eigenvalues of covariance and decoupled gradients. And also the good bulding of contextual gradients from the optimization process. All above brings the stability. \n3. promising experimental results\n\nWeakness:\n1. Somewhat small scale experiment and also only regression is tested in the paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity & Quality: this paper is well-written. \nNovelty: Novel\nReproducibility: The code was release on github.",
            "summary_of_the_review": "This paper is well-done except the experiments could be extended to larger-scale and more settings.The violation of anonymity is also found in the github repo.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
            ],
            "details_of_ethics_concerns": "The github link https://github.com/f6c334/iclr2023-stablenll gives some direct identity link to Denis Megerle, which might be a violation of anonymity.\n\n\n",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3175/Reviewer_YtD7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3175/Reviewer_YtD7"
        ]
    },
    {
        "id": "gF6h-_Kx4h",
        "original": null,
        "number": 3,
        "cdate": 1666676011962,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676011962,
        "tmdate": 1666676011962,
        "tddate": null,
        "forum": "hmuLHC5MrG",
        "replyto": "hmuLHC5MrG",
        "invitation": "ICLR.cc/2023/Conference/Paper3175/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the problem of Gaussian likelihood maximization. Estimating the conditional mean and the conditional covariance of a Gaussian likelihood model under neural modeling assumptions on the mean and the covariance leads to difficult optimization problems. The paper proposes a method to solve such optimization problem",
            "strength_and_weaknesses": "Pros:\nA fairly novel method to solve the ensuing Gaussian likelihood optimization problem. \nThe experimental results do seem to indicate better performance than other competing methods.\n\nCons:\nBy very nature the methods seems to be unscalable (as it estimates a full covariance matrix) to higher dimension.s",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the analysis was insightful. The algorithm is novel. I am not sure how easy it is to code up this algorithm and there seems to be no available implementation, as of now.",
            "summary_of_the_review": "Interesting algorithmic contribution, but not very scalable.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3175/Reviewer_WXLj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3175/Reviewer_WXLj"
        ]
    },
    {
        "id": "3Htb_B5rdX",
        "original": null,
        "number": 4,
        "cdate": 1666871081447,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666871081447,
        "tmdate": 1670104968636,
        "tddate": null,
        "forum": "hmuLHC5MrG",
        "replyto": "hmuLHC5MrG",
        "invitation": "ICLR.cc/2023/Conference/Paper3175/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the problem of optimising multivariate Gaussian likelihoods, specifically wrt to its mean and covariance matrix. They identify the root causes of various failure modes in Gaussian likelihood optimisation through the lens of spectral analysis. Specifically, they characterise these modes based on the covariance matrix's eigenvalues and propose combining numerous techniques into a pipeline to mitigate these issues. In particular, the methods consist of structured natural gradients (NG) and trust region projection layers (TRPL).",
            "strength_and_weaknesses": "\n## Strength\n\nThis paper seeks to tackle a timely and important problem, the solution for which would have a significant impact.\n\n## Weakness\n\nThere is no strong theoretical/analytical justification for the proposed approach. It is well-known that small eigenvalues can cause the matrix to be non-PSD in practice due to the limitations of finite-precision floating-point arithmetic. We can usually reason about this precisely or construct failure examples by first assuming some unit round-off error u, constructing a PSD matrix with some properties in terms of u, and showing that finite-precision arithmetic can lead to small negative eigenvalues (and thus non-PSD-ness). For the proposed method to be convincing, one would ideally like to see some analysis and guarantees that it is robust or can recover, regardless of how large the round-off error is. This may be true by construction, but is not immediately obvious, and needs to be discussed in the manuscript. There is some discussion on Pg. 5, under \"Structural Integrity\", but the argument is a little hand-wavy. I am not entirely convinced that the proposed optimization method can actually always mitigate these issues. Instead, they may simply be less likely to encounter these problematic regions.\n\nConcerning the experimental results, it appears the authors did tune the hyperparameters of their proposed approach on an intermediate validation/development set (according to A.3), but it is unclear whether the baselines against which they benchmarked their method benefited from any kind of similar tuning? Looking at Table 6, there appears to be a significant number of hyperparameters. \n\nSecond, I found the results themselves to be a bit underwhelming. In particular, the final test MSE often appears to be worse lower than the best-performing baseline? (e.g. Table 1-2)\n\nFinally, there is no indication of performance in terms of runtime. Instead of merely comparing convergence in terms of the number of epochs (e.g. Figure 5), it is important to get a comparison in terms of at least one of wall-clock time, throughput (samples per sec), or FLOPs.\n\n### Misc questions and issues\n\nHere is a (non-exhaustive) list of some miscellaneous issues and questions:\n\nPage 1:\n- \"fit only subpar\"\nPage 2:\n- \"empirically performing regions\": meaning ambiguous\n- \"destructive gradients\": meaning ambiguous\n- \"_Albeit_ all these efforts\": \"despite\"\n- \"improved and less volatile fit in terms of [...]\": vague\nPage 3:\n- \"allow to\"\nPage 4:\n- Figure 2 (a) \"Univariant variance\": univariate\nPage 5:\n- Figure 3 (a): this figure is quite difficult to parse. Also, there is nothing in the text dedicated to explaining what each item in the legend represents exactly. Most of these can be inferred but 'SqrtCovariance' -- does this just represent the matrix square root (Cholesky factorization)?\n- \"compensates large, but does not converge\"\n- \"Vice versa\"\n- \"rescaling is faulty\"\n- \"potentially destructive changes\"\n- \"disproportional\": \"disproportionate\"\n- \"cannot sustain structure integrity\": meaning ambiguous. Also, \"structural integrity\" is an odd choice of words.\n- \"requires to\"\n- \"the covariance quickly drifts out of the parameter space of PSD matrices\": please be more specific here. Presumably, you are referring to parameterizations that result, by construction, in a PSD covariance matrix such as the low-rank (Cholesky) or diagonal parameterization, and the reason they are not PSD is due to floating-point precision issues?\nPage 6:\n- \"the gradient is _sensitised_ by\"\n- \"degrading gradients further away\": meaning ambiguous\n- \"we argue that these gradient explosions harm learning for neural networks\": sentences like this are examples of where this manuscript is unnecessarily verbose. This \"argument\" can be unsaid because nobody would ever argue with this.\n- \"_inciting_ plateau regions\"\n- \"parameter space integrity\": the use of the word \"integrity\" here is non-standard and carries ambiguous meaning. I understand this to mean numerical stability and PSDness but this is not clear from the writing\n- Normalization: I would use terms like \"whitened\", \"standardized\", or \"de-correlated\" here, because normalization could mean so many other things (e.g. projected onto unit hypersphere, affine transformed to be in unit hypercube, etc.)\nPage 7:\n- \"_delivering_ favorable gradients\"\n- \"cholesky\": Upper case\n- \"respective the old distribution\"\nPage 8:\n- \"trade off the function approximator's _allocation_\": meaning ambiguous - I understand you are trying to say its \"representational capacity\" or something along these lines but this is difficult to decipher.\nPage 9: \n- \"without mean bound\"\n- \"once converged, is stable\": this sentence is tautological - if it's not stable, then it can't have converged?\n- \"multiple _windings_\": ?\n- \"_especially_ the covariance cannot be fit\"\n- \"but beta have a hard time fitting a full covariance\"",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity/Quality. I found this manuscript unclear and difficult to follow. Several sections are unnecessarily wordy and dense. Additionally, there are a few grammatical mistakes and quite a lot of odd choices of words or phrases that made the true underlying meaning ambiguous. Generally, the writing in this manuscript is of poor quality and could benefit from a few iterations of copy-editing. Lastly, the content is far from self-contained and requires familiarity with a number of existing works (Seitzer et al., 2022; Lin et al., 2021, Otto et al., 2021 to name the main ones) to understand the full extent of this paper's contribution.\n\nNovelty. The novelty is somewhat limited. The main argument is that small eigenvalues in the covariance matrix cause the gradient to blow up, and conversely, large eigenvalues cause the gradient to saturate. This is somewhat obvious if not well-known. Furthermore, the proposed technique to mitigate the issues straightforwardly chains existing solutions together into a pipeline consisting of structured NG and TRPL.\n\nReproducibility. There is no mention of source code availability, nor any indication it will become available upon publication. I am not confident the results reported in this manuscript can be reproduced without it, due to the finicky nature of the constituent methods. Appendix A.3 contains hyperparameter settings and further training details which certainly helps, but I am not sure that this is sufficient.",
            "summary_of_the_review": "I recommend Rejection at this time. There are a number of significant issues in this paper that make it not yet ready for publication. In particular, as outlined above, there are gaps in the analysis of the proposed method and in the empirical results. Furthermore, there is room for improvement in the writing and presentation. That being said, this paper seeks to tackle an important problem, and could become a great contribution once the aforementioned issues are addressed. \n\nEDIT: I am not inclined to update my score. The authors have not provided a response, and I have not found supportive arguments from other more positive reviews compelling enough to change my views.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3175/Reviewer_zKvQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3175/Reviewer_zKvQ"
        ]
    }
]