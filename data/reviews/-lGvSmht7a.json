[
    {
        "id": "jMKt09EI5mM",
        "original": null,
        "number": 1,
        "cdate": 1666672471049,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672471049,
        "tmdate": 1666672471049,
        "tddate": null,
        "forum": "-lGvSmht7a",
        "replyto": "-lGvSmht7a",
        "invitation": "ICLR.cc/2023/Conference/Paper4624/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Gradient Coding is a general approach distributed computation (of gradients) in the presence of straggling nodes. The paper introduces two new schemes for GC, where coding is not only done across compute nodes, but also across the time dimension, measured by compute rounds. The first scheme uses the original GC but with selective repetition of unfinished tasks. The second scheme, and main contribution, combines two classes of tasks, one that uses GC and the other that uses repetition. The two classes are multiplexed over workers and rounds in an adaptive manner.  An experimental evaluation is performed on AWS with 256 worker nodes, showing the performance of the new scheme. ",
            "strength_and_weaknesses": "Strength:\nUsing the temporal dimension in GC improves the normalized load per worker. The schemes cover various straggler models (e.g., bursty, arbitrary, fixed number per round). The experimental evaluation is performed in a real AWS environment, with a sizable number of nodes. \n\nWeaknesses:\nThe presentation is often hard to follow, due to the heavy and involved notation, although the high level ideas are relatively clear.",
            "clarity,_quality,_novelty_and_reproducibility": "The high level ideas and goals are clear, and explained well, but the notation and details are rather cumbersome.",
            "summary_of_the_review": "The paper proposes new CG schemes where that use the temporal dimension, and coding is done both across worker nodes and across compute rounds. As a result the normalized load per worker is reduced. Various design parameters are used to accommodate for different straggler models. The experimental evaluation is very good, and shows the performance of the new scheme.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4624/Reviewer_euSx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4624/Reviewer_euSx"
        ]
    },
    {
        "id": "jPVVKY6fA1",
        "original": null,
        "number": 2,
        "cdate": 1666742581670,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666742581670,
        "tmdate": 1666742581670,
        "tddate": null,
        "forum": "-lGvSmht7a",
        "replyto": "-lGvSmht7a",
        "invitation": "ICLR.cc/2023/Conference/Paper4624/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper propose 2 extensions of gradient coding, by adding extra tolerance of stragglers and introducing asynchrony (staleness) to the updates. The theoretical analysis is provided for the computational load. The empirical results show that the proposed schemes are more efficient than the previous works. ",
            "strength_and_weaknesses": "Strength:\n1. This paper propose 2 extensions of gradient coding, by adding extra tolerance of stragglers and introducing asynchrony (staleness) to the updates. The idea is natural and makes sense.\n2. The theoretical analysis is provided for the computational load. \n3. The empirical results show that the proposed schemes are more efficient than the previous works. \n\nWeakness:\n1. The experiment setup (CNN + MNIST) is too simple and small for distributed training. Such a simple task can be finished quickly on a single computer. To justify the efficiency of the proposed methods in practice, larger tasks such as resnet on cifar-100 or imagenet is highly recommended.\n2. I recommend to show the sensitivity to the hyperparameters, i.e., how the curve of convergence (training loss) vs. time changes when using different B and W. It is important to show the readers how robust the proposed algorithms are, which will also be helpful for the new users when tuning these hyperparameters.\n3. It seems that no convergence proof is provided. Although I think the convergence proof will be trivial since the proof can be reduced to the one for a simple asynchronous SGD algorithm, a convergence proof will make this work more thorough.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written with enough details provided for reproducing the experiments.\nThe proposed algorithms are novel.",
            "summary_of_the_review": "This paper propose 2 extensions of gradient coding, by adding extra tolerance of stragglers and introducing asynchrony (staleness) to the updates. The theoretical analysis is provided for the computational load. The empirical results show that the proposed schemes are more efficient than the previous works. I recommend to add experiments for the algorithms' sensitivity to the hyperparameters, and theoretical analysis for the convergence. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4624/Reviewer_dEGh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4624/Reviewer_dEGh"
        ]
    },
    {
        "id": "9edZtStq5s",
        "original": null,
        "number": 3,
        "cdate": 1666937797399,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666937797399,
        "tmdate": 1666937970903,
        "tddate": null,
        "forum": "-lGvSmht7a",
        "replyto": "-lGvSmht7a",
        "invitation": "ICLR.cc/2023/Conference/Paper4624/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper develops schemes to protect distributed gradient descent from stragglers building on previous work that induces redundancy in computation via error correcting codes. The authors specifically build on a technique called gradient coding that enables a master node to recover the sum of the gradients from $n$ distributed works, even if a certain prescribed number ($s$) nodes straggle (i.e., fail to provide their computation within a time-out). \n\nWhile gradient coding can protect each of the $\\binom{n}{s}$ straggling patterns in every iteration, the authors here aim to be less conservative and aim to protect a subset of the straggling patterns that seem to occur more commonly in practice. They make two assumptions (1) there is some negative correlation in the straggling patterns - a node that  straggles in a given iteration is assumed to not straggle in $B$ iterations later, and (2) that the number of distinct stragglers in any given time-window is limited. For these assumptions, the authors develop new coding schemes that improve upon gradient coding in terms of the amount of redundancy required.",
            "strength_and_weaknesses": "Strengths:\n+The redundancy of gradient coding can be quite a lot, and the recognition of structure among the set of stragglers in AWS lambda and reducing the overall redundancy is a positive contribution.   \n+The developed coding schemes are novel and clever - even though they do build upon the idea of gradient coding.\n+ The results are experimentally validated, and the experimental results are quite promising, and indicate the usefulness of carefully designed redundancy in AWS Lambda.\n\nWeaknesses:\n- The main use case for the developed technique is the application of training $M$ distinct models on the same data set. This seems to be a narrow - perhaps even contrived - use case as I see it. It is also inevitable that the developed technique cannot be applied to the most common use case of training a data set, due to the coding scheme incurring a delay in decoding (the sum of partial gradients at iteration $t$ is decoded at iteration $t+B$).\n\n- In gradient coding, the load is $s+1$ times the load of the common case. Note that this is the same overhead as if the data sets were repeated $s$ times (which is much simpler!). The main benefit of gradient coding, as I understand it, is that it does not necessarily require $s+1$ to divide $n$, whereas replication would require this. In fact, notice that this replication scheme is also a candidate for the experimental results in Fig. 4, as $s=15,n=256$.\n\nI would like the authors to clarify if the above property is also true for the developed coding schemes. If we allowed some additional divisibility criteria to be satisfied among the parameters, then do simpler schemes such as replication match the performance of the developed schemes?\n\n- There is no statement/result on whether the developed schemes can be improved, or gap to optimality.\n\n\n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "In Example 3.3.1, the authors claim that \"if the master did not receive $g_{2i}(t)$ in round $t$ and round $t+2, ...$.\nHowever, as per my understanding of the model, this can never happen for that example. If $g_{2i}(t)$ does not return, it is because the node straggles in iteration $t$. If it straggles in iteration $t$, it then returns that answer in round $t+B = t+2$, and as per the coding scheme, $g_{2i}(t)$ is necessarily repeated in round $t+2$. Please correct this or add clarifying explanations.\n\nApart from the above detail, the system model and coding scheme (which is quite intricate) is presented quite clearly.  \n\n--> A small clarification in the explanation of Fig. 14: For a given normalized load, are there 80 \"dots\" on the y axis corresponding to each job? If this is the case, the figure is the result of several hundreds of jobs? \nIf not, then it is unclear what the dots represent.",
            "summary_of_the_review": "The paper advances the development of introducing coding based redundancy in training towards mitigating stragglers, and builds on a recent line of work that is of interest to the machine learning community. The recognition of spatial and temporal dependencies in stragglers, and the corresponding coding techniques are novel. However, the application of the techniques is limited to a relatively narrow use case of concurrent training of multiple models on the same data set. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4624/Reviewer_9BzY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4624/Reviewer_9BzY"
        ]
    },
    {
        "id": "nvX8_E3oWd",
        "original": null,
        "number": 4,
        "cdate": 1667201657091,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667201657091,
        "tmdate": 1671063919605,
        "tddate": null,
        "forum": "-lGvSmht7a",
        "replyto": "-lGvSmht7a",
        "invitation": "ICLR.cc/2023/Conference/Paper4624/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new coded distributed gradient descent approach to mitigate the effect of stragglers in the distributed computation of gradients. The authors combine prior schemes on coded distributed gradient computation with repetition of unfinished tasks from straggling nodes across computation rounds with the assumption being that the overall computation can tolerate a finite amount of delay. Experiments on a distributed gradient descent task on AWS Lambda shows improvements over prior uncoded and coded approaches.",
            "strength_and_weaknesses": "Strengths:\n\n1. The approach considers sequential computations and is designed based on the trends in straggling over time. The temporal aspect of straggling has not been considered by most prior coded distributed computing works and is arguably more practical in real scenarios (as also demonstrated by the experiments in this work).\n\n2. The proposed approach is faster than baselines in real serverless computing settings (AWS Lambda) and reduces the computation load at workers which may be especially relevant for emerging frameworks like serverless computing where the individual instances do not have a lot of computing capacity.\n\nWeakness:\n\n1. The approach will improve over prior gradient coding works only when a finite amount of delay ($T>0$) can be tolerated. The authors consider a scenario where multiple neural networks are trained on the same dataset in their experiments where such a delay can be tolerated. However, it is unclear when this would happen in practice because typically gradient descent is a sequential operation with successive iterations of a single model being performed where such a delay would not be acceptable because the gradient from each round is needed for the next round. Please provide some examples/references to back this claim.\n\n2. The proposed approach seems to strongly depend on the choice of hyperparameters ($B$, $W$, etc.). Indeed, the authors perform additional experiments to select the right hyperparameters. Given, the extra cost/latency of performing these added experiments, and the extra cost/latency of decoding the coded gradients, neither of which is included in the time measurements, it is unclear if the proposed schemes will give any net improvements over the uncoded scheme, especially since model training is a one-time operation where a small amount of straggling may actually not be a very significant issue. Therefore I would like to see either that the gain is significant after taking these factors into consideration or that incorrect/different choices of hyperparameters for the same setting do not lead to much differences in performance since that would imply that the extra experiments to choose the hyperparameters will not be needed.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear overall and the proposed scheme is novel within the context of coded distributed gradient descent. I have the following three comments/suggestions on the writing:\n\na. The audience, especially at ICLR, may not be familiar with the use of erasure codes for straggler mitigation in distributed computing. Therefore, I feel it would be better to provide some background on the general area of coded computing to make the paper more accessible at this venue.\n\nb. Likewise it may be beneficial to provide some details on the Gilbert Elliot model and its deterministic counterpart to clarify why it is a suitable model for stragglers.\n\nc. Please include a conclusion section.",
            "summary_of_the_review": "The paper introduces a new coded distributed gradient computation approach for mitigating stragglers in distributed gradient descent. While the approach improves over baselines if a finite delay in the computation can be tolerated, it is unclear where such delays would be acceptable in practice. Moreover, given the added cost/latency associated with choosing the hyperparameters for this approach the net gain over the uncoded approach is not yet clear.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4624/Reviewer_82EB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4624/Reviewer_82EB"
        ]
    }
]