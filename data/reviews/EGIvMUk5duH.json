[
    {
        "id": "66gEsQbfpKs",
        "original": null,
        "number": 1,
        "cdate": 1666527286723,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666527286723,
        "tmdate": 1666527570896,
        "tddate": null,
        "forum": "EGIvMUk5duH",
        "replyto": "EGIvMUk5duH",
        "invitation": "ICLR.cc/2023/Conference/Paper4846/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies in what situation training samples are vulnerable to membership inference attacks.\nFirstly, the authors empirically argue that membership inference performance may have a weak connection with the OOD property of training data.\nThen, they hypothesize that membership inference advantages may be related to how well the samples are memorized by models.\nThrough experiments, the authors find a clear correlation between the label memorization score of data and the corresponding membership inference attack accuracy, which justifies their claim.",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper is written clearly and the studied topic is important and interesting.\n\n2. The empirical results are strong enough (especially Fig. 1 and Fig. 2) to justify the correlation between label memorization and membership inference.\n\n\nConcerns & Questions:\n\n1. According to the definition of label memorization score, one needs to access the learning algorithm to calculate such a score. However, in real-world attack scenarios, the learning algorithm (as well as hyperparameters) of a black-box model may be agnostic by adversaries, which would result in difficulties in obtaining memorization scores. Please discuss the practicality of leveraging memorization scores in membership inference attacks.\n\n2. Why the conjecture in Section 4.3 is appropriate? Please justify.\n\n3. In Table 1, every model is trained with MNIST and a subset of under-represented data. However, from Table 2, it seems that every model is instead trained with MNIST and the singletons from the under-represented data (which results in the size of the under-represented population being reduced). I think the comparison between Table 1 and Table 2 is unfair, and a more fair comparison may be that: (1) train model on MNIST + subset of under-represented data, and (2) compare the classification accuracy of the model on random under-represented data and under-represented singletons. Please comment.\n\n4. Only MNIST is used as the over-represented dataset. I suggest using more datasets as the over-represented datasets.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow. The research topic is important and interesting.",
            "summary_of_the_review": "This paper studies an important and interesting problem, i.e., the relationship between data memorization and membership inference advantages. In general, the empirical results are strong enough to justify most of the claims in the paper. Therefore, I tend to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4846/Reviewer_zop7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4846/Reviewer_zop7"
        ]
    },
    {
        "id": "BCpgurgRax",
        "original": null,
        "number": 2,
        "cdate": 1666609146149,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666609146149,
        "tmdate": 1666609146149,
        "tddate": null,
        "forum": "EGIvMUk5duH",
        "replyto": "EGIvMUk5duH",
        "invitation": "ICLR.cc/2023/Conference/Paper4846/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the connection between label memorization and membership inference.\nThe paper shows that examples that are considered OOD according to some empirical metrics may not be more vulnerable to MI. Instead, examples with large label memorization (as defined by Feldman) are more likely to be vulnerable to MI attacks.",
            "strength_and_weaknesses": "Strengths:\n- understanding what makes data vulnerable to privacy attacks is an important problem\n- comparing different ways of \"defining\" OOD data is valuable\n\n\nWeaknesses:\n- Very imprecise. Terms like \"singleton\" or \"OOD\" are never formally defined, and the conjecture in Section 4.3 and its consequences are stated in terms vague enough that the conjecture cannot meaningfully be falsified\n- Circular reasoning: ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written, but understanding its main points is difficult because many important concepts are left underspecified.\nFor example, the concept of a \"singleton\", which is central to many of the paper's arguments, is never formally defined.\n\nSimilarly, the term \"OOD\" is used throughout the paper without ever being clear on what its formal definition actually is. As discussed below, there isn't any single canonical definition of OOD. Instead we just have different metrics that try to capture something like OOD. But it would be worth clarifying this early on in the paper, as the paper's results have a very different interpretation in this light.\n\nSection 4.3 suffers from vagueness the most. The given conjecture uses terms such as \"singleton\", \"well generalized\" without any proper definition. The notation \"-> 1\" is also not defined. It is unclear what it would mean for Strat to \"tend towards 1\" (my interpretation of the meaning) as there are no variable quantities here. As a result, the conjecture is essentially meaningless as one cannot hope to confirm or refute it.",
            "summary_of_the_review": "The paper's main claim is that OOD points are not more susceptible to MI, but this depends entirely how one defines \"OOD\". In fact, an entirely natural way of defining OOD data is simply by using the result of an MI attack.\nWhat the paper actually shows (in Figure 1) is that some canonical OOD metrics (like MSP) are not aligned at all with privacy scores from MI attacks. But this can be interpreted in two ways: either privacy scores aren't impacted much by OOD data, or MSP is not actually a very good OOD metric!\nLooking at Figure 1, it seems that the latter conclusion is most at \"fault\" here: indeed, in this experiment you have ground truth on what data is OOD (i.e., the under-represented data). As we can see, MSP is terrible at distinguishing the under-represented and the over-represented data. So it seems like a poor OOD metric.\nIn contrast, the privacy score appears to distinguish the under- and over- represented data a lot better.\nThere is a prior work by Carlini et al (https://arxiv.org/abs/1910.13427) that looks at the agreement between different measures of what constitutes an outlier, including privacy metrics. It would be worth discussing the relation of this work to that paper.\n\nAs figure 2 shows, the privacy score of Carlini et al and the memorization score of Feldman are more closely correlated. This is not particularly surprising though: from the definition of label memorization (which should be credited to Feldman 2020, rather than Feldman & Zhang), it is obvious that high memorization of a given point implies the ability to do membership inference on that point well.\nIn fact, the two metrics are essentially measuring the same thing, just at different levels of granularity: the score of Carlini et al. measures the distinguishability of the loss distributions when training on D vs D \\ {i}. Label memorization takes a much coarser approach by just collapsing these distributions to a single scalar, the probability of outputting the correct label. Through this view, it is not clear why it should be surprising that these metrics are very strongly correlated.\n\nDue to this very strong correlation, the results from section 5 also seem somewhat circular. Of course, if we focus the MI attacks on those samples where the label probabilities for members and non-members are most different, then the MI attack will work better.\n\nSo overall, it is not clear to me what conclusions to draw from this paper. There are some interesting results that seem to show that MI attacks are better at identifying OOD data compared to other metrics. But these results are not at all interpreted this way, and rather taken to imply that prior work somehow drew incorrect conclusions regarding the success of MI attacks on OOD data.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4846/Reviewer_eP6o"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4846/Reviewer_eP6o"
        ]
    },
    {
        "id": "alrSG6GW6z",
        "original": null,
        "number": 3,
        "cdate": 1666664835627,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664835627,
        "tmdate": 1669248848727,
        "tddate": null,
        "forum": "EGIvMUk5duH",
        "replyto": "EGIvMUk5duH",
        "invitation": "ICLR.cc/2023/Conference/Paper4846/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the relationship between label memorization and membership inference. It showed that examples with high memorization are more easily attacked. Based on this, it formulated membership inference attack where the attacker choose highly memorized examples, and show that the attack performance improved significantly.",
            "strength_and_weaknesses": "**Strength**: This paper systematically studied the relationship between memorization and MI attack performance, and also evaluated common OOD detection algorithms.\n\n**Weakness**\n\n1. The connection between label memorization and membership inference attack performance is known. The attack algorithm that this paper primarily use (Carlini et al. 2022a) is directly built on this relationship: it performs hypothesis testing based on the distribution of two group of shadow models to make the MIA prediction. Since the (practical estimator of the) label memorization is just the difference between the mean of the two distributions, the larger the difference between the two distributions, the easier the hypothesis testing is, and subsequently the higher the MIA performance is. So this observation, while not formally characterized yet, is essentially known in the literature.\n\n2. This paper claims that OOD is not well correlated with MIA performance. But this is not well supported, and the main reason for the observation seems to be that the OOD detector used in the paper are not a good one. From Fig. 1, x-axis, we can see that the OOD score failed to make a clear separation between the over-represented data and under-represented (out of distribution) data.\n\n3. The paper propose to perform MIA in a MI Game setting where the attacker choose the points being attacked according to the susceptability of them being memorized. This ended up making the MIA performance higher. I'm not an expert in security, but showing that the attackers could have high success rate when they choose to only attack the most outlier examples does not seem surprising or concerning, comparing to, e.g., showing that an attacker can achieve high rate on average across randomly sampled training examples.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and relatively easy to follow. ",
            "summary_of_the_review": "This paper studies the relationship between memorization and membership inference. My main concern over this paper is that the main empirical observations are essentially known, and the proposed MIA that focus on attacking the memorized examples is just a straightforward implication of the observation applied to existing MI attacks, and does not seem to bring new insights to the table.\n\n---------------------\nPost rebuttal: Thanks to the authors for the responses. After reading the response and other reviewers' comments, I'm keeping my current rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4846/Reviewer_j9KE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4846/Reviewer_j9KE"
        ]
    },
    {
        "id": "kWQ2zi3gTNA",
        "original": null,
        "number": 4,
        "cdate": 1667600719946,
        "mdate": 1667600719946,
        "ddate": null,
        "tcdate": 1667600719946,
        "tmdate": 1667600719946,
        "tddate": null,
        "forum": "EGIvMUk5duH",
        "replyto": "EGIvMUk5duH",
        "invitation": "ICLR.cc/2023/Conference/Paper4846/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This study focuses on the existence of samples in which membership inference is more likely to succeed and samples in which it is less likely to succeed and discusses the causes of this difference. It was generally believed that out-of-distribution samples were more likely to succeed in membership inference, but the study argues that whether the sample is in-distribution or out-of-distribution has little to do with whether the sample is in-distribution or out-of-distribution. Instead, they argue that the larger the gap between the probability that a sample is identified as the target label when trained on a training data set that contains the target sample and the data set that does not, the more likely it is that membership inference will be successful. In addition, theoretical and experimental considerations regarding this hypothesis are provided.\n\n\n",
            "strength_and_weaknesses": "Strength\nTheoretical considerations on the success of membership inference.\n\nWeakness\nContains unexplained or implicit assumptions\nContains gaps in the logic. Due to this gap, it s difficult to examine whether it is making a valid argument adequately",
            "clarity,_quality,_novelty_and_reproducibility": "Contains unexplained or implicit assumptions.\nContains gaps in the logic. Due to this gap, it s difficult to examine whether it is making a valid argument adequately.",
            "summary_of_the_review": "The author's argument could not be fully understood because it either assumes knowledge and understanding of existing research and assumptions contained in them or because there are gaps in the logic. The paper's claims must be structured in such a way that they can be understood on their own.\n\nPage 4\nThe explanation of the privacy score is too simple.\n\nPage 5\nWhy do you only include samples with large memorization values in your attempt to correlate memorization and privacy scores? Isn't it impossible to evaluate the correlation unless we also include samples with small memorization values?\n\nFigure 2, bottom\nHow were these image samples selected? Are they not cherry-picked?\n\nPage 6\nHow were samples in D_A  drawn? Sampled from out-of-distribution?\n\nProof of Lemma\nWhat does the hybrid argument mean? \n\nPage 7\nIn Reason 1, do you assume \\theta is optimal?\nIn Reason 2, do you assume the training procedure is probabilistic?\n\n\n\n\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4846/Reviewer_HK7T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4846/Reviewer_HK7T"
        ]
    }
]