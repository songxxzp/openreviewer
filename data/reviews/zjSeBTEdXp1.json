[
    {
        "id": "bQV9VleRaN3",
        "original": null,
        "number": 1,
        "cdate": 1666360263547,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666360263547,
        "tmdate": 1666360263547,
        "tddate": null,
        "forum": "zjSeBTEdXp1",
        "replyto": "zjSeBTEdXp1",
        "invitation": "ICLR.cc/2023/Conference/Paper2542/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper under consideration proposes a method (DGGF) for solving generative modeling related problems. The method is based on Wasserstein gradient flows with potential function given by entropy-regularized $f$-divergence. Essentially, the authors try to approximate density ratio function (DRE) via Bregman divergence combined with Euler-Maruyama simulation. The density ratio function is used at the inference stage allowing a practitioner to sample from target distribution.",
            "strength_and_weaknesses": "**Strength**\n\n* The paper provides a comprehensive list of practical applications and shows practical advantages of the method in several use cases. For example, the proposed density-ratio based method could be adapted for conditional generation on-the-fly. \n* DGGF approach seems to overcome the alternative methods based on density ratio function (EPT) and doesn\u2019t require additional latent space to data space generators. The proposed Algorithm is easy to implement (compared to JKO-based Wasserstein gradient flows methods)\n\n**Weaknesses**\n* My main concern regarding the proposed method is the lack of theoretical interpretability. The theory of Wasserstein gradient flows is nice but it is not clear if the Algorithm 1 is indeed turning the function $r_{\\theta}$ to be the density ratio function $q_t(x)/p(x)$. At first, $r_{\\theta}$ does not depend on time which limits the interpretability from a theoretical point of view. There are cases when the prior distribution $q_0(x)$ is significantly different from target distribution $p(x)$ and one can expect that the density ratio function significantly changes along the gradient flow (for example, when the uniform distribution is used as the prior). Secondly, the convergence analysis of the Algorithm 1 itself is also unclear. Bregman divergence optimization is anticipated by stochastic procedure (EM simulation) and it is hard to see if the optimization converges to something relevant. Therefore, special attention should be paid to the convergence analysis of the Algorithm 1 and what is the physical meaning of $r_{\\theta^*}$ it converges to. How does $r_{\\theta^*}$ resemble the real density ratio function? How does it depend on $K$?\n* In light of my previous point I have some doubts concerning the  interpretation of  \u201cModel Estimate of KL over Flow\u201d section. The Figure 4 shows that the quantity $E_{x_{t_k}} \\log r_{\\theta}(x_{t_k})$ is indeed converges to zero, but if we can interpret the charts as KL convergence remains unclear for me.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** The paper is over-all well-written.\n* The link to the article [1] seems to be missed (it introduces density ratio fitting under Bregman divergence):\n\n[1] Sugiyama et. al. Density-ratio matching under the Bregman divergence: a unified framework of density-ratio estimation https://doi.org/10.1007/s10463-011-0343-8\n\nWhat is `stop_gradient` procedure in the Alorithm 1 in the appendix?\nSome hrefs in the paper don\u2019t work (do not redirect to the bibliography section). For example, Gao et. al., 2022. Song and Ermon et. al., 2019 \n\n**Quality and Novelty:** The idea of utilizing Wasserstein gradient flows (in particular, with entropy regularized $f$-divergence) is not new, therefore the main contribution the authors present is methodological. In spite of the proposed approach seems to work, there are some problems with theoretical interpretability listed in the weaknesses section. Therefore:\n* The questions raised in the weaknesses section should be answered. Probably it is worth considering some toy scenarios with known density ratio function or give up Wasserstein gradient flows interpretation in favor of some other theoretical basis.\n* Minor comment, but yet: please add the comparison with JKO-flow approach [2]. This related method seems to surpass the proposed one at CIFAR10 image generation problem. \n[2] Fan et. al. Variational Wasserstein gradient flow, https://arxiv.org/pdf/2112.02424.pdf\n\n**Reproducibility:** I didn\u2019t run the code provided but it seems to be clearly written and I have no special doubts regarding reproducibility. \n",
            "summary_of_the_review": "In spite of practical applicability of the proposed DGGF approach the theoretical interpretability should be significantly improved. For now there is a gap between Wasserstein gradient flows theory and proposed methodology. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2542/Reviewer_YDPG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2542/Reviewer_YDPG"
        ]
    },
    {
        "id": "t9q7LF5_9b",
        "original": null,
        "number": 2,
        "cdate": 1666591693790,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591693790,
        "tmdate": 1666591693790,
        "tddate": null,
        "forum": "zjSeBTEdXp1",
        "replyto": "zjSeBTEdXp1",
        "invitation": "ICLR.cc/2023/Conference/Paper2542/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work re-explores deep generative modeling by minimizing the entropy regularized f-divergence in the Wasserstein-2 space of probability measures and derives an algorithm based on an ambiguous discretization method of the relevant Wasserstein gradient flows. Some experimental results on image synthesis are presented to support the proposed method.",
            "strength_and_weaknesses": "The work is not theoretically sound, and the proposed algorithm lacks justifications of its correctness both intuitively and technically.\nIt is well-known that Wasserstein gradient flows can be utilized in deep generative modeling. There are a series of publications that have introduced Wasserstein gradient flows in deep generative modeling and explored their potential in analyzing or developing deep generative modeling algorithms, for example please see [1-9]. Most of these relevant papers are not mentioned in the related work. It is not novel to leverage the Wasserstein gradient flow of the entropy regularized f-divergence in deep generative modeling and to further simulate the gradient flow with a density ratio estimation procedure [8, 9]. The empirical evaluation of the proposed method seems reasonable.\n\nThe central part of the proposed method is to numerically simulate the relevant Wasserstein gradient flows with density ratio estimation between the pushforward distribution $q_t$ and the target distribution $p$. According to the authors on page 4, \u201cThe simulation of Eq. 8 requires a time-dependent estimate of the density ratio; however, we train the density ratio estimator $r_\\theta$ without explicit dependence on time\u201d. As far as I\u2019m concerned, the numerical simulation formula in Eq. 8 should explicitly depend on a time-dependent density ratio which is a basic result of Euler-Maruyama discretization. It is hard to capture the basic intuition of recasting this density ratio as a time-independent one, especially from a perspective of simulating a time-dependent Wasserstein gradient flow path. In practice, the proposed method does only estimate a time-independent density ratio and plugs it in Eq. 8 as indicated by the pseudocode of the training algorithm in the appendix. Furthermore, a natural question would be raised regarding the convergence properties of the proposed algorithm. Suppose that the training of the proposed model is completed, the pushforward distribution $q_{t_K}$ is approximately equal to the target distribution $p$ when $t_K$ is large enough. Accordingly, the estimated density ratio between $q_{t_K}$ and $t_K$ would be expected to approximate a constant 1 almost everywhere. This observation is basically inspired by the convergence behavior of Wasserstein gradient flows. Then a straightforward question is how to generate new samples at test time just using a time-independent density ratio while the estimated density ratio is approximately equal to 1. Due to this possible issue, it is recommended to justify the convergence property of the iterations in Eq. 8 when a claimed time-independent density ratio estimator is deployed.\n\u00a0\n[1] Geometrical Insights for Implicit Generative Modeling, 2017.\n[2] Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions, ICML 2019.\n[3] Sobolev Descent, AISTATS 2019.\n[4] Maximum mean discrepancy gradient flow, NeurIPS 2019.\n[5] Deep generative learning via variational gradient flow, ICML 2019.\n[6] Refining deep generative models via discriminator gradient flow, ICLR 2021.\n[7] On the convergence of gradient descent in GANs: MMD GAN as a gradient flow, AISTATS 2021.\n[8] Re\ufb01ning deep generative models via discriminator gradient \ufb02ow, ICLR 2021.\n[9] Deep generative learning via Euler particle transport, MSML 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "It is suggested to provide a pseudocode of the training algorithm in the main body instead of the appendix. If possible, please also include a sampling algorithm for reference.\n The results might be important for developing deep generative models if the correctness can be justified.",
            "summary_of_the_review": "The paper lacks of both theoritical and computational justification.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2542/Reviewer_NwTe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2542/Reviewer_NwTe"
        ]
    },
    {
        "id": "1EyS9U3Rku",
        "original": null,
        "number": 3,
        "cdate": 1666666709965,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666709965,
        "tmdate": 1666666709965,
        "tddate": null,
        "forum": "zjSeBTEdXp1",
        "replyto": "zjSeBTEdXp1",
        "invitation": "ICLR.cc/2023/Conference/Paper2542/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper explores a unconditional learning generation model using Wasserstein Gradient Flow directly on the data space. To make the training feasible, the authors propose to use a deep density ratio estimator for learning in Wasserstein space.Authors demonstrate the proposed method on Cifar10 and CelebA, and the result shows that DGGF is able to generate high-quality images.\n\nFrom a conceptual point of view, the work differs from related approaches, in the sense that it tackles the problem of learning unconditional generative models directly on data space. However, from computational point of view, I found it quite simililar to Energy-based models.",
            "strength_and_weaknesses": "Strengths\n* [S1]: The paper addresses an important problem in an interesting way, based on a clear motivation.\n* [S2]: The method is sensible and clearly presented, I enjoyed reading it.\n\nWeaknesses\n*[W1] Insufficient empirical evidence; several experimental comparisons are absent and should be included (see below for further details).\n\n\nDetails:\n* Experiments(baselines):  Considering there are few related work that addressing the intractable chanlledge, some baseline method([1][2]) should be included for comparision.\n* Can you further investigate the algorithm's computational complexity considering the training is ? Is it possible to quantify the performance on larger data sets?\n* Can a pseudo-code be added to the training/inference procedures?\n* Can a discussion be included to this concurrent work? https://arxiv.org/pdf/2209.11178v4.pdf \n\n[1] Large-scale wasserstein gradient flows. \n[2] Variational wasserstein gradient flows.",
            "clarity,_quality,_novelty_and_reproducibility": "The submission is concise and of excellent quality. The submission consists of code.",
            "summary_of_the_review": "Overall, I tend to give a negative vote, but the score can still be adjusted if my concerns can be addressed. My main concern comes from the fact that the experimental results of the article are not strong, and there are still many aspects that could be discussed quantitatively (e.g. on a larger datasets, reporting the training speed of the algorithm, etc.), which makes the strengths of WGGF unclear to me.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2542/Reviewer_yo3y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2542/Reviewer_yo3y"
        ]
    }
]