[
    {
        "id": "R5r5sBrVTw",
        "original": null,
        "number": 1,
        "cdate": 1666543901713,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666543901713,
        "tmdate": 1666660097006,
        "tddate": null,
        "forum": "KwmPfARgOTD",
        "replyto": "KwmPfARgOTD",
        "invitation": "ICLR.cc/2023/Conference/Paper734/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "An equivariant Transformer/GNN model is proposed to deal with the 3D symmetries (rotation, transliteration, and inversion). The main targets are atomic graphs such as molecules. \nThe main technical advancement of this paper is the equivariant attention operation with multi-layer perception and nonlinear activations. \nExperimentally, the proposed model records better figures consistently compared to the existing equivariant-aware Transformer/GNNs. ",
            "strength_and_weaknesses": "(+) Developed a generic MLP attention operator with a non-linear activation function. \n\n\n(+) Experimental results show the efficacy of the proposed method against the existing equivariant models. \n\n(-) difficult to read especially for readers who are unfamiliar with equivariance or group theory.\n\n(-) As a result, the technical novelty of the proposed method is unclear (to me, at least)\n\n(-) The difficulty of the tackled technical challenge is not discussed. This makes it difficult for me to understand the significance of the proposed solution in the equivariant GNN literature. ",
            "clarity,_quality,_novelty_and_reproducibility": "I'm a novice at the equivariance or the group theory. Therefore, the overall impression on this paper is \"difficult to read\", The detailed appendix sections (thank you, authors!!) helped me a lot to understand, but I still need some questions to (better) understand the contributions of this paper. \n\nPlease feel free to correct my misunderstandings!\n\n[main contributions]\nConsists of the following two items. Is this correct? \n* A Transformer that can handle any equivalent features (more than type-0 and type-1 vectors)\n* Developed an equivalent operator for a MLP-driven attention with nonlinear activations \n\n[technical novelty]\n* All the contents of Sec 4.1 are basically established in the existing literature? \n* I understand Sec 4.2 is the core of the technical novelty, especially Eqs. (2-4). Please give me some more explanations about (I) where the difficulty lies, (ii) how difficult the challenges are, and (iii) how you tackle the challenges. \n* Nonlinear equivariant messaging (Eq.5) is a known result (e.g. by SEGNN)? \n\n[some naive questions]\n* Please consider presenting some simple examples. For example, how the type-L vectors look like for each symmetry? \n* Fig. 2 What are the meanings of the colors of nodes?\n\n",
            "summary_of_the_review": "A Transformer with 3D equivariant awareness is presented. The main contribution is the MLP-based attention that is capable of equivariant outcomes. \nExperimental results are strong. \nThe manuscript is not easy to read for readers unfamiliar with equivariant topics. More explanations through feedback is expected to more correctly evaluate the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper734/Reviewer_8jGX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper734/Reviewer_8jGX"
        ]
    },
    {
        "id": "Jxa3M3DdV9",
        "original": null,
        "number": 2,
        "cdate": 1666689226000,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666689226000,
        "tmdate": 1668674289839,
        "tddate": null,
        "forum": "KwmPfARgOTD",
        "replyto": "KwmPfARgOTD",
        "invitation": "ICLR.cc/2023/Conference/Paper734/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel transformer framework that involve equivariance for the inputs. It shows high performance on OC20 dataset and comparable results on MD17 and QM9.",
            "strength_and_weaknesses": "Strength:\n1. The proposed method considers various equivariances in transformer frameworks.\n2. The experimental results of IS2RE for OC20 dataset outperforms the other baselines.\n\nWeakness:\n1. In Sec. 4, all the components of equiformer are existing modules. The authors should emphasis their contributions or modifications to these existing modules.\n2. In Sec. 4, the analyses of equivariance are missing. Why the proposed components still keep equivariance?\n3. It seems that the performances on MD17 and QM9 are marginal. Why the baselines evaluated on MD17, QM9, and OC20 are different?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality is pool. Sec. 4 just introduce each component one-by-one. The authors should point out their own contributions in the paper and why the proposed method is able to keep the equivariance.",
            "summary_of_the_review": "Overall, the novelty, experimental evaluations, and the motivation are enough in the current form. However, the paper is hard to follow because the novel part of the proposed method and the existing modules are mixed in the paper. The writing should be improved",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper734/Reviewer_1yMo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper734/Reviewer_1yMo"
        ]
    },
    {
        "id": "Borl1l8meQ",
        "original": null,
        "number": 3,
        "cdate": 1667217715043,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667217715043,
        "tmdate": 1667218099579,
        "tddate": null,
        "forum": "KwmPfARgOTD",
        "replyto": "KwmPfARgOTD",
        "invitation": "ICLR.cc/2023/Conference/Paper734/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an E(3)/SE(3) equivariant transformer on 3D molecular graphs. The central point is to apply MLP attention + non-linear message for better capturing the interaction between atoms.  The attention weights are computed by a non-linear MLP attention mechanism based on the type-0 irreps features. Then the attention weights are multiplicated with other irreps features with type >0. The evaluations are carried out on QM9, MD17 and OC20, which supports the benefit of the proposed method.",
            "strength_and_weaknesses": "Strengths:\n\n1. This paper did a good job on explaining and comparing the difference between different equivariant models and transformers: TFN, SE3-Transformer, NeuIP, SEGNN.\n\n2. Besides QM9 and MD17 (which are actually well-explored datasets and could be overfitted by many recent methods), the authors evaluate their method on OC20 (the direct setting), a new but more desirable dataset for performance comparison. It is valuable to see that the proposed method achieves good results on IS2RE. Necessary ablation studies are also performed.\n\nWeaknesses:\n\n1. I totally understand that MLP attention + non-linear message is the most desirable combination as supported by this paper. Extending current methods to the form proposed in this paper is not that surprising. The authors are suggested to futher highlight the novelty of the proposed method. The current writing makes people think that it is just a simple combination of serveral components (Sec. 4). Readers are interested in knowing which part is novel and which part is previously proposed. \n\n2.  The ablation studies are insufficient. From Table 6-7, it seems the contribution of this paper only lies in the form of the attention it used. If this is the case, the contribution is weak. In other words,  the authors are suggested to perform more experiments to show what other novelty (other than the attention) is and why it works.\n\n3. For the OC experiments, did you try the relaxation setting?",
            "clarity,_quality,_novelty_and_reproducibility": "As stated above, this paper did a good job on explaining and comparing the difference between different equivariant models and transformers in Related Work. However, there are still certain concerns for the presentation. \n\n1. Section 4 only focuses on SE(3) equivaraince. What about E(3) equivariance? By the way, SH is SE(3)-equivariant not E(3).\n\n2.  It is hard to justify the sufficient novelty by the current presentation. \n",
            "summary_of_the_review": "Overall, this paper has made some variable efforts on improving current equivariant GNN models, by considering the techiniques from Transformers.  However, for its current presentation, it seems the technical novelty is marginal and the experimental evaluations are still insufficient to support the contributions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper734/Reviewer_d6yC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper734/Reviewer_d6yC"
        ]
    },
    {
        "id": "K6qHnlIu_-",
        "original": null,
        "number": 4,
        "cdate": 1667430479942,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667430479942,
        "tmdate": 1667430479942,
        "tddate": null,
        "forum": "KwmPfARgOTD",
        "replyto": "KwmPfARgOTD",
        "invitation": "ICLR.cc/2023/Conference/Paper734/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new equivariant neural network to process molecular graphs.\nThe authors successfully combine two already effective equivariant architecture designs - i.e. non-linear message passing and transformers - and achieve improved performance and computational gains on multiple datasets and tasks.\nMoreover, a large ablation study is included to validate the importance of each contribution.\n",
            "strength_and_weaknesses": "\n\nWhile the main theoretical contribution is limited to combining existing methods, I think the paper has high practical relevance.\nIndeed, besides the state-of-the-art results, the extensive experimental section provides insights about benefits and costs of different sub-modules.\n\nEven if many related works often don't do it, I encourage the authors to report mean and std of their results, aggregated over multiple runs (the difference in performance in some experiments is relatively small).\nI think this is especially important in the ablation study: a more statistically sound comparison would increase the impact of this work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the related literature is sufficiently discussed.\nThe complex architectures proposed are much easier to understand thanks to the clear figures.\n\nThe supplementary materials describe in details the experiments performed and the architectures.\n",
            "summary_of_the_review": "SUMMARY\n\nThe work combines existing methods in a new, successful way and is supported by strong empirical results.\nThe authors could improve the work by proving the statistical significance of some results (e.g. reporting mean and std over multiple runs of their experiments).\n\n\n\nOTHER QUESTION\n\n\nYou argue that (1) the MLP attention is more computationally efficient and more expressive than (2) Dot-Product attention.\nIn C.4, you mention that the higher cost in (2) is partially because the query and the key need to have the same dimension of the value.\nWhy can't smaller keys and queries be used?\nIf only frequency-0 keys and queries were used in (2), as is done in (1), would its computational cost be lower than (1) ?\n\n\nThe description of radial basis and radial functions at page 6 is a bit unclear. What does it mean that the function consists of *two* MLPs? Do you mean a 2-layers MLP?\nAlso, I am not sure I understood the first part of this sentence: \n\"[We transform radial basis with a learnable radial function] to generate weights for those DTP layers.\"\n\nSince this work builds mostly on top of SEGNN and SE(3)-Transformer, isn't it better to use the train/val/test split used by those works in the QM9 experiments?\nDo you think the different split could make a significative difference in the results?\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper734/Reviewer_sfAh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper734/Reviewer_sfAh"
        ]
    }
]