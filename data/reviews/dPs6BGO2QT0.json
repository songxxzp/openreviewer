[
    {
        "id": "Z7l0mYcCin",
        "original": null,
        "number": 1,
        "cdate": 1666483861955,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666483861955,
        "tmdate": 1666483861955,
        "tddate": null,
        "forum": "dPs6BGO2QT0",
        "replyto": "dPs6BGO2QT0",
        "invitation": "ICLR.cc/2023/Conference/Paper1212/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces SimDRC for dialogue representation learning. SimDRC capture the dialogue structure by locality loss and isotropy loss. The locality loss maximizes the cosine similarity of the representations of the tokens within an utterance, while the isotropy minimizes the cosine similarity of the representations of the tokens from different utterances. The authors applied SimDRC to fine-tune BART on 3 different dialogue tasks, i.e., multi-turn dialogue response generation, conversational response retrieval, and conversational semantic role\nlabeling. The experimental results showed that SimDRC improved the naive fine-tuning strategy in terms of automatic and human evaluation metrics.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper is well organized and easy to follow.\n- The authors presented extensive experimental results on 3 different dialogue tasks with 2 different language model (BART and DialoGPT).\n\nWeakness:\n\n- I am not fully convinced that \"most task-oriented datasets, like MultiWOZ are essentially not conversational and contextualized\" as stated in the paper. There are plenty of multi-turn task-oriented datasets (e.g., SGD, STAR, RiSAWOZ, BiTOD, CrossWoz ). It would make the paper stronger if authors could show the advantage of the proposed method in some task-oriented datasets.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity, Quality, Novelty: good.\n\nReproducibility: lack implementation details (e.g., model training), code is not available.",
            "summary_of_the_review": "This paper proposed a novel method to improve dialog representation learning. But the experiments are only limited to open domain dialogue datasets. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1212/Reviewer_uyyF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1212/Reviewer_uyyF"
        ]
    },
    {
        "id": "KaDBfQZb5JH",
        "original": null,
        "number": 2,
        "cdate": 1666624352947,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624352947,
        "tmdate": 1666624352947,
        "tddate": null,
        "forum": "dPs6BGO2QT0",
        "replyto": "dPs6BGO2QT0",
        "invitation": "ICLR.cc/2023/Conference/Paper1212/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes techniques for generation dialog representation that is both local and isotropic in order to address the problem of anisotropy in conversation modeling. The paper attempted to apply locality and isotropic constraints at both the token and utterance levels. The locality constraint ensures that each token representations is close to the utterance representation while the isotropic constraint ensures that the utterance representations are pushed away from each other. The proposed representation was then applied to 3 dialog related tasks including multi-turn response generation, response retrieval and dialog semantic role labeling. The paper is well written and well motivated. The paper is also easy to follow.",
            "strength_and_weaknesses": "Pros:\nThe use of locality and isotropic constraints is adequately demonstrated as show in the embedding similarity heat map in Figure 1&4. When applied to response generation, the technique was able to improve the vanilla BART model on both DailyDialog and LCCC datasets. \n\nCon:\nMost of the cons are already mentioned in section 5 under in-depth analysis. \nConversational Coherence - One would expect the isotropy between utterance representations to reduce the conversational coherence. However, the analysis showed that SimDRC is actually more conversationally coherent than the vanilla BART and SimCTG. While this is encouraging, it is not clear how the dialog representation v_{context} is obtained in the article since dialog representation is not explicitly stated in the problem formulation. Can the author(s) explain how this is calculated? Would be great if the coherent score can be evaluated on Ubuntu dialog also.\n\nSensitivity to loss weight and margin value -  The optimal values of these hyper-parameters seem to be dataset dependent which make it unreliable. However, the authors providing ranges of value for each task alleviates this problem to a considerable extent. Nevertheless, it would still be desirable to have more robustness over these hyper-parameter values.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well motivated and well written with appropriate illustrations to ensure clarity. The idea of using locality and isotropic constrained to added conversational anisotropy problem is new to the best of my knowledge. However, the sensitivity to training hyperparameters raises concerns for reproducibility.",
            "summary_of_the_review": "The main idea of the paper seems novel and reasonable. However, there's concern with some of the conclusions drawn by the paper especially the broad applicability to different dialog modeling related tasks. \n\nIt is not clear to me that the isotropic constraint did not impact the conversational coherence despite the results shown in Table 5. Would be great to understand how the dialog representation is calculated and if explicitly adding to the problem formulation to ensure conversational coherence would be more beneficial. \n\nAlso, the sensitivity for training hyper-parameter raises concerns for reproducibility. I wonder if using contrastive learning loss would have been better than using max-margin losses, which would minimize the number of hyperparameters to be searched for.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1212/Reviewer_uSDJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1212/Reviewer_uSDJ"
        ]
    },
    {
        "id": "-Ncyd99_L5",
        "original": null,
        "number": 3,
        "cdate": 1666699083900,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699083900,
        "tmdate": 1669282318148,
        "tddate": null,
        "forum": "dPs6BGO2QT0",
        "replyto": "dPs6BGO2QT0",
        "invitation": "ICLR.cc/2023/Conference/Paper1212/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on the problem that the existing large language models fail to learn the dialogue-specific features. The paper tries to solve the locality and isotropy problem for dialogue generation modes by encouraging the model to aggregate the representation of tokens within an utterance and push away the representation of distinct utterances. ",
            "strength_and_weaknesses": "The motivation of this paper is not convincing. I think the inter-speaker correlation can not be well-modeled by the generation model. It would be better to use the social network to enhance the generation model rather than use the model itself. \n \nIt is not novel to define a distance and add it to the model via a loss function. \n \nBy using the locality loss, and isotropy loss, the token-level similarity will be revised to the pattern shown in Figure 1 (c). But I do not think the inter-speaker correlations and conversational structure information are well-captured by the generation model. There is a gap between the motivation and the proposed model.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written, but the idea is not novel. ",
            "summary_of_the_review": "The motivation of this paper is not convincing. There is a gap between the motivation and the proposed model.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1212/Reviewer_bCi9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1212/Reviewer_bCi9"
        ]
    },
    {
        "id": "F1ZlYGGUDtq",
        "original": null,
        "number": 4,
        "cdate": 1666720874559,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666720874559,
        "tmdate": 1666720874559,
        "tddate": null,
        "forum": "dPs6BGO2QT0",
        "replyto": "dPs6BGO2QT0",
        "invitation": "ICLR.cc/2023/Conference/Paper1212/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper focuses on the problem of how to make it more conversational for the generated dialogue representation from Transformers or large-scale pretrained language models. The authors studied related literatures on the issues that dialogue modeling is not isotropic and conversational, and analyzed the recent representation calibration approaches. Based on these, the authors proposed a new dialogue representation calibration method, namely SimDRC focusing on locality and isotropy to justify the challenges. \n\nThe paper first gave the definitions of locality and isotropy in dialogue modeling, then defined the locality loss, isotropy loss and SimDRC loss to adjust feature space to model the dialogue representation. \n\nThen the paper applied the new approach on the state-of-the-art models for 3 dialogue tasks: multi-turn dialogue response generation, conversational response retrieval, conversational semantic role labeling. The results of the experiments demonstrated that by combining SimDRC and state-of-the-art models in these tasks, better results have been achieved across all the 3 tasks than other related methods. \nThe analysis of conversational coherence, effects of loss weight and margin value, the measurements of locality and isotropy as well as the visualization of token similarity metrics were presented. ",
            "strength_and_weaknesses": "Strength: \n\nThis paper defines a new approach for generated dialogue representation calibration based on locality and isotropy. The experiments showed a consistent performance gain in 3 conversational tasks. \nThe literature study in the paper was complete and the analysis of the experiments was also in-depth. \nThe paper was organized very well with good articulation and the analysis of the technical problems, the methodology and the corresponding results. \n\nWeakness: \n\nSimCTG seems to be the major focus that the paper is comparing SimDRC with. The paper articulated the difference between the two methods in section 2 (token-level vs. multi-granularity levels), and the results of the experiments in section 4, as well as the conversation coherence analysis in section 5.1. It is better if the authors could include the following analysis: \n\na) why is SimDRC a better methods that SimCTG in dialogue related tasks? what is missing in SimCTG that make it is not suitable for conversational tasks? \n\nb) what other tasks will SimDRC be good for other than the 3 tasks included in the paper? Is it good for document generation tasks? ",
            "clarity,_quality,_novelty_and_reproducibility": "As I said in the strength of the paper, it is well written and it articulated very well the new ideas and the new methods, experiments and results, as well as the analysis. It is novel and it will benefit the ICLR audience. \nIt is also a good extension of the state-of-the-arts.  ",
            "summary_of_the_review": "This paper created a new approach for generated dialogue representation calibration based on locality and isotropy, demonstrated a consistent performance gain in 3 conversational tasks. It was organized very well to articulate the new methods, experiments and results. It is a good extension of the state-of-the-arts.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1212/Reviewer_YMbd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1212/Reviewer_YMbd"
        ]
    }
]