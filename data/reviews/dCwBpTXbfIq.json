[
    {
        "id": "woreDJ_q_HK",
        "original": null,
        "number": 1,
        "cdate": 1666386857235,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666386857235,
        "tmdate": 1669993313201,
        "tddate": null,
        "forum": "dCwBpTXbfIq",
        "replyto": "dCwBpTXbfIq",
        "invitation": "ICLR.cc/2023/Conference/Paper1515/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a solution to the problem of causal representation learning, which is about identifying latent variables and how they are causally related by observing only a nonlinear transformation of them. They propose a model in which the latent causal graph is linear gaussian and an auxiliary variable u is observed and has an influence on both the distribution of the gaussian noise and the coefficients of the linear models. They provide conditions under which this model is identifiable up to permutation and rescaling of the latent factors. Motivated by this result, they provide a method based on a simple variational lower bound to estimate the mixing functions. Compelling experimental results validates that the method can indeed identify the latent factors.",
            "strength_and_weaknesses": "Strengths:\n- I follow rather closely this literature and I found the main theoretical contribution (Theorem 4.1) of this work to be novel and quite interesting. I believe other ICLR participants will be interested by this result. \n\nWeaknesses:\n- Many claims are either imprecise or wrong (see below).\n- Some sections (especially Section 3.2) lack clarity (see below).\n- Thm 4.1 should be contrasted with CausalVAE further, which shares many similarities with this work.\n- Many experimental details are missing: For example, what is the number of latent causal variables used in Figure 5a? What is the number of segments available in Figure 5b? How\u2019s the causal graph extracted from the SuaVE\u2019s parameters to produce Figure 5c? This can be a problem for reproducibility.",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty: \n- To the best of my knowledge, the main theoretical result presented in this work is novel and not superseded by another theoretical work. I looked quickly at the proof and it seems sound (although some steps require clarification, see below). It reuses existing proof techniques but also introduces new ones.\n\nImprecise or wrong claims:\n- In introduction: \u201cA brief review is provided in Section 2. For the \nsupervised approach, where causal graphs or/and labels are known, the challenging identifiability \nproblem in latent space has been transferred to an identifiability problem in the observed space, \nfor which some commonly-used functional classes have been proven to be identifiable (Zhang & \nHyvarinen, 2012; Peters et al., 2014)\u201d. I don\u2019t understand how knowing the causal graph reduces the problem to causal discovery on observed variables\u2026 If the graph is known, there is no graph to learn, no?\n- Section 3.1: \u201crecent progress in nonlinear ICA (Hyvarinen et al., 2019; Khemakhem et al., 2020), which has shown that the independent latent noise variables n_i are identifiable under relatively mild assumptions, if they are modulated by an additionally observed variable u.\u201d Please clarify that n_i are *conditionally* independent *given u*.\n- Section 3.1: It is said that the n_i\u2019s are identifiable, by recent nonlinear ICA advances. However, I think this is true only if the mapping from the n_i\u2019s to the z_i (later referred to as g) is bijective. This should be made clear.\n- Section 3 should be called \u201cThree indeterminacies in latent causal models\u201d, or something like this. The current title is confusing.\n-Corollary 4.2: The introduction as well as the paragraph just before the corollary points toward the fact that the causal graph is identifiable, but the Corollary in itself guarantees only the identifiability of the MEC. The text should be adjusted to better reflect what is actually shown in the paper.\n\nClarity:\n- Section 3.2: \u201cAccording to the graph structure in the right column of Figure 1, assume \nthat z1 := n1, z2 := z1 + n2 and x := f(z1, z2) + \u03b5 (case 1).\u201d But Figure 1 has only one column\u2026 Was it meant to point at figure 2? But the graphs of figure 2 do not correspond to what is described in words\u2026 \n- Section 3.2: This section is really hard to follow. I believe the authors want to argue that two different causal models can yield the same distribution over observations (which is clearly true), but they provide virtually no explanation.\n- Thm 4.1: Do we assume that k, the dimensionality of the minimal sufficient statistic is known?\n- Thm 4.1: The statement \u201cwe can recover the latent causal variables z up to\u2026\u201d is too vague to be part of a theorem. What does it mean \u201cto recover the latent causal variables\u201d?  \n- Section 6.1: The authors refer to the Appendix for more details on how the data was generated, but the Appendix does not contain these details. I was trying to confirm that the ground-truth causal graph is truly complete right?\n- The experimental section shows the SHD of the graph learned by SuaVE, but it is unclear how one goes from the parameter of SuaVE to an actual estimated graph. Is it by thresholding the learned parameter \\lambda(u) to some value? How was this threshold selected? Did the authors consider adding regularisation to the learned graph? Without it, I suspect the graph would always be complete, no?\n\nProof of Thm 4.1: \n- I am not sure I understand how the authors obtain equation (42). I quickly tried to show it myself using an argument similar to what is done for the other terms, but it appears to be more subtle. Given how crucial this step is to showing identifiability up to permutation, I believe this step should be made completely transparent.\n\nSuggestions for experiments:\n- It would be interesting to see what happens when the ground-truth graph is empty, which would correspond to a setting covered by iVAE\u2019s theory, and thus it should perform well. Does SuaVE perform similarly to iVAE?\n- CausalVAE performs very well (although not as good as SuaVE) on Figure 5a. The gap seems much more important for Figure 5b. Could the authors comment on that? How many latent causal variables are used in Figure5a?\n\nMinor:\n- In introduction: \u201c...disentangled representation learning can be viewed as a special case of causal representation learning where the latent variables have no causal influences\u201d. Many authors use the term disentanglement more broadly, without requiring the latent factors to be independent. Actually, Bengio et al (2013) made that point (see section 3.1 under \u201cSimplicity of Factor Dependencies\u201d).\n- Related work: I disagree with the authors\u2019 classification of Kuegelgen et al (2021) and Brehmer et al (2022) as \u201cSupervised Causal Representation Learning\u201d. The authors consider the knowledge of the graph as a supervision, which is true to some extent, but this kind of supervision is typically referred to as \u201cweak\u201d. Also, Brehmer et al (2022) does not assume the ground-truth causal graph is known.",
            "summary_of_the_review": "Although the identifiability result introduced in this work appears to be novel and very interesting, I cannot recommend acceptance due to (i) Imprecision/wrongness of certain claims, (ii) lack of clarity and (iii) the lack of details regarding some experiments. I strongly encourage the authors to improve the writing of this work, since I believe the main theoretical result would be of interest to the community.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_FaeD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_FaeD"
        ]
    },
    {
        "id": "MPzDnV_TZMx",
        "original": null,
        "number": 2,
        "cdate": 1666611538136,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611538136,
        "tmdate": 1666611538136,
        "tddate": null,
        "forum": "dCwBpTXbfIq",
        "replyto": "dCwBpTXbfIq",
        "invitation": "ICLR.cc/2023/Conference/Paper1515/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Observed signals are often generated from latent variables with significant structure that can provide significant understanding into the dynamics of the phenomenon under investigation. This structure is often unidentifiable without strong prior assumptions. The authors study a particular model, in which noise is Gaussian and associations between latent variables are linear, and show that some aspects of the latent structure are identifiable up to some transformations.",
            "strength_and_weaknesses": "**Strengths**\n- The paper is clearly written. The authors give a lot of details into why latent structure is not identifiable which serves to motivate the proposed approach. Their model is general enough that it can be a plausible explanation for latent structure in practice.\n\n**Questions / comments**\n- It is not clear what the authors are identifying in Corollary 4.2. When marginalizing over latent variables, the structure is a mixed graph and not a DAG, and thus the equivalence class is a PAG. Moreover with the assumed model in Sec. 4, all variables Z are dependent so the distribution of the data really doesn't narrow the class of compatible models.\n- The graph in Fig. 4 is non-standard, if z_i is a function of u, then u should have an arrow into z_i. Is there any difference between Eq. (2) and $z_i := g(\\mathbf u,\\mathbf z) + n_i$?\n- Thm. 4.1 depends on a lot of unverifiable, and one may argue, unrealistic assumptions about the latent structure. I understand that these may be necessary from a theoretical standpoint but also makes this method inapplicable in practice. At the very least one needs sensitivity analysis to understand whether this method can be used in practice. The current experiments are probably insufficient to understand the proposed approach.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and I particularly enjoyed reading the background on identifiability issues in latent models. I don't have the expertise to truly understand how novel the identifiability result is in relation to existing literature, but on a causal and practical side my view is that this method is unlikely to be useful.",
            "summary_of_the_review": "Well written paper with potentially some interesting results on latent model identifiability. The assumed structure however is quite restrictive and it is not clear what the effect of these assumptions are for performance in practice. Since this method is presented as a contribution to the causal literature I don't think it is likely to have a strong impact, simply because it relies on too many unverifiable assumptions not only on the causal structure of the underlying system but also on the functional dependencies and distributions of all variables.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_P5md"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_P5md"
        ]
    },
    {
        "id": "v7VWLSPYvgO",
        "original": null,
        "number": 3,
        "cdate": 1666633978640,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633978640,
        "tmdate": 1666633978640,
        "tddate": null,
        "forum": "dCwBpTXbfIq",
        "replyto": "dCwBpTXbfIq",
        "invitation": "ICLR.cc/2023/Conference/Paper1515/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a theory for identifying latent causal structure using observational data when the weight-variant linear gaussian model assumption holds.",
            "strength_and_weaknesses": "## Strength\n1. This paper is very well and claerly written.\n2. A detailed and interesting motivation in Section 3 helps a lot in understanding the problem.\n3. This paper contains extensive simulation results.\n\n\n## Weakness\n1. More background information for understanding the paper (e.g., what\u2019s nonlinear ICA?) could help.\n2. More detailed example in the problem of causal representation learning (e.g., why do we want to learn the relations between variables Zi?) is needed.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I think this paper is very clearly written, and the quality is good. Also, it contains novel theories on the problem. Some questions that helps improving the paper are the following: \n\n**Questions.** \n1. In what practical scenarios are we given an observed variable U? \n2. I don\u2019t understand what the red edges are in Figure 4. In the standard structural causal models, there are no edges from variables to edges. \n3. Is the assumption 2 in Theorem 4.1. can be held in practice? If X=x is a binary or discrete variable and the function f is bijective, then the latent variable Z must also be binary or discrete. Therefore, in which scenarios can we think that assumption 2 holds?",
            "summary_of_the_review": "Overall, I think this paper is very clearly written, and the quality is good. Also, it contains novel theories on the problem. I think the paper can be improved by if more preliminaries and detailed example of causal representation learning are provided.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "This paper doesn't contain ethical concerns.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_2uCg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_2uCg"
        ]
    },
    {
        "id": "HzZ9PE0oI8",
        "original": null,
        "number": 4,
        "cdate": 1666654681076,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654681076,
        "tmdate": 1666654681076,
        "tddate": null,
        "forum": "dCwBpTXbfIq",
        "replyto": "dCwBpTXbfIq",
        "invitation": "ICLR.cc/2023/Conference/Paper1515/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors extend the nonlinear ICA results on identifiable latent factors to when the latent factors have a causal structure. ",
            "strength_and_weaknesses": "There are certain inconsistencies in the assumptions that need to be addressed by the authors. \n\nThe assumptions might be a bit too strong/unrealistic. This is evidenced by the fact that the equivalence class reduces to a single graph given the specified assumptions. If the authors showed that these assumptions are also necessary, I think that would be an interesting contribution. But I don't see such an argument. ",
            "clarity,_quality,_novelty_and_reproducibility": "The exposition is very clear. Novelty is not too high given the existing work in nonlinear ICA. This is not a problem though. ",
            "summary_of_the_review": "Thank you for your submission. I have some major concerns below. I would be happy to update my score based on the authors' clarifications/ addressing of these issues. \n\n1. The discussion in the first paragraph of Section 3.1 is inconsistent with the causal graph given in Figure 1: It is extremely unlikely that n_i's are independent if they are all caused by the same variable u. This assumption that n_i's are independent seems to be necessary to make the nonlinear ICA framework applicable here so this issue should be addressed by the authors. \n\n2. The result inherits the proof of Khemakhem et al. Therefore it inherits some of the the strict assumptions of their theorem. For example, the proof, when one looks closely, assumes that for any two causal models, the noise distributions have to be identical. This is quite a strong assumption and needs to be mentioned explicitly. I recommend the authors explicitly list the set of assumptions including this one in their manuscript. \n\n3. The assumption that U is a variable that is observed, and it can be used to \"intervene\" on every edge, i.e., every configuration of latent variables and also several times as required by the theorem might be a strong assumption. It might be better to dissect this assumption a bit more to justify when it can be achieved in practice.\n\n\nMinor Comments and Suggestions: \n\nThe main contribution is in Section 4. Although Section 3 gives a nice exposition with examples, the observations here are not surprising that the mapping can absorb such indeterminacies. \n\n\"causal supergraph\"\nI don't believe there is a need for a new name here. This fully connected DAG is known as a tournament in graph theory. Also known as a total order.\n\n\"To address this issue, we allow causal influences among latent\ncausal variables to be modulated by an additionally observed variable\"\nthe word \"modulated\" is not well-defined here.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_DXAx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_DXAx"
        ]
    },
    {
        "id": "a_WrdFg5rV",
        "original": null,
        "number": 5,
        "cdate": 1666661136941,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661136941,
        "tmdate": 1670010129797,
        "tddate": null,
        "forum": "dCwBpTXbfIq",
        "replyto": "dCwBpTXbfIq",
        "invitation": "ICLR.cc/2023/Conference/Paper1515/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed SuaVE, a new identifiable model for causal representations learning. SuaVE is an extension of i-VAE model in the sense that: 1, an additional latent causal variables layer is generated conditioned on the latent noise variables;  2, the latent causal variables are modelled via linear Gaussian models with $u$-dependent causal weights. Under certain conditions, the authors shows that 1), the latent causal variables can be identified up to permutation and linear scaling; 2), the causal graph among latent causal variables can be identified up to its Markov equivalence class. The authors then proposed practical algorithms for identifying the latent causal variables and conducted both synthetic and real-world experiments. ",
            "strength_and_weaknesses": "Utilizing recent advances in nonlinear ICA identifiability to propose new causal representation learning algorithms is an interesting point of view. This paper is very well-written and I expect the results can make novel contribution to the field. I also like the way that the authors explain the non-identifiability of latent causal representations, motivates how to tackle them, and avoid the constraint-based optimization of no-tears by pre-defined causal orders. \n\nCompared to other reviews, I might be less worried about the empirical evaluations.  Based on my own understanding the technical details, all the proofs seems to hold except Corollary 4.2. \n\n- The main issue is that I am not convinced how Corollary 4.2 can justify the identification algorithm proposed by the paper. To be more specific, Corollary 4.2 claims that the linear scaling of latent variables does not affect the identifiability of the latent causal structure. This only true if the identification is based on e.g. conditional independence relationships. However, this is not what the learning algorithm proposes. As I understand, SuaVE implicitly models the adjacency matrix via continuous causal weights + thresholding pruning, which I guess is inspired by the parameterization used in No-tears and many other causal discovery papers. This is fine when the dataset is fully observed. However, in the case of SuaVE, $z$ can be only recovered up to linear scaling. Therefore, the scaling of $\\lambda(u)$ *will* get affected by linear scaling of $z$. This is especially the case since the authors also uses a non-zero thresholding (0.1) to round up the weights; which essentially means that this threshold also needs to be scaled proportionally to the linear scaling of $z$, which is very tricky and not considered by the paper. Also, the $u$-dependent parameterization for causal weights $\\lambda(u)$ introduces additional issues since the existence of the corresponding edges might depend on the values of $u$ as well, which is not dealt with in Huang et al. 2020. I guess the above issue might be avoided by an alternative parameterization, in which you could decouple the parameterization of causal graphs and the edge functions (as in SCMs), where the causal graph can be modeled as via binary variables, that can be used to 0-1 mask the functions of its parent nodes.\n\n- The other concern is I am not sure how to justify the modelling choice of SuaVE (Figure 4) and how reasonable this assumption is in the real-world causal representation learning applications. To me it seems a bit unintuitive to treat the latent variables in i-VAEs as non-exogenous noises here ($n$); also the fact that the latent causal graph might vary across data points seems like a non-necessary complication. Could the authors provide some examples to justify those assumptions?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality: the paper would benefit more from adding motivations for the model structures, not just for the sake of identifiability but also its usefulness in practical scenarios.\n\nNovelty: see above.",
            "summary_of_the_review": "This is an interesting paper that addresses an important question in causal representation learning; and seems to possess enough novelty to the best of my knowledge. However, due to my certain technical concerns, I can only recommend a weak accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_kexo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_kexo"
        ]
    },
    {
        "id": "hLGepgSI6j",
        "original": null,
        "number": 6,
        "cdate": 1666780091038,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666780091038,
        "tmdate": 1670279788870,
        "tddate": null,
        "forum": "dCwBpTXbfIq",
        "replyto": "dCwBpTXbfIq",
        "invitation": "ICLR.cc/2023/Conference/Paper1515/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In causal discovery, one starts with input variables and then learns a causal graph describing the relationships between the input variables. In recent years, a complementary question, which asks \"how to learn causal variables from low-level raw data (e.g., images)?\" has gained attention and this task is often termed \"causal representation learning\". Many recent works in the area either leverage simplistic assumptions on the underlying causal DAG (e.g., independence of latents conditional on observed auxiliary variable in iVAE). Other works leverage temporal data to deal with more general causal DAGs (e.g., CITRIS). In this work, the authors tackle the question without making assumptions on the structure of the underlying causal DAG. To tackle the question, the authors assume a linear structural equation model that governs interactions between the latents and also require to observe an auxiliary variable that impacts the weights of the weights of the structural equation model. Under the above assumptions, the authors extend the theory from iVAE to achieve identification in the proposed setting. Having shown identification of the latents the authors show that the causal DAG of the latents can be identified up to the Markov equivalence class. Lastly, authors propose an extension of the iVAE based procedure by adapting the prior based on the linear SEM that is assumed. The authors show that the procedure is effective in synthetic and semi-synthetic experiments.",
            "strength_and_weaknesses": "**Strengths** \nThe authors study a very important problem and propose an interesting approach that extends works such as causal VAE. A main limitation of causal VAE is that the each latent is independent of the rest conditional on one component of the auxiliary information vector. To address this limitation, authors propose a new data generation model, the weight variant linear causal model. The weight variations in the linear causal model allow the authors to achieve identification. These weight variations are connected to interventions, which makes the setup interesting. \n\n**Weaknesses** \n\nI have several concerns that I highlight below. \n\n1. **Regarding the weight variant causal model** The new data generation process considered by the authors seems quite artificial to be honest. Having access to an auxiliary variable that impacts all weights in a linear SEM seems too much of a stretch. Do authors have some good examples that show otherwise? If yes, then authors should provide a convincing case of such examples during the rebuttal. My understanding tells me that data generation process seems to have been reverse engineered to fit the requirements of the proof. In other words, DGP has been engineered to allow for structural dependency but at the same time leverage as much of iVAE as possible.\n\n2. **Regarding Corollary 4.2** The paper leverages the work of Huang et al. to achieve identification of causal DAG up to the Markov equivalence class. Since this is quite important to the paper the authors should have fully explained if the assumptions that are required in Huang et al. are fully met. Also, the authors should have described the exact steps of the procedure to go from Markov equivalence of z and u to z only. If this was excluded due to space constraints, I am a little surprised as this seems quite important. I also do not see a discussion in depth on this in the Appendix. \nRegarding the proof of Corollary 4.2. The proof of the corollary is incomplete. The authors talk about recovery of MEC of z union u. That can be done from good old works of Pearl and Verma that date back to 1990. How to go from there to the MEC of z? \n\n\n3. **Regarding experiments** In the set of experiments that are carried out on FMRI data. I do not see a description of what is the auxiliary information. In the absence of auxiliary information how does the framework of authors even kick in. Further, the authors seem to learn the DAG just by thresholding the weights learned. I was expecting to see a set of DAGs that are Markov equivalent and some of them would correspond to the true one. \nAlso, to validate Corollary 4.2 the authors should have carried out synthetic data based experiments to show that indeed the MEC is recovered. \n\n4. **Role of weight variation model** Since weight variation is the key part of the paper. Some ablation experiments that explain how different choices for weight variation model help in different types of identification would have been useful. In the current form the model carries out global interventions and that is not realistic. What would happen if the interventions were localized, i.e., some components of u only interact with some edges, do we get partial identification at least?  \n\n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. However, some important things related to the method for learning the MEC are missing as I highlight in the weaknesses above. The work needs significant revision but it is promising. I do not see the code to reproduce the experiments. ",
            "summary_of_the_review": "The work proposes a new DGP to tackle the question of causal representation learning. The DGP proposed seems to be quite contrived and that is one major limitation. The proof of the theorems are correct, but the proof of the corollary is incomplete or has details missing. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_4oGs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1515/Reviewer_4oGs"
        ]
    }
]