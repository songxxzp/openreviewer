[
    {
        "id": "NZgVHfcUF1e",
        "original": null,
        "number": 1,
        "cdate": 1666535249874,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666535249874,
        "tmdate": 1666535249874,
        "tddate": null,
        "forum": "LOTGOB5_Xh2",
        "replyto": "LOTGOB5_Xh2",
        "invitation": "ICLR.cc/2023/Conference/Paper2521/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper analyzes the essence of MIM and introduces a universal MIM method that can be applied to both CNNs and Transformers. There are several designs that together build the introduced method, including the mean RGB replacement, the intermediate mask, the frequency reconstruction targets and the HOG targets. Experimental results on both ResNet-50 and ViTs are shown.",
            "strength_and_weaknesses": "### Strengths\n1. The paper is well-organized and easy to follow.\n2. The figures, tables, and visualizations indeed help to understand the author's point of view.\n3. It is new to apply MIM on convolution neural networks.\n\n### Weaknesses\n1. I have the most concern about the motivation for this work. As we know, ViTs lack the capture of inductive bias and MIM solves this issue to some extent by enforcing visual context reasoning. However, CNNs are different, they are good at capturing inductive bias. Furthermore, as observed in the authors' results (Tab. 1), a lot of effort has been done to do MIM pre-training, however, the final results are almost the same as supervised counterparts. So is it a false proposition to apply MIM to CNNs?\n2. I have another concern about the authors' declaration that MIM enables the network with a better feature extraction ability. As a common observation in previous MIM works, the linearly probing results of MIM-pretrained models are unsatisfactory, worse than the contrastive methods. MIM is known to provide transferable model parameters rather than extracting out-of-the-box features.\n3. In Sec. 3.1, the authors analyze different augmentation methods, however, the setup is not fair. MIM is employed for pretraining while the others (e.g., CutMix) are for fine-tuning. \n4. There are many components in the introduced method, including the mean RGB replacement, the intermediate mask, the frequency reconstruction targets and the HOG targets. How much does each component affect?",
            "clarity,_quality,_novelty_and_reproducibility": "+ **Clarity**: The paper is well written, however, I do not agree with several claims (see Weaknesses).\n+ **Quality**: Comprehensive experiments are conducted, however, the premises should be verified, i.e., do we need MIM on CNNs?\n+ **Novelty**: Applying MIM to CNNs is new.\n+ **Reproducibility**: Implementation details are provided. ",
            "summary_of_the_review": "I tend to weakly reject this paper in the initial comments due to the unfounded premises. I am looking forward to the authors' opinions in the rebuttal period on why we need MIM for CNNs.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2521/Reviewer_nEcw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2521/Reviewer_nEcw"
        ]
    },
    {
        "id": "VOpndu5oY8",
        "original": null,
        "number": 2,
        "cdate": 1666601395929,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601395929,
        "tmdate": 1666601395929,
        "tddate": null,
        "forum": "LOTGOB5_Xh2",
        "replyto": "LOTGOB5_Xh2",
        "invitation": "ICLR.cc/2023/Conference/Paper2521/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper inspects MIM pre-training approaches and finds MIM essentially helps the model to learn better middle-order interactions between patches. Motivated by that, this paper proposes a novel MIM-based method, dubbed A$^2$MIM, that works well for both (small-sized) ConvNets & ViTs. Extensive experiments are conducted to verify the effectiveness of the proposed approach.",
            "strength_and_weaknesses": "### Strength \n1. This paper provides a thoroughgoing study of MIM, and highlights the essence of MIM pre-training is to help models learn middle-order interactions between patches.\n2. Based on the study of middle-order interactions of MIM, this paper proposes a novel approach that works well for both ConvNets as well as ViTs. What's more, I appreciate the management of technical details in this paper, such as ``Filling Masked Tokens with RGB Mean'' is technically sound. \n3. Extensive experiments on several downstream visual recognition tasks are conducted to verify the effectiveness of the proposed approach.\n   \n### Weaknesses\n1. The 2nd paragraph's name of Sec. 2 Related Work is inaccurate.``Autoregressive Modeling'' means that the output depends linearly on its own *previous* values (e.g, GPTs), while masked modeling is *bidirectional* modeling.\n2. I don't see a strong connection between Sec. 3.1 and the proposed approach.\n3. The scalability of A$^2$MIM is unknown, which is a crucial property for pre-training.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality, clarity and originality are sound.\n",
            "summary_of_the_review": "Overall, I think this is a solid and well-written paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2521/Reviewer_yVqw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2521/Reviewer_yVqw"
        ]
    },
    {
        "id": "wPGr2xr6AH",
        "original": null,
        "number": 3,
        "cdate": 1666604939867,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604939867,
        "tmdate": 1666605800573,
        "tddate": null,
        "forum": "LOTGOB5_Xh2",
        "replyto": "LOTGOB5_Xh2",
        "invitation": "ICLR.cc/2023/Conference/Paper2521/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studied the problem that MIM is compatible with the Transformer family but is incompatible with CNNs. To this end, it proposed an Architecture-Agnostic Masked Image Modeling framework (A2MIM) that is compatible with both Transformers and CNNs in a unified way. Specifically, this paper used RGB Mean as masked tokens and added mask tokens on the intermediate feature maps. A frequency domain reconstruction loss is also applied to improve the ability of learned models. Experiments are conducted on ImageNet-1K, downstream COCO detection and segmentation, and ADE20K dataset with ViT models and ResNet-50.",
            "strength_and_weaknesses": "Strengths:\n\n    - MIM is an interesting problem/framework to study, and understanding how MIM-based methods perform well in the vision domain is important for this field.\n\n    - The proposed method is clear to understand and is easy to follow by other researchers.\n\n    - The experiments are extensive on ImageNet-1K, COCO, and ADE20K datasets.\n\nWeaknesses:\n\n   - Some statements are incorrect or even a little bit over-claimed, such as:\n\n1)\tIn the abstract, the authors stated: \u201cMIM primarily works for the Transformer family but is incompatible with CNNs.\u201d And in the introduction section, this paper stated \"To the best of our knowledge, we are the first to carry out MIM on CNNs that outperforms contrastive learning counterparts.\" This is not true as ConvNext [1] is adequate to handle MIM learning strategy. And this paper with CNN is actually similar to it when using patchified images with CNN.\n\n[1] Liu, Zhuang, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. \"A convnet for the 2020s.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11976-11986. 2022.\n\n2)\t\u201cBased on this fact, we propose an Architecture-Agnostic Masked Image Modeling framework (A2MIM), which is compatible with both Transformers and CNNs in a unified way.\u201d The proposed framework is basically similar to the regular MIM approach with trivial or not significant modifications (use RGB Mean as the Masked Tokens and add mask token on the intermediate feature map). Thus, the called \"a unified way\" seems incorrect. The used Fourier/Frequency domain is also not related to the core framework in this paper and can straightforwardly be applied in the vanilla MIM models.\n\n3)\tThe authors stated that the proposed framework has \u201c(a) No complex or non-generic designs are adopted to ensure compatibility with all network architectures. (b) Better middle order interactions between patches for more generalized feature extraction.\u201d I think this is because the proposed framework is too close to the original MIM architecture.\n\n4)\tThe authors claimed they \u201cdelved deep into MIM and answered the question of what exactly is learned during MIM pre-training.\u201d This statement is very strong, however, after reading this paper carefully, I still did not get the insights or intuition about what MIM pre-training learns from the input data from the paper\u2019s descriptions. Proper explorations and explanations are necessary to support this claim.\n\n - The performance in this paper is not competitive. As shown in Tables 1, 2, 4, etc., the improvement is fairly marginal (0%~0.2%). Considering that this paper used extra Fourier/Frequency domain reconstruction supervision/loss, I\u2019m not sure whether the proposed strategy is truly effective or not.\n\n - The writing and organization of this paper can be improved. The used Fourier/Frequency domain reconstruction loss seems not related to the key approach of the architecture-agnostic masked image modeling framework, since it can also be applied to the regular MIM frameworks. Moreover, the insights of using this additional supervision are not clearly expressed. This part seems fragmented from others in the method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly presented, however, the originality of this work is slightly insufficient.",
            "summary_of_the_review": "This paper conducted sufficient experiments. However, the method itself is not novel with marginal improvement, the novelty and originality of the method are basically not very strong. Also, many statements in this paper are slightly over-claimed (details please refer to \"Strength And Weaknesses\"). Thus, I tend to reject.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2521/Reviewer_dEJJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2521/Reviewer_dEJJ"
        ]
    },
    {
        "id": "r63j3gCaCLQ",
        "original": null,
        "number": 4,
        "cdate": 1666727426251,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727426251,
        "tmdate": 1666763169626,
        "tddate": null,
        "forum": "LOTGOB5_Xh2",
        "replyto": "LOTGOB5_Xh2",
        "invitation": "ICLR.cc/2023/Conference/Paper2521/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a variant of BERT-like pretraining method for computer vision (the so-called Masked Image Modeling). The method can generalize well to both vision transformers and CNNs, thus is described as architecture agnostic.\n\nThe main contributions of this work are:\n- using the mean color values to fill the corrupted pixels, which makes the self-supervised pipeline suitable for different neural network architectures;\n- giving a nice insight that the middle-order interactions are important for visual representation, and making two improvements to standard BERT algorithm to facilitate their learning (i. masking intermediate features and ii. introducing supervision in the frequency domain);\n- and providing some empirical evidence that supports its validity.\n\n&nbsp;",
            "strength_and_weaknesses": "**[strengths]**\n- [S1] This work presents a *fresh* perspective to rethink the self-supervised learning in computer vision, say, the *middle-order* interaction. The \"interaction strength\" mentioned in the paper is a useful indicator to show how good the model is at modeling long-range dependency, which is actually considered to be a valuable aspect in advancing pre-training for NLP [1,2].\n- [S2] The authors have put considerable effort into how to fairly compare the established algorithms, which is appreciated and allows the hypotheses presented in their paper to be fully tested. The ablation experiments also demonstrate the validity of different components in their method.\n\n\n&nbsp;\n\n\n**[weakness]**\n\nThere are some parts of the article that are not very easy for the reader to follow, which could be the main weakness.\nI would suggest authors to explain more about the following aspects (which I find enlightening) and eventually add them to their manuscript or appendix:\n  - [W1] In the Introduction, the authors claim that \"it is not straightforward to directly apply mask token to CNNs\". Can we replace the masked part (e.g., 16x16x3=768 pixels) with a \"mask token\" (e.g., 768 learnable scalars) like BERT does? This is a straightforward way to mask. Or would it be more appropriate to describe this way as \"underperforming\" than \"not straightforward\"? (considering the mediocre performance of MAE, SimMIM, etc. in Tab. 1 of the paper)\n  - [W2] \"MAE takes the reorganized the unmasked input patches of 112x112 as the input\", can the authors explain more on this? I wonder if a substitution like the above (768 pixels to a 768-dimensional learnable vector) or some other operation has been performed.\n  - [W3] According to the analysis in Sec. 3.2, middle-order interactions seem to manifest a medium- or long-range inter-patch dependency. Why the authors say \"middle-order interactions could be enhanced via guiding the network to learn features of certain frequencies\"? Does \"certain frequencies\" refer to medium or high frequencies? Why does increasing the richness of these frequencies on feature maps can promote middle-order interactions?\n\n[W4] In addition, there appears to be some related work that has not been adequately discussed. See the \"Clarity, Quality, Novelty And Reproducibility\".\n\n&nbsp;\n\n**[open questions]**\n\n[O1] The methodological improvements to BERT-like pre-training in this paper seems to be incremental. \nConsidering that the authors propose some quantitative metrics to describe middle-order operations, is it possible to design a more principled algorithm to explicitly facilitate such middle-order interactions? I believe the insights on middle-order are valuable, but the solutions proposed in the article do not seem to fully exploit their values.\n\n&nbsp;\n\n------------------\n[1] Jawahar, Ganesh, Beno\u00eet Sagot, and Djam\u00e9 Seddah. \"What Does BERT Learn about the Structure of Language?.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.\n\n[2] Xu, Jiacheng, et al. \"Discourse-Aware Neural Extractive Text Summarization.\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.\n\n&nbsp;\n",
            "clarity,_quality,_novelty_and_reproducibility": "**[missing discussions that may weaken the novelty]**\n\nIn the Related Work section, the authors refer to the work CIM [3].\nThis is an Electra-like [4] method that fills the corrupted pixels via a small network instead of mean color values, and thus [4] claimed they are the first to demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework.\nAuthors are encouraged to discuss A$^2$MIM and CIM in more detail, and to update some of the corresponding descriptions.\nAlthough this would weaken the novelty of A$^2$MIM, the related works deserve to be discussed fairly and pertinently.\n\n\n**[Reproducibility]**\n\nI believe one can easily reproduce the main results in this work given the detailed experimental configurations and source codes.\n\n\n------------------\n[3] Fang, Yuxin, et al. \"Corrupted image modeling for self-supervised visual pre-training.\" arXiv preprint arXiv:2202.03382 (2022).\n\n[4] Clark, Kevin, et al. \"Electra: Pre-training text encoders as discriminators rather than generators.\" arXiv preprint arXiv:2003.10555 (2020).",
            "summary_of_the_review": "Overall, I find this work provides some interesting insights, but the writing and presentation of the paper leaves much to be desired.\nIn addition, the methodological improvements to BERT pre-training in this paper seems to be trivial and piecemeal.\nI hope to understand this article better in further communication with the authors and that may influence my judgment.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2521/Reviewer_RicF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2521/Reviewer_RicF"
        ]
    }
]