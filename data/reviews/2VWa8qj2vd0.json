[
    {
        "id": "KSqKRnPem",
        "original": null,
        "number": 1,
        "cdate": 1666234165060,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666234165060,
        "tmdate": 1666234261115,
        "tddate": null,
        "forum": "2VWa8qj2vd0",
        "replyto": "2VWa8qj2vd0",
        "invitation": "ICLR.cc/2023/Conference/Paper1595/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors propose to alleviate computation cost of ViT for video modeling. To achieve this goal, they use linear attention with a feature fixation module. Specifically, they introduce two types of fixation by gating or attention on feature channels. Moreover, they use spatial and temporal shift for feature association.",
            "strength_and_weaknesses": "1 Strength\n\n1) The computation problem of ViT is a key issue for video modeling.\n\n2) The paper is well-written.\n\n2 Weakness\n\n1) Novelty. The novelty is limited. \n\nFirst, adding such fixation (or gating) mechanism has been widely used in CNN, e.g., SE or CBAM. I understand the detailed difference between this paper and other gating method. But the key idea or design is similar. \n\nSecond, temporal shift operation has been proposed in TSM.\n\n2) Results. The results are not quite convincing. \n\nFirst, for SSV2, there are many better solutions which are ignored in this paper, e.g., (MViT-B, Top1:67.1, Top5:90.8, GFLOPs: 170\u00d73\u00d71), (UniFormer-B, Top1:70.4, Top5: 92, GFLOPs: 290\u00d73\u00d71), etc. Linear attention based ViTs are not the popular or well-known approaches in video modeling. I suggest to compare with the well known efficient models in video recognition literature. For Kinetics, please report the GFLOPs to evaluate computation cost.\n\nSecond, it is not convincing if the feature fixation is the most important design in this work. From table 3, one can easily see that, temporal shift is the key operation. However, this shift operation has been proposed in TSM. Hence, the contribution in this work may not be convincing.\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "1 Clarity\n\nThe paper is well-written. \n\n2 Quality\n\nThe results are not quite convincing. It would be better to compare with popular effcient video models in the linterature. \n\n3 Novelty\n\nThe novelty is limited. The fixation mechanism is simply like feature reweighting. Table 3 does not support that such mechanism is the most important design.\n\n4 Reproducibility\n\nIt seems to be OK for simple re-implementation.\n",
            "summary_of_the_review": "Overall, this paper proposes to reduce computation burden for video modeling. The problem is important. But the fixation design is limited. The experiments are not quite convincing too.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1595/Reviewer_uvWv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1595/Reviewer_uvWv"
        ]
    },
    {
        "id": "LBKsrjwvtH1",
        "original": null,
        "number": 2,
        "cdate": 1666576790368,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576790368,
        "tmdate": 1666576790368,
        "tddate": null,
        "forum": "2VWa8qj2vd0",
        "replyto": "2VWa8qj2vd0",
        "invitation": "ICLR.cc/2023/Conference/Paper1595/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors study how to close the performance gap between a linear transformer and a standard (softmax) transformer. The motivation is that linear transformer can be computationally beneficial when the sequence length is high. The main proposed method is called \"feature fixation\", which re-weights the Query and Key in channel dimension in attention. The authors perform a few ablation studies and compare the proposed method with a number of prior works.\n",
            "strength_and_weaknesses": "Strengths:\n+ Reducing the computational cost for video models can potentially be highly impactful due to the very high computational cost of most video models.\n+ Bringing expressiveness to linear transformer to make it work better makes sense.\n\nWeaknesses:\n- The improvement brought by feature fixation is small (1.2%). I'm not sure if the gain worths the complexity and additional computations added to the model.\n- Table 3 shows that the component that brings most of the improvement is \"temporal shift\", which is proposed in prior work (e.g. [45, 51]), instead of the proposed feature fixation. Thus, even if the overall method outperforms some prior works, it's not convincing that it's due to the proposed method.\n- Models compared in Table 1 use different pre-training datasets. In particular, the proposed method uses a strong IN21K pre-training, which could put prior works that do not at a disadvantage. Thus overall, without discussing on the effect of pre-training datasets, it's hard to draw conclusions from the table.\n- Some state-of-the-art results are missing. For example, Swin-B achieves 69.6 and MViTv2 achieves 73.3 accuracy on SSv2, which are both significantly higher than the 65.5 presented, but are not compared with.\n- minor points on writing: I'm not a big fan of using a new term to describe something that's already used. For example, prior works have widely used \"excitation\" ([31]) and \"gating\" ([55]) for the \"fixation\" operators. I find using new terms make reading more difficult.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity could be improved. In particular, I find the use of some terms, such as \"cooperative feature fixation\" for feature reweighting or \"neighborhood association\" for shifting, confusing and making reading unnecessarily harder.\n\nQuality of the experiments can be improved. I think a more detailed discussion on pretraining and adding missing prior works for Table 1 and showing flops & params in Table 2 would help.\n\nI think the novelty is ok. ",
            "summary_of_the_review": "Overall, my major concern is that the experiments do not support the usefulness of the method. The small improvement brought by feature fixation is unfortunately a bit disappointing. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1595/Reviewer_xSk5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1595/Reviewer_xSk5"
        ]
    },
    {
        "id": "OKzJlI-k3Jh",
        "original": null,
        "number": 3,
        "cdate": 1666620985512,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620985512,
        "tmdate": 1666621166777,
        "tddate": null,
        "forum": "2VWa8qj2vd0",
        "replyto": "2VWa8qj2vd0",
        "invitation": "ICLR.cc/2023/Conference/Paper1595/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for more efficient video transformer networks based on linear attention. The main contribution is using a context-gating like mechanism to re-weight the features before attention. The approach is evaluated on kinetics and something-something datasets.",
            "strength_and_weaknesses": "The paper is well written, and the experiments are well done, showing the benefit of the approach. The approach is largely a combination of existing components, however, this combination works. The experiments and ablations are thorough and show the benefit of all the piece of the approach.\n\nHowever, some of the results are concerning. For example, T3 seems to suggest that the shift is beneficial, rather than the reweighting. There's also some differences in the setup of model training in T1 (e.g. pretraining datasets, learning rates settings, etc), which makes it harder to tell if the benefit of this approach is the pretraining or the model/method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and should be reproducible from the provided details. The approach is not especially novel, being a combination of existing linear attention and re-weighting approaches. However, this combination is effective, which is valuable.",
            "summary_of_the_review": "The experiments show the benefit of the approach, and it reduces the FLOPs vs. softmax attention, while still having strong performance. The approach is not greatly novel, but the combination of components works, which is a valuable contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1595/Reviewer_8R83"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1595/Reviewer_8R83"
        ]
    },
    {
        "id": "jM1bd-_ike",
        "original": null,
        "number": 4,
        "cdate": 1666935649724,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666935649724,
        "tmdate": 1666935649724,
        "tddate": null,
        "forum": "2VWa8qj2vd0",
        "replyto": "2VWa8qj2vd0",
        "invitation": "ICLR.cc/2023/Conference/Paper1595/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "The paper focuses on improving the efficiency of standard vision transformers, which have quadratic computational complexity. The paper follows previous methods using linear attention, which does reduce the computational complexity, but results in worse performance. The paper proposes to mitigate the performance drop caused by linear attention with feature fixation, and also improves feature fixation by neighborhood association. The paper demonstrated the effectiveness of the proposed method on SSv1 and SSv2, Kinetics400 and Kinetics600.\n",
            "strength_and_weaknesses": "The following are some detailed questions and comments about the paper:\n\n1, Does the paper introduce additional parameters to the model, such as in Eq. (4)? The description in section 4.1 can be more clear, especially which parameters are learnable.\n\n2, The throughput improvement over softmax seems to be limited based on results from Table 8. However, the theoretic FLOP saving is more significant from Table 1. Is this due to not very well optimized implementation of the proposed method? Please provide more insight if possible.\n\n3, The proposed method is very generic. But the paper doesn\u2019t provide results to prove how well the proposed method can generalize to other cases. For example, can we apply the method to other backbones, such as TimeSformer, MViT, etc? Can we apply the method for image classification as well? If the proposed method is coupled with a specific backbone, such as XViT, the impact of the paper will be less significant.\n\n4, In Table1, why use 2x3 views for the proposed method, and 1x3 views for XViT? Since the proposed method is using XViT as the backbone, it should use the same setting. Will the performance gap between the proposed method and XViT become much larger, if they both use 1x3 views?\n",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "See above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1595/Reviewer_nema"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1595/Reviewer_nema"
        ]
    }
]