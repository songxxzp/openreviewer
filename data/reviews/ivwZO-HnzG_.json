[
    {
        "id": "vh97T2QG4Z",
        "original": null,
        "number": 1,
        "cdate": 1666594546851,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594546851,
        "tmdate": 1666594546851,
        "tddate": null,
        "forum": "ivwZO-HnzG_",
        "replyto": "ivwZO-HnzG_",
        "invitation": "ICLR.cc/2023/Conference/Paper2343/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper tackles conflicting gradients in multi-task learning (MTL). Specifically, the idea is to identify the shared layers of the multi-task model that exhibit large degrees of conflicting gradients, and replace these layers with task-specific parameters. The reported experiments suggest that only a small number of additional task parameters are necessary to improve performance, and reduce the occurrence of conflicting gradients in the remaining shared layers.\n\nThe experiments cover four multi-task benchmarks: multi-fashion+MNIST, CityScapes, NYUv2, and PASCAL-Context. The main finding is that the proposed \u201cRecon\u201d MLT approach improves the performance of all base models. Notably, the performance increase can be obtained with only a small increase in the total number of parameters (1.42%, 12.77%, 0.26%, and 9.8%, respectively for each benchmark).\n\nFurther analysis is provided in the way of an ablation study, showing that Recon significantly reduces the occurrence of conflicting gradients. Additionally, a theoretical analysis is presented, showing that a single gradient update on the model parameters of Recon achieves lower loss than on the original model parameters.\n",
            "strength_and_weaknesses": "While the approach is straightforward, the simplicity and effectiveness of the proposed Recon method are appealing.\n\nA potential concern with resolving conflicting gradients at training time is that they may, in some cases, provide a regularisation benefit that leads to positive transfer on held-out data. I would have liked to see more discussion on this issue.\n\nThe experiments are limited in two respects: (1) they focus only on vision problems; and (2) three out of four multi-task settings involve semantic segmentation. I would have liked to see more diversity of benchmarks as well as benchmarks with larger numbers of tasks.\n\nA downside of the proposed approach is that the resulting model is not actually multi-task anymore, as it contains task-specific weights, which could be problematic in some settings for deployment.\n\nFinally, the hyperparameters (S, K) appear to be quite sensitive, which increases the cost of the approach as they require fine-tuning for the different domains / datasets.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and proposes a novel approach. Although the approach is simple, reproducibility would be easier if code can be shared. \n\nDefinition 1 has a typo: \u201cangel\u201d -> angle\n",
            "summary_of_the_review": "Overall, this paper describes a simple idea to improve MTL by introducing a relatively small number of task-specific parameters, in order to reduce the impact of gradient conflict. The experiments are fairly comprehensive.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2343/Reviewer_Tqjk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2343/Reviewer_Tqjk"
        ]
    },
    {
        "id": "aRIaGNNhOHC",
        "original": null,
        "number": 2,
        "cdate": 1666627659299,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627659299,
        "tmdate": 1666627659299,
        "tddate": null,
        "forum": "ivwZO-HnzG_",
        "replyto": "ivwZO-HnzG_",
        "invitation": "ICLR.cc/2023/Conference/Paper2343/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a multi-task learning method to reduce conflict gradients during multi-task optimisation. The proposed method, name Recon is very simple in design and intuitive \u2013 it first computes the cosine similarity score for all combinations of task pairs in each shared layer, and sets this shared layer to be task-specific if its cosine similarity score is negative for multiple task pairs. As such, the proposed method can be combined with any gradient-based multi-task optimisation methods and can be applied on any multi-task architecture. The paper verifies this design on a range of multi-task benchmarks with different architectures and different optimisation methods.",
            "strength_and_weaknesses": "Strength. \n+ The paper is very well written and clearly motivated, supported with a wide range of experiments.\n+ The proposed method is general in design and can be attached to any optimisation methods and any architecture.\n+  The proposed method can achieve smaller loss supported with theoretical analysis.\n\nLimitations.\n+ Scalability and Running Time. Clearly, one inevitable problem for the proposed method is to compute cosine similarity for all combination of task pairs which might be unscalable when the number of training tasks is large. Though I agree that the newly initialised task-specific parameters might be small compared to the total network parameters, but the running time might be much slower. It would be clearer if the authors could the list the increased running time for such method and how it scales with increasing number of tasks. One paragraph that related to this calming that \u201crequires extra 25% of training iterations\u201d is clearly not sufficient and definitely not fast considering the fact that we only 2 tasks.\n\n+ Conflict from Architecture Design. It\u2019s very interesting to see that the authors found out that the conflict layers are consistent across multiple methods. This observation makes me wonder that the conflict itself was mainly due to the multi-task architecture design. As such, I am wondering whether these conflicts would reduce if we re-train the model from scratch after we set these conflict layers to be task-specific. If so, the running time would not be a problem since we only need to run this method once.  Just to be clear, the consistency is from different methods on the same dataset or different methods on different datasets as well?\n+ Severity criteria. Is this hyper-parameter needs to be tuned for each dataset as the paper lists the percentage of increased parameters vary quite a lot from different datasets, which will make the method a bit less general.\n+ Minor visualisation suggestion. Please add the attached method before (w/ Recon) or have a midrule to separate other methods. This would make the comparison much clearer.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear and should be easy to reproduce.",
            "summary_of_the_review": "The paper proposes a simple multi-task optimisation method: to make task-shared layers to be task-specific if the gradient conflict is severe. The paper in general is very well written with comprehensive empirical and theoretical support. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2343/Reviewer_DXpE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2343/Reviewer_DXpE"
        ]
    },
    {
        "id": "BPMTCi7TPI7",
        "original": null,
        "number": 3,
        "cdate": 1666703984181,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703984181,
        "tmdate": 1666704084503,
        "tddate": null,
        "forum": "ivwZO-HnzG_",
        "replyto": "ivwZO-HnzG_",
        "invitation": "ICLR.cc/2023/Conference/Paper2343/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to mitigate the negative transfer, i.e., the conflicting gradients, of different tasks on the shared layers. After showing that the gradient surgery methods cannot reduce the occurrence of conflict by investigating the gradient angle distributions of different tasks, the authors propose to split those layers with the most conflicting gradients. \n\nSpecifically, the authors train the network for some initial steps to calculate an S score to indicate the gradient conflict, where for a specific layer, its S score is the number of tasks whose gradient angle is larger than a threshold. Then, the shared layer with a higher S score (i.e., higher gradients conflict) is split for T times for each of T tasks. Finally, the new network with split task-specific layers is continued to train until convergence. Theoretical analysis is also provided to guarantee that applying the proposed Recon reduces the training loss.\n\nThe experiments are conducted on Multi-Fashion+MNIST, CityScapes, NYUv2, and PASCAL-Context with ablations validating the promising performance.",
            "strength_and_weaknesses": "Strength:\n\n1. Important problem with the pilot study probing existing issues and the concise solution addressing them.\n2. The theoretical analysis is also carried out that guarantees a lower training loss.\n3. This paper is well-organized and easy to follow.\n\nWeaknesses:\n\n1. In order to avoid , there is another category of MTL methods aims to avoid the negative transfer/gradient conflict by designing proper network architectures. For example, [1-4] perform layerwise feature fusing between tasks, which do not consider splitting from a shared network, but starting with several individual networks to merge (fuse features). Can the authors discuss, and preferably compare with, those methods?\n2. Would it help to control the starting iteration step when calculating (i.e., summing up) the s-scores? In other words, currently s^(k) = \\sum_{i=1}^{I} s_i^(k), would it help if i does not start with 1 as the s-scores of the very initial iterations can be noisy?\n3. This could be a minor issue: there exists a much larger MTL dataset, i.e., Taskonomy [5], to validate the proposed method.\n\n[1] Misra et al., Cross-stitch networks for multi-task learning. CVPR 2016.\n\n[2] Gao et al., NDDR-CNN: Layerwise feature fusing in multi-task CNNs by neural discriminative dimensionality reduction. CVPR 2019.\n\n[3] Ruder et al., Latent multi-task architecture learning. AAAI 2019.\n\n[4] Gao et al., MTL-NAS: Task-Agnostic Neural Architecture Search towards General-Purpose Multi-Task Learning. CVPR 2020.\n\n[5] Zamir et al., Taskonomy: Disentangling task transfer learning. CVPR 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: this paper is clearly written with good organization.\n\nQuality, Novelty: this paper probed an important issue of the existing methods and proposed original solutions.\n\nReproducibility: though the proposed method is fairly concise and should be easy to implement, the reproducibility still depends on if the authors release the codes.",
            "summary_of_the_review": "Overall I think this is a good paper with clear (empirical) problem analysis and concise solution. For those further possible improvements, please see the Weakness section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2343/Reviewer_2d79"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2343/Reviewer_2d79"
        ]
    },
    {
        "id": "IkixExqly_",
        "original": null,
        "number": 4,
        "cdate": 1666946368568,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666946368568,
        "tmdate": 1666947494568,
        "tddate": null,
        "forum": "ivwZO-HnzG_",
        "replyto": "ivwZO-HnzG_",
        "invitation": "ICLR.cc/2023/Conference/Paper2343/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper provides a new perspective on solving the conflicting gradients problem in MTL. It checks the conflicting level at layer scale, and turns the layers with high conflict scores into task-specific ones. Some theoretical analysis and experimental verifications are provided. ",
            "strength_and_weaknesses": "Strengths:\n\n+ The perspective on identifying conflicting gradients at layer scale is interesting. The proposed method that turn layers with high conflicts to task-specific ones seems to work well.\n+ The overall structure of this paper is easy to follow.\n\nWeaknesses:\n+ I am not convinced by the claim that \"gradient manipulation can not reduce the occurrence of conflicting gradients\". In my practice, I find these techniques can alleviate the conflicting gradients. Your results are only based on one dataset. It's surely not enough. Moreover, how do you calculate the conflicting angles? I think Figure 1 and Section 3.3 need more in-depth explanations. Otherwise, this claim seems to be exaggerated.\n+ The key algorithmic design \"turn the layers into task-specific ones\" is hard to figure what its exact meaning. I struggled to understand this point very concretely. Some figures with network structures may help better illustrate this clearly. \n+ The applicability of this method to other commonly used MTL structure such as MMoE is not provided.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Generally, this paper is easy to follow. But the key part of the algorithmic design is not clear. Readers will find it hard to understand what are the precise actions of \"turn the layers into task-specific ones\".\n\nQuality and Novelty: Fairly good\n\nReproducibility: The clarity affects the reproducibility to some extent.",
            "summary_of_the_review": "The paper is in general interesting. But in the current form, some parts of this paper are questionable or not clear. The authors need to clarify two main points raised in the review. \n\n(1) The claim on the effects  of gradient manipulation to conflicting gradients. \n\n(2) What does \"set these layers task-specific\" mean?\n\n(3) More experiments or discussions are needed to see whether the proposed applies to typical MTL structures such as MMoE.\n\nIf the concerns are well addressed, I will raise my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2343/Reviewer_5G4Q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2343/Reviewer_5G4Q"
        ]
    }
]