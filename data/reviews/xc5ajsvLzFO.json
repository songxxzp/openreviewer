[
    {
        "id": "X2YVMA6JV-Q",
        "original": null,
        "number": 1,
        "cdate": 1666298080952,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666298080952,
        "tmdate": 1666298080952,
        "tddate": null,
        "forum": "xc5ajsvLzFO",
        "replyto": "xc5ajsvLzFO",
        "invitation": "ICLR.cc/2023/Conference/Paper3533/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper performs a mixture of things to try to visualize ViTs as opposed to CNNs. First, they slightly extended the methodology of (Simonyan et al. 2014) by adding some data augmentations, and ran the optimization to find images that maximally activate a certain filter. This is met largely with adversarial examples, and sometimes I thought got even less informative results than the original paper.  Then, they explored the effect of the classification token, which I kinda consider obsolete because of the increasing usage of global average pooling in modern ViT architectures. Finally, they performed experiments comparing ViTs and CNNs on separated foreground/background (images with foreground/background regions cropped), and showed that ViTs perform better than CNNs when presenting only the foreground, as well as only the background. This is one concrete result of the paper.",
            "strength_and_weaknesses": "Strengths:\n\n-- Any attempt of explaining transformers and illustrating their differences with CNNs is interesting at the moment.\n-- The result that transformers perform better than CNNs when showing only the foreground, as well as only the background, is concrete and significant.\n\nWeaknesses:\n\n-- Trying to run the methodology of (Simonyan et al. 2014), although with the added data augmentation pieces, resulted in adversarial perturbations that are hard to interpret. I almost would believe there would be ways to do this better, e.g. by constraining more to the natural image manifold (such as using a VAE latent space perturbation). And the results in this part are a mixed bag at the best.\n\n-- Understanding that ViTs/CNNs have some segmentation capability is hardly something novel -- this has been shown in various prior works e.g. GradCAM and all the weakly supervised segmentation work that utilized GradCAM.\n\n-- Work on the classification token is a bit obsolete with the stronger adoption of global average pooling at the last layer.\n\n-- There has been no work that tries to check the effect of distillation and large-scale pretraining on the ViTs explored. E.g. what is the difference between distilled ViTs and non-distilled versions? What is the difference of something trained only on ImageNet-1k (e.g. a masked autoencoder) vs. JFT? Distillations and large-scale training sets are important aspects of transformer training that could bring material changes to their behavior and would be nice to explore. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. Quality and novelty is moderate -- largely used the same methodology from (Simonyan et al. 2014) with small modifications.",
            "summary_of_the_review": "The novelty is this paper is moderate since the methodology is largely similar with (Simonyan et al. 2014) with small tweaks. The results are a mixed-bag as well -- a few interesting and concrete results such as that the ViTs classify background better than the CNNs, but the visualizations are largely adversarial and sometimes confusing. I think this paper really can go either way, but would like to see some more concrete conclusions if we accept it to the conference.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3533/Reviewer_sVcT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3533/Reviewer_sVcT"
        ]
    },
    {
        "id": "-EhH3LW6bg",
        "original": null,
        "number": 2,
        "cdate": 1666302241428,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666302241428,
        "tmdate": 1666302241428,
        "tddate": null,
        "forum": "xc5ajsvLzFO",
        "replyto": "xc5ajsvLzFO",
        "invitation": "ICLR.cc/2023/Conference/Paper3533/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to use Activation Maximization method to visualize different layers of vision transformers (the feed-forward and attention layers) and compare them with CNNs. More specifically, the author uses gradient descent to maximize the sum of a particular feature over all patches to create a maximally activating image, starting from noisy versions of images in the dataset. In the paper, Activation Maximization was implemented with Total Variation regularization and several augmentations similar to previous works. The authors examine the optimized activated image and original images in the dataset that maximize a feature activation. Most of the examination was done on ViT-B16 but almost all other popular ViT variants are also examined in the Appendix.\n\nThe authors find out that Activation Maximization method does not yield good interpretable visualization from the attention layers (query, key and value) while the feed-forward layers exhibit the same patterns as in saliency maps. The ViTs layers also produces progressively more complex representations as CNN and, although lacking the limited receptive field, retains much of the spatial information of the image. The authors also provides experiments that show ViTs make better use of background information than the CNN models and rely less on textural attributes. The final examination, the authors show that training ViTs with language supervision makes the model aware of various high-level concepts, such as viewpoints, number of object or semantic concepts such as morbidity and music.",
            "strength_and_weaknesses": "Strength: The paper shows that the Maximum activation method can also work in ViTs to visualize the features in feed-forward layers. In most situation, the ViTs show very similar patterns compared to CNN. Some observations are quite interesting such as the spatial information perseverance, the better use of background information, and the attention visualization.\nWeakness:\n1. The method of visualization is not significantly novel.\n2. Although the paper focuses on visualization, the comparison between CNNs and ViTs uses mostly empirical results. Does the visualization of CNNs and ViTs also show that ViTs are more sensitive to background features, the current visualization is very similar to what expected from a CNN model.\n3. Some observations need more explanation: The attention components do not respond well to activation maximization. The author mentioned that this is because \"ViTs pack a tremendous amount of information in 768 features\", which is not quite clear to me. The author mentions that the neuron in ViTs maybe multi-modal neurons, is there some evidence/visualization to suggest this? \n4. The visualization does not show the conceptual categories activation well, as in Fig 9. It is more of a particular example case rather than some types or patterns. However, this is outside the scope of the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper's novelty in term of visualization method is limited, and since most of the behaviors are not totally unexpected given the results from CNNs and previous work, the insights provided in the paper (as discussed above) is also limited.\n\nThe results in the paper should be easily replicated.",
            "summary_of_the_review": "The paper does have some good visualizations and some interesting observation. However, ultimately, the method used is not novel and the insights provided from the analysis are not significant enough. The paper needs deeper analysis, for example a better analysis of why the attention visualization fail, is this because the maximum activation loss is not applicable? is there better ways of analyze this components? ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3533/Reviewer_eSYz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3533/Reviewer_eSYz"
        ]
    },
    {
        "id": "wvkht9O7TW",
        "original": null,
        "number": 3,
        "cdate": 1666623069046,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623069046,
        "tmdate": 1666623069046,
        "tddate": null,
        "forum": "xc5ajsvLzFO",
        "replyto": "xc5ajsvLzFO",
        "invitation": "ICLR.cc/2023/Conference/Paper3533/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper visually analyzes the behavior of ViT and CLIP models. The paper also shows the difference between ViT and CNNs and provides many observations. ",
            "strength_and_weaknesses": "Strength\n---\n- To my knowledge, this is the first work that visualizes the features of transformers. \n\nWeakness\n---\n- The paper is missing references that study the CNNs and transformers (e.g., [1]). And, the related section should be more elaborated.\n\n- Some of the observations are already studied (e.g., high-frequency in [1], the receptive field in [2])\n\n- It is not clear the novel findings from previous works.\n\n- While some observations related to the background are interesting, most of the experiments are done qualitatively but no quantitative analysis is done.\n\n- The experiment in Table 1 does not look convincing to me.\n\n[1] Park, Namuk, and Songkuk Kim. \"How Do Vision Transformers Work?.\" ICLR'22\n[2] Do Vision Transformers See Like Convolutional Neural Networks?, NeurIPS'21",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to read.",
            "summary_of_the_review": "In overall, the novelty is limited in that there are not many interesting findings compared to prior works on studying transformers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3533/Reviewer_gujJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3533/Reviewer_gujJ"
        ]
    },
    {
        "id": "NasI0Vu-zkd",
        "original": null,
        "number": 4,
        "cdate": 1666645935073,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645935073,
        "tmdate": 1666645990197,
        "tddate": null,
        "forum": "xc5ajsvLzFO",
        "replyto": "xc5ajsvLzFO",
        "invitation": "ICLR.cc/2023/Conference/Paper3533/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work adapts visualization techniques originally proposed for CNNs to ViT and observe the similarities and difference between CNN and ViT in terms of these visualization. For example, CNNs and ViT explores low-level features in the earlier layers and more semantic features in the higher layers. Some discoveries are interesting. For example, they observe that CLlP-based ViT image encoders can be activated by semantic concepts in the text information, e.g., such as prepositions, adjectives, and conceptual categories. ",
            "strength_and_weaknesses": "(+) They perform visualization on multiple ViT variants.\n\n(+) Experiments related to ViTs with text information (CLIP models) are interesting. \n\nWeakness:\n(-) The paper is not well-organized and the writing needs to be improved.\n\n(-) The visualization is corresponding to different layers displayed in different figures without justifications. Therefore, it is hard to tell if these results are cherry-picked.\n\n(-) Most conclusions drawn from this work are based on qualitative visualization without quantitative results, which makes these conclusions less convincing.\n\n(-) Some of the results are in the expectation without significant contributions to the communities. For example, the spatial information is lost in the last layer since the classification is performed based on the class-token. In addition, ViT relies more on background information has also been explored in previous works, e.g, patch-based infill transformation in Qin et al \"Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation\" (NeurIPS-2022) and Naseer et al \"Intriguing Properties of Vision Transformers\" (NeurIPS-2021). ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not well-organized and the writing needs to be improved. In addition, the novelty is not strong enough to support ICLR.",
            "summary_of_the_review": "This work does not have strong novelty and lack of quantitative analysis for most of the conclusions presented in the paper. In all, I lean toward a bordline reject. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3533/Reviewer_myKF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3533/Reviewer_myKF"
        ]
    }
]