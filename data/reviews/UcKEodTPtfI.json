[
    {
        "id": "qPIAmoy6dtb",
        "original": null,
        "number": 1,
        "cdate": 1666428966592,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666428966592,
        "tmdate": 1670269817890,
        "tddate": null,
        "forum": "UcKEodTPtfI",
        "replyto": "UcKEodTPtfI",
        "invitation": "ICLR.cc/2023/Conference/Paper5209/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a methodology to optimally sample datasets to train neural\nnetwork based surrogates of programs written in Turaco, a programming language\nintroduced by the authors. The programs are represented as stratified functions,\nwhich are functions that behave differently in different regions (strata) of the\ninput space (these correspond to different traces of the program according to\nthe branches chosen during execution). An independent neural network surrogate\nis trained for each strata. The question the paper tackles is how to sample the\ndataset to train these networks, that is, given a budget of how many samples we\nhave, how many samples to allocate to the training of each strata.\n\nThe authors determine the optimal number of samples $n_i$ for the strata by\nminimizing the upper bound of the error of the learned stratified surrogate of\nthe stratified function, with the constraint that the sum of the number of\nsamples must be equal to the budget (Theorem 3.1).\n\nTo compute the number of samples using this theorem, the complexity of the\ncomponent functions $\\zeta(f_i)$ is required. To this end, the authors introduce\na programming language called Turaco, in which all programs denote learnable\nstratified functions. They also provide the semantics and a complexity analysis\nfor the language which makes it possible to calculate an upper bound on the\ncomplexity of traces $\\zeta$, and so the number of samples $n_i$.\n\nThe theoretical results are verified empirically on a 60 line shader program\nwhich is part of a larger 3D renderer. The authors find considerable reductions\nin error compared to the baselines of uniform and frequency-based sampling.\n",
            "strength_and_weaknesses": "\n### Strengths\n\nThe paper presents a solid theoretical analysis with proofs, which is also\nverified empirically on a real-world program.\n\n### Weaknesses and Questions\n\nThe paper works with upper bounds both for Theorem 3.1 and when determining the\ncomplexity of a trace of the program, so we substitute upper bounds of the\ncomplexities of traces into an estimate arrived by minimizing the upper bound of\nthe error. Even though the authors present empirically verified gains compared\nto the baselines, it would be good to know something about how tight these\nbounds are.\n\nThe complexities of the traces in Table 1 are very similar to each other in\nmagnitude, probably because the branches in the program don't differ that much\nin complexity. It would be good to see a program where the complexities are more\nskewed. The path frequencies are the opposite of this: they are very skewed\ntowards rrrlrr (the rightmost one). Here it would be good to see a less skewed\ndistribution.\n\nThe Turaco programming language lacks iterative constructs like loops or\nrecursion, which is very important for programming. Could they also be included,\nand how difficult would it be?\n\nI think that the original C++ code of the shader (for which there is a GitHub\nlink on page 7) should be included in the Appendix as it may change on GitHub.\n\nThe captions of Table 1 and 2 should be below the tables.",
            "clarity,_quality,_novelty_and_reproducibility": "\n### Clarity\n\nThe paper is well-written and fairly easy to follow despite the many different\nelements of the topic which are combined. Section 2 with the example is\nespecially helpful.\n\n### Quality\n\nI found the paper to be of high quality.\n\n### Novelty\n\nThe paper has novel and interesting results, and it addresses novelty directly\nin Appendix B.\n\n### Reproducibility\n\nThis is a concern for the paper. I believe that in order for the results to be\nreproducible, the implementations of the Turaco programming language and the\nmethodology to compute the complexity bounds of the traces should be made\npublic.\n",
            "summary_of_the_review": "Overall I found the paper to be a significant contribution, well-written, and\nconvincing. Reproducibility is a concern, but that could be easily addressed.\n\nI did not verify the proofs in the Appendix.\n\n------------------------------------------------------------------------------\n\nUpdate:\n\nSimilarly to reviewer #2, I read the comments of reviewer #1 and their\ndiscussion with the authors. I agree with reviewer #1's points that calling the\nmethod \"optimal sampling\" and the method converging to uniform sampling are\nsignificant issues, so I'm decreasing my score.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5209/Reviewer_RLWc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5209/Reviewer_RLWc"
        ]
    },
    {
        "id": "hVNjjosbBXc",
        "original": null,
        "number": 2,
        "cdate": 1666622892826,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622892826,
        "tmdate": 1670206010739,
        "tddate": null,
        "forum": "UcKEodTPtfI",
        "replyto": "UcKEodTPtfI",
        "invitation": "ICLR.cc/2023/Conference/Paper5209/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is concerned with the problem of developing surrogates of programs. A major difficulty in this context is how train surrogates for programs with control flow. To overcome this challenge, this paper represents the program as a stratified function and uses stratified surrogates to model such functions. To ensure the training accuracy, an optimal allocation approach allocating different number of training samples by considering the complexity of the different paths of the program is proposed. A new programming language is presented to realize the proposed method. Finally, a demonstration 3D renderer is used for case study to verify the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength: \nThe authors proposed an optimal allocation approach, which allocates different number of samples by considering different complexity of the path in the program.\nIn addition, they also presented a new programming language to realize the proposed method.\n\n\nMajor comments:\nPage 2 \"Opitmal sampling\". The authors mentioned that \"Using neural network sample complexity bounds for learning analytic functions\". The reviewer is wondering are there other complex bounds that can be used to learn analytic functions? If so, what are the differences between these bounds, and why did the authors choose this bound?\n\nPage 3 \"Optimal path sampling\". The authors mentioned that \"Using this bound (as implemented by our TURACO analysis described in Section 4.2), we determine that the twilight path takes 1.5\u00d7 as many samples to train a surrogate to a given error as the nighttime path, and the daytime path requires5\u00d7 as many samples.\" The reviewer is confused about this result. The frequency of these three paths is in the order: nighttime>daytime>twilight, but the complexity order is: nighttime <  twilight <  daytime. Could you explain how this happened?\n\nPage 4, Figure 2 (b). It seems that when the total data size is small (around 10^1) the error decreases faster under the frequency sampling. Could you provide a possible reason for this?\n\nPage 4, \"Training methodology\".  Is the proposed method sensitive to these parameters?  How do these parameters influence the result?\n\nPage 4, Section 3 \"A given function f is probably approximately correctly learnable...\". Please provide the definition of \"probably approximately correctly learnable\".\n\nPage 6. \"For a 2-layer neural network trained with stochastic gradient descent...\" Instead of stochastic gradient decent, can other methods be used?\n\nPage 6, section 4. The motivation to create this new language \"TuARO\" is not clear. Specifically, why not use other existing languages, e.g. C++, python? Compared with these existing languages, what are the advantages of the presented language? \n\nPage 12, A1. Could you please list the pros and cons of your offline method compared with the online method in Coetes et al (2019)? In addition, could you explain how the different choice on the sample complexity affects the result?\n\nPage 13 \"Optimal sampling\". It is suggested that the restrictiveness of the assumptions should be discussed.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is mostly clear, well-written, and easy to follow.\n\nQuality: The quality of this paper looks reasonable and they achieve more accurate surrogates than existing works.\n\nNovelty: The novelty of the proposed approach is medium. But they present a new programming language, which is quite novel.\n\nReproducibility: The author has uploaded their code in the github, so this work is reproducibility.",
            "summary_of_the_review": "This paper is well written in general, and the simulation is nice.\nThe contribution is interesting and enough, but the method seems not very novel.\nThe motivation for the presented programming language should be improved.  \n\n---------------------------------\nAfter reading reviewer #1's comments and discussions, I tend to decrease the score. Although interesting, the paper seems not very rigorous in defining \"optimal sampling\", \"formulation\", and the contributions of the paper seem not significant.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5209/Reviewer_E5p2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5209/Reviewer_E5p2"
        ]
    },
    {
        "id": "kNL930FXA0f",
        "original": null,
        "number": 3,
        "cdate": 1667424630109,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667424630109,
        "tmdate": 1667724776623,
        "tddate": null,
        "forum": "UcKEodTPtfI",
        "replyto": "UcKEodTPtfI",
        "invitation": "ICLR.cc/2023/Conference/Paper5209/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes improved sampling for training program surrogates. The proposed sampling scheme takes into account both data distribution and sample complexity of different paths. A language suitable for application of the proposed sampling scheme is introduced, and the scheme is evaluated on a graphics program.",
            "strength_and_weaknesses": "Strengths: the paper takes a more thorough approach than earlier works to distribution of samples for training program surrogates. Theoretical error bounds are used to derive practical sample distribution rules. The results are empirically supported.\n\nWeaknesses: the paper has many technical inconsistencies which impair significantly my trust in the presented results.\n\nFirst, the phrase 'optimal sampling' is used through the paper, including formal statements, while it is clear that the sample distribution is based on theoretical error bounds, which are not tight, and has sampling is not 'optimal'.\n\nSecond, the proof, provided in the appendix, of the central theorem of the paper, 3.1, contains errors. 1) the derivative of the 'Lagrangian' is wrong, the derivative of n^{-1/2} is -1/2n^{-3/2} rather than 1/2n^{-3/2} (the minus is omitted). 2) the last line is obviously wrong. 3) this is not a proof of constrained optimization but rather a sketch of it, for a normal proof one has to show that the zero-gradient point is indeed the minimum (it does not have to be). \n\nThird, the formula in Theorem 3.1 has a paradoxical property --- as the number c of strata/branches increases, their relative complexity affects 'optimal' sample distribution less and less. I am not convinced that this is indeed 'optimal' behavior rather than an artifact of a particular form of upper bound used for the proof (and more suitable for theoretical analysis than for practical sampling). This should at least be discussed.\n\nFourth, there is no supplementary material, so one cannot reproduce the results, or even look at the source code. The only artifact of the empirical evaluation is the 'full code' of the renderer case study in Figure 18, which is a single function written in a version of Turaco different from introduced in the paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has inconsistencies, certain things are used before they are introduced (e.g. the language for the didactic example in figure 1). I do not see an easy way to attempt to reproduce the results presented in the paper.",
            "summary_of_the_review": "The paper addresses an interesting topic but does not stand by publication standards. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5209/Reviewer_mndd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5209/Reviewer_mndd"
        ]
    }
]