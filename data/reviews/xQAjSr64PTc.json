[
    {
        "id": "U_hqOT_hw7",
        "original": null,
        "number": 1,
        "cdate": 1666485772941,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666485772941,
        "tmdate": 1670358943915,
        "tddate": null,
        "forum": "xQAjSr64PTc",
        "replyto": "xQAjSr64PTc",
        "invitation": "ICLR.cc/2023/Conference/Paper2265/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a method called EUCLID for the unsupervised reinforcement learning (URL) setting, that consists of two phases: an unsupervised pretraining phase, and then a downstream task where task-specific rewards are provided. The method uses a model-based pretraining approach, where the model has multiple heads that are trained to be experts on particular subsets of the state space. During finetuning, the model is used in a model-based planning setup to complete downstream tasks. The method is evaluated on state-based tasks from the URLB benchmark. The paper also contributes empirical studies of which unsupervised RL benchmarks improve the performance of EUCLID and ablation studies of the method.",
            "strength_and_weaknesses": "Strengths:\n- The idea behind the method of the paper is well-motivated, and it conceptually makes sense that a pre-trained dynamics model would be one way to take advantage of an unsupervised pre-training phase.\n- The EUCLID method is agnostic to the choice of intrinsic reward, as the authors point out, and the empirical study on the choice of unsupervised RL method is quite comprehensive.\nWeaknesses:\n- I think that the paper in its current form overclaims the contributions of the multiple-choice dynamics model learning for the final performance. In Table 1, it appears that EUCLID without MCL achieves very similar performance (often within the error bars) as EUCLID with MCL. Although the qualitative results presented show that certain task heads perform better than the other ones, it seems reasonable for that to happen because the heads are specifically trained to be better at specific tasks. But the claim that \u201cThis indicates that multi-choice learning reduces the prediction error of the downstream tasks corresponding to local dynamics and obtains better results\u201d doesn\u2019t seem substantiated.\n- I also think that the results for TDMPC @ 100k, which are in the appendix, should be included in Table 1. Because EUCLID uses many of the components of TDMPC, the performance of TDMPC @ 100k is actually quite competitive with EUCLID. \n- The method is only demonstrated on tasks from state, although because of the world model formulated, it seems like it would be readily transferable to learning from pixels. I think it would be helpful to see results on image-based tasks to show the methods\u2019 efficacy.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- A few technical sections could have been presented more clearly. For example, particularly I think it could be more clear how the policy that is described in Section 3.1 is used (my understanding is that it is both an exploration policy during pretraining and also used to generate candidate action samples during finetuning). \n- The context for Figure 1 is confusing \u2013 it\u2019s not clear which method was used for pretraining in this example. \n- Another point is that the statement in the introduction is that \u201cpre-training via diverse exploration is not enough for guaranteeing to facilitate downstream learning\u201d. But doesn\u2019t this method also fall into the category of pre-training using diverse exploration?\n\nQuality: The experimental evaluations seem to have been thoroughly executed and there are many ablation analyses, which I find helpful. See my points in \u201cWeaknesses\u201d.\n\nNovelty: I think the novelty of the work is somewhat limited when compared to TDMPC. The combination of a TDMPC-like method with unsupervised RL algorithms is novel to me. The other main contribution claimed is the multi-choice learning framework, but as mentioned earlier, I am not convinced of its effectiveness.\n\nThe paper describes the implementation in significant detail and appears reproducible.",
            "summary_of_the_review": "In summary, I think this is a nice paradigm for the unsupervised RL setting and includes thorough experimental evaluation and ablations. However, I have a few concerns regarding the novelty and improvements compared to TDMPC, effectiveness of the multi-headed model approach, and clarity of the paper. Therefore I think the paper currently shouldn\u2019t be accepted, but am willing to change my score based on discussion. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2265/Reviewer_wBqh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2265/Reviewer_wBqh"
        ]
    },
    {
        "id": "w76ExacO38",
        "original": null,
        "number": 2,
        "cdate": 1666615517459,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666615517459,
        "tmdate": 1666615517459,
        "tddate": null,
        "forum": "xQAjSr64PTc",
        "replyto": "xQAjSr64PTc",
        "invitation": "ICLR.cc/2023/Conference/Paper2265/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces EUCLID, a new paradigm for the unsupervised reinforcement learning task. In the pre-training phase, the paradigm first introduces a multi-choice prediction model to learn the dynamics of the environment. The learned dynamics can adapt the policy to downstream tasks efficiently and effectively. The experimental results show significant improvements over baselines.",
            "strength_and_weaknesses": "Strength:\n\n1. The motivation is clear and the proposed method make senses.\n2. The idea is novel in this field, although it has been proposed in dynamics generalization task.\n3. The experimentally comparisons are comprehensive and the improvement is impressive.\n\nWeakness:\n\n1.  This paper employs a multi-choice model to learn the environmental dynamics; however, Table 1 demonstrates that the performance difference between with MCL and without MCL is marginal, which raises my concern that MCL is unnecessary for this paradigm. If so, could \"multi-choice\" be removed from the title? Can authors provide further clarification?\n2. According to my knowledge, the scale of rewards in the training phase and the fine-tune phase are vastly different. I am concerned that the scale difference may be greater than the differences between prediction heads in some situations, rendering the MCL ineffective. Can authors provide additional experiment information and observations?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe presentation is clear and easy to understand.\n\nNovelty:\nThe idea is new in this area although it has been proposed in dynamics generalization task.\n\nReproducibility:\n\nThe authors provide enough details to reproduce their methods.",
            "summary_of_the_review": "Overall, I think that the paper is above the accpetance bar considering the idea, difficulty to impletement and the experimental improvements,  but I still have some concerns about the metods details.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2265/Reviewer_E7J9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2265/Reviewer_E7J9"
        ]
    },
    {
        "id": "Uh5CKBTT7E",
        "original": null,
        "number": 3,
        "cdate": 1666680347375,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680347375,
        "tmdate": 1666680347375,
        "tddate": null,
        "forum": "xQAjSr64PTc",
        "replyto": "xQAjSr64PTc",
        "invitation": "ICLR.cc/2023/Conference/Paper2265/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a framework for joint pretraining of dynamic model and exploration policy and the adaptation to downstream tasks. The method achieves superior performance compared to prior works with a high sample efficiency. ",
            "strength_and_weaknesses": "Strength\n* The pretraining and finetuning phases, with a specific design of multi-choice dynamics model, is well-motivated. \n* The method achieves SOTA performance on standard benchmarks with high sample efficiency. \n* Rationale behind several design choices, such as using multiple heads for dynamic model training, are empirically supported in experiments. \n\nWeaknesses\n* The method combines multiple components such as MBRL and MCL into a pretraining and finetuning framework. These components are directly taken from the existing literature. While the paper does offer comparisons of which backbones offer the best empirical results, the novelty of the work is limited. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written. The proposed multi-choice model is novel. ",
            "summary_of_the_review": "This work proposes a model-based training framework that leverages dynamic information and skills learnt from the pretraining phase to speed up learning in downstream tasks. The novelty of the framework is limited since the components in this framework are directly taken from the literature, but I think this itself is not a big limitation given the good experimental performance in standard benchmarks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A. ",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2265/Reviewer_486X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2265/Reviewer_486X"
        ]
    },
    {
        "id": "lrxC5ovBuqt",
        "original": null,
        "number": 4,
        "cdate": 1667550834550,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667550834550,
        "tmdate": 1670358808968,
        "tddate": null,
        "forum": "xQAjSr64PTc",
        "replyto": "xQAjSr64PTc",
        "invitation": "ICLR.cc/2023/Conference/Paper2265/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an approach for pretraining an RL agent in a reward-free environment by fitting a dynamics model on the data generated by an exploration policy. The authors position this work in contrast to the body of prior work in unsupervised reinforcement learning (URL) involving model-free policy pretraining, which suffers from what they refer to as the \u201cmismatch problem\u201d: prior URL model-free approaches, like most exploration algorithms, optimize non-stationary reward functions and therefore end up oscillating in state distribution, meaning that transfer performance (which relies on overlap between the state distribution of the pretrained policy and that of the optimal policy for the target task) is highly dependent on the learning update at which the pretraining is stopped. The authors instead propose to pretrain a model-based policy by pretraining a dynamics model on data produced by an exploration policy; to improve the diversity of the pretraining data distribution for the dynamics model, the authors propose to use an ensemble of exploration policies which are additionally incentivized to visit different, and therefore specialize to, subspaces of the latent state space. The authors demonstrate that the pretrained learned dynamics model and policies can be transferred to downstream tasks by using the dynamics model for planning and selecting an expert from the ensemble for guiding the planner. In experiments on the URLB benchmark, they demonstrate that their method outperforms model-free URL baselines, and ablate choices pertaining to the exploration strategy and components of the objective for learning the dynamics model.",
            "strength_and_weaknesses": "Strengths:\n\nExperiments demonstrate that the resulting pretraining procedure is effective on URLB, compared to baselines. In particular, their approach requires significantly fewer samples under the downstream task.\n\nExperiments demonstrate that unlike the model-free URL pretraining approaches they compare to, the resulting model benefits from more pre-training steps (and data) in a monotonic manner. \n\nThe authors provide a comparison between a significant number of exploration strategies (APT, Disagreement, DIAYN). While this is certainly an added bonus, this also introduces many hyper-parameter tuning challenges. \n\nThe authors provide additional comparisons on tasks based on humanoid, which are typically not considered in such pretraining settings. \nIn general, this work provides a useful case study of the model-based paradigm applied to the URLB benchmark.\n\nWeaknesses:\n\nThe work is mainly positioned with respect to work in URL, but it also relates more broadly to literature in model-based RL, and the general challenge of collecting data for model fitting. It might be more impactful to consider prioritizing the model-based RL perspective and how the proposed method , in which case URLB may serve as one of many benchmarks.\n\nThe authors do not spend much time discussing how the mismatch problem can be circumvented with existing URL approaches, to be effectively transferred to downstream tasks; for example, skill discovery algorithms with discrete skill variables such as DIAYN (Eysenbach et al, 2018) can be interpreted as producing a set of experts which can then be treated as a mixture of experts by a master policy. Such an approach should serve as a baseline. \n\nThe novelty is limited in so far as the approach combines a variant of existing model-based approaches (such as T-CML, Seo et al 2020) with existing exploration strategies.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The exposition is mostly clear, but the method relies on prior work as solutions to subproblems, details of which are not always fully explained; for example, there is not much explanation for how the correct expert is chosen from only zero-shot model evaluation (which seems to follow Seo et al 2020). \n\nThe novelty is limited in so far as the approach combines a variant of existing model-based approaches (such as TDMPC, Hansen et al 2021, and T-CML, Seo et al 2020) with existing exploration strategies.\n\nIt would be useful to verify that the positive effect of multiple choice is not simply due to increase in capacity of the dynamics model.\n",
            "summary_of_the_review": "Overall, this work serves as an interesting case study of a model-based approach on the URLB benchmark. That said, the stated contribution of providing a novel model-fused approach for unsupervised pre-training is mainly positioned with respect to existing model-free approaches in the URL space, even though it ends up being quite related to existing work in model-based RL, and in particular, the challenge of obtaining data for fitting the dynamics model. Moreover, the comparison to transferring e.g. competence based URL methods by learning to control discovered skills is missing, which should serve as a stronger baseline.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2265/Reviewer_yoch"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2265/Reviewer_yoch"
        ]
    }
]