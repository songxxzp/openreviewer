[
    {
        "id": "t8vznsnILGJ",
        "original": null,
        "number": 1,
        "cdate": 1666290280980,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666290280980,
        "tmdate": 1666400351829,
        "tddate": null,
        "forum": "JRFSLFyYAII",
        "replyto": "JRFSLFyYAII",
        "invitation": "ICLR.cc/2023/Conference/Paper2770/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In summary, from top to bottom, the paper: \n* Attempts to explain why a population of agents may be more useful than a single agent, under a theoretical formulation of argmax(multiple agent outputs).\n* Presents a lower bound objective which leads to a variant of policy gradient which involves only updating the policy which generated the best trajectory per problem instance.\n* Sets up the architecture as a shared encoder but with multiple decoders to denote multiple policies.\n* Demonstrates competitive experimental performance against classic and deep-learning based baselines in the literature, especially normalizing for wall-clock time.",
            "strength_and_weaknesses": "# Strengths\n* Extensive experimental results demonstrate that the method empirically is solid. A brief read over the Appendix also contains relevant ablation studies, such as checking the diversity of the population of policies.\n* I applaud the paper for providing Figures 1 and 2, which makes the paper more engaging to read, and also help explain the encoder/decoder method much better. \n\n\n# Weaknesses\n* The theoretical motivations are quite subtle and I have quite a few questions, which if answered, could help make the paper much stronger in its theoretical/fundamental contributions:\n    * How is the framework similar/different from e.g. describing evaluation objective as a maximum of $K$ sampled trajectories from a (potentially very expressive) policy $\\pi_{\\theta_{meta}}$? In the $K=2$ case, $\\pi_{\\theta_{meta}}$ for example, could be a very bimodal policy which can also produce diverse behaviors from its two modes.\n    * Is Algorithm 1 an unbiased policy gradient estimator of $J_{poppy}$? \n    * The method has no explicit objective/regularizer to maximize behavioral diversity, but instead implicitly assumes that this occurs through the \"argmax\" training in Algorithm 1. Is there a way to theoretically capture/analyze this \"implicit regularization\"? Currently it seems that the behavioral diversity isn't theoretically guaranteed, but rather occurs I suspect mainly if the beginning of training allows each agent to \"latch on\" to their own specific strategy.\n    * Algorithm 1 is written somewhat specifically for this particular case involving a shared encoder and multiple decoders. Can it be re-written as a general RL method? ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and Quality:** My main concern is with the theoretical motivations. Please see above in \"Weaknesses\". Furthermore, there are some presentation issues, e.g. Figure 3 is not described very well / the plots are confusing and unclear of their purpose. Otherwise the experimental results appear to be solid.\n\n**Novelty and Originality:** The proposed idea has **potential** to be quite novel, if the theoretical components are analyzed better. However, currently it is presented as a somewhat ad-hoc experimental method.",
            "summary_of_the_review": "The experimental results are fine. However, I would really like to see more theoretical analysis of the proposed method, which would significantly increase the impact of the paper and also broaden its scope. Otherwise, currently from a devil's advocate point of view, the proposed method may be considered somewhat of a \"hack\" currently.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2770/Reviewer_rRMu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2770/Reviewer_rRMu"
        ]
    },
    {
        "id": "KfLRSVlGfsw",
        "original": null,
        "number": 2,
        "cdate": 1666475234004,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666475234004,
        "tmdate": 1671142322658,
        "tddate": null,
        "forum": "JRFSLFyYAII",
        "replyto": "JRFSLFyYAII",
        "invitation": "ICLR.cc/2023/Conference/Paper2770/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes Poppy, an RL-based approach to learn a population of constructive heuristics for combinatorial optimization problems.   The presented architecture is based on the encoder-decoder Attention Model, with a shared encoder and one decoder per agent of the population. Starting from a pretrained model, Poppy training is based on a policy gradient that only depends on the performance of the best agent for each instance; and serves to only update that agent\u2019s parameters. Poppy is evaluated on three well-known CO problems: the TSP, CVRP and KP. ",
            "strength_and_weaknesses": "**Strengths**\n1. The paper is clear and very well-written.\n1. The idea of using a population of agents is well motivated.\n1. The proposed loss that enables an effective diversity without explicitly guiding the specialization seems general enough and could be used in other contexts.\n1. Experimental evaluation shows strong results on the selected tasks.\n\n**Weaknesses**\n1. A very similar idea of training multiple decoders for the Attention Model was explored in MDAM [1] and applied to the TSP and CVRP:\n   * As far as I can see, the main contribution/difference of the present paper w.r.t MDAM is the different loss -- which considerably limits the novelty of the paper. \n   * Although the experimental results of the present paper seem stronger, this can also come from using POMO [2] training which is known to significantly improve the AM performance.\n   * Including a precise discussion of the differences w.r.t to MDAM and including it in the experiments baselines would be useful.\n\n1. The experimental evaluation focuses on datasets (synthetic TSP/CVRP instances with 100, 125 or 150 nodes and KP with 100 or 200 items) that do not seem challenging enough, today, for learning-based methods. Indeed:\n    * The reported optimality gap of existing learning-based methods is already very small: less than 0.01% for TSP100 and 0.2% for TSP150, and less than 0.005% for the KP. These are already very good to excellent for heuristics in general.\n    * In this context, even dividing the opt gap by 5 (as claimed in the abstract for TSP) does not seem very significant to me. This is especially visible in Table 3.\n    * The scale of these problems is quite small compared to what other learning-based methods are able to solve (e.g. [3] solves TSP instances with up to 10,000 nodes, with supervision only on small graphs)\n    * I would suggest to explore more challenging problems (e.g. larger sizes) to really showcase the experimental value of the proposed approach.\n \n1. I can see the interest of having a population of agents in Fig 3;  however, (again) the gains seem very small: \n    * in Fig 3 bottom left, we see that removing 1 agent from the population would lead to a deterioration of the opt gap of at most ~ 0.0004%. \n    * This raises the question of is it because the opt gap is already very small (cf previous point) or is it the influence of the individual agents that is not significant (i.e. how different are the agents). \n    * I wonder how, for example, taking the best of 4 samples x 4 agents would compare to greedily using 16 different agents. \n\n\n[1] Xin et al, Multi-Decoder Attention Model with Embedding Glimpse for Solving Vehicle Routing Problems, AAAI 2021\n\n[2] Kwon et al, POMO: Policy Optimization with Multiple Optima for Reinforcement Learning, Neurips 2020\n\n[3] Fu et al, Generalize a Small Pre-trained Model to Arbitrarily Large TSP Instances, AAAI 2021\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity & Quality**:\nThe paper is clear and very well-written, I enjoyed reading it. The idea of using a population of agents for CO is well motivated and illustrated on an intuitive example. Different plots (Fig 3) help to understand the effects of the proposed approach.\n\n**Originality**:\nBecause of an existing closely related work [1], the technical originality is limited (see Weakness 1). In terms of experiments, the datasets and settings are standard.\n\n**Reproducibility**: \nThe experimental setting is described in detail; the code and datasets are not provided but it looks like they would be after the double-blind reviewing period.\n\n**Questions/remarks:**\n\n1. In Algorithm 1, the two last lines, I could not see how these formulas allow to only update the parameters of the \u201cbest\u201d decoder for each trajectory.\n\n1. About the Training Procedure, I wonder if there is any intuition on how to fix the number of training steps of phase 2. If too small, the mentioned imbalance between the decoders is an issue; if too large I guess the decoders may become too similar. Aside from tuning this number experimentally, I wonder if there is any criteria that can be used to switch from phase 2 to 3. \n\n1. A related work that's worth mentioning: [4].\n\n[4] Kim et al, Learning Collaborative Policies to Solve NP-hard Routing Problems, Neurips 2021\n\n",
            "summary_of_the_review": "I would vote for reject, because of the combination of:\n\n(i) The limited novelty, especially w.r.t [1]: since the population aspect has already been explored with a similar architecture for similar problems, the main novelty seems to be the loss -- although I encourage the authors to make a precise comparison and update their claims.\n\n(ii) The experimental evaluation is not convincing because, in my opinion, it shows marginal gains on tasks where learning-based methods have already achieved excellent results. Evaluation on more challenging problems/settings (e.g larger instances) would greatly increase the empirical value of this paper.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2770/Reviewer_rpzd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2770/Reviewer_rpzd"
        ]
    },
    {
        "id": "cgKf4YfCy3W",
        "original": null,
        "number": 3,
        "cdate": 1666669817406,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669817406,
        "tmdate": 1666669817406,
        "tddate": null,
        "forum": "JRFSLFyYAII",
        "replyto": "JRFSLFyYAII",
        "invitation": "ICLR.cc/2023/Conference/Paper2770/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper is in the line of works that uses reinforcement learning for combinatorial optimization (CO) problems. The main contribution is the idea of training a population of complementary agents for a given distribution of CO problems, with the objective function being optimizing the performance of the best agent instead of the average. The proposed training approach, POPPY, is evaluated on three benchmark tasks, including TSP, CVRP, and KP. It shows state-of-the-art performance on the TSP task. ",
            "strength_and_weaknesses": "Strengths: \n1) Using RL for CO problems is an interesting topic and of practical importance. \n2) The idea of training a (diverse) population agents for CO problems is interesting and novel. \n3) The rationale behind POPPY is well-motivated with the discusison of the line of related works, as well as using the simple example in Figure 1.\n4) It shows considerable improvement over SOTA RL methods on the TSP benchmark task. \n\nWeaknesses:\n1) The empirical evaluation is not entirely convincing -- \n1a) The improvements over SOTA RL methods like POMO are generally small on all of the three tasks. POMO itself is already pretty close to optimal solution, so that the edge of POPPY is barely significant (it is unclear if the authors have done t-tests). The evaluation would be more convincing if the authors can show that POPPY has more significant improvements. \n1b) On the CVRP task, POPPY is outperformed by EAS. \n1c) The baselines are missing for the KP task, making it even more challenging to evaluate the performance of POPPY. \n\n2) There are many engineering heuristics to make the system work (e.g., when to freeze or unfreeze the encoder, whether or not the optimal solution depends on the starting point). Therefore, the overall performance may be fragile to the specific training architecture. Especially that the edge of POPPY seems to be small compared to the baselines, so that it is questionable whether these kind of improvements are due to engineering tricks or structured improvements due to maintaining a population of agents. Also, the results may be hard to reproduce.  \n\n3) It is unclear to me where the \"diversity\" in the population of agents lies in the objective function. It appears that POPPY is training a large set of agents with different policies, so that one of them \"happens\" to be good at a certain problem instance. As a fair comparison, perhaps you should separately train 16 POMO/LIH/EAS, and compare POPPY-16 with the best performance of those 16 policies. It is questionable whether POPPY can beat such a strong baseline. ",
            "clarity,_quality,_novelty_and_reproducibility": "My evaluation has 4 levels: Excellent, good, fair, poor. \n\nClarity: fair. The claim that the method is \"theoretically grounded\" seems not to be supported. The lower bound of the objective is too weak to be a theory and people generally think of a gaurantee of some kind of performance when you say that. \n\nQuality: fair. \n\nNovelty: Excellent. This is a novel idea! \n\nReproducibility: Fair. The description of the method seems clear and the method itself (the key idea) is not complicated. But there are some engineering tricks that make it a bit challenging to reproduce. ",
            "summary_of_the_review": "The idea of training a diverse population of agents to solve CO problems is generally interesting, novel, and well-motivated. But it is not clear where diversity is encouraged, and the evaluation is not entirely convincing. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2770/Reviewer_5SRy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2770/Reviewer_5SRy"
        ]
    }
]