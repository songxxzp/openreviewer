[
    {
        "id": "qzkdrUtm5d",
        "original": null,
        "number": 1,
        "cdate": 1666023196224,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666023196224,
        "tmdate": 1670572846093,
        "tddate": null,
        "forum": "rLwC0_MG-4w",
        "replyto": "rLwC0_MG-4w",
        "invitation": "ICLR.cc/2023/Conference/Paper841/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to employ denoising diffusion models to decode error correcting codes (ECCs). In particular, the authors introduce a diffusion process that models well the decoding of an ECC, and the denoising is performed by conditioning on the number of errors in the parity check matrix. In order to choose the step size of the reverse diffusion, a line search is adopted. The authors compare their results to the transformer-based decoder recently proposed by Choukron & Wolf, (2022). Some comparisons with 'classical' decoders (BP) are also provided.",
            "strength_and_weaknesses": "Strengths:\n\n(1) The idea of using diffusion processes in the context of error correction is, at the best of my knowledge, new. Compared to existing neural decoders which often do not scale well to larger block lengths, this novel framework has the potential to scale to codes with block lengths of a few hundreds of bits.\n\n(2) Clear gains in a number of different settings with respect to ECCT, i.e., the transformer-based approach of Choukron & Wolf, (2022).\n\n\nWeaknesses:\n\n(1) My main concern is that the comparison with 'classical' (i.e., non neural-network-based) decoding algorithms is not sufficiently thorough. It could be that DDECCT does better than ECCT, but ECCT is not quite SOTA, in the sense that there is a 'classical' algorithm that performs much better with roughly the same complexity. More specifically, the standard decoder of polar codes is *not* the BP decoder taken into account here, but rather the successive cancellation list (SCL) decoder. Is DDECCT able to match SCL performance? Let me highlight that, at the lengths considered here (64 in Figure 4, but also 512 in Figure 5), SCL reaches the optimal ML performance with a relatively small list size. \n\n(2) In page 8, the authors give an expression of the overhead of the proposed method over ECCT. However, it is not clear what the overhead is in terms of running time. Suppose that one wishes to compare ECCT with the method proposed here, keeping the complexity fixed. This means that ECCT could use a larger N or d than DDECCT (provided that the complexity of the two procedures is the same). Would DDECCT still compare favourably?\n\n(3) Comparing the performance of different decoding algorithms in the tabular form of Table 1 is not really standard in wireless communications. In particular, what really matters in practice is the SNR needed to reach a certain target BER. Target BERs typical in wireless communications are e.g. in the range $10^{-3}-10^{-5}$. This difference in reporting the results can be quite substantial: if the error curves are very steep, the improvement in the error probability at a fixed SNR could seem very large, while the improvement in the SNR needed to reach the target BER may be limited. I strongly suggest that the authors provide a plot of the error probability as a function of the SNR in addition to Table 1, so that the improvement with respect to the baselines is more clear. As a final (minor) note, I would suggest that the authors put in bold the number corresponding to the best performance in Table 1 to improve readability.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clear and well written.",
            "summary_of_the_review": "Overall, the paper provides a new framework based on denoising diffusion models, which results in improvements over the existing work by Choukron & Wolf, (2022). My borderline initial opinion is due to the weaknesses pointed out above concerning the experimental results. If such points are properly addressed during the rebuttal, I can raise my score.\n\n--- \n\nMy concerns have been addressed and I have increased my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper841/Reviewer_U25g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper841/Reviewer_U25g"
        ]
    },
    {
        "id": "eQgI7k8QHXn",
        "original": null,
        "number": 2,
        "cdate": 1666880478321,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666880478321,
        "tmdate": 1666880763633,
        "tddate": null,
        "forum": "rLwC0_MG-4w",
        "replyto": "rLwC0_MG-4w",
        "invitation": "ICLR.cc/2023/Conference/Paper841/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a new decoding framework in algebraic block codes by employing a denoising diffusion model. \nThe idea is to learn the process of the codeword corruption using the diffusion model and to detect the noise contribution in an output y by computing the reverse process of the corruption using the learned model. To implement this idea, the authors make some approximations in the reverse process and propose some additional ideas, such as learning the binarized multiplicative noise instead of the additive one and learning the noise process conditioned by the number of parity check errors. The experimental results of the proposed method significantly outperform the other comparative methods, such as the transformer-based framework and the standard decoding scheme based on Belief propagation in terms of the bit error rate (BER), demonstrating the superiority of the proposed method.\n",
            "strength_and_weaknesses": "Strengths:\n- A novel idea about how to use the diffusion model in decoding.\n\n- The high decoding performance of the proposed method compared to other methods.\n\nWeaknesses:\n- The ideas for implementation, such as learning the binarized multiplicative noise instead of the additive one and learning the noise process conditioned by the number of parity check errors, have no theoretical reasoning.\n\n- There seems to be no guideline on how to choose the variance schedule {\\beta_t}_{t=0}^{T}.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The presentation is not clear. I think the derivation/explanation of the algorithms can be much simpler or convincing. For example, the current explanation does not provide sufficient reasoning as to why the multiplicative noise is treated and inferred instead of the additive noise that is directly affecting the codeword; hence the reader may wonder what happens if inferring the additive noise directly. Also, there are typos and misleading expressions at some important points in the overall manuscript. The following is the list of them I have realized:\n******************************************************\nPage 3, below eq. (2): The equation \\hat{x}=y\\cdot \\epsilon_{\\theta} should be \\hat{x}=y\\cdot (\\epsilon_{\\theta})^{-1} by the definition of the multiplicative noise. I know it finally becomes multiplicative sign noise so (\\epsilon_{\\theta})^{-1}=\\epsilon_{\\theta}, but at this point it is not explained that only the sign of the multiplicative noise is important.\n\nPage 3, above eq. (5): The sentence ``a model \\epsilon_{\\theta}(x_t,t) that predicts the additive noise \\epsilon'' can be misleading since later the symbol \\epsilon_{\\theta} is used as an estimate of the ``multiplicative noise''. One should use different notations between these two things.\n\nPage 4, eq. (10): q(x_t|x_{t-1}) should not be q(x_{t-1}|x_t)? The same is true for q(x_t|x_{t-1}) above eq. (10)\n\nPage 4, eq. (12): Why use ``log'' with two arguments to denote binary cross-entropy? I think it is better to use another notation. The same is true for eq. (13)\n\nPage 6, Alg. 2: The meaning of the function ``e'' should be explained.  \n\nPage 6, Alg. 2: How to compute \\lambda is not well explained. How to relate y and x_t in eq. (16)? I think it is better to define a function to compute \\lambda and use it for explanation.\n\nPage 6, in Sec. 4.5: Although the variance schedule is fixed to \\beta_t=0.01, the range of t is not explained. \n\nPage 9, in Sec. 5.2: Figure 2 > Figure 6\n******************************************************\nOverall, I think the level of clarity is low. \n\nQuality: Although the presentation is not good, the idea itself is interesting. The implementation seems to work well and the proposed method largely outperforms the other methods. These points make the quality of the papers high.\n\nNovelty: The base idea using the diffusion model in decoding is novel. The additional ideas for implementation are also original. The level of novelty is thus high.\n\nReproducibility: Although I did not check the code, the algorithms and the learning details are explained and thus one can reproduce the result in principle. Hence there is no serious problem in reproducibility.\n",
            "summary_of_the_review": "The core idea of the paper is novel and interesting, and the experimental result of the proposed method is good. Hence, I basically vote for accepting the paper, though the presentation should be improved.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper841/Reviewer_YkMp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper841/Reviewer_YkMp"
        ]
    },
    {
        "id": "DqNe4PQiMO",
        "original": null,
        "number": 3,
        "cdate": 1667186351394,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667186351394,
        "tmdate": 1669260545508,
        "tddate": null,
        "forum": "rLwC0_MG-4w",
        "replyto": "rLwC0_MG-4w",
        "invitation": "ICLR.cc/2023/Conference/Paper841/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a novel decoding principle for error correcting codes (ECC) based on diffusion models.  The denoising diffusion probability model (DDPM) by Ho et al. can be seen as a basis of the proposed algorithm. A reverse diffusion process in DDPM is used as a decoding process of an ECC. The noise estimator neural network is conditioned on the number of parity errors that  indicates the level of corruption of the original codeword. A line search procedure is introduced for determining a proper diffusion step size. Their experimental results indicates that the proposed algorithm surpasses known state-of-the-art decoding algorithms such as belief propagation(BP). Especially, proposed algorithm has strong advantage for short length codes. ",
            "strength_and_weaknesses": "[Strength]\nI am working in the field of error correcting codes, especially for LDPC codes. As far as I know, the idea to use the DDPM as a decoding algorithm is definitely novel one.  The idea of the proposed method is natural and reasonable. The method proposed in the paper heavily depends on the idea and the DDPM architecture presented in Ho et al. but  the paper contains non-trivial contributions such as conditioning based on the Hamming weight of the syndrome and line search procedure. Furthermore, the decoding principle presented in the paper has not been discussed in the coding community. The decoding principle presented in the paper  can be a new direction for the further research in coding theory. The experimental results are promising and supports this optimistic view.\n\n[Weekness]\n(1) I felt that some details of the algorithm is not well documented in the paper (I will describe it in the next section).\n(2) Computational complexity and scalability for longer codes: I got rough understanding on a computational procedure of the decoding process. In order for the proposed method to be a real competitor of the known decoding algorithm such as BP, fair comparisons of the computational complexity will be needed.  For example, BP can be practically applied to LDPC codes of length up to 10^3-10^4. It is better to include some discussions regarding scalability. If this paper concentrates on the ``proof of the concept'', the computational complexity issue is not a large problem so far but some more discussions help the readers. \n(3) I would like to see the results of the numerical experiments on AWGN channel in graphs like Figure 4 (SNR versus BER).  It is common to   display decoding performances in such a way in coding community. It makes easier to grasp the difference in decoding performances.",
            "clarity,_quality,_novelty_and_reproducibility": "Most part of the paper is easy to read and well organized. From the attached codes, one interested in reproducing the results may be able to reproduce the results presented in the paper. One thing I am not satisfied with is the lack of description on the noise estimator network \\epsilon_\\theta. In subsection 4.5, it is described that the network \\epsilon_\\theta is based on the architecture of ECCT but no details are given in the manuscript. Thus, how to make conditioning with the syndrome weight is not so clear to understand. Of course, a reader can read DDECC.py directly for details but some more descriptions are needed.\n\n[Minor comments]\n(1) In many places, SNR is described in EbN0.  It should be Eb/N0 because it is not a product but a ratio.\n(2) The work uses the Hamming weight of syndrome as a conditioning parameter (indicating the noise level). The justification of this is heuristically explained based on Fig.3. Are there any convincing ways for this?\n(3) The line search procedure improves the overall performance but it produces additional computational overhead. The overhead should be explained.\n",
            "summary_of_the_review": "In summary, the decoding principle presented in the paper may indicate a new direction of decoding algorithms. Although I am not so sure whether the proposed algorithm is really competitive against known algorithms such as BP in a practical situation, I am inclining to be positive about this paper because it definitely contains something new from the view point of coding theory. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper841/Reviewer_8AQR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper841/Reviewer_8AQR"
        ]
    }
]