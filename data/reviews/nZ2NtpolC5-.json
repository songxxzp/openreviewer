[
    {
        "id": "lKPuq1-0Ako",
        "original": null,
        "number": 1,
        "cdate": 1666633384714,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633384714,
        "tmdate": 1668529307815,
        "tddate": null,
        "forum": "nZ2NtpolC5-",
        "replyto": "nZ2NtpolC5-",
        "invitation": "ICLR.cc/2023/Conference/Paper2936/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the learning dynamics of neural networks in the mean field limit. The authors focus on gradient descent, Hebbian learning, feedback alignment and direct feedback alignment on networks of arbitrary depth, as well as gated linear networks. For these cases we are presented with a set of self-consistent stochastic differential equations to be solved numerically.\nThe self consistent stochastic equations simplify in the static limit and the linear network case and reduce to other results in the literature.\nSpecifically, the authors obtain an explicit characterisation of the dynamics of two layer linear networks.",
            "strength_and_weaknesses": "Strengths:\n1) The paper is reasonably well-written and easy to follow.\n2) The analytical procedure to obtain a theoretical characterisation of the dynamics is easily generalisable to different learning rules and architectures beyond what is shown in the paper.\n3) The predictions of the theory are compared with experiments and the plots display for the most part quantitative agreement between theory and experiments.\n4) The authors check numerically that the large width limit is approached polynomially fast in the width of the network.\n\nWeaknesses:\n1) The role of the data on the theory is not explicitly stated in the main test.\n2) The authors propose a fixed point scheme to solve the self-consistent equations. Do you have a theoretical argument for its convergence and some bounds on the number of iterations required?\n3) In many plots (for example figure 3a, blue line) there is a small disagreement between theory and experiments. What could be the reason for the discrepancy?\n4) While the numerical checks of the finite width effects are very convincing, the theoretical argument for the finite width expansion seems not so transparent. Can you plot the finite width corrections to the theory? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The analytical results of this paper are an interesting tool to probe the learning dynamics of neural networks and the derivation is easily understandable and could be adapted with minimal change to different learning rules. While this computation is essentially the same as Bordelon & Pehlevan '22 here it's applied to a number of learning rules, so it's a significant contribution on its own.\n\nThe authors choose to call their analytical result dynamical mean field theory (DMFT in short). This name is slightly confusing as there are other works claiming to have DMFT for neural networks, where here DMFT is intended as an effective theory that maps the high dimensional dynamics of the weights to the low dimensional evolution of a set of order parameters. A modern derivation of this kind was done by Agoritsas et al. '17 (Arxiv:1710.04894) for the perceptron and then expanded by Mignacco et al. '20 (Arxiv:2006.06098). This approach was also recently proven rigorously by Celentano et al. '21 (Arxiv:2112.07572) and Gerbelot et al. '22 (Arxiv:2210.06591).\nThe main difference between this line of works and the paper at hand is in the treatment of the data, and while this is clear from the derivations it would be clarifying to point it out more explicitly in the main text.\n\nThe authors provide a code in the supplementary material reproducing the figures in the paper, and the numerical method proposed seems to converge without significant issues.",
            "summary_of_the_review": "The paper presents a set of self-consistent stochastic differential equations that describe the learning dynamics of deep networks for a number of learning rules. The paper is well-written and clearly details the analytical derivation as well as the numerics involved. The results are easily reproducible and a code is provided in the supplementary materials. \n\nWhile the treatment of the dynamics and the phenomenology extracted from it are not completely novel it's a significant contribution that could be expanded in many directions",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2936/Reviewer_BwSf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2936/Reviewer_BwSf"
        ]
    },
    {
        "id": "4WgssX47JX",
        "original": null,
        "number": 2,
        "cdate": 1666648693246,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648693246,
        "tmdate": 1668569159596,
        "tddate": null,
        "forum": "nZ2NtpolC5-",
        "replyto": "nZ2NtpolC5-",
        "invitation": "ICLR.cc/2023/Conference/Paper2936/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors investigate an important and open question: how does the the learning rule (gradient descent, Feedback alignment, Direct Feedback Alignment, error-modulated Hebbian learning, Gated linear networks) influence the learning dynamics during neural network training. The authors focus on wide neural networks, and study these using dynamical mean field theory. Their approach allows them to make statements about the learning dynamics with different learning rules. Some of these statements are expected (but still good to formally analyze), e.g. that error-modulated Hebbian learning does not enable effective learning. Other statements are more surprising (e.g. that smaller initial feedback alignment later leads to better feature learning in the Feedback Alignment method). ",
            "strength_and_weaknesses": "# Strengths:\n\n- The paper is relevant to a big portion of the ICLR community, and provides theory and insights that may result in follow-up research, and possible better learning rules. \n- The papers release their code in the supplementary material. This includes code to generate the graphics included in the paper. \n- The paper is well-written. \n\n# Weaknesses:\n\n- The empirical work done in the paper is on fairly small tasks, considering that many of the learning rules considered in the paper are supposed to work on networks as complex as the human brain. \n\n- Some of the paper\u2019s insights are not very surprising. For example, the fact that Hebbian learning converges slowly, or that it does not closely follow the gradient, are generally accepted as true in the community. \n",
            "clarity,_quality,_novelty_and_reproducibility": "# High-level remarks\n\n- The paper is generally well-written, though some portions are hard to follow. The authors can probably improve the concerned lines of text easily. Overall, the paper is structured well. \n\n- The authors include Jupyter notebooks with their experiments and the figure-generating code. This should make the work easy to reproduce. \n\n\n# Detailed questions/suggestions\n\n- In Equation (2), is there an extra gamma_0 that should not be there? \n- In equation (3): why is there $\\dot\\phi$ in the line for GD? From the paragraph above equation (2), it seems that $\\dot\\phi$ is only used for GLNs. \n- In equation (5), the authors use $z_\\mu^l(t)$, which I believe is the pre-gradient signal from page 2 of https://arxiv.org/pdf/2205.09653 . Could the authors introduce this new symbol in their text as well, just before or after equation (5)?\n\n- Figure (1) is for a 2-layer neural network. How well would DMFT approximate learning if the network were deeper?\n- \u201cGausssian \u201d should be \u201cGaussian \u201d\n\n- The end of this sentence in the appendix is missing: \u201cthis learning rule will continue to update the weights even once the task is fully learned, leading \u201d\n\n- Equation (9) in Appendix B.1 needs a period at the end, as do a few other equations in the appendices. \n\n",
            "summary_of_the_review": "The paper is relevant to a big portion of the ICLR community, and provides theory and insights that will likely result in follow-up research, and possible better learning rules. \n\nWhile the paper is generally well-written, the authors should make small fixes/improvements during the rebuttal period. \n\nIn my opinion, the biggest weakness of the paper is that many of the findings that the paper works out are unsurprising. The authors could strengthen the paper by highlighting which of the findings are \"common knowledge\", but have not yet been investigated formally. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2936/Reviewer_ZSh1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2936/Reviewer_ZSh1"
        ]
    },
    {
        "id": "tzTJxpOBIre",
        "original": null,
        "number": 3,
        "cdate": 1666691581203,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691581203,
        "tmdate": 1666691581203,
        "tddate": null,
        "forum": "nZ2NtpolC5-",
        "replyto": "nZ2NtpolC5-",
        "invitation": "ICLR.cc/2023/Conference/Paper2936/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies several biologically (more) plausible alternatives to backprop in the lazy/rich NTK regime. This is done using the recent developments in dynamic mean field theory for backprop, and produces non-trivial predictions for activity of those learning rules in wide networks.",
            "strength_and_weaknesses": "### Strengths\n\nThe paper clearly theoretical differences between solutions found by different learning rules.\n\nThe theoretical analysis of learning rules' dynamics is important for determining how the brain learns.\n\nThe calculations are written in detail and can be repeated for other learning rules.\n\n### Weaknesses\n\nThe paper seems to be a straightforward application of [Bordelon & Pehlevan, 2022] to other learning rules.\n\nIt's not clear to me if the rich NTK regime contributes to any of the final conclusions.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nThe paper is hard to read due to the density of theoretical results, but it seems inevitable given the nature of the work. On the upside, the key take-home messages of the theory are clearly formulated.\n\n### Quality \n\nThe technical side of the paper seems solid (although I haven't carefully checked the appendix). The experiments are standard for an NTK paper.\n\nQuestion 1: how width-dependent are the specific results on non-SGD rules (e.g., for feedback alignment)? Fig. 1e-f and Fig. 3d show that the experimental kernels more or less match the DFMT ones for large networks, but would the *structure* (rather than the precise match) change for e.g. N=100? If yes, would that imply different predictions for networks far from the rich NTK regime?\n\nQuestion 2: are any of the conclusions require the rich training regime? $\\rho$-FA result is highlighted as being rich regime-specific, but it seems that the lazy regime already tells us that $\\rho=0$ results in less feature learning than non-zero $\\rho$. \n\n### Novelty\n\nMath-wise, the paper seems to be a straightforward (but interesting) extension of [Bordelon & Pehlevan, 2022]. Are there other technical contributions besides re-deriving DFMT for alternative learning rules?\n\nThere's one very relevant background citation worth mentioning (doesn't change the contribution):\nCharacterizing emergent representations in a space of candidate learning rules for deep networks\nY Cao, C Summerfield, A Saxe\n\n### Reproducibility\nThe code is provided but I did not run it.\n",
            "summary_of_the_review": "Good paper with a mostly technical contribution. Some parts could be improved/better explained, but overall this could be an important result for biologically plausible learning studies.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2936/Reviewer_k9iv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2936/Reviewer_k9iv"
        ]
    },
    {
        "id": "5qOss1NzeKf",
        "original": null,
        "number": 4,
        "cdate": 1666854839832,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666854839832,
        "tmdate": 1666854839832,
        "tddate": null,
        "forum": "nZ2NtpolC5-",
        "replyto": "nZ2NtpolC5-",
        "invitation": "ICLR.cc/2023/Conference/Paper2936/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the learning dynamics of infinite-width networks trained using various biologically plausible variations to gradient descent, including feedback alignment, direct feedback alignment, error modulated Hebbian learning, and gated linear networks. The paper derives an effective Neural Tangent Kernel (NTK) that governs the learning dynamics. Through theory and simulation, the paper identifies a number of interesting phenomena about the learning dynamics of these biologically plausible learning rules.",
            "strength_and_weaknesses": "Strengths:\n- Nice use of the theory of infinite width neural networks to study learning dynamics in networks trained with variants of gradient descent.\n- Identifies a number of interesting phenomena regarding similarities and differences between the representations learned using the different learning rules.\n\nWeaknesses:\n-  Analysis is limited to MLP networks",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is clearly written. To my knowledge the contributions are novel.",
            "summary_of_the_review": "Overall, I think this paper is an important contribution in our understanding of the learning dynamics of networks trained with biologically plausible (or at least biologically inspired) learning rules. Feedback Alignment and subsequent variants are an intriguing learning rule because they seem so far from gradient descent; this paper helps us understand when and why they work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2936/Reviewer_6oec"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2936/Reviewer_6oec"
        ]
    }
]