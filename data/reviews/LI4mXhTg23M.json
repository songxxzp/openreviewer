[
    {
        "id": "EGim7EiYtfo",
        "original": null,
        "number": 1,
        "cdate": 1666195830478,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666195830478,
        "tmdate": 1666255102391,
        "tddate": null,
        "forum": "LI4mXhTg23M",
        "replyto": "LI4mXhTg23M",
        "invitation": "ICLR.cc/2023/Conference/Paper970/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed an **autoencoder** architecture based on **trigonometric functions** for **unsupervised disentangled representation learning**, introduced a new disentanglement metric (based on a strong assumption), and evaluated the proposed architecture on six datasets.",
            "strength_and_weaknesses": "## Strengths\n\n- Unsupervised disentanglement is an important problem.\n- The author used the algebraic definition of disentanglement.\n- The author compared the proposed architecture with many autoencoder-based methods.\n\n## Weaknesses\n\n- The author tried to support the proposed method with theoretical guarantees, but the theorems seem not to guarantee the learned representation is disentangled.\n- The proposed metric, Grid Fitting Score, relies on the assumption that \"performing independent disentangled actions on a symmetry group causes the corresponding subspace to form a grid-shape latent space\", which may not hold (details below).\n- The groups and group actions in experiments are not given, so the experimental settings are a bit questionable.\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity and Quality\n\n### Euler encoding\nFor starters, the name \"Euler encoding\" is confusing. Where does \"Euler\" come from? The closest thing I can think of is Euler's formula in complex analysis. However, the complex exponential function does not appear here. This term was not used in previous work (~164 results from Google), either. If the author coined this term, they should explain why it relates to Euler.\n\nLeaving aside the name issue, it is unclear why the author chose this trigonometric function form. The author stated that the purpose of introducing Euler encoding was \"a linear disentangled representation is achieved if changes of components are independent of one another\" (inaccurate statement, by the way). However, it is unclear which properties of cos/sin functions can lead to this. The group is not always cyclic, and the latent variable should not necessarily be bounded.\n\n### Disentanglement\n\nThe author cited [Locatello et al. 2019] to support the statement, \"it is fundamentally impossible to learn disentangled representations without having inductive biases\". However, it is unclear how their results can be applied here because their work used a different definition of disentanglement, but this work only focuses on the algebraic aspect.\n\n### Group and group action\n\nIt is nice that the author used the algebraic definition of disentanglement. However, I do not think the author ever precisely stated the \"clear underlying group structure\" anywhere (including the appendix), e.g., which group and group actions were used for each attribute. For example, does the author use a cyclic group for the X/Y position, or do they have some special treatment? Is the group for the shape a cyclic group or an arbitrary permutation group? For 3D objects, was $O(3)$ or $SO(3)$ used? It is hard to evaluate the experimental settings and results without such information.\n\n### Theorems\n\nIt is nice that the author tried to analyze the methodology theoretically. However, the theorems are verbose, and I did not see how they can guarantee the proposed model can successfully disentangle the data in an unsupervised fashion.\n\nTheorem 3.1 basically says the composition of equivariant maps is equivariant. This is well-known, but there is a hidden condition. A map is equivariant to two specific group actions, and a group can act on a set in many ways. Say, $b: W \\to O$ is equivariant to actions $\\mathrm{act}_1$ on $W$ and $\\mathrm{act}_2$ on $O$, and $h: O \\to Z$ is equivariant to actions $\\mathrm{act}_3$ on $O$ and $\\mathrm{act}_4$ on $Z$. $f : W \\to Z := h \\circ b$ is equivariant to $\\mathrm{act}_1$ and $\\mathrm{act}_4$ if $\\mathrm{act}_2$ and $\\mathrm{act}_3$ on $O$ are the same.\n\nThen, the paragraphs in p.4 below Theorem 3.1 basically say, \"$\\mathrm{id}_G$ + an isomorphism $\\leadsto$ isomorphic group actions\". I do not see how it supports the statement \"equivariant map can be learned by an autoencoder\" (no matter how the group and group actions are chosen).\n\nLastly, if I didn't misunderstand anything, we can prove Theorem 3.2 by saying, since $T_i^\\alpha: \\\\{1, \\dots, n\\\\} \\times (0, 1) \\to Z$ as a function of $i$ and $\\alpha$ ($Z \\subset \\mathbb{R}^n$ is the hypercube), $E: Z \\to \\mathbb{R}^{2n}$, and $A: \\mathbb{R}^m$ are injective, $A \\circ E \\circ T_i^\\alpha$ must be injective. Again, I do not see how it is related to disentanglement.\n\n### Grid\n\nI think the statement \"ideal learned latent space should cover a two-dimensional grid\" in Figure 1 caption and the assumption \"performing independent disentangled actions on a symmetry group causes the corresponding subspace to form a grid-shape latent space\" are unjustifiable. Not all group action/representation leads to this. For example, how do you get a group representation of $C_3$ or $D_4$ on a grid (let alone linear)?\n\n### Other issues\n\n- \"*To the best of our knowledge, this is the first* deterministic model that <a long description of the model>\" sounds redundant. It's called plagiarism if the author already knows they are not the first.\n- The author should follow \"Formatting Instructions for ICLR 2023 Conference Submissions\" and learn to use $\\verb|\\citet{}|$ and $\\verb|\\citep{}|$ (Section 4.1).\n- \"The quality of the disentanglement is fully reliant on ideal or near-ideal priors\" has no support.\n- It's unclear why \"values smaller than unity are replaced with hyperparameter $\\alpha$\" and what \"unity\" means.\n- Does \"Gaussian interpolation is used to map unseen examples to known examples\" mean that the model has completely no generalization ability?\n- The meaning of the interpolation layer (Algorithm 2) and \"weight-sensitive Gaussian noise\" is unclear.\n\n## Novelty\n\nTo the best of my knowledge, the so-called \"Euler encoding\" (Eq. 6) is novel. Other aspects of the model exist in prior work.\n\n## Reproducibility\n\nThe author provided a brief review of the encoder/decoder architectures but did not provide any training details, such as hyperparameters. No code was found.",
            "summary_of_the_review": "The author tried to solve a challenging problem (unsupervised disentanglement) and conducted abundant experiments. However, the grid assumption is unreliable, their theory does not support the proposed method, and the detailed group theoretical treatment in experiments is unclear. Therefore, I cannot recommend this paper for acceptance to ICLR.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper970/Reviewer_LTGT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper970/Reviewer_LTGT"
        ]
    },
    {
        "id": "gxSgiRBD5y",
        "original": null,
        "number": 2,
        "cdate": 1666632162203,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632162203,
        "tmdate": 1666632162203,
        "tddate": null,
        "forum": "LI4mXhTg23M",
        "replyto": "LI4mXhTg23M",
        "invitation": "ICLR.cc/2023/Conference/Paper970/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a non-probabilistic approach to use autoencoder for learning disentanglement in unsupervised manner. It also proposes a new metric to quantify the disentanglement. ",
            "strength_and_weaknesses": "The paper explains the new framework in detail and compares against other autoencoder models on a range of disentanglement metrics.\nAt the same time, all the comparisons are against other autoencoder models and ignores other types of unsupervised deterministic models e.g. \nLearning the Multilinear Structure of Visual Data\nM Wang, Y Panagakis, P Snape, S Zafeiriou\nCVPR 2017\nThe current formulation does not cover the case of disentanglement with missing data.\nEvaluations are based purely on disentanglement metrics and it is not clear how much difference in downstream tasks some gains achieve. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and seems to propose a novel method. ",
            "summary_of_the_review": "Overall the paper proposes a novel framework to address the problem of disentanglement with unsupervised data. Though the paper is overall sound, the evaluation could be improved by highlighting the differences achieved on some downstream tasks as it's unclear whether the gains are significant. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper970/Reviewer_9cKh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper970/Reviewer_9cKh"
        ]
    },
    {
        "id": "4Xu4f6KrZs",
        "original": null,
        "number": 3,
        "cdate": 1666678828373,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678828373,
        "tmdate": 1666678828373,
        "tddate": null,
        "forum": "LI4mXhTg23M",
        "replyto": "LI4mXhTg23M",
        "invitation": "ICLR.cc/2023/Conference/Paper970/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a deterministic VAE for unsupervised disentanglement learning with Euler encodings. By introducing an architectural inductive bias with the Eular layer, this approach can learn disentanglement without using independence regularization terms or additional supervisions. In addition, the authors present several ways to tackle practical issues with PCA, min-max normalization, and Gaussian interpolation. An interesting disentanglement metric that does not need label information is also proposed. Experiments were conducted on several benchmarks with different metrics.",
            "strength_and_weaknesses": "Strength\n- I think designing good explicit inductive biases is particularly important for unsupervised disentanglement learning. The architectural design that enforces the neworks to learn disentangled features can resolve many difficulties in previous methods: For example, the necessity of independence-inducing regularization loss (which can cause the tradeoff between disentanglement and reconstruction) can be removed. In this aspect, I believe this work addressed an important problem and did a good job.\n- I feel the Euler encoding is technically sound and the mathematical notations and derivations are quite solid.\n- The experiments are comprehensive in terms of the number of datasets and evaluation metrics.\n- The paper is well backed-up by supplementary material that contains sufficient details of the method and experiments and presents additional results.\n\nWeakness\n- I was not able to find ablation studies and/or in-depth discussions to argue the effectiveness of the three elements described in Section 3.3. For example, how did the authors set the hyperparameter alpha in Algorithm 1 and what happens with varying the value of alpha? What happens if different normalization schemes are used or any normalization is not applied? Is the Gauassian interpolation layer indeed necessary? It would be very helpful to add experimental/theoretical support regarding them.\n- The architectures used in the experiments consist of vanilla convolutional and transposed convolutional layers with relatively shallow depths. I feel they are a bit outdated; I would recommend the authors to include experimental results using the networks with residual blocks and deeper depths.\n- My major concern is the applicability for real-world datasets; the proposed method was validated melely on simulated datasets. Although many unsupervised disentanglement algorithms also face this issue regarding their practical usage, it would be very helpful to present visual disentanglement results on CelebA or other real-world images to show the potential of the proposed method.",
            "clarity,_quality,_novelty_and_reproducibility": "- The idea that introduces the Eular layer for unsupervised disentanglement learning is interesting and quite novel.\n- The paper is clearly written and easy to follow.\n- Ablation studies and detailed analyses are missing.\n- Although the experimental validation is quite good, the applicability for real-world data is not demonstrated.\n",
            "summary_of_the_review": "The main idea is quite good and this paper includes several interesting points. However, the in-depth analyses of the proposed method are missing and thus the sources of good performance are not identifiable. Furthermore, the empirical support is weak in terms of the applicability for real-world data. I thus find it difficult to argue for acceptance of the work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper970/Reviewer_T3ur"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper970/Reviewer_T3ur"
        ]
    },
    {
        "id": "yr86g6mRzjL",
        "original": null,
        "number": 4,
        "cdate": 1667401920960,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667401920960,
        "tmdate": 1667401920960,
        "tddate": null,
        "forum": "LI4mXhTg23M",
        "replyto": "LI4mXhTg23M",
        "invitation": "ICLR.cc/2023/Conference/Paper970/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the author proposed a new perspective for learning disentangled representation. The Euler encoding is introduced to force the latent space to achieve a linear disentangled representation. What's more, the author adopts the PCA, normalization layer, and Gaussian interpolation to address the issues caused by the Euler encoding. The author also designed a new metric for comparing the disentanglement of the representation. The proposed method is validated on several toy datasets.\n ",
            "strength_and_weaknesses": "\nStrengths:\n1. The author proposed a new perspective for learning disentangled representation, which introduces the Euler encoding to force the latent space to achieve a linear disentangled representation.\n2. The author provides a detailed theoretical analysis.\n3. The proposed method achieves good results on toy datasets.\n\n\nWeaknesses:\n1. Some classic metrics are not adopted to measure the differences between the proposed method and SOTA methods, such as Disentanglement, Completeness, Informativeness\uff0c et al.  [1]\n2. Some related multi-dimensional disentangling methods (which are not probabilistic)  with AE as backbone are not included [2,3].\n3. The proposed method is only validated on the toy / synthetic dataset. The author is suggested to add experiments on natural image datasets.\n4. Some typos, such as ' of each of', 'commutative Lie groups'\n5. I am concerned that the proposed method will fail on natural images. The reasons are concluded as follows: the PCA achieves poor results on complicated nature images; The nature images contain more noise.\n\nReferences:\n[1]A Framework for the Quantitative Evaluation of Disentangled Representations[J]. International Conference on Learning Representations, 2018.\n[2]Dual Swap Disentangling, NIPS 2018.\n[3]Image-to-image translation for cross-domain disentanglement, NIPS 2018.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well written and clear. The idea of using Gram matrices to do the clustering in the first state is novel and also leads to improved performance. The author also provides a detailed  theoretical analysis.\n",
            "summary_of_the_review": "Overall, the ideal is novel. The author provided a new insight for representation learning with Euler encoding.\nHowever, the experiments on the nature image dataset are not given. Some classic metrics are not adopted in the comparison experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper970/Reviewer_JNBF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper970/Reviewer_JNBF"
        ]
    }
]