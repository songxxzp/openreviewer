[
    {
        "id": "r-ppv_tj3e1",
        "original": null,
        "number": 1,
        "cdate": 1666381953218,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666381953218,
        "tmdate": 1670299747351,
        "tddate": null,
        "forum": "xWutyHiLtwP",
        "replyto": "xWutyHiLtwP",
        "invitation": "ICLR.cc/2023/Conference/Paper917/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper presents a new fairness metric that can measure fairness for multiple sensitive attributes of any type. The author also developed a learning algorithm by using their fairness criteria as a regularization term.",
            "strength_and_weaknesses": "The paper aims to solve an important problem which is ensuring fairness for any number and any type of protected attributes.\n\nBut the paper suffers from lots of weaknesses:\n- The multiple protected attribute-based fairness notions are not new. There are several existing works on this as follows: Multicalibration (U. Hebert-Johnson et al, 2018), subgroup fairness (M. Kearns et al, 2018), and intersectional fairness (J. Foulds et al, 2020). The paper doesn't motivate enough why we need a new multiple attribute-based fairness metric?\n- The proposed learning algorithm is also not new. Adding fairness metric as a regularization term with a trade-off parameter is the standard approach in many existing fair ML work. For example, Foulds et al, 2020 developed a backpropagation-based learning algorithm with the regularization term, designed using intersectional fairness criteria. \n- Regarding the discrete and continuous outcomes. Agarwal et al 2018 proposed a reduction-based constrained optimization method that can ensure fairness in classification task for single or multiple protected attributes. Furthermore, they extended their reduction based method for regression task (Agarwal et al 2019) with fairness metric for continuous outcomes.\n- I am very surprised that authors didn't compare/analyze their proposed metric and learning algorithms with any of the multiple protected attribute-based existing fair ML works in their experiments. This is the biggest weakness of the paper.          \n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is somewhat clear, but some important details are missing or unclear. There are enough resources and details to reproduce the results. But the main ideas of the paper are not novel or have limited novelty. Furthermore, provided experimental analysis is not enough to support the claim. ",
            "summary_of_the_review": "See the comments in the \"weakness\" section. \n\n..........................................\nI thank the authors for explaining some of the concerns. However, I still think that the novelty of the paper is limited and not enough for ICLR. However, after reading other reviewers comments and authors feedback, I've decided to increase my overall rating.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper917/Reviewer_iTEE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper917/Reviewer_iTEE"
        ]
    },
    {
        "id": "FtCNch-dFjN",
        "original": null,
        "number": 2,
        "cdate": 1666843293585,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666843293585,
        "tmdate": 1666843389697,
        "tddate": null,
        "forum": "xWutyHiLtwP",
        "replyto": "xWutyHiLtwP",
        "invitation": "ICLR.cc/2023/Conference/Paper917/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes to practical method called FairCOCCO that can incorporate multitype/multivariation sensitive attributes for fairness-aware learning. The method simply develops the kernel-based measure of fairness and uses it as a regularizer for learning a predictive model. The experimental results seem promising, but the comparing baselines are relatively old. \n",
            "strength_and_weaknesses": "Strength\n- The paper proposed a novel in-processing based fairness-aware learning method based on FairCOCCO, which can be computed via using kernel based conditional cross covariance operator. \n- The experiments show that the method can be applied in various setting that involve multi-variate, continuous-valued sensitive attributes. \n- It outperforms several baselines on multiple settings. \n- Detailed analyses are given in Supplementary. \n\nWeakness\n- Some recent baselines are missing, e.g., \n [Jung et el.,  Learning fair classifiers with partially annotated group labels, CVPR 2021]\n[Quadrianto et al., Discovering fair representations in the data domain. CVPR 2019]\n[Jiang et al., Identifying and correcting label bias in machine learning. AISTATS 2020]\n- Result on COMPAS is a bit dubious -- DEO is 0.00??\n- While the result on the relaxation on computing COCCO score is given in the appendix, what is the actual computational complexity? Fairly comparing with the CPU time would be also beneficial. \n- Some more results on vision dataset such as UTKFace would be also helpful. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly. ",
            "summary_of_the_review": "Overall, while the proposed method is simple, the experimental results seem promising. The actual computational complexity comparison would be helpful for making the final decision. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper917/Reviewer_V8La"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper917/Reviewer_V8La"
        ]
    },
    {
        "id": "T5ISjojNuw",
        "original": null,
        "number": 3,
        "cdate": 1667431229774,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667431229774,
        "tmdate": 1667431229774,
        "tddate": null,
        "forum": "xWutyHiLtwP",
        "replyto": "xWutyHiLtwP",
        "invitation": "ICLR.cc/2023/Conference/Paper917/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses the extension of existing fairness notions in ML such as Demographic Parity, Equal Opportunity, and Calibration to settings where the both the target and sensitive attributes are potentially multivariate and continuous.\n\n It proposes a metric (FairCOCCO) based on the cross covariance operator over reproducing kernel Hilbert spaces that can be used to both measure and regularize existing ML systems for fairness-aware learning, and shows comparable or better results over existing methods on multiple real-world datasets.",
            "strength_and_weaknesses": "Strengths:\n\nThe paper does a good job on explaining the merits of the cross covariance operator as a viable tool to extend DP, EO and Calibration to multivariate and potentially continuous target and sensitive attributes. The presentation is solid, the regularization objective is well motivated, and the experimental results  show improvement over the examined baselines.\n\nWeaknesses:\n\nI think the novelty of the work is somewhat limited. The main theoretical results presented in the paper are taken directly from Fukumizu 2007, though the application to fairness metric is, to my knowledge, novel.\n\nSome key implementation details are shown in the appendix, rather than discussed in the main paper, such as the closed form expression for FairCOCCO in Appendix A, which also seems taken from Fukumizu 2007. \n\nUpon seeing the discussion on Appendix C.2 on the impact of batch size on the FairCOCCO measure, I would like to ask the authors if comparable hyper parameter optimization was also performed for the competing baseline methods. I perhaps missed a similar analysis on how important the regularization strength term $\\lambda$ was to the final results.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, there is enough detail on the proposed method to reproduce the proposed method, though some details such as hyper-parameter tuning on regularization strength seem to be omitted.\n\nThe novelty of the paper is somewhat limited, since it seems to be a straightforward application of Fukumizu 2007 in the context of fairness. However, the proposition seems solid and interesting.",
            "summary_of_the_review": "I think the paper has merit, and proposes a well-motivated and grounded application of existing literature to easily extend existing fairness definitions into multivariate target and attribute scenarios. The main drawback of the paper seem to be the relatively equivalent results to existing methods, and a potential lack of technical novelty.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper917/Reviewer_yfN2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper917/Reviewer_yfN2"
        ]
    },
    {
        "id": "QlZ-cnldsfD",
        "original": null,
        "number": 4,
        "cdate": 1667446362620,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667446362620,
        "tmdate": 1667446362620,
        "tddate": null,
        "forum": "xWutyHiLtwP",
        "replyto": "xWutyHiLtwP",
        "invitation": "ICLR.cc/2023/Conference/Paper917/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new fairness metric, FairCOCCO SCORE, that can quantify unfairness among groups are distinguished based on multiple sensitive attributes of different types. It measures the unfairness in reproducing kernel Hilbert space based on cross-covariance operators. Based on this measure, the paper further proposes a regularization term that can be used for learning fair predictors.\n",
            "strength_and_weaknesses": "Strength:\n1. Unlike most existing studies that focused on measuring/remedying unfairness against a single binary sensitive attribute, this paper focuses on the more refined (sub-)groups that are potentially distinguished based on multiple, continuous sensitive attributes  \n2. The paper is well-organized, and the literature review seems to be extensive.\n3. The paper conducted extensive experiments on multiple datasets and compared the performance of the proposed method with many existing algorithms.  \n\nWeakness: \n1. The paper proposes a new kernel method-based fairness measure. However, there is no theoretical support to justify why the proposed measure is better than the existing measures. Its performance is only evaluated empirically in experiments. \n3. On the comparison in experiments: the authors compared the proposed FairCOCCO with other methods on various datasets. Many results are competitive, and it is not obvious that FairCOCCO outperforms other methods. For example, in Table 9, the bolded results are not the best, and the accuracy and fairness are very similar under different algorithms. ",
            "clarity,_quality,_novelty_and_reproducibility": "1. On clarity: the paper is well-written and easy to follow. \n2. On novelty: the problem of measuring unfairness against continuous sensitive attributes is not new and has been studied in the literature. Using kernel-based methods to measure unfairness has also been explored before. As far as I can tell, the main contribution of the paper is the FairCOCCO SCORE, which is an extension of COCCO and conditional fairness (Lemma 3.1) proposed by Fukumizu et al. 2007. The regularization term that incorporates FairCOCCO SCORE for fairness-aware learning is directly adopted from the existing fair learning method, i.e., using Lagrangian relaxation to transfer constrained optimization into unconstrained optimization.  \n",
            "summary_of_the_review": "While the paper proposes a new fairness measure that is applicable to settings where groups are distinguished by (continuous) multiple sensitive attributes, it is not clear to me why the proposed measure is better than the existing unfairness measures that are also applicable to continuous sensitive attributes.   ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper917/Reviewer_stjQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper917/Reviewer_stjQ"
        ]
    }
]