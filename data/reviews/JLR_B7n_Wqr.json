[
    {
        "id": "rYvg518qSG",
        "original": null,
        "number": 1,
        "cdate": 1665984238465,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665984238465,
        "tmdate": 1670380715683,
        "tddate": null,
        "forum": "JLR_B7n_Wqr",
        "replyto": "JLR_B7n_Wqr",
        "invitation": "ICLR.cc/2023/Conference/Paper4381/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to address the latent graph inference problem, i.e., inferring the intrinsic graph structure from point cloud-like data where connection is not available in the original data. Based on the discrete Differentiable Graph Module (dDGM) proposed by previous work, this paper extends dDGM with Riemannian geometry by calculating distance (which is used for edge modelling) in not only Euclidean space but also hyperboloid/hypersphere spaces. Extensive experiments are conducted to show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "### Strengths:\n\nS1: A great number of experiments are conducted. \nThe authors conduct the experiments on various of datasets, including homophilic, heterophilic, and real-world application-based datasets. Visualization experiments are also used to exhibit the quality of the learned graph structure.\n\nS2: The paper is well presented and written.\nThe definitions and notations in the paper are explicit. The proposed method is clearly described. The paper is written in good quality.\n\n### Weaknesses:\n\nW1: The novelty of this paper is limited.\nAs is described by the authors, the proposed method is based on dDGM (Kazi et al. 2022). The main difference between dDGM and the proposed method is that, dDGM mainly learns on Euclidean, while the proposed method learns on both Euclidean and other non-constant curvature spaces. However, in Section 5.1.2 and Table 3 of paper (Kazi et al. 2022), this paper already discusses the potential of applying dDGM to non-Euclidean geometry latent space and conducts an experiment on Hyperbolic space. Such a previous discussion makes the contribution of this paper less significant. In this case, the authors are suggested to better clarify the novelty and contribution of this paper. \n\nW2: Some critical related works lack in the literature review.\nThis paper focuses on learning latent graph structure from raw data, which is described as \u201cthe latent graph inference problem\u201d in this paper. However, in the community of GNNs, there is a branch of works aims to address \u201cgraph structure learning problem\u201d which has a consistent learning target with this paper. In this case, it\u2019s of great significance to review the related papers about graph structure learning and discuss the similarity/difference between the proposed method and these works. For this topic, the authors are suggested to refer recent survey [1] and some classic methods (e.g., LDS-GNN [2], IDGL [3], and ProGNN [4]). \n\n[1] Zhu, Yanqiao, et al. \"Deep graph structure learning for robust representations: A survey.\" arXiv preprint arXiv:2103.03036 (2021).\n\n[2] Franceschi, Luca, et al. \"Learning discrete structures for graph neural networks.\" International conference on machine learning. PMLR, 2019.\n\n[3] Chen, Yu, Lingfei Wu, and Mohammed Zaki. \"Iterative deep graph learning for graph neural networks: Better and robust node embeddings.\" Advances in neural information processing systems 33 (2020): 19314-19326.\n\n[4] Jin, Wei, et al. \"Graph structure learning for robust graph neural networks.\" Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 2020. \n\nW3: The experiments seriously lack baselines for comparison.\nIn the experiments, the proposed method is only compared to MLP and GCN, making the results not convincing enough. Readers may be curious about how the proposed method performs compared to state-of-the-art methods. At least, the authors should add the original dDGM for comparison (is that equal to GCN-dDGM-E?). Moreover, some representative reviewed methods (e.g., DGCNN and DGMs) and the graph structure learning methods (listed in W2) should also be considered.\n",
            "clarity,_quality,_novelty_and_reproducibility": "As is discussed in S2, the paper has good clarity and quality. In specific, the research problem and proposed method is well formulated. The paper is well written.",
            "summary_of_the_review": "Clarity/Quality:\n\nAs is discussed in S2, the paper has good clarity and quality. In specific, the research problem and proposed method is well formulated. The paper is well written.\n\nNovelty:\n\nAs is discussed in W1, the novelty of this paper is limited. In specific, compared to the basic model dDGM, the technical improvement of the proposed method is relevantly minor. \n\nReproducibility:\n\nThis paper has great reproducibility. The experimental details are described. The source code is not provided.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4381/Reviewer_R63Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4381/Reviewer_R63Y"
        ]
    },
    {
        "id": "y6974tEc5W",
        "original": null,
        "number": 2,
        "cdate": 1666741285493,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666741285493,
        "tmdate": 1666741285493,
        "tddate": null,
        "forum": "JLR_B7n_Wqr",
        "replyto": "JLR_B7n_Wqr",
        "invitation": "ICLR.cc/2023/Conference/Paper4381/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper generalizes discrete Differentiable Graph Module (dDGM) to learn latent graphs in the product space of Euclidean planes, hyperboloids and hyperspheres of constant curvature. The learned graphs are then used for graph neural networks. Experiments are done on heterophilic and homophilic datasets, as well as some real-world data without a graph given as a prior.",
            "strength_and_weaknesses": "Strength: Modeling data points on different manifolds to embed more complicated relationship information into the graph is an interesting idea, and seems to work well for experiments with heterophilic datasets. Visualization of latent graph evolution in Figure 2 is intriguing. \n\nWeakness: Notations used in section 3.1. can be hard to understand. In section 4.2., if the data doesn't come with a graph and are just pointclouds, why would one decide to use GNNs in the first place?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written, and the authors created a new PyTorch layer to help with reproducibility.",
            "summary_of_the_review": "It's an interesting approach to latent graph learning that builds on prior work on differentiable Riemannian manifolds. The experimental results are promising.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4381/Reviewer_vGZV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4381/Reviewer_vGZV"
        ]
    },
    {
        "id": "Ug_EBcDRFbi",
        "original": null,
        "number": 3,
        "cdate": 1666850124425,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666850124425,
        "tmdate": 1666850124425,
        "tddate": null,
        "forum": "JLR_B7n_Wqr",
        "replyto": "JLR_B7n_Wqr",
        "invitation": "ICLR.cc/2023/Conference/Paper4381/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed to generalize the discrete Differentiable Graph Module (dDGM) for latent graph learning, by incorporating the Riemannian geometry into the model and generating more complex embedding spaces. The authors further propose a computationally tractable approach to produce product manifolds of constant curvature model spaces that can encode latent features of varying structures. The numerical results demonstrate that the proposed method outperforms the original dDGM model.\n\n\n",
            "strength_and_weaknesses": "Strength:\n\n1. The authors provide a novel method to generalize the discrete differentiable graph module for learning latent graphs.\n\n2. It is interesting to incorporate Riemannian geometry into the model to generate more complex embedding spaces. The resultant performance is improved. \n\nWeakness:\n\n1. It is unknown the scalability of the proposed method.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-organized. The novelty is sufficient and the contributions are significant.",
            "summary_of_the_review": "The authors present a new method to generalize the discrete differentiable graph module for learning latent graphs, which could be interesting to people working on graph neural networks. Such a method could provide a way to learn a better graph topology, which helps to improve the performance of graph neural networks in the downstream task. One concern to be solved is the scalability of the proposed method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4381/Reviewer_DMwU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4381/Reviewer_DMwU"
        ]
    }
]