[
    {
        "id": "dAkpqUuFXEj",
        "original": null,
        "number": 1,
        "cdate": 1665836303543,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665836303543,
        "tmdate": 1669628520070,
        "tddate": null,
        "forum": "b0RuGUYo8pA",
        "replyto": "b0RuGUYo8pA",
        "invitation": "ICLR.cc/2023/Conference/Paper4840/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies transfer learning for tabular data. The authors take MetaMIMIC for experiments, where there are 12 medical targets. They train on 11 targets and test on the leaveout target. They find that transfer learning provides definitive advantages over gradient boosted decision trees. They further compare self-supervised learning and supervised pretraining on tabular and find that supervised pretraining is more helpful. Finally, they propose a new feature imputation method to learn tackle missing values.",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper studies transfer learning on 12 medical targets.\n\n2. The paper demonstrates the benefits of using supervised pretraining for tabular data.\n\n3. A method for feature imputation is proposed. \n\n\nWeaknesses:\n\n1. All the experiments are performed in the medical domain. The 12 selected targets are closely related. Although the authors also validate on other datasets including Yeast functional genomics data and Emotions data, the targets in each dataset are too similar. Does the author test on other domains? For example, the authors can train on MetaMIMC and validate on Yeast functional genomics data.\n\n2. Similarly, the authors should provide some analysis about the 12 targets of MetaMIMC. How correlated these targets are?\n\n3. There are several works studying transfer learning for tabular data, e.g. TransTab (https://arxiv.org/abs/2205.09328). The novelty of this paper is ok but not significant. Although the paper cites the TransTab, it should not be regarded as concurrent work anymore because the paper is already on arxiv in May. The paper may need to carefully compare the differences with respect to these previous work. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. \n\nThe quality is sound. \n\nThe novelty is ok but not significant given existing works about transfer learning on tables.  \n\nReproducibility is good.",
            "summary_of_the_review": "1. The chosen targets may be too similar.\n2. No analysis about the targets used. \n3. Missing comparison with previous work. Unclear about the novelty.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4840/Reviewer_ADx7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4840/Reviewer_ADx7"
        ]
    },
    {
        "id": "NuLMe8V7zW",
        "original": null,
        "number": 2,
        "cdate": 1666466755362,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666466755362,
        "tmdate": 1666466755362,
        "tddate": null,
        "forum": "b0RuGUYo8pA",
        "replyto": "b0RuGUYo8pA",
        "invitation": "ICLR.cc/2023/Conference/Paper4840/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors study the highly important problem of transfer learning with DNNs on tabular data. This avenue has huge potential in many practical applications. The problem and DNNs for tabular data are generally underexplored so this contribution could be important for the community. They propose a way to use DNNs for transfer tasks in several settings. The proposed schemes are compared to strong tree-based methods on many datasets, indicating the potential of transfer learning on tabular data.",
            "strength_and_weaknesses": "Strengths: The paper is well-written and easy to follow. The approach is simple but seems to work in many datasets and across different settings. Expireneltal evaluation is strong and demonstrates the potential of the approach.\nWeaknesses: some related work on regularizations for tabular data is missing; for example\n\n[1] Kadra et al. Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data 2021\n\n[2] Yang et al. Locally Sparse Neural Networks for Tabular Biomedical Data 2022\n\n[3] James Fiedler, Simple modifications to improve tabular neural networks\n\nTo improve the presentation, it would be worthwhile to remind the reader what are MLM and FS in the caption of figure 3.\nAlso, please explain the procedure for downstream pseudo features in the caption of figure 4.\n\nDo you have intuition why this procedure is beneficial? Was this done (or analyzed) in any prior work? This seems related to the procedure commonly used to impute missing values.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The problem and solution are novel. The quality of the writing and explanations are good. The authors detail all hyperparameters and use Optuna to optimize all methods. I feel that this approach is fair, and reproducibility considerations are satisfied. ",
            "summary_of_the_review": "Overall I enjoyed reading the paper. I think new solutions to challenges in tabular data are really important for the ML community. The authors did a good job of analyzing and presenting their work. The proposed solution is simple yet seems to be effective even when the number of labeled samples in the target distribution is not high. This can open the door to many follow-up works. I believe that the paper should be accepted.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4840/Reviewer_gnrQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4840/Reviewer_gnrQ"
        ]
    },
    {
        "id": "XpjXcF7oCvA",
        "original": null,
        "number": 3,
        "cdate": 1666644407617,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644407617,
        "tmdate": 1666644407617,
        "tddate": null,
        "forum": "b0RuGUYo8pA",
        "replyto": "b0RuGUYo8pA",
        "invitation": "ICLR.cc/2023/Conference/Paper4840/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "## Summary\n\n* This paper investigates whether deep learning methods are effective for transfer learning on tabular data. In comparison to domains such as NLP and CV, tabular data transfer learning is not as well studied.\n\n* The overall contributions of the paper are: (1) a thorough empirical evaluation of transfer learning methods on tabular data (considering gradient boosted decision trees, training deep models from random initialization, supervised pre-training, and self-supervised pre-training); (2) a strategy for transfer learning with tabular data when the pretraining and finetuning datasets differ in what features are measured; (3) a distilled set of recommendations for empirical work in this space.\n\n\n## More details \n* The paper outlines an empirical testbed for understanding the benefit of transfer learning with tabular data. The authors adopt a medical intensive care unit dataset, MIMIC-IV, for the primary evaluation. This dataset has structured tabular data for a number of patients together with 12 downstream labels of interest. The evaluation setup for supervised pretraining involves: (1) conducting pre-training on 11 of the labels; (2) fine-tuning on the 1 remaining label, with different finetuning dataset sizes, to gauge performance. \n\n* There is also an evaluation on two non-medical datasets in the appendix. \n\n* In this experimental evaluation, the authors find that supervised pre-training outperforms training strong gradient-boosed decision trees (GDBT) baselines  with tuned hyperparameters (Xgboost and Catboost). This is an interesting result, since it demonstrates that neural models can be effective for transfer learning on tabular data. \n\n* The paper evaluates a variety of different neural network strategies in the experimental evaluation, including recently proposed strategies for tabular data such as FT-Transformer and TabTransformer, in addition to simpler methods such as MLPs and ResNets. A variety of different finetuning strategies are also considered for these models (e.g., linear evaluation, end2end training with a linear head/MLP head).\n\n*  The second part of the experimental evaluation compares different pre-training strategies, namely supervised pre-training as compared to self-supervised strategies such as masked imputation. Overall, supervised pretraining strategies perform better than self-supervised pretraining methods.\n\n* The paper then studies a setting where the pretraining and finetuning sets may contain different features, so straightforward transfer learning is not as readily appropriate. To tackle this, the paper suggests a process to impute the missing feature using an additional trained model, before re-running pretraining/finetuning. They find that this strategy improves over just excluding the missing feature (though the evaluation in this setting appears less complete). \n\n* The paper presents detailed information about experimental setups, including considerations of: how to handle the small data regimes, how to do hyperparameter search, ranges of considered hyperparameters, how early stopping applies in such extreme small data regimes. ",
            "strength_and_weaknesses": "## Strengths\n\n* The empirical evaluation in the paper is thorough and clear. The range of investigated methods (state of the art deep learning methods for tabular data, popular tree-based models, self-supervised methods) is very helpful in understanding trends and how these different strategies perform. Additionally, evaluation on multiple datasets of different types helps strengthen the empirical findings.\n\n* The range of experiments and domains studied, together with distilled findings, is useful for practitioners to understand how best to apply these strategies in practice.\n\n* The related work is fairly comprehensive as far as the ML space goes. There are other works that would be helpful to refer to, which are mentioned later in these comments.\n\n## Neutral \n\n* One could argue that the main methodological novelty in this paper comes from the last section, which concerns how to handle situations when the pretraining and finetuning tasks differ in what features they consider. Personally I think the strength of the empirical study and evaluation is valuable as a standalone contribution, even if this is the only 'new' methodological aspect.\n\n## Weaknesses\n\n### Details on empirical study\nGiven the work's main contribution is the empirical study, there are some more details that would be good to include for full completeness.\n1.  A clearer discussion of how many different seeds/splits were considered in experiments.\n2. Why were these particular low data regimes considered, as opposed to other values? What happens if we scale up to larger dataset sizes (even a single run with a large data scale would help round off the discussion)? \n3. Was splitting into train/val/test on the upstream and downstream tasks on a per-patient level? What were upstream dataset sizes on average?\n4. Dataset details are particularly crucial for the medical application domain -- could you provide a 1-2 sentence description of the tasks, and the label prevalences?\n5. What do error bars on the ranks look like (even for a subset of the experiments)?\n\n### Other related works\nAlthough the related work surveys the ML field well, there are some specific relevant works in the medical ML literature that would be worth mentioning somewhere. They deal with structured data timeseries rather than prediction from a single vector, but they also investigate contrastive pretraining, masked imputation pretraining, and supervised pretraining: \n* https://proceedings.mlr.press/v139/yeche21a.html (ICML 2021)\n* https://dl.acm.org/doi/pdf/10.1145/3450439.3451877 (CHIL 2021).\n\n### Other things\n* In the contrastive experiments, how were the corruption hyperparameters chosen? These might significantly impact performance, so should they also be incorporated in the hyperparameter search?\n* This ICLR paper from this year: https://openreview.net/forum?id=CuV_qYkmKb3 studied contrastive learning on tabular data, and performed comprehensive experiments. This might be a better augmentation strategy to compare with, rather than the one studied.\n* In the pseudo feature experiments, what happens if more than one feature needs to be imputed? How does this impact performance? Do the experiments consider removing different features in each run? What does the variance in performance/rank look like?",
            "clarity,_quality,_novelty_and_reproducibility": "The work is clear, and figures and writing are easy to understand. The work is of high quality, with clear contributions. The work's findings are mostly original, with some aspects being studied at least in part in prior works.",
            "summary_of_the_review": "Overall, I think the experimental evaluation in this paper is thorough and has useful insights for practitioners. I would therefore vote to accept it. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4840/Reviewer_HXuq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4840/Reviewer_HXuq"
        ]
    },
    {
        "id": "uSN5mwIxtm4",
        "original": null,
        "number": 4,
        "cdate": 1666951181642,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666951181642,
        "tmdate": 1669627471276,
        "tddate": null,
        "forum": "b0RuGUYo8pA",
        "replyto": "b0RuGUYo8pA",
        "invitation": "ICLR.cc/2023/Conference/Paper4840/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies transfer learning on deep tabular models. Authors create a transfer learning benchmark from the MetaMIMIC repository, and then conduct transfer learning experiments on it. Specifically, authors find that fine-tuning pre-trained FT-Transformer consistently outperforms classical models such as XGBoost and Catboost with different training data sizes. Authors also find self-supervised pre-training is inferior to supervised pre-training on tabular data. Additionally, to deal with applications with missed features, authors propose to use pseudo values for feature alignments. ",
            "strength_and_weaknesses": "Strength\n1. Deep learning with tabular data is an important and unsolved problem. \n2. This paper is clearly written. \n3. The proposed approach of pseudo-value completion seems interesting and effective. \n\nWeaknesses\n1. Previous studies (such as FT-Transformer) have already shown that DNNs can generally outperform XGBoost/CatBoost. Therefore, the main findings of this paper sound less surprising. Authors provide no insights about when DNNs transfer better than tree models and when not. \n2. All experiments use 11 splits as the upstream task and 1 as the downstream task. This may not be representative enough for various applications. I wonder whether using less splits in pre-training still results in the same conclusion. \n3. A minor typo: \u201cwith without\u201d",
            "clarity,_quality,_novelty_and_reproducibility": "Good clarity, quality and reproducibility. Novelty is a bit limited.  ",
            "summary_of_the_review": "Overall, this paper presents useful comparisons of transfer learning between deep Transformers and tree models on tabular data, though the findings are not so surprising. My major concerns are, 1) when (on which kinds of data) shall we use deep Transformers and when to use tree models, and 2) whether is the conclusion still right with fewer pre-training tasks? ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4840/Reviewer_1RK9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4840/Reviewer_1RK9"
        ]
    }
]