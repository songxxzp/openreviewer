[
    {
        "id": "kWgbqUwYM_o",
        "original": null,
        "number": 1,
        "cdate": 1666665577174,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665577174,
        "tmdate": 1670688474723,
        "tddate": null,
        "forum": "SM7XkJouWHm",
        "replyto": "SM7XkJouWHm",
        "invitation": "ICLR.cc/2023/Conference/Paper4906/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the overs-moothing problem of graph neural network (GNN) and Transformers, and proposes to avoid the dimensional collapses in the representation, borrowing the idea from self-supervised learning. It thus proposes a normalization layer ContraNorm, which aims to learn a more uniform distribution in the embedding space that can ensures slighter dimensional collapse, motivated on the uniform loss from self-supervised learning. The proposed normalization layer can be inserted into GNNs and Transformers, and improves these baselines based on the experiments on three different scenarios: ViT for image classification, BERT for natural language understanding, and GNNs for node classifications.",
            "strength_and_weaknesses": "**Strengths:**\n\n+ The motivation of the proposed method is clear. It is good this paper try to connect the designed normalization method with the uniformity loss, which provides stronger motivation/foundation for this paper, even though I have concerns for its connections and analyses.  \n\n+ The effectiveness of the proposed method over baseline is empirically verified in several architectures (e.g., GNN, Transformer) on several datasets. \n\n**Weaknesses:** \n\n1.One severe problem of this paper is that it misses several important related work/baselines to compare[1,2,3,4], either in discussion [1,2,3,4]or experiments[1,2]. This paper addresses to design a normalization layer that can be plugged in the network for avoiding the dimensional collapse of representation (in intermediate layer). This idea has been done by the batch whitening methods [1,2,3] (e.g, Decorrelated Batch Normalization (DBN), IterNorm, etal.). Batch whitening, which is a general extent of BN that further decorrelating the axes, can ensure the covariance matrix of the normalized output as Identity (IterNorm can obtain an approximate one). These normalization modules can surely satisfy the requirements these paper aims to do. I noted that this paper cites the work of Hua et al, 2021, which uses Decorrelated Batch Normalization for Self-supervised learning (with further revision using shuffling). This paper should note the exist of Decorrelated Batch Normalization. Indeed, the first work to using whitening for self-supervised learning is [4], where it shows how the main motivations of whitening benefits self-supervised learning.\n\n2.I have concerns on the connections and analyses, which is not rigorous for me. Firstly, this paper removes the $AD^{-1}$ in Eqn.6, and claims that \u201cIn fact, the operation corresponds to the stop-gradient technique, which is widely used in contrastive learning methods (He et al., 2020; Grill et al., 2020). By throwing away some terms in the gradient, stop-gradient makes the training process asymmetric and thus avoids representation collapse with less computational overhead. It verifies the feasibility of our discarding operation\u201d. I do not understand how to stop gradients used in SSL can be connected to the removement of $AD^{-1}$, I expect this paper can provide the demonstration or further clarification.  \nSecondly, It is not clear why layerNorm is necessary. Besides, how the layer normalization can be replace with an additional factor (1+s) to rescale H shown in claims \u201cFor the convenience of analysis, we replace the layer normalization with an additional factor 1 + s to rescale H\u201d. I think the assumption is too strong.   \nIn summary, the connections between the proposed contraNorm and uniformity loss requires: 1) removing $AD^{-1}$ and 2) add layer normalization, furthermore the propositions for support the connection require the assumption \u201clayer normalization can be replace with an additional factor (1+s) to rescale H\u201d. I personally feel that the connection and analysis are somewhat farfetched. \n\n\n\n**Other minors:**\n\n1)Figure 1 is too similar to the Figure 1 of Hua et al, 2021, I feel it is like a copy at my first glance, even though I noted some slightly differences when I carefully compare Figure 1 of this paper to Figure 1 of Hua et al, 2021. \n\n2)The derivation from Eqn. 3 to Eqn. 4 misses the temperature $\\tau$, $\\tau$ should be shown in a rigorous way or this paper mention it. \n\n3)In page 6. the reference of Eq.(24)? \n\n**References:**\n\n[1] Decorrelated Batch Normalization, CVPR 2018  \n[2] Iterative Normalization: Beyond Standardization towards Efficient Whitening, CVPR 2019  \n[3] Whitening and Coloring transform for GANs. ICLR, 2019  \n[4]Whitening for Self-Supervised Representation Learning, ICML 2021  \n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is clear but I have certain concerns on the claims, the quality is overall good, and the novelty is somewhat incremental. I believe the experiments can be somewhat reproduced based on the descriptions of this paper.",
            "summary_of_the_review": "This paper is a well-motivated paper with empirical success in experiments, but it misses several important related work for comparison and some claims are not well clarified. In summary, I am slightly negative to this paper. \n\n==================after rebuta==================\n\nI look over the revised paper, and the authors almost address all my concerns. E.g., add the comparison to the missing related works, and re-organize the clarity. I still have concerns on the claims that \u201cIn fact, the operation corresponds to the stop-gradient technique, which is widely used in contrastive learning methods\u201d. I believe removing $AD^{-1}$ may improve the performance based on the authors experiments, but it does not correspond to the stop-gradient in contrastive learning (they are different mechanisms, e.g., stop-gradient is essential to prevent collapse in simsiam/BOYL, while I believe removing $AD^{-1}$ is not the essential part in the proposed method.). Even though this concern holds, the revised version is more rigorous than the previous one, and I tend to positive to this paper. I thus raise my score from 5 to 6. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4906/Reviewer_uxeV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4906/Reviewer_uxeV"
        ]
    },
    {
        "id": "AopFbYyZw2",
        "original": null,
        "number": 2,
        "cdate": 1666746761767,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666746761767,
        "tmdate": 1666746761767,
        "tddate": null,
        "forum": "SM7XkJouWHm",
        "replyto": "SM7XkJouWHm",
        "invitation": "ICLR.cc/2023/Conference/Paper4906/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the phenomenon of oversmoothing in GNNs and transformers; and presents the ContraNorm layer. The experiments on real-world dataset show the efficacy of this method.",
            "strength_and_weaknesses": "Strength:\nIt\u2019s a nice method with good motivations and theoretical insights. \nThe paper is generally well written.\nThe experiments show the efficacy of the model. It indeed can improve the performance in general.\n\nweakness\n\n1, I would ask one particular extreme case on any dataset: the authours can train a very deeper model that encounter the collapse issue, and the model performance is not good. However, with the help of ContraNorm layer, the issue can be solved. This may be important to show the efficacy.\n\n2, Eq.(2) uniform loss part:  what\u2019s the relation between symbol i^+ and x_k? seems some typos here.\n\n3, could the authours give more reasons/motivations why directly discarding the second terms in Eq. (6)? And \u201cEmpirically, the two terms play a similar role in our method.\u201d for any empirical evidence?\n\n4, After reading the paepr, I feel that the ContraNorm is not necessarilty related to/address the collapse issues. As least, there is not enough evidence now. Some visualization of Fig. 1 in real world dataset and backbone is thus needed to directly show that the collapse issues happened when training the model; and ContraNorm can change the distribution of features. This can be shown on any dataset. Otherwise, we can just take ContraNorm as one typical type of contrast learning strategy to better train the model.\n And particularly, note that ContraNorm is built upon the layer norm.\n",
            "clarity,_quality,_novelty_and_reproducibility": "the paper is well written, and very clean. \n",
            "summary_of_the_review": "please refer to the weakness part. I would ask for some direct evidence and visualization that ContraNorm can solve the collapse issue.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4906/Reviewer_Q9GP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4906/Reviewer_Q9GP"
        ]
    },
    {
        "id": "wjbN32640_",
        "original": null,
        "number": 3,
        "cdate": 1667096383042,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667096383042,
        "tmdate": 1667096555597,
        "tddate": null,
        "forum": "SM7XkJouWHm",
        "replyto": "SM7XkJouWHm",
        "invitation": "ICLR.cc/2023/Conference/Paper4906/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper discusses an over smoothing issue of representation in Graph Learning and message-passing like computation unit (attention) as the depth of the models increase. The paper argues that attention maps are not a very good indictors of over smoothing, due to the fact that a feature can have low similarity while having high attendance score. Subsequently, an optimization based normalization unit is proposed to encourage large variance across features and decorrelation of dimensions. Results, especially for image and GCN, show good improvement.\n ",
            "strength_and_weaknesses": "Comments and questions:\n- The paper is very well written with good organization. Its topic of discussion is a well-motivated and focused problem with clear solution which makes it easy to follow. Results, especially for GCN and vision tasks, show good improvement as the depth of models increase. \n- Questions: \n    - In Subsection 3.1 and Figure 2, it was argued that similarity in attention does not imply feature collapse. The argument is based on using attention map, which is a cosine similarity between query and key, as a measure of representation collapse. The question is, what is referred as feature here? I am assuming the value embedding. If so, similarity between attention map naturally does not imply similarity between value pairs. What is new here or am I missing something?\n    - The uniformity term in contrastive learning is between the representation in question and all its negative examples. However, $\\dot{L}_{uniform}$ seem to measure similarity across all representation. Is that the case?  \n     - The normalization layer Eq(7) and Eq(8), are designed as a single gradient step that updates the features in the direction of minimal similarity. Could this be amortized? as opposed to approximating a solution with single step. \n- Questions regarding experiment:\n     -  What average performance in Table 1?\n     -  The average performance for both BERT and ALBERT is reported inconsistently in the **Results** section and **Table 1**. Cloud you correct/clarify?\n     -  Reported results in **Table 1** show tiny improvement. Are these the right models and experimental setup to show case your solution for the GLUE task? \n\n- Corrections:\n    - missing fullstop in the second line form the last, in the second paragraph of the introduction section.",
            "clarity,_quality,_novelty_and_reproducibility": "Very clear, good quality. seems reproducible. ",
            "summary_of_the_review": "The paper motivated the problem attempts to solve, suggested a solution and provided insight into why it would work with theoretical justification. It further showed, empirically, performance improvement as the depth of the model increases. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4906/Reviewer_Zh7A"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4906/Reviewer_Zh7A"
        ]
    },
    {
        "id": "ecrfK36JDW",
        "original": null,
        "number": 4,
        "cdate": 1667159795979,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667159795979,
        "tmdate": 1670577294279,
        "tddate": null,
        "forum": "SM7XkJouWHm",
        "replyto": "SM7XkJouWHm",
        "invitation": "ICLR.cc/2023/Conference/Paper4906/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the collapse problem within nodes (tokens) of a sample if I correctly got the idea of it. The authors employ the effective rank to measure the quailty of the feature. Then, they borrow the idea of uniformity loss from the contrastive learning algorithm, and convert the gradient through back-propagation to the forward-propagation as the Contranorm algorithm. Finally in the experiments, the authors provide several results that the Contranorm boost the performance.",
            "strength_and_weaknesses": "Strength:\n\nThe paper presents a nice idea to convert the gradient to the form of normalization.\n\n\nWeaknesses:\n\nInstinctively, I would like to ask why it is not possible to apply uniformity loss straightforwardly unless it doesn't converge, the authors should provide a comparison between the proposed method and uniformity loss. Since, theoretically, the ContraNorm is equivalent to applying the loss, and appending the ContraNorm to the neural network does increase the computational costs during the inference time.\n\nThe authors did not provide an analysis of the computational costs of the proposed Contranorm, I wonder if the number of tokens grows, it will significantly decrease the training speed.\n\nIn the ImageNet1K experiment, the performance for #L=24 is still lower than that of #L=16, which means the proposed method does not actually address the mentioned over-smoothing problem, despite the effective rank depicted in figures 2 & 3 seems to be much higher than the reference model. Therefore, I do not consider the mechanism of feature collapsing discussed in this paper to be comprehensive or even correct, which challenges the motivation of this study.\n\nI do not understand how the layer norm can be replaced by the factor 1+s in Eq.9, the layer norm should remove the bias which the factor 1+s cannot.\n\nThe clarity of this paper is not acceptable to this conference.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThis paper is hard to understand, and the main reason is the notations used in Section 3. I am heavily confused by the $\\mathbf{H}$ in Eq. 6~12. It seems that the authors are updating the $\\mathbf{H}$, however, I wonder if Eq. 10 still holds for $\\mathbf{H}^2$ and $\\mathbf{H}^1$, what does the superscript stand for? layers or what else?\n\nQuality:\n\nThe overall quality of this paper is not satisfying because of the weaknesses mentioned above.\n\nNovelty: \n\nThe idea to convert the contrastive uniformity loss to the forward pass is very sophisticated, the method should be helpful to this community.\n\nReproducibility:\n\nThe paper lacks details on how the methods are actually implemented in the Transformers. For instance, how the stop-gradient is used? How the layer norm is configured in terms of the bias and scale factor? How is the former layer norm dealt with when you apply the contranorm before it?",
            "summary_of_the_review": "I did really spend a lot of time understanding this paper, and it is not because the technical details are difficult, it is simply because the paper is low in clarity and representation quality. I suggest the authors carefully revise this paper and submit it to another conference.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4906/Reviewer_HpT5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4906/Reviewer_HpT5"
        ]
    }
]