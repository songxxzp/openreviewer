[
    {
        "id": "eZmnkBRrrZ",
        "original": null,
        "number": 1,
        "cdate": 1666340919473,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666340919473,
        "tmdate": 1666340919473,
        "tddate": null,
        "forum": "pOnhudsvzR",
        "replyto": "pOnhudsvzR",
        "invitation": "ICLR.cc/2023/Conference/Paper1220/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an unsupervised method for bias-mitigation. The presented approach leverages proxy labels derived from comparing the instance-wise loss values to a threshold. These proxy labels are consolidated/smoothed through nearest neighbors in latent space (through embedding). The labels indicate poorly and well represented groups, which can be used in combination with arbitrary supervised methods for obtaining fairer classification models. The approach is extensively evaluated and is competitive with group-label supervised methods for bias mitigation.",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper is well written. I enjoyed reading this paper and found it well-structured and good to follow.\n2. The paper addresses an important and timely problem. Bias is common in machine learning and methods to deal with it are relevant in practice. Furthermore, bias is often introduced implicitly, i.e., without group labels known at model training time. Therefore, the proposed method may be a valuable step towards fairer automated decisions.\n3. The experimental evaluation is extensive and rigorous and overall convincing. It includes several data modalities (tabular and text), fairness measures, datasets and baselines. Furthermore, important sanity checks such as the validity of the nearest-neighbor approximation for smoothing are in place. I briefly looked at the experimental code which appears to be in a good state. Intersectional discrimination of multiple attributes is also considered.\n4. The approach is simple and can be implemented in a few lines of code. Also, it only has one hyperparameter (the number of neighbors used for smoothing). This makes the method ready-to-use for practical applications.\n\nWeaknesses:\n\n1. There is no comparison between proxy labels and ground truth groups (e.g., given by race, gender, etc.). However, it is frequently claimed that the proxy labels correlate with demographic groups (for instance, right in the abstract, it says: \u201c[...] that correlate with unknown demographic data\u201d). The claim is repeated in 5.1, however, the performance gap is compared to the proxy labels only and not the ground truth group labels. This could be achieved by providing the statistics, e.g., confusion matrices, with ground truth group labels.\n2. The theoretical grounding of the work is weak, which raises the question whether there exist other simpler and principled solutions. While the paper provides some intuition for why the proposed method works, it lacks theoretical evidence from point to point. For instance, it is not clear why U should be generally replaced with the Loss in Section 3.2 and how the group-based formulation is transformed to an individual one. I see that the experimental results are promising, but the derivation could be backed with more evidence or theoretical results.\n3. There are no theoretical guarantees that the proposed method will lead to fairer models and that the proxy objective corresponds to the actual one (or bounds on the error). This also raises the question if the fairness could possibly be decreased. This could be a problem in high-stakes scenarios. In such settings, a black-box method like the one presented is unlikely to be deployed.\n\nMinor comments:\n1. I would have appreciated further evidence or intuition for some claims made in the paper. Here are some examples: \n(1) Because TrustPilot is already fair, the ground truth labels cannot be aligned with the TPR gaps (Section 5.1) \n(2) Why is GD-CLA more robust to conflated classes (final paragraph of 4.2) \n(3) The discussion of upstream and downstream debiasing (Section 2, \u201cUnsupervised bias mitigation\u201d). As far as I understand, upstream debiasing is also unsupervised? Thus, a comparison with one of these methods could be insightful in terms of future work, since these methods could be applicable to the same set of problems.\n2. Related work: Please use \\citep (end of first paragraph, Point (1) in second paragraph).\n\nQuestions:\n1. How exactly was the ROC-RFC computed? Were the scores normalized? For instance, looking at Figure 5b), the area under the curve is well above 0.8 x 0.8=0.64 for both methods. Even if you normalize the curve (taking the left and right-most point as references), the areas are well above 0.5. However, in Table 1, values reported for Bios-G are all below 0.5. How can this be explained?\n2. ROC-PFC values for Trust Pilot, Adult, Compas are extremely low (~0.1). This indicates that performance is hurt drastically even with slowly introduced fairness. Why is this the case? Is the AUC-PFC a still a representative measure in this case? Maybe this is connected to the first question.\n3. Table 3 shows the best values for the nearest neighbors smoothing which are different across datasets. How would you choose a suitable value for k in practice, without having access to ground truth fairness measures?\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Overall, the work is well presented. The method and experimental setup are well described. \nNovelty: Some works follow a similar high-level sprit (e.g, Hashimoto et al.) but the follow-up ARL method is a competitor in the experiments and an improvement over this method is shown. Therefore, I would consider this work as sufficiently novel, but there might be some works which I have missed.\nReproducibility. The authors provide the full source code which appears to be in a good shape. Therefore, I am confident that the results are reproducible.\n",
            "summary_of_the_review": "The topic of bias mitigation is relevant and the empirical results in this paper are interesting. While the empirical evidence suggests that the method improves fairness on real datasets, there are no theoretical guarantees or considerations, making it hard to deploy this method in high-stakes applications. However, I found the experiments well-designed and executed and the results very promising, which is why I am currently leaning towards acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1220/Reviewer_y5Cm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1220/Reviewer_y5Cm"
        ]
    },
    {
        "id": "0UI8uUpqpk",
        "original": null,
        "number": 2,
        "cdate": 1666443577183,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666443577183,
        "tmdate": 1666443577183,
        "tddate": null,
        "forum": "pOnhudsvzR",
        "replyto": "pOnhudsvzR",
        "invitation": "ICLR.cc/2023/Conference/Paper1220/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Bias creeps into ML models trained from human behavioural data, which can usually be mitigated against but (typically) with labeled data for that purpose. This paper proposes a meta-algorithm for debiasing representations called Unsupervised Locality-based Proxy Label assignment. It is evaluated over five datasets, from NLP to structured data, and this evaluation shows that the provided technique recovers proxy labels that correlate with \u2018unknown demographic data\u2019. Performance is competitive with supervised methods. ",
            "strength_and_weaknesses": "Stengths\n\n- Bias remains a crucial (and ongoing) challenge across many subdomains of ML. It is commendable that this work generalized beyond one of these subdomains in its analysis. Moreover, the focus on unlabeled demographics is more ecologically valid and therefore useful.\n- Good, relevant, and (mostly) recent literature is effectively compared and contrasted in a well-structured manner.\n- The datasets and baselines for empirical evaluation are well chosen\n- The questions asked in each of the sub analyses of Sec 5 are well-chosen and mostly well executed\n\nWeaknesses\n\n- There are multiple group-wise measures of fairness beyond the one adopted in Sec 3.1. A minor point, but it would be preferable to show some rationale for the method chosen, and some indication of the alternatives.\n- Certain assumptions seem to be glossed over in Sec 3.2, such as how (or whether) over- or under-represented groups can be binarized according to z_i\u2019. This may of course be a useful heuristic empirically, but whether it has anything to do with actual groups or is merely correlated with them for other, more spurious reasons should be better explained (e.g.., in sec 5.2). \n- Empirical results are fairly similar across rows, for multiple datasets and attributes, in Table 1. That this is the case for the unsupervised method is actually a positive outcome, but claims of \u2018consistently outperforming\u2019 other approaches seems to require some tempering, and some statistical tests of significance should nevertheless be run.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is written clearly, and mostly with sufficient depth although some assumptions are glossed over. As the authors note, other approaches exist to unsupervised approaches in this space, but the simple approach proposed here is technically novel. The baselines are run with the fairlib library. The Appendix is fairly complete\n",
            "summary_of_the_review": "\n- The positives of moderate novelty, fairly good empirical approaches (sub analyses, datasets, baselines), and the importance of this space outweigh negatives of glossed-over rationales and assumptions, and perhaps some overstatement of performance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1220/Reviewer_djVz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1220/Reviewer_djVz"
        ]
    },
    {
        "id": "xbEFQzTSVN",
        "original": null,
        "number": 3,
        "cdate": 1666583467974,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583467974,
        "tmdate": 1666583467974,
        "tddate": null,
        "forum": "pOnhudsvzR",
        "replyto": "pOnhudsvzR",
        "invitation": "ICLR.cc/2023/Conference/Paper1220/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new framework to achieve fair outcomes in settings where access to sensitive attributes is not available. The method is called Unsupervised Locality-base Proxy Label assignment (ULPL) and is based on assigning proxy labels according to model predictions for poor vs well-modeled instances. The authors perform various experiments on datasets from ML and NLP domains.",
            "strength_and_weaknesses": "**Strengths:**\n1. The paper studies an important subject. Achieving fair outcomes in settings where access to protected attributes is limited is important and interesting.\n2. Authors perform various experiments on datasets both from NLP and ML domains to showcase various aspects of the paper.\n3. The method is easy and straightforward.\n4. Paper is well-written and easy to follow.\n\n\n**Weaknesses:**\n1. It seems like the method is data dependent. Maybe results from section 5.1 on TrustPilot dataset is exactly due to this reason? I think further analysis or discussion need around this issue.\n2. Related to my above comment, while the paper does a good job in terms of empirical analysis, theoretical discussion is lacking. This theoretical discussion maybe address my above comment (1). That this approach can be applicable regardless of specific data properties.\n3. The performance of the approach is not impressive considering that comparison with the only unsupervised approach ARL is not done in a fair manner. \n4. How much do you think the base supervised approach that ULPL is implemented over is having effect on the results. Although authors discuss this briefly in \"Different ULPL methods:\" section. More in depth discussion might be useful to improve the paper.\n5. The approach is dependable on some parameters such as k which might make it inefficient. Is there a good way to make this more flexible?\n\n**Additional Questions that needs clarification:**\n\nThe framework based on indicating poorly vs well modeled instances seems more aligned with equalized odds or equality of opportunity notions of fairness. Do authors know why their approach works for statistical parity which is different in nature than what is proposed here? \n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\n\nThe paper was written clearly, and it is easy to follow. \n\n**Quality:**\n\nThe paper is of high quality in terms of empirical experiments, but not so much theoretically. \n\n**Novelty:**\n\nWhile the problem on its own is not very novel, the approach proposed can be considered to some extend novel. \n\n**Reproducibility:**\n\nAuthors provided an anonymous source code which will be really useful for reproducibility. Discussions in the Appendix also is comprehensive which can make it easier to reproduce the exact results in the paper.",
            "summary_of_the_review": "Overall, the paper is a well-written and motivated paper; however, some deeper theoretical discussions can improve the paper drastically. The approach has also some flaws in terms of dependability to data, parameters, as well as some performance related concerns which I discussed in detail under weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1220/Reviewer_r7Nv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1220/Reviewer_r7Nv"
        ]
    },
    {
        "id": "o_NPFiLMkZV",
        "original": null,
        "number": 4,
        "cdate": 1666670325546,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670325546,
        "tmdate": 1670858028986,
        "tddate": null,
        "forum": "pOnhudsvzR",
        "replyto": "pOnhudsvzR",
        "invitation": "ICLR.cc/2023/Conference/Paper1220/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the problem of debiasing representations learned by encoder-decoder models when demographic information is missing. The authors propose Unsupervised Locality-based Proxy Label assignment (ULPL), which assigns a binary \u201cproxy\u201d label to each example based on whether the model incurs a larger or smaller than average loss on that example. Once examples have been assigned proxy labels, it is possible to apply existing debiasing methods. In order to be robust to noisy outliers, the paper also proposes smoothing the proxy label for each example via a majority vote among that example's neighbors in the encoded space. In experiments on 5 datasets (with various protected attributes in focus for each dataset) they show that ULPL leads to larger areas under the performance fairness curve than ARL and a baseline that does not employ fairness interventions; ULPL is claimed to lead to similar performance as some supervised methods. Analysis showing that proxy labels generated by ULPL are generally correlated with protected labels is provided, as well as some analysis of kNN smoothing and an experiment where groups are identities by cross-products of protected attributes.\n",
            "strength_and_weaknesses": "Strengths:\n- The crux of the approach in this paper is to try to equalize training loss among all examples. The proposed method, ULPL, simply groups the examples into examples that experience lower than average loss, and those that experience higher.  Once labeled, other debiasing methods are used to equalize the loss. This seems reasonable as a way to improve fairness. The argument made in the paper is that examples with lower loss are likely to be of the same (under-represented) demographic group. While there is some empirical evidence that supports this claim in previous work, I find it weaker and somewhat inconsequential: if training loss is more evenly divided across examples (and training loss is a good proxy for performance) then fairness should improve across all groups.\n- This paper is clearly written and easy to follow.\n- Experiments are performed with 5 datasets. Significant analysis is also presented.\n\nWeaknesses:\n- The proposed label smoothing is not consistently effective. Table 3 shows that in 50% of experimental conditions, no label smoothing is better than label smoothing.\n- Label smoothing seems to be impossible to tune in realistic settings (in terms of the number of neighbors k). A standard approach of tuning k would be selecting k that yields the best results on the validation set. Since the metric being optimized is AUC-PFC, demographic group information is required to compute the metric. But the motivation of this paper includes the assumption that no demographic information is available.\n- The results are difficult to interpret. Table 1, which includes the main results, reports AUC-PFC, which characterizes the tradeoff between performance and fairness. I assume that the proposed method helps to improve fairness, but does it also improve performance? A more detailed characterization of how ULPL affects performance and fairness would help. Moreover, the difference between the proposed approach and other competing methods is often a few thousandths of a unit of area, which seems small, but as a reader it is difficult to be certain. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: while the paper is easy to follow, after reading I\u2019m unsure of the following:\n- For each dataset, are the protected attributes in question always binary? How many values can each take on? This could help shed more light on the results.\n- For ULPL, is the loss per instance taken from the final model or summed over all training iterations?\n- What is the precise way to apply this? Train a model, then label the examples, assign new labels with knn, then debias? Or does this whole process happen at each iteration?\n- During neighborhood smoothing, are points relabeled with their smoothed label or do they retain their unsmoothed labeled? I imagine that an example\u2019s smoothed label can be different from its unsmoothed label, leading it to vote differently in smoothing decisions for other examples. \n- In the sensitivity to k-NN hyperparamters section, what is the variable p?\n- In the same section: what is class-specific vs. ins-pecific smoothing?\n\nQuality:\n- ULPL seems like reasonable approach to debiasing without group information\n- The experiments seem to be technically sound, however the results do not seem very significant.\n- The writing is clear, but some of the details about the method and experimental methodology have been left out.\n\nNovelty:\n- The primary novelty is the idea to group examples into those that experience more/less than average loss (ULPL) as a proxy for protected groups when none are provided. As previously mentioned, while I don\u2019t believe that examples with more loss will always (or even most of the time) correspond to specific protected groups, using existing debiasing methods to more evenly distribute the loss seems like a reasonable way to encourage parity across groups, when groups are unknown.\n- The idea to use k-NN smoothing is not particularly novel and moreover, as previously mentioned, it seems problematic from a hyperparameter-tuning perspective. Moreover, it only helps half of the time. \n- The problem of debiasing without group information is interesting, relevant, and has not received much attention.\n\nReproducibility:\n- A link to the code used to run the experiments is provided, so I believe the results could be replicated. However, I have not tried to replicate them myself.\n\nOther comments:\n- Please bold the best results in the table to make them easier to read.\n- Consider adding a citation to work on dataset cartography, which also groups points based on how difficult they are to predict correctly\n",
            "summary_of_the_review": "The paper is written clearly but some important details are missing. One of the methods proposed in this paper (label smoothing) does not consistently provide benefit and seems impossible to tune. The primary method proposed (ULPL) seems reasonable but doesn\u2019t lead to significant gains. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1220/Reviewer_sY6r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1220/Reviewer_sY6r"
        ]
    }
]