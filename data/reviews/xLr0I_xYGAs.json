[
    {
        "id": "-6vz25GyHs_",
        "original": null,
        "number": 1,
        "cdate": 1666432184956,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666432184956,
        "tmdate": 1669820885039,
        "tddate": null,
        "forum": "xLr0I_xYGAs",
        "replyto": "xLr0I_xYGAs",
        "invitation": "ICLR.cc/2023/Conference/Paper307/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the problem of 'Unified Open-Set Recognition' (UOSR), a variant of open-set recognition (OSR) which also considers how to deal with misclassified closed-set test samples. Concretely, the closed-set test samples are split into InW and InC (wrongly and correctly classified samples respectively), and the model must predict 'unknown' (i.e have a high open-set score) for InW. \n\nI would summarise the central finding of this paper as \"False Positives in standard OSR tend to be misclassified closed-set samples\" (by False Positives, I mean examples from the training classes which have been predicted as 'open-set', and by misclassified, I mean wrongly predicted within the closed-set classes). \n\nBesides the above, the authors provide an empirical investigation into OSR and OoD techniques for the UOSR problem, presenting some interesting findings such as that Outlier Exposure helps the InC/open-set decision but hurts the InW/InC decision. They further tackle the UOSR problem in a few-shot manner, and present a combination of KNN and a softmax score to solve it.",
            "strength_and_weaknesses": "The main strength of this paper is the finding that, for most OSR methods, 'false positive' predictions tend to be misclassified closed-set samples. Though this may seem obvious, I do not believe this has been reported in the OSR literature. The main weaknesses for me revolve around the way this finding is phrased, and some of the experimental setups.\n\nStrengths:\n* As mentioned above, it is interesting to know that false positive open-set predictions tend to be misclassified closed-set examples. This is useful to researchers as the authors highlight a specific sub-problem, separating InC from open-set samples, as a key challenge. It is also important to practitioners, as both open-set examples and incorrectly predicted samples should be rejected by a closed-set classifier.\n* The paper provides a thorough set of experiments (though I have some questions about their execution and details, see weaknesses) to demonstrate interesting empirical findings, such as the one regarding outlier exposure mentioned above. \n* As needed for an empirical paper, the authors provide extensive additional implementation details in the appendix. They further include code to reproduce the paper's empirical findings, which is greatly appreciated.\n\nWeaknesses:\n* I am not sure that framing UOSR as an entirely new task is warranted or helpful in this case (e.g with new evaluation datasets). Rather, it seems that this paper highlights that a specific subset of the OSR problem (delineating InC vs open-set) is the most interesting part of the problem, and a new evaluation metric which focusses just on this decision could be interesting. \n* I am a little confused as to why authors have used non-standard datasets for evaluation. The authors rely heavily on a CIFAR100 (ID) vs TinyImageNet/LSUN (OoD) experiment to demonstrate their findings. This is more similar to an OoD evaluation rather than OSR. Standard practise in the OSR literature is to use held-out categories from a single dataset as 'unseen' (this is especially an issue for the LSUN experiment, whose taxonomy is unrelated to CIFAR100). \n* I am not sure why the authors have used different methods to compare video and image datasets, rather than a unified evaluation. A number of the image-based methods could be equally run on video and vice-versa. This lessens the clarity of the findings.\n* The authors compare and contrast training with Outlier Exposure to using pretrained models. However, I think it is important to distinguish the *data* from the *method* used. As such, I think a better experiment would be to pre-train models with the OE data and see if the findings hold. I would also suggest the authors discuss how much overlap there is in the semantics of the pre-training/outlier training set with the OoD test set.\n* (Minor): The authors do not introduce AURC, which I understand is from the Selective Prediction literature. Given this paper is targeted at the OSR community, I would suggest explaining it.",
            "clarity,_quality,_novelty_and_reproducibility": "I believe the quality of this paper is high, with all experimental details clearly specified and claims substantiated. \n\nHowever, I believe that the clarity of the paper could be substantially improved in the following ways.\n\n1) The take home from the paper, that 'false positive' predictions tend to be misclassified closed-set samples, is not presented in a straightforward manner but typically in a more roundabout way (e.g 'the uncertainty distribution of almost all existing methods designed for OSR is actually closer to the expectation of UOSR than OSR'). This hurts readability. \n2) As mentioned above, AURC should be clearly explained to the reader early on in the paper. On a related note, Fig1b is difficult to parse.",
            "summary_of_the_review": "Overall, I believe this paper has an interesting central finding for OSR which is intuitive but has, to my knowledge, not been explored in the OSR literature before. The paper further conducts relatively rigorous experiments and makes effort to provide all details and code. That said, I believe there are issues with the exact execution of some experiments and presentation of findings, which prevent the findings from being easily accessible to the reader.\n\nUPDATE AFTER REBUTTAL:\n\nI have now looked through the other reviews and the corresponding authors' responses. The authors have responded to my main concerns and clarified some of my confusions.\n\nMy fellow reviewers' main concern seems to be that Vaze et al. [1] already show a correlation between ID accuracy and OSR performance. Though it is, perhaps, a corollary that the 'False Positive' OSR predictions tend to be InW samples, I believe it is an important point which has not received sufficient emphasis in the literature. It further seems to me that the authors have provided substantial additional empirical evidence to respond to all reviewers' concerns. Thus I believe this paper provides a valuable empirical contribution to the field.\n\nAs such, I maintain my previous rating (8, Good Paper) and recommend the paper for acceptance.\n\n[1] OSR: A Good Closed-Set Classifier is All You Need? Vaze et al, ICLR 2022",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper307/Reviewer_sZtz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper307/Reviewer_sZtz"
        ]
    },
    {
        "id": "cYMBoSHkY9",
        "original": null,
        "number": 2,
        "cdate": 1666540389479,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666540389479,
        "tmdate": 1666540389479,
        "tddate": null,
        "forum": "xLr0I_xYGAs",
        "replyto": "xLr0I_xYGAs",
        "invitation": "ICLR.cc/2023/Conference/Paper307/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This article demonstrates the uncertainty distribution of almost all existing OSR methods is actually closer to the expectation of UOSR than OSR. This article also introduces a new evaluation setting into UOSR, which is few-shot UOSR. Then, the FS-KNNS method is proposed, which achieves state-of-the-art performance under all settings.",
            "strength_and_weaknesses": "Strength:\n- This article deeply analyzes the UOSR task under different training and evaluation settings through extensive experiments.\n- Although Vaze et al. [7] have demonstrated the importance of a good closed set, this paper further illustrates the impact of InW samples on OSR performance, providing a meaningful reference for future OOD detection research.\n- The article analyzes how pre-training and outlier exposure influence the UOSR.\n- This paper proposed a new evaluation setting called few-shot UOSR, and proposed a new method named FS-KNNS.\n- Detailed ablation experiments are included in the appendix of this paper, and the results of the paper are reproducible.\n\nWeaknesses:\n- In Section 2, the authors mentioned the definition of UOSR, which rejects InW and OoD samples. The authors may also consider discussing outlier detection [1,2] as related work.\n- Some OSR methods need to use the generated OoD samples to assist in training [3,8], while some OSR methods only need to train InD samples to identify OoD samples [4]. This article should discuss whether the InW samples will play an equally important role in the above cases in Tables 2 and 3. Perhaps it is possible to report the performance of OSR methods with/without OoD sample-assisted training separately in the table.\n- The authors discuss the impact of InW on UOSR. Did the authors consider the effect of some noisy outliers in the training set on the results? Or just consider the impact of misclassified samples in the test set on the results? How to discover them and use them to improve the model? I am glad to see related discussions.\n- In Table 5, the authors compare the methods of outlier exposure and claim that the method of outlier exposure can effectively improve UOSR. However, I am concerned that this is the superiority of the OOD detection method itself, since the methods not based on outlier exposure are older. Should the author try to introduce outlier exposure settings for the same method and observe the AUROC changes to confirm the final effect, or try to compare some SOTA methods that do not require outlier exposure, e.g., [5,6]?\n\nSome Questions:\n- Appendix C mentioned the outlier datasets used for the study. However, the outliers of other methods appear to be generated from known data [3,8], while the authors use additional data directly. Should the authors discuss the difference between using real and virtual outliers, and how they relate to the conclusions drawn in this paper?\n- The InD dataset used for OSR in this paper is CIFAR-100, a relatively simple dataset. Whether the author considers using InD dataset with large-scale semantic space such as TinyImageNet, because this dataset seems to be more challenging and more realistic to reflect the negative effects of InW, I am glad to see related discussions.\n\nOverall:\nThis paper mentions many important issues that have not been paid attention to by the current research on OoD detection. However, there are still some problems with this article. If the authors can provide convincing replies to the above queries, I am willing to improve my initial score.\n\n\n[1] Goodge, Adam, et al. \"Lunar: Unifying local outlier detection methods via graph neural networks.\" AAAI. 2022.\n[2] Chauhan, Kushal, et al. \"Robust outlier detection by de-biasing VAE likelihoods.\" CVPR. 2022.\n[3] Wang, Yezhen, et al. \"Energy-based open-world uncertainty modeling for confidence calibration.\" CVPR. 2021.\n[4] Bao, Wentao, Qi Yu, and Yu Kong. \"Evidential deep learning for open set action recognition.\" ICCV. 2021.\n[5] Cao, Senqi, and Zhongfei Zhang. \"Deep Hybrid Models for Out-of-Distribution Detection.\" CVPR. 2022.\n[6] Wang, Haoqi, et al. \"ViM: Out-Of-Distribution with Virtual-logit Matching.\" CVPR. 2022.\n[7] Vaze, Sagar, et al. \"Open-Set Recognition: A Good Closed-Set Classifier is All You Need.\" ICLR. 2021.\n[8] Du, Xuefeng, et al. \"VOS: Learning What You Don't Know by Virtual Outlier Synthesis.\" ICLR. 2021.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The language of the article is clear and the overall quality is good. \n- Although the article does not propose a novel model in terms of method, it raises the problem of existing OSR, which will provide guidance for future work, and the article is relatively novel. \n- The authors provide the source code as supplementary material and the results are reproducible.",
            "summary_of_the_review": "This paper mentions many important issues that have not been paid attention to by the current research on OoD detection. However, there are still some problems with this article. If the authors can provide convincing replies to the above issues, I am willing to improve my initial score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper307/Reviewer_ZHBz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper307/Reviewer_ZHBz"
        ]
    },
    {
        "id": "afTT-d6I1_L",
        "original": null,
        "number": 3,
        "cdate": 1666623970906,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623970906,
        "tmdate": 1672455660382,
        "tddate": null,
        "forum": "xLr0I_xYGAs",
        "replyto": "xLr0I_xYGAs",
        "invitation": "ICLR.cc/2023/Conference/Paper307/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses an extended problem of open-set recognition (OSR), called Unified Open-set Recognition (UOSR). Unlike OSR, which only rejects testing samples from classes that the model has not seen during training, UOSR aims at rejecting both samples from unseen classes and also samples from seen classes but wrongly classified. The paper aims at providing a comprehensive study of the UOSR problem by adopting existing OSR methods. Meanwhile, analysis is also given on the effects of supervised pretraining on ImageNet or Kinetics 400 and the effects of training the model with outlier exposure with extra out-of-distribution samples. Finally, the paper introduces a simple technique for few-shot UOSR by introducing a KNN-based margin in SoftMax for uncertainty estimation, showing improved performance compared with the baselines.",
            "strength_and_weaknesses": "Strength\n- The paper introduced a relatively thorough analysis of OSR methods for UOSR, revealing that the wrongly classified samples from known classes have a huge impact on the performance.\n- The paper is generally well-written and easy to follow.\n\nWeaknesses\n- Though a good set of analysis is given, some of them appear to be straightforward and intuitive. For example, The wrongly classified samples from known classes have a strong impact on the final performance, which appears to be very obvious especially when the correlation between InD performance and OoD performance is founded (Vaze et al, 2022) because better  InD performance means less InW and more InC.  Hence, the additional value of the study here seems to be limited.\n- It is unclear why state-of-art methods such as ARPL, ARPL+CS, and OpenGAN are missed in the comparison and analysis.\n- The SSB benchmark introduced by Vaze et al, 2022 appears to be better suited for OSR performance evaluation (thus UOSR), especially when there are fine-grained datasets and ImageNet-scale OSR data. It would be more convincing to evaluate the method on SSB.",
            "clarity,_quality,_novelty_and_reproducibility": "- Paper is generally well-written and easy to follow.\n- Novelty is somewhat limited as discussed above. \n- Code is provided in the supplementary, though I didn't get a chance to run to verify.",
            "summary_of_the_review": "Overall, I find this paper interesting with promising analysis. However, given the concerns above on the significance of the findings, which appear to be quite straightforward after having the findings by Vaze et al 2022, as well as the concerns on compared methods and datasets, I would hold a relatively conservative view about the paper and not recommend for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper307/Reviewer_zcng"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper307/Reviewer_zcng"
        ]
    },
    {
        "id": "o6dMS46nKhO",
        "original": null,
        "number": 4,
        "cdate": 1667232423997,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667232423997,
        "tmdate": 1670863263446,
        "tddate": null,
        "forum": "xLr0I_xYGAs",
        "replyto": "xLr0I_xYGAs",
        "invitation": "ICLR.cc/2023/Conference/Paper307/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work authors evaluate various open set recognition methods on the task of Unified Open set recognition(UOSR) where we need to detect novel classes but also misclassified/incorrect prediction. Authors also investigate training methods which reportedly improve open set recognition on UOSR. In addition, provide few-shot UOSR evaluation.\n",
            "strength_and_weaknesses": "Strengths:\n\nPaper is easy to follow and task of UOSR & evaluations is important to community.\n\n\nWeakness:\n\nLacks novel insights, methods for UOSR based on observed phenomenon, please see next section for more details. \nIs there any particular reason to not consider baselines like Deep ensembles [2] which is best performing method in initial UOSR evaluation benchmarking study? Also, as the paper makes a point w.r.t entropy distributions, it might be worthwhile to evaluate calibrated models (e.g. temperature scaling) and see the relative change in behavior compared to baseline models.",
            "clarity,_quality,_novelty_and_reproducibility": "Regarding finding that entropy distributions are more closer to UOSR than OSR. We know entropy is a good anomlay detecion measure on-par with max-logit, max-softmax probability for OOD. For a well calibrated classifier we expect entropy to be high for both OOD & mis-classified samples. All visualizations of OSR/OOD in 2D shows OOD occupies center of viz or high entropy region especially for openset detection, meaning difficult to make decision which is the case for misclassified examples too. \n\nRegarding devil is in wrongly classified samples, I think it is known phenomenon in openset detection works and [1] whole claim that good closed set improves open set detection also supports this. In addition, for e.g. in review of  Open-Set Recognition: A Good Closed-Set Classifier is All You Need, reviewer HAFU makes point about overlap of misclassified/incorrect predictions and openset predictions. \n\n\nIt is interesting to see that pre-training improves UOSR while outlier exposure does not, this can be expected as by pre-training we obtain richer or more-informative representations (lower bounded by training from scratch) whereas outlier exposure is more like a regularization technique w.r.t in-distribution decision boundaries & OE also makes the point that small amount of diverse data is useful further addition of data doesn't necessarily improve performance.\n\nRegarding few shot UOSR, there are works on few shot open set detection but also GCD[4] and CCD[5], which do not even use labels as initial representation can cluster Near OOD classes reasonably well. [3] Decomposes representational capacity with unknown/anomaly detection measure like max-logit and argues that in-distribution classes-based representation has significant ability to detect OoD classes so not sure if few shot extension to UOSR is worthwhile novel contribution.\n\nReferences:\n[1] Vaze et al. Open-Set Recognition: A Good Closed-Set Classifier is All You Need (ICLR 2022) \n[2] J. Kim et al. A Unified Benchmark for the Unknown Detection Capability of Deep Neural Networks\n[3] R. Garrepalli et al. Oracle Analysis of Representations for Deep Open Set Detection\n[4] Vaze et al. Generalized Category Discovery (CVPR 2022) \n[5] Grow and Merge: A Unified Framework for Continuous Categories Discovery (NeurIPS 2022)",
            "summary_of_the_review": "Authors evaluate methods on important task of UOSR but given lack of contributions in terms of providing new method, insight or unknown empirical behavior. Given UOSR is already established & evaluated in earlier work, and the emphasized phenomenon of wrongly classified examples well. Based on GCD, we know clustering works even in unsupervised fashion for OSR and hence few-shot learning would work well too albeit with lesser data. Although it is interesting to validate empirically that expectation of Uncertainty distributions is close to UOSR, in my opinion not sure if the work is worthwhile of ICLR acceptance. \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper307/Reviewer_q3SW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper307/Reviewer_q3SW"
        ]
    }
]