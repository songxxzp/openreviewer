[
    {
        "id": "aFOZ0Nrdxb3",
        "original": null,
        "number": 1,
        "cdate": 1666590241542,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666590241542,
        "tmdate": 1666590241542,
        "tddate": null,
        "forum": "7bJizxLKrR",
        "replyto": "7bJizxLKrR",
        "invitation": "ICLR.cc/2023/Conference/Paper3109/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper empirically evaluates how machine learning models forget the training samples during the training process. The paper empirically demonstrates that machine learning models do memorize the training samples once it sees them, but tend to forget them gradually once the training proceeds without seeing them again. The paper claims that the non-determinism in the training process might be one possible reason, by showing that empirically and theoretically that forgetting does not happen in deterministic training, but could happen when non-determinism is introduced.",
            "strength_and_weaknesses": "\n* The hypotheses on why forgetting happens in practice is interesting and could spur future research on this topic\n\nWeaknesses:\n\n* The empirical results on forgetting are not surprising. In fact, as discussed in Section 2.3, similar insights have already been discussed in prior work. (Although there is still value in demonstrating the empirical results).\n\n* Overall the paper is well-written. However, there are still some unclear points:\n    - Page 4: \"we instead design a procedure to measure an algorithm\u2019s worst-case forgetting.\" How can this algorithm measure the *worst-case* forgetting?\n    - Figure 1a: the accuracy increases before the injection (the vertical line). How could that happen?\n    - Page 6: \"...by differential privacy with \u03b5 \u2248 0.6\". How do you get this epsilon? The detail is missing.\n    - The last paragraph of Section 4: \"We \ufb01nd that examples injected before the decay are forgotten signi\ufb01cantly slower than examples injected after.\" I do not see how to draw this conclusion from Figure 3.\n    - Continuing the above question: figure 3 uses #steps to label the bars, but uses inject epoch as the x-axis. Without knowing the number of steps per epoch, it is unclear how to read this figure.\n    - The discussion in Section 5.1 is interesting. However, it slightly disconnects from the main story of the paper. While the non-convexity in the illustrated k-means example \"can prevent forgetting\", it does not give much insight into the neural networks, as the non-convexity in the two settings is very different. The story goes much better without Section 5.1.\n    - Section 6: \"We \ufb01nd the size of the dataset is key\". Which results in the main text illustrate this?\n    - Section 6: \"forgetting is more likely to happen when ... these examples are sampled from a large training set (e.g., when training modern language models).\" Which results in the main text illustrate this?\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity: The paper is overall well-written but there are still some issues (see the above points).\n\nQuality: Overall, the paper is of good quality (except for the above points).\n\nReproducibility: the code is not provided.\n\nNovelty: Some insights illustrated in the experimental results are already known in the prior work, but the systematic experiments conducted in this paper are still useful.\n",
            "summary_of_the_review": "\nAlthough the insights illustrated in the experiments are not surprising, having such a systematic empirical study is still useful to the community. The proposed metric can be useful for follow-up work. The hypothesis on the reason for forgetting can spur future research. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3109/Reviewer_2BVh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3109/Reviewer_2BVh"
        ]
    },
    {
        "id": "XXoTiqpRjp",
        "original": null,
        "number": 2,
        "cdate": 1666635786825,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635786825,
        "tmdate": 1666635786825,
        "tddate": null,
        "forum": "7bJizxLKrR",
        "replyto": "7bJizxLKrR",
        "invitation": "ICLR.cc/2023/Conference/Paper3109/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the connection between training data memorization and data forgetting. By measuring the forgetting with privacy attacks (member inference attack and data extraction attack), this paper shows that examples that are not observed by models in recent training iterations are more likely to be forgotten (i.e. more robust to privacy attack). In addition, this paper also conducts some studies to better understand forgetting and shows that the k-means algorithm does not experience a forgetting and non-deterministic (like random sampling) may be one of the reasons causing forgetting in the training.",
            "strength_and_weaknesses": "Many thanks for the paper. I thoroughly enjoyed reading this paper and thinking about how privacy risk is related to forgetting and memorization problems in deep learning. I listed the strengths and some of my concerns of the paper as follow.\n\nStrength:\n+ The paper studies an interesting problem which is the connection between data forgetting and memorization.\n+ Different from current forgetting measuring for catastrophic forgetting, this paper proposes a novel method based on privacy attack: an example that is easier to be inferred by a privacy attack is less likely to be forgotten by models. The evaluation results show that, if examples have not been seen recently, they are less susceptible to privacy attacks.\n+ This paper also shows that the k-means method is resilient to forgetting and non-deterministic can be the reasons behind forgetting in training.\n+ The writing of the paper is clear and good to follow. \n\nWeakness:\n+ The paper proposes to use a privacy attack to evaluate the forgetting of examples in training, but I am not quite convinced why the proposed forgetting measure is better than the existing forgetting measuring method. Although section 3 talks about the difference, there is no empirical evaluation to compare this difference. What could be the connection between the privacy-based forgetting metric and the traditional forgetting metric (like accuracy or training loss)? If we use the traditional forgetting metric, can we get similar conclusions discussed in the paper? Like, if examples have not been seen recently, the examples are less likely to be forgotten?\n\n+ I am not sure if the proposed method indeed measures the forgetting of examples. By measuring by inference attack, it is more like measuring how much models memorize examples. Examples never learned by models have low privacy risks but should not be treated as forgotten examples. I feel that the proposed method measures the privacy risk, but not necessarily if models forget examples.\n\n+ Why do we need to \u201cthe worst-case\u201d examples to measure forgetting? Is the proposed method a general method to measure forgetting of all training examples (i.e. \u201caverage-case\u201d in the paper)?  \n\n+ One of the main arguments in this paper is that, if examples have not been seen recently, they are less susceptible to privacy attacks, which may not be so surprising. Privacy leakage on datasets usually comes from unwanted memorization of training data. Examples are expected to be less memorized if they have not been seen recently due to catastrophic forgetting features of deep learning models, which makes them less susceptible.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to read. There lacks some empirical evidence to show why the proposed forgetting measure method is better than traditional measuring methods. The appendix includes details on the experiment implementation.",
            "summary_of_the_review": "I thoroughly enjoyed reading this paper and thinking about how privacy risk is related to forgetting and memorization problems in deep learning. My major concern is that, as a novel method to measure forgetting, there may lack some evidence to show how the proposed method is better than existing methods. Besides, the reason behind the design of the measuring method is a bit unclear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3109/Reviewer_TqY1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3109/Reviewer_TqY1"
        ]
    },
    {
        "id": "tHuzjpJhu-O",
        "original": null,
        "number": 3,
        "cdate": 1666698810051,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666698810051,
        "tmdate": 1669110892124,
        "tddate": null,
        "forum": "7bJizxLKrR",
        "replyto": "7bJizxLKrR",
        "invitation": "ICLR.cc/2023/Conference/Paper3109/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work aims to quantify how much models forget the training samples. To do so, the authors connect the forgetting and privacy, then utilize privacy-related algorithms to measure the forgetting. In addition, the authors claim that nondeterminism could be the potential explanation and conduct some experiments and calculations to support it.",
            "strength_and_weaknesses": "Strength:\n- Exploring the forgetting phenomenon is a fundamental topic for the machine learning community, thus, the work is interesting and promising. \n- This work is inspiring, as connecting privacy and forgetting is novel to me. One interesting direction may be evaluating the effectiveness of DP algorithms using similar connections. \n- This work provides a novel measurement for modeling forgetting.\n- The paper is well-written and easy to follow.\n\nI do enjoy reading the paper, but several concerns make me give the current rating. I will raise the score if the authors can address my concerns.\n\nWeakness:\n- My main concern is about nondeterminism, i.e., Sec. 5. I think the experimental and theoretical analysis seem weak to support the perspective. Specifically,\n1) (Sec. 5.1) the authors claim that non-convexity can prevent forgetting, but the designed experiments are not good, where adding more samples when performing the k-means algorithm has little impact on forgetting samples.  I think the reason should be that the employed algorithm does not change the features of samples, thus, always remember the samples, which is different from the scenario of training DNNs, i.e., updating makes features change. Thus, remembering comes from unchanged features rather than nondeterminism.\n2) (Sec. 5.2) further experiments conducted to support the perspective are given in Sec. 5.2, which also seems not good enough. Specifically, the adversary holds the same data D, extra data D_p, and even the same batch order. This means that the adversary knows everything. I cannot understand why the adversary is interested in attacking the target because the adversary even knows exactly what the difference is from D_p.\n3) (Sec. 5.3) the authors draw a conclusion using the introduced example of mean estimation: introducing two different samples causes the difference between models, but the nondeterminism can cause forgetting, i.e., hard to distinguish the difference. However, the difficulty in distinguishing actually results from more training iterations, k in Theorem 1, rather than the nondeterminism.\nI hope the authors could clarify the above questions.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Good Quality, novelty, and reproducibility.",
            "summary_of_the_review": "Surprisingly novel, but with limited reasonable support.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3109/Reviewer_hxrq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3109/Reviewer_hxrq"
        ]
    },
    {
        "id": "kAk71bwIrhd",
        "original": null,
        "number": 4,
        "cdate": 1666729195302,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666729195302,
        "tmdate": 1666729195302,
        "tddate": null,
        "forum": "7bJizxLKrR",
        "replyto": "7bJizxLKrR",
        "invitation": "ICLR.cc/2023/Conference/Paper3109/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors study the effect of the forgetting of machine learning models on privacy. To this end, they focus on validating the two hypothesis, including the pessimistic hypothesis and the optimistic hypothesis. Then, they design a method to measure the example forgetting in the training and show several analytic and empirical findings with this method. ",
            "strength_and_weaknesses": "Strength:\n\n-  This work is well motivated. In this paper, the authors aim to study the connections between memorization and forgetting in the context of data privacy.\n- The paper writing is easy to follow. The authors give a clear definition of forgetting before introducing the measure method, which makes it much clear for the reader.\n- This work gives several interesting empirical findings and provides in-depth understandings. Specifically, the authors show the two cases for when the forgetting happens, which is useful for the future investigation.\n\nWeakness:\n\nThe contribution of this work is limited due to lack of theoretical guarantee. Although this work provides many empirical studies, it is not clear if the conclusions can extend to other settings, like different model architectures. It would be better if the authors can give any theoretical guarantee.",
            "clarity,_quality,_novelty_and_reproducibility": "Claritya and Quality: This paper is well-written and provide sufficient empirical results to support their claim.\n\nOriginality: This work is novel from my perspective. Before this work, there is no study to exactly show when the forgetting happens.\n\n",
            "summary_of_the_review": "In summary, this work is well motivated and easy to follow. Besides, the work provides in-depth understandings with empirical findings, which, I believe, can bring some new insights to inspire future research. However, the contribution of this work is still limited without theoretical results. I am not an expert in this area, so I give a weak accept for this work before the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3109/Reviewer_Qkp9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3109/Reviewer_Qkp9"
        ]
    }
]