[
    {
        "id": "4cG3mdSALj",
        "original": null,
        "number": 1,
        "cdate": 1666635054777,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635054777,
        "tmdate": 1666635054777,
        "tddate": null,
        "forum": "vuD2xEtxZcj",
        "replyto": "vuD2xEtxZcj",
        "invitation": "ICLR.cc/2023/Conference/Paper3846/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to accelerate training by pruning the calculated gradients. More specifically, the authors propose 1:2 and 2:4 minimum-variance unbiased estimators and show their effectiveness on a number of tasks, where accuracy is not severely affected by the proposed pruning scheme.",
            "strength_and_weaknesses": "The paper is well written and proposes very attractive improvements to training speed without affecting the accuracy.\nGiven a relatively general title, I'd be curious to see some results on using the approximate method in setups with different sparsity setups, e.g, 4:8, and also higher sparsity, e.g., 1:4.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the results are presented in a clear way.\nProposed pruning scheme seems straightforward to implement, but having access to a reference implementation may still be valuable.",
            "summary_of_the_review": "A well written paper with interesting and relevant results for practical applications of DL.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3846/Reviewer_ccBg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3846/Reviewer_ccBg"
        ]
    },
    {
        "id": "KjLbB3pNhxE",
        "original": null,
        "number": 2,
        "cdate": 1666659705703,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659705703,
        "tmdate": 1666659705703,
        "tddate": null,
        "forum": "vuD2xEtxZcj",
        "replyto": "vuD2xEtxZcj",
        "invitation": "ICLR.cc/2023/Conference/Paper3846/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper develops a new scheme for pruning the gradients passed through a network during back-propagation. In particular, the proposed method designs a stochastic pruning operator which is unbiased and provides the minimum variance output. Both 1:2 (2 inputs, 1 non-zero output) and 2:4 (4 inputs, 2 non-zero outputs) variants of this operator were developed. In testing, networks pruned with the proposed operator demonstrated little-to-no degradation in performance across a range of network architectures and classification problems.\n",
            "strength_and_weaknesses": "# Strengths\n\nThe paper provides a novel method for pruning the back-propagation gradients.\n\nThe proposed method does not hurt network performance, whereas naive greedy baselines hurt performance significantly.\n\nThe paper is well-written and reasonably easy to follow.\n\nThe paper provides a significant discussion of the implications the paper's results have on hardware acceleration and design\n\n# Weaknesses\nThe paper doesn't make a strong case for why the purning operator needs to be unbiased and only points vaguely to Chmiel et al.'s finding that \"for the neural gradients... it is critical to use unbiased quantization (i.e., Bias[\u03b8(a)] = 0).\"\n\nFor completely understandable reasons (no hardware support for pruning during training), the paper is unable to demonstrate any actual training time reductions. Hopefully this changes as new hardware is developed.\n\n# Suggestion\nThe introduction of the MSE of the pruning operation describes $a$ as scalar, rather than a vector. The resulting discussion is then confusing as there's no stochasticity in $a$ and one is left asking why not just let $\\theta(a)=a$. I suggest introducing $a$ as a vector from the start, as this will make the resulting discussion easier to follow -- there are many choices for the pruning operator which would produce different MSEs. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Besides the above suggestion, the paper is generally easy to follow.\n\nThe results demonstrate the proposed 2x pruning procedure doesn't hurt performance significantly.\n\nThe proposed pruning operator is, to my knowledge, novel and seems simple to reproduce from the provided equations.\n",
            "summary_of_the_review": "The paper provides a simple gradient pruning procedure that could accelerate future hardware systems. While the form of the pruning operator could use further motivation, I'm overall in favor of this paper's publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3846/Reviewer_B9mF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3846/Reviewer_B9mF"
        ]
    },
    {
        "id": "t0y3Kc9JrM5",
        "original": null,
        "number": 3,
        "cdate": 1666944891762,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666944891762,
        "tmdate": 1666944891762,
        "tddate": null,
        "forum": "vuD2xEtxZcj",
        "replyto": "vuD2xEtxZcj",
        "invitation": "ICLR.cc/2023/Conference/Paper3846/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a method for a stochastic unbiased masking of the gradients such that half of them are 0 and they can be used to accelerate the matrix multiplications in an accelerator such as a GPU. In particular, the method induces a form of sparsity called N:M sparsity where N out of M consecutive (in memory) elements are 0. The authors propose to use a masking such that the gradients are still unbiased but also such that they have the minimum variance. Thorough experiments show that using their approximate 2:4 sparsity algorithm as well as the exact 1:2 algorithm, allows training a variety of neural network architectures on both images and text with minimal loss in final performance, if any. Moreover, the authors show that their algorithm can be combined with similar algorithms for the forward and backward pass as well as quantization without significant drop in performance.",
            "strength_and_weaknesses": "Strengths\n------------\n\n- The theoretical analysis is well written and intuitive\n- Accelerating GEMMs is a very important topic of research as it affects all modern neural network architectures\n- Thorough experimental evaluations show that the method indeed does not hurt performance and can be used to train a variety of models\n\nWeaknesses\n--------------\n\n- The main weakness of the paper is the lack of real world performance improvements due to lack of custom kernels or support from currently available accelerators. The speedup that is mentioned is a best case scenario and makes the method seem more useful than it is probably.\n- Another weakness is the lack of comparisons with MSE based sparsity for the gradients. Even though previous work has shown that unbiased estimators are more important for the gradients than the weights, it would strengthen the paper significantly if instead of the greedy estimator the paper also compared with MSE optimal biased estimators.\n\nTypos\n------- \n\n- eq. 8 typo it should be $sign(a_2) \\left(\\|a_1\\| + \\|a_2\\|\\right)$\n- Fig. 2 typo $a_4 >> \\max(a_1, a_2, a_3)$",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with clear analyses and provides code for the proposed method.",
            "summary_of_the_review": "The method is very intuitive and straightforward. It may be slightly incremental work, however I believe it is a clear contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3846/Reviewer_e1dv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3846/Reviewer_e1dv"
        ]
    }
]