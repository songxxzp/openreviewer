[
    {
        "id": "-Mp5S7CY0E",
        "original": null,
        "number": 1,
        "cdate": 1666513092441,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666513092441,
        "tmdate": 1669092752102,
        "tddate": null,
        "forum": "LGkmUauBUL",
        "replyto": "LGkmUauBUL",
        "invitation": "ICLR.cc/2023/Conference/Paper2450/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to combine white-box meta-learning methods and\ndistributional RL. The main benefit of this combination is that the\ndistributional return can not be adaptive, and perhaps allow for more\neffective RL agents. In particular, the authors propose a meta-gradient\nthrough the parameters that define the quantile and its loss. The\ndistributional meta-update is said to be compatible will almost all\nmeta-gradient RL work. Experiments are first conducted on a small\nnon-stationary grid world problem, but the majority of the results are\non Atari 200M, where they beat distributional baselines and a\nrepresentative meta-gradient baseline in aggregated score. They note,\nhowever, that the SOTA meta-gradient baseline - STACX - is difficult to\nreproduce and do not compete directly.\n\n",
            "strength_and_weaknesses": "# Strengths\n\n-   Strong empirical results, versus the baselines presented. This is,\n    however, somewhat overshadowed by STACX missing as a baseline. It is\n    also to be expected that adapting more hyperparameters allows for a\n    more flexible learner as it is more general than the baselines\n    compared against.\n\n-   The contribution is clear: both meta-gradient RL methods and\n    distributional RL are strong methods in current deep RL. Combining\n    them together is a clear and interesting research direction.\n\n# Weaknesses\n\n-   Some important baselines and experimental details are missing that\n    would help contextualize the merit of distributional meta-gradients.\n-   Overall lacking in novelty. While the contribution is clear and\n    interesting, combining meta-gradients and distributional RL is\n    rather obvious as both methods are orthogonal.\n-   The experiments are somewhat limited. Atari 200M is a large and\n    difficult problem, and success on this benchmark is commedable.\n    Being a large benchmark, however, limits the accuracy of the\n    conclusions because of the high degree of variability. Although the\n    results are averaged over 5 runs, no error bars are reported and the\n    ablation on various games is somewhat ad hoc.\n",
            "clarity,_quality,_novelty_and_reproducibility": "# Detailed Comments (Clarity, Quality, Novetly, Reproducibility)\n\n-   Section 1, Paragraph 1 (Meta-gradient RL): It is limiting to say\n    that meta-gradient RL only provides better objectives. While this is\n    true in Xu et al. 2022, meta-gradients can also be used to\n    meta-learn hyperparameters unrelated to the target or objective,\n    such as the learning rate or epsilon for exploration.\n\n-   Section 1, Paragraph 2: \"This is essential for RL updates and even\n    more critical for meta-gradients that enjoy multiple satisfying\n    properties\"\n\n    It is not clear what \"this\" refers to, but if you mean value\n    distributions then this is not true nor demonstrated in any\n    meta-gradient RL work.\n\n-   \"On the other hand, distributional RL methods have good optimization\n    properties in the loss form\"\n\n    I do not think this is true either. While there are worse options\n    than the KL divergence for stability, that does not mean that the KL\n    loss in distributional RL has particularly good proprerties in\n    comparison to non-distributional RL.\n\n-   Section 2, Paragraph 3: This paragraph is key and really helped me\n    understand the goal of this paper more than the abstract or\n    Section 1. I would encourage the authors to rework this into the\n    introduction.\n\n-   Section 3 \" the Bellman operator for quantile distribution does not\n    need to perform projection (lose precision) or manually define the\n    return boundaries, like C51 (Bellemare et al., 2017) does.\" This\n    seems at odds with the claim that distributional RL is a good method\n    for meta-learning because of the additional hyperparameters it\n    requires.\n\n-   Section 4 ( Actor-Critic with Distributional \u03bb-Return): this\n    majority of this subsection seems better suited to the preliminaries\n    section. It is not clear which parts are new contributions or\n    relating to previously established work on meta-gradient\n    actor-critic.\n\n-   Section 4: \"Our proposed distributional adaptive return can\n    naturally boost up the scope of self-tuning hyperparameters in the\n    meta-gradient RL algorithm.\"\n\n    Not clear what this means exactly, but the principle of self-tuning\n    hyperparameters is not limited in scope in any fundamental way. Using meta-gradients to tune specific hyperparameters related to distributional return is merely an application.\n\n-   Section 5.1 (Two Colors Grid): While the results are compelling, in\n    the sense that DrMG adapts quicker than MG, it is not clear whether\n    this is due to Distributional meta-gradients or just the\n    distributional base algorithm. Unfortunately, the appendix is short\n    on details here.\n\n-   Section 5.2 (Atari 2600 200M): Again, the results here are\n    compelling, albeit difficult to ascertain their true significance.\n    Atari 200M is a large and difficult task, and the reported\n    performance seems promising. But, as an evaluation benchmark, it is\n    quite limited due to its size and inability to run many seeds with\n    confidence intervals.\n\n-   Section 5.3 (Ablating adaptive vs non-distributional return): It is\n    true that an important ablation study would compare the importance\n    of adaptive and non-adaptive distributional return. However, an even\n    more important baseline would be a meta-gradient algorithm that uses a distributional return but does\n    not take a meta-gradient through the distributional hyperparameters. Without this very important baseline, it's difficult to tell how important meta-gradients are for distributional RL in particular. \n\n# Minor Comments\n\n-   Section 3 \"computed from the updated model parameters \u03b8\"\n\n    Should this be: \"computed from the updated model parameters \u03b8'\"?\n\n    If not, you should be clear what \u03b8\u2032 is, for example\n    \u03b8\u2032\u2004=\u2004\u03b8\u2005\u2212\u2005f(\u03c4,\u2006\u03b8,\u2006\u03b7).\n\n-   Section 4, Distributional Meta-Gradient (usage of \u03b1): earlier you\n    used \u03b1 as a step-size so it is confusing to use it for the leak\n    v-trace as well.\n\n-   Section 4, \"for our method; with STACX and MG it would be 3 \u00d7 N + 3,\n    7 \u00d7 3 and 2, respectively.\" What is N here?\n\n-   Section 5: \"We toggle the reward at each 1e5 steps to investigate\n    the agent\u2019s adaptation ability. \" What does this mean exactly? If\n    the blue key gives a reward of 0.7 and the red key gives a reward of\n    0.3, does toggling switch the color-reward correspondence.\n\n-   Section 5.1 title: \"GIRD-WORLD\" -> \"GRID-WORLD\"\n\n",
            "summary_of_the_review": "\n# Decision\n\nWhile there is obvious appeal in combining two currently separate but\nsuccessful RL methods (that of distributional and meta-gradient RL), I\nthink the paper as a whole does not provide enough insight into\nchallenges and benefits of such a combination. The paper clearly\ndemonstrates that the proposed method (DrMG) is able to outperform\ndistributional baselines and a meta-gradient baseline. The paper also\ndemonstrates that adaptive distributional returns are important in some\nAtari games. As a whole though, the Atari benchmark makes sound\nempirical comparison difficult due to the fact that no statistical\nevidence is presented. In addition, some important baselines are missing\n(STACX, which cannot be reproduced to the reported performance as well\nas a distributional meta-gradient baseline that does not adapt the\ndistributional hyperparameters). As the paper currently stands, I am\nleaning towards weak reject with the possibility of increasing to weak\naccept if some of the gaps in the baselines are addressed.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2450/Reviewer_LNTi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2450/Reviewer_LNTi"
        ]
    },
    {
        "id": "Bw8-s7-s9C",
        "original": null,
        "number": 2,
        "cdate": 1666628376698,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628376698,
        "tmdate": 1666628376698,
        "tddate": null,
        "forum": "LGkmUauBUL",
        "replyto": "LGkmUauBUL",
        "invitation": "ICLR.cc/2023/Conference/Paper2450/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper is considering meta-gradient RL, algorithms that consider the gradient of the gradient descent update to learn better objectives. The authors are proposing an algorithm called DrMG which is based on two foundational choices: (a) the use of a quantile distribution for the return and (b) the use of a white-box meta-gradient RL model where the loss function depends on two sets of parameters, the low-level parameters theta and the high-level parameters eta. The algorithm used for RL is a actor-critic class algorithm, with the critic approximating a quantile distribution. ",
            "strength_and_weaknesses": "+ Convincing justification that the estimation through a quantile distribution represented through a uniform mix of Dirac's deltas. \n+ In the evaluation on both a toy problem and on Atari games, the overall performance shows a clear improvement over the comparable algorithms.\n\n- When replacing a numerical value with a distribution, the reader would be naturally interested in how this distribution looks like. Is it Gaussian, skewed, fat tailed, multimodal distribution? The paper makes no effort to investigate this. ",
            "clarity,_quality,_novelty_and_reproducibility": "+ Clear description of the justification of the proposed approach and how it fits in the state of the art.\n+ The primary novelty of the paper is to approximate the distributional value in the value update of the RL, and finding an efficient method to do this.",
            "summary_of_the_review": "A novel contribution to the field of model free RL providing faster learning and higher performance than comparable algorithms on the standard benchmarks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2450/Reviewer_K6pA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2450/Reviewer_K6pA"
        ]
    },
    {
        "id": "hhz6ND6KiJ",
        "original": null,
        "number": 3,
        "cdate": 1667482130665,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667482130665,
        "tmdate": 1667482130665,
        "tddate": null,
        "forum": "LGkmUauBUL",
        "replyto": "LGkmUauBUL",
        "invitation": "ICLR.cc/2023/Conference/Paper2450/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In the existing meta-gradient studies, the adaptive return is simply formulated in the form of expected cumulative rewards. In this paper, the authors address the meta learning problem of the distributional actor-critic method with quantile distributions by considering the distributional $\\lambda$ return and apply the standard meta-gradient method to self-tune a set of hyperparameters. Some of the hyperparameters are quantile-specific, which offers a larger scope of hyperparameters compared to the prior works. This new algorithm outperforms the existing meta-gradient methods in Grid-world and Atari games, achieving the state-of-the-art performance.",
            "strength_and_weaknesses": "**Strength**\n\n- This paper provides the first distributional attempt of meta-gradient RL.\n- The empirical results are quite promising in Atari 2600 benchmarks.\n\n\n**Weakness**\n\n- The proposed algorithm appears to be a direct combination of distributional actor-critic and the standard meta-gradient method. The novelty of the algorithm is somewhat limited.\n- Comparison with baselines: This paper empirically compares DrMG mainly with the standard MG and a more recent method STACX. However, the most recent baseline Bootstrapped Meta Gradient (BMG) was excluded for comparison. The paper mentioned that \u201c... which is excluded here for comparison because it alters some important experimental settings for 200M evaluation to boost the performance.\u201d Despite this, it still appears feasible to adjust the setting of BMG to ensure a fair comparison. I do not fully get the difficulty here. More justification would be needed.\n\n\nAdditional issues: Some explanations of the methodology are incorrect or confusing. For example: \n- The distributional Bellman operator in Eq (8) appears incorrect (cf. Eq. (5) in [Bellemare et al., 2017]).  \n- There shall be some typos in Eq. (14), e.g., the learning rates are missing. \n- The last quality of Eq. (14) would require explanation in more detail.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is overall well-written and easy to follow in most places. \n- Quality: The experimental results are very promising, and the ablation study is quite helpful in explaining the effect of the adaptive return and the lambda return. \n- Novelty: The idea of applying a meta-gradient method to distributional RL has never been substantiated or evaluated before. That said, the overall algorithm design appears to be a rather direct combination of meta gradient and quantile-based distributional RL.\n- Reproducibility: This shall be acceptable given that the critical parts of the pseudo code and the hyperparameters are provided in the appendix.\n \n",
            "summary_of_the_review": "This paper proposes an effective meta gradient method to boost the performance of quantile-based distributional RL. Despite that the idea is not very novel, the experimental results on Atari are very promising, and the ablation study verifies the efficacy of the adaptive return.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2450/Reviewer_ApMf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2450/Reviewer_ApMf"
        ]
    },
    {
        "id": "xkBfduvvi4B",
        "original": null,
        "number": 4,
        "cdate": 1667502186524,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667502186524,
        "tmdate": 1670562875064,
        "tddate": null,
        "forum": "LGkmUauBUL",
        "replyto": "LGkmUauBUL",
        "invitation": "ICLR.cc/2023/Conference/Paper2450/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a Distributional return Meta-Gradient algorithm (DrMG) algorithm, which combines distributional RL (like C51) and meta-gradient RL. Specifically, the authors formulate a distributional return in a meta-gradient RL setup and derive a meta-update rule to learn the adaptive distributional return with meta-gradients. Empirically, the proposed algorithm is compared with the existing meta-gradient RL methods on 57 Atari tasks.",
            "strength_and_weaknesses": "The strength and weaknesses of this paper are both significant.\n\n## Strength\n\n- The idea of combining distributional RL and meta-gradient RL is natural and intuitive.\n\n- The proposed algorithm DrMG seems to achieve great performance on the Atari benchmark compared to existing meta-gradient RL methods.\n\n- The ablation study considers several important design choices.\n\n\n\n## Weakness\n\nThe weaknesses mainly come from the experiments.\n\n- The toy experiment \"Two Colors Grid-World\" is very confusing to me. Specifically,\n  \n  - In Sec 5.1, this paper mentions \"We toggle the reward at each 1e5 steps to investigate the agent\u2019s adaptation ability.\" What is the exact meaning of \"toggle the reward at each 1e5 steps\"? According to the appendix, it seems the key collection reward values toggle between the two keys. However, if this is the case, then why a key-agnostic optimal agent could collect a +1.35 expected return? I think it should be 1.5.\n  \n  - In figure 1, the dashed line denoting the key-agnostic agent is neither 1.5 nor 1.35. What is the exact number of it and how do you get it?\n  \n  - Why the reward is modified in the middle of training? This is an uncommon rare practice in RL. The authors claim that \"Since the RL agents are unaware of the reward toggling during policy training, their adaptation ability is characterized by their efficiency in modeling the return during the training process.\" I do not really understand this point. Why the \"efficiency in modeling the return\" needs to be shown in such a weird setting?\n\n- The experiments on Atari are not conducted properly\n  \n  - First, it seems all the curves come without std. It is unconvincing if the experiments are only run with one random seed, especially when the curves look very unstable. I understand the computation cost of those experiments is high, but it is still necessary to try a least a few more random seeds. In STACX, they use 3 random seeds.\n  \n  - It is unclear why the experiments shown in figure 3 adopt an early stopping strategy but the experiments shown in figure 2 do not. Given that the full curves (training with 200M) should be available according to figure2, why not show the full curves in figure 3?\n\n- Minor points\n  \n  - The legends in the figures should be consistent. For example, \"MG\" in figure 2 but \"MetaGrad\" in figure 3.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity\n  \n  - The proposed method is clearly described and easy to follow.\n  \n  - The experiments section can be improved with more details and accurate descriptions.\n\n- Quality\n  \n  - The quality of this paper is ok but can be polished a little bit more.\n\n- Novelty\n  \n  - To the best of my knowledge, this paper is the first to combine distribution RL with meta-gradient RL.\n\n- Reproducibility\n  \n  - A lot of implementation details are presented in the appendix, but it would be great if the authors could release the code to the public.",
            "summary_of_the_review": "Overall, I feel this paper proposes a good approach that combines distributional RL and meta-gradient RL. However, the empirical evaluation of their approach is not done properly. Given that the idea itself is not extremely fancy, I feel proper evaluation is very important in this case. Therefore, I cannot recommend acceptance.\n\n\n=========\nUpdate after the rebuttal period:\n\nI previously leaned toward rejection. My main concern is that the experiments are not sufficient to support their claims fully. Given that this paper's technical novelty is not super significant (combining distribution RL and meta-gradient RL), I feel proper evaluation is very important in this case.\n\nHowever, during the rebuttal period, the authors provided additional experiment results, including more random seeds for each experiment, more baselines, and more ablations. The additional results make the empirical evaluation much stronger. Therefore, my main concern is addressed, and I lean toward acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2450/Reviewer_s6vn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2450/Reviewer_s6vn"
        ]
    }
]