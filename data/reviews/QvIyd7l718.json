[
    {
        "id": "Sh-ny5th7bq",
        "original": null,
        "number": 1,
        "cdate": 1666606015893,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666606015893,
        "tmdate": 1666606015893,
        "tddate": null,
        "forum": "QvIyd7l718",
        "replyto": "QvIyd7l718",
        "invitation": "ICLR.cc/2023/Conference/Paper4651/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a multi-task training strategy for improving the performance of the knowledge graph embedding (KGE) model in downstream tasks, e.g., entity classification or regression. In addition to the conventional link prediction training of KGE, the proposed model is pre-trained with 4 extra tasks, e.g., relation prediction, entity neighboring, etc. The experiments on FB15K-237, YAGO3-10, and WRNN show the multi-task trained model achieves better MRR on the pretraining tasks but lags behind LP than the single training (as expected). The performance on downstream tasks are marginally better for certain KGE model, e.g., RotatE on FB15K-237. Overall, the performance of graph-structure prediction tasks (e.g., NBE) is not stunning as dedicatedly trained. The performance of downstream tasks is also limited.",
            "strength_and_weaknesses": "Strength:\n1.\tSome observations are interesting, e.g., KGE performs poorly on graph-structure prediction tasks other than link prediction. \n2.\tThe paper is clearly written. \n\nWeakness:\n1.\tThe results are not stunning. The multi-task training does not lead to consistently better performance on all graph-structure prediction tasks. The performance improvement on downstream tasks is not effective. \n2.\tSome experimental settings are not rigorously designed. E.g., MTT should test on unseen tasks. The experimental results could be further discussed or explained, i.e., the variation of model behavior under different datasets/settings.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the representation is good. However, the model behavior should be further discussed or explained.\nThis paper contributes little technical novelty. Albeit some observations are interesting, the proposed method does not show enough effectiveness. \n\n",
            "summary_of_the_review": "1.\tThe results of applying multi-task training are not stunning at all. The model trained on the single link prediction task (STD) still has the best performance on the link prediction task (Table 3). And the model trained on extra tasks (MTT) no doubt performs well on the extra tasks. I would suggest authors train models ablative, i.e., for the unseen test tasks. In addition, the performance improvements on downstream tasks are also limited. E.g., On the FB15K-237 dataset, the STD strategy excels the proposed MTT for nearly all settings of EC (except for RotatE). In this regard, the effectiveness of the proposed multi-task training strategy is nor promising.\n\n2.\tThe domain and range predictions seem not widely used tasks for KGE. Are there any reasons for choosing these two tasks? E.g., do they have any potential for benefiting downstream applications?\n\n3.\tAs the SOTA performances of KGC are achieved by pretrained LM-based models, such as KG-BERT (Yao et al., 2019), KGT5 (Saxena et al., ACL\u201922), and KG-S2S (Chen, et al., COLING\u201922). The applicability of the proposed MTT strategy should be further discussed.\n\n4.\tIt is not clear how the wildcard entities (MTT part) approach the max-aggregation (MTR part).  The max-aggregation search for the best relation over entire entity set, while the wildcard entities make use of an abstract \"wildcard\" embedding to get the result. Although using the wildcard entity would be easier as it reduces the computational cost for searching, but I believe the two methods The methodologies for the two methods are conceptionally different and may have different results.\n\n5.\tWhy STD has better results than the dedicated MTT on NBE task (0.21 v.s. 0.152, Table 3) needs further explanation. \n\nMinor comments:\n(1) Authors should give the description of abbreviation STD (i.e., the model trained with single task, i.e., LP.) when it is first introduced in Section 4.1.\n(2) .. replace wildcards by their the corresponding .. -> remove the (page 5)\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns as far as I can see.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4651/Reviewer_x7Bp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4651/Reviewer_x7Bp"
        ]
    },
    {
        "id": "h88igodc1B8",
        "original": null,
        "number": 2,
        "cdate": 1666668488389,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668488389,
        "tmdate": 1666668488389,
        "tddate": null,
        "forum": "QvIyd7l718",
        "replyto": "QvIyd7l718",
        "invitation": "ICLR.cc/2023/Conference/Paper4651/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to explore the suitability of various Knowledge Graph Embeddings (KGE) for graph structure prediction tasks such as entity and relation neighborhood prediction, type/range of a given relation, and relation prediction. This text also investigates the performance of these models on several downstream tasks. Building upon these two investigations, the authors then propose a Multi-Task Training (MTT) objective to potentially improve the performance on graph structure prediction tasks and downstream applications without further task-specific training. ",
            "strength_and_weaknesses": "Strengths\n\n+ Motivation for the work is well-founded in literature and concretely defined in the work. \n+ Authors introduce several new basic graph structure prediction tasks (Neighborhood and Domain prediction). They also evaluate existing models on these tasks showing the poor generalizability of current KGE methods to such tasks.\n+ Further, this work proposes an MTT objective that includes the well-known Link and Relation Prediction tasks in conjunction with other graph structure prediction tasks introduced in this work.\n+ Evaluation shows that MTT training does help improve the existing KGE model's performance on graph structure prediction tasks and downstream tasks. \n\nWeaknesses / Questions\n\n+ Authors should provide an ablation study in order to better understand the role of each task introduced in MTT. \n+ In my opinion, this method loses the Efficiency vs Performance tradeoff. It takes 2x-4x (worst) more time to train but seems to marginal gain in performance on downstream tasks.\n+ It would have been interesting to see the results of KE-GCN on basic graph structure prediction tasks as well (treating these tasks as downstream and training on them). \n+ Instead of YAGO3-10, why didn't the authors use one of the [OGB](https://ogb.stanford.edu/docs/linkprop/) datasets for large-scale experiments? I would like to know if this is a conscious design choice by the authors.\n+ \"We also use the so-obtained wildcard embeddings for prediction in the same fashion; e.g., we set $$s(s, r^{'}, *) = s(s, r^{'}, any_{O})$$\". I am not sure which prediction the authors are talking about here. \n+ Please provide the code used for all the experiments. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written and the motivation of this paper is clear. While there are some minor weaknesses in the work, I feel that the research problem is well-founded and the authors have done a good job overall. \nAuthors use a well-known KGE framework for implementing the experiments in this paper but do not provide their code, so reproducibility takes a hit. \n",
            "summary_of_the_review": "Overall, I feel this is a good piece of work with a well-known and well-founded research problem. Authors have done extensive evaluation (with some minor weaknesses) to explore the use of Multi-Task Training objectives to improve the generalizability of Knowledge Graph Embeddings methods. To that extent, I vote to accept the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4651/Reviewer_1Rac"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4651/Reviewer_1Rac"
        ]
    },
    {
        "id": "NKNpVs-tRp4",
        "original": null,
        "number": 3,
        "cdate": 1666679823129,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679823129,
        "tmdate": 1666679823129,
        "tddate": null,
        "forum": "QvIyd7l718",
        "replyto": "QvIyd7l718",
        "invitation": "ICLR.cc/2023/Conference/Paper4651/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a thorough empirical investigation of the performance of knowledge graph embedding models on a variety of graph structure prediction tasks, including link prediction, relation prediction, domain prediction, and neighborhood prediction. The results show that these models do not perform well on the tasks except link prediction. The authors go on to propose and test multi-task learning to solve this problem. Multi-task learning can also help with KG downstream tasks like entity classification and regression.",
            "strength_and_weaknesses": "This paper presents some \"interesting\" findings regarding KG embedding models and applications. It would motivate us to reconsider the suitability and generalizability of KG embedding models. However, I have the following concerns.\n\nFirst, as a KG embedding expert, the finding does not surprise me that a model trained on the link prediction task performs poorly on other structure prediction tasks. There is a significant gap between the training and evaluation tasks in this experiment. It is also not surprising that multi-task learning can improve the model's performance on some structure prediction tasks, because, just as stated by the authors in Section 4.2, \"these tasks have been introduced as auxiliary training objectives\". So, I think this paper is not so attractive to the KG embedding researchers, although I appreciate the authors' hard work. It lacks in-depth analysis of the reasons behind the performance.\n\nSecond, the selected four KG embedding models in the empirical study, i.e., TransE, DistMult, ComplEx, and RotatE, are old. I think these four models could not adequately represent the large and fast-growing family of KG embedding models. In Section 4.1, the authors state that ComplEx is the SOTA factorization model. But in my view, the SOTA factorization-based KG embedding model is TuckER [1]. Also, some recent GNN-based [2] or Transformer-based KG embedding models are ignored in this study. So, a natural question is, whether the recent KG embedding models still suffer from the claimed issue in this paper.\n\n[1] Ivana Balazevic, Carl Allen, and Timothy Hospedales. 2019. TuckER: Tensor Factorization for Knowledge Graph Completion. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5185\u20135194, Hong Kong, China. Association for Computational Linguistics.\n\n[2] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal A. C. Xhonneux, Jian Tang: Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction. NeurIPS 2021: 29476-29490\n\n[3] Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao, Ruofei Zhang, and Yangfeng Ji. 2021. HittER: Hierarchical Transformers for Knowledge Graph Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10395\u201310407, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the work is of acceptable quality and originality. But, there is still room for improvement in writing:\n\n* \"Knowledge graph embeddings (KGE) models provide low-dimensional representations of the entities and relations in a knowledge graph (KG).\" -> Knowledge graph embedding (KGE) models provide low-dimensional representations of entities and relations.\n\n* \"..., STD training approach performs poorly on all graph-structure tasks\" -> ..., the STD training approach performs poorly on all graph-structure tasks\n\n* \"The performance for these tasks is significantly increased using MTT training.\" -> The performance of these tasks is significantly improved by using MTT training.\n",
            "summary_of_the_review": "Overall, this paper sheds some light on both KG embedding models and their applications. However, most of the findings, in my opinion, are common knowledge in the related field. Some recent representative KG embedding models and in-depth analysis of the reasons for the performance are lacking in the empirical study. So, I think the contribution of the paper is not enough.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4651/Reviewer_RUJy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4651/Reviewer_RUJy"
        ]
    }
]