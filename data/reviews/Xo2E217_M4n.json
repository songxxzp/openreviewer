[
    {
        "id": "UcDUbCStrA",
        "original": null,
        "number": 1,
        "cdate": 1666295634375,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666295634375,
        "tmdate": 1669652068309,
        "tddate": null,
        "forum": "Xo2E217_M4n",
        "replyto": "Xo2E217_M4n",
        "invitation": "ICLR.cc/2023/Conference/Paper6082/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a defense against backdoor attacks to federated learning. Existing defenses do not study how hardening benign clients can affect the global model (and the malicious clients).  The paper proposes a three stage defense that aims to harden benign clients and connect cross-entropy loss, attack success rate, and clean accuracy.  The proposed defense is evaluated on three datasets and shows better performance than previous defenses. \n\n\n",
            "strength_and_weaknesses": "Strengths\n+ The studied problem is important\n+ The paper proposes a three stage defense against backdoor attacks to federated learning\n\n\nWeaknesses\n-Theoretical results are inaccurate\n-Theoretical results do not provide meaningful guidance about why the propose defense works\n-Evaluation is not sufficient \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n\n\n\n=====Theoretical results====\n\nMy main concern is about the two theoretical results, i.e., Theorem 1 and Theorem 2, that are inaccurate. \n\nIn Theorem 1, the loss Lg\u2019 and Lg defined is unclear, so does x. When I went through the appendix, I realized  Lg\u2019 and Lg are defined on a single data point x, instead of a training set.     \nHowever, when generalizing it to the set of samples, the relationship considered in Eqn (3) is inaccurate. \nMore specifically, the accurate form in Eqn (3) should sum_{s=1}^n_b  min_t (x \\nabla W),  instead of min_t (sum_{s=1}^n_b  x \\nabla W), as these two terms are not equal in general.  \nTherefore, the results in theorem 2 based on Eqn(3) is inaccurate. \n\nMoreover, the theoretical results are under a very simple linear model. A key reason making backdoor attacks be successful is due to the overfitting/memorization of deep models on the trigger.  Hence, I am doubtful that these theoretical results can guide the defense effectiveness. \n\n====Evaluations====\nWhat are the recovered triggers by the proposed defense? Neural Cleanse often only recovers very blurred trigger. What if benign clients just inject a set of randomly chosen triggers for adversarial training?  What is this defense performance? Further, is the best performance achieved when the true trigger is used for adversarial training?  Basically, I would like to see how the  trigger affects the performance. \n\nHow many extra augmented data samples are used?  \n\nNon-IID is an important factor that affects the defense performance. What\u2019s the defense performance against more non-IID data across clients?  \n\nIn adaptive attacks, is the setting that the malicious clients set the inverted trigger + clean images as a targeted label, while benign clients set the inverted trigger + clean images as the true label? If this is correct, what\u2019s the insight that makes the defense effective? More specifically, if malicious clients have more poisoned samples than benign clients\u2019 clean samples, and the deep model memorizes the trigger, how could it be possible that the defense is effective? \n  \nThe proposed defense does not compare with tee state-of-the-art, e.g., \n\nShejwalkar and Houmansadr, Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated\nLearning, In NDSS, 2021\n\nHow the class distance is defined? Just using the data samples from the source-target class? How the number of samples affect this distance? \n",
            "summary_of_the_review": "The studied problem is important. The proposed defense is somewhat novelty. However, the proposed defense is not well justified, and the theoretical results may be inaccurate and cannot be generalized to general deep nonlinear models. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6082/Reviewer_zr2z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6082/Reviewer_zr2z"
        ]
    },
    {
        "id": "wiJYzKBjIaQ",
        "original": null,
        "number": 2,
        "cdate": 1666556884121,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556884121,
        "tmdate": 1666556884121,
        "tddate": null,
        "forum": "Xo2E217_M4n",
        "replyto": "Xo2E217_M4n",
        "invitation": "ICLR.cc/2023/Conference/Paper6082/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In FL many clients participate. Malign/adversary clients can potentially join and impact training by submitting malicious weights. To cope with such scenarios benign clients can employ algorithms that are more robust for such injections. The authors propose the use of data augmentation through adversarial training that benign clients should use. \nThe also provide a theoretical analysis of the difference in the loss function if benign clients employ such as strategy vs not. \nOn standard datasets they find that their algorithm usually outperforms benchmark algorithms. ",
            "strength_and_weaknesses": "Strengths: \nThe underlying FLIP algorithm is rather novel. It combines existing concepts but in an interesting way. \nThe theoretical analysis is also interesting. The statements state results that are definitely relevant and intriguing to show. \n\nWeaknesses: \nPerhaps FLIP can be enhanced to improve its performance. It would be great if it beats other benchmarks in more cases. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to understand. \nThey claim to open source the code. ",
            "summary_of_the_review": "The contributions are significant. The paper is a nice mixture of algorithmic designs and theoretical analyses. \n\nThe literature review section can be improved by more explicitly stating how does prior work differ from the current work. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6082/Reviewer_cMnB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6082/Reviewer_cMnB"
        ]
    },
    {
        "id": "HMimsgHsMt",
        "original": null,
        "number": 3,
        "cdate": 1666672479109,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672479109,
        "tmdate": 1666672659244,
        "tddate": null,
        "forum": "Xo2E217_M4n",
        "replyto": "Xo2E217_M4n",
        "invitation": "ICLR.cc/2023/Conference/Paper6082/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper propose a trigger reverse engineering based defense and show that the method can achieve robustness improvement with guarantee (i.e., reducing the attack success rate) without affecting benign accuracy. The paper conducts comprehensive experiments across different datasets and attack settings. The results on eight competing SOTA defense methods show the empirical superiority of the  method on both single-shot and continuous FL backdoor attacks. ",
            "strength_and_weaknesses": "1. The paper presents a new provable defense framework that can provide a sufficient condition on the quality of trigger recovery such that the proposed defense is provably effective in mitigating backdoor attacks.\n2. The paper empirically evaluate the effectiveness of FLIP at scale across MNIST, Fashion-MNIST and CIFAR-10, using non-linear neural networks. The results show that FLIP significantly outperforms SOTAs on the continuous FL backdoor attack setting. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors will publish codes after acceptance. ",
            "summary_of_the_review": "See the above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6082/Reviewer_cwn7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6082/Reviewer_cwn7"
        ]
    },
    {
        "id": "NH67ZGKbwgc",
        "original": null,
        "number": 4,
        "cdate": 1666874953624,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666874953624,
        "tmdate": 1666874953624,
        "tddate": null,
        "forum": "Xo2E217_M4n",
        "replyto": "Xo2E217_M4n",
        "invitation": "ICLR.cc/2023/Conference/Paper6082/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Federated learning is vulnerable to backdoor attacks. The authors propose a new defense against backdoor attacks to federated learning. The key idea is to combine some techniques developed to defend against backdoor attacks to centralized learning. Some theoretical analysis is performed to analyze the loss and learnt parameters. Evaluation is extensive, covering both existing and adaptive attacks. ",
            "strength_and_weaknesses": "Strength\n\n+ This paper proposes a new federated learning method to defend against backdoor attacks. Instead of designing new aggregation rules or changing how the server aggregates local model updates, the method modifies how benign clients train their local models. \n\n+ The paper extends centralized learning backdoor defense methods to federated learning. This is an interesting direction to explore. \n\n+ Some theoretical analysis is performed. \n\n+ Evaluation is extensive, covering multiple datasets, multiple baselines, and adaptive attacks. \n\nWeakness\n\n- One potential weakness is that the meaning of provable is slightly different from what provable defense usually means. My first impression about provable defense is that it provides certified accuracy, like what ensemble federated learning provides. But this is a minor clarification issue. \n\nOther minor comment\n\nCan you describe the setting for FLTrust? In particular, what root dataset is used by the server in FLTrust? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the above comments. ",
            "summary_of_the_review": "This paper extends centralized learning backdoor defense to federated learning, which is an interesting direction. Extensive evaluation is performed to show the effectiveness of the method. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6082/Reviewer_i7z3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6082/Reviewer_i7z3"
        ]
    }
]