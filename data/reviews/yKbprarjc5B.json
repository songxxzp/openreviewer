[
    {
        "id": "rk2mQ7ui_Mv",
        "original": null,
        "number": 1,
        "cdate": 1666569787222,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666569787222,
        "tmdate": 1666625646787,
        "tddate": null,
        "forum": "yKbprarjc5B",
        "replyto": "yKbprarjc5B",
        "invitation": "ICLR.cc/2023/Conference/Paper6195/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper tackles the problem of using Large Pre-trained Language Models (LLMs) for multiple-choice question-answering. Instead of using the standard cloze formulation, the paper suggests presenting the question and answer choices to the model and have the model output the answer symbol (e.g., A, B,C, ...etc.). The authors conduct a thorough study on multiple datasets and using different LLMs and show that the new formulation improves over variants of the standard cloze prompting technique. In addition, they measure the effect of shuffling the order of answers on the performance and note which LLMs show better invariance to the order of the answers. ",
            "strength_and_weaknesses": "Strengths:\nThe paper presents an interesting idea for Multiple-Choice Question-Answering (using the answer symbol instead of the answer itself), motivates the idea well and does a thorough analysis over multiple datasets, and LLMs to analyze its performance in different settings (including few-shot settings). \n\nWeaknesses:\nThere is no contribution/novelty from the modeling/methods side. The paper is mainly an empirical analysis of a different way of formulating an existing problem. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite well written and easy to follow and the code will be made available which facilitates predictability. \nThe paper presents an interesting and thorough empirical analysis but there is not much novelty from the methods/modeling side.",
            "summary_of_the_review": "Overall, while the paper doesn't have modeling/methods novelty, it presents a thorough and interesting analysis of how a different formulation of Multiple-Choice Question-Answering affects the performance of LLMs on the task. The analysis includes multiple datasets and several LLMs and looked at issues affecting the performance such as order invariance which are sometimes ignored in other studies.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6195/Reviewer_nm2P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6195/Reviewer_nm2P"
        ]
    },
    {
        "id": "fg3s5OAzNGd",
        "original": null,
        "number": 2,
        "cdate": 1666623986010,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623986010,
        "tmdate": 1666624252191,
        "tddate": null,
        "forum": "yKbprarjc5B",
        "replyto": "yKbprarjc5B",
        "invitation": "ICLR.cc/2023/Conference/Paper6195/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors identify a better method to prompt LLMs for multiple-choice question answering. Instead of (the usual) comparing the probability of producing each answer, they present all options to the model and then identify the right option by producing just the letter that identifies the answer.\n",
            "strength_and_weaknesses": "Strengths: The authors explain their approach well. They also discuss the (somewhat surprising) variance between different models in their ability to separate the letter from the answer. (They call this Multiple Choice Symbol Binding.) The approach is evaluated on a wide range of (20) datasets.\n\nWeaknesses: The approach is not new, just discussed and evaluated. The authors differentiate their suggested prompting from \u201cprompt engineering\u201d, which they seem to define as fine-tuning of prompts to increase model performance. However, I\u2019m not convinced that these are fundamentally different, and would include research such as theirs in the general domain of prompt engineering.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and I believe the experiments are verifiable with the given information, i.e. it should be possible to reproduce them.\n\nRegarding novelty, I am less convinced. The authors mention others having used the MCP approach. So the main addition here is the systematic discussion and wide range of experiments.\n",
            "summary_of_the_review": "The authors discuss an alternative (but not novel) way to prompt LLMs for better results on multiple-choice tasks. The prompt is well-motivated and thoroughly discussed with a good range of experiments that support the author's arguments. However, it is not novel: it is a fairly obvious way to prompt and has been tried before.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6195/Reviewer_jqnn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6195/Reviewer_jqnn"
        ]
    },
    {
        "id": "_wSHGs29hxY",
        "original": null,
        "number": 3,
        "cdate": 1666709210245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666709210245,
        "tmdate": 1666709210245,
        "tddate": null,
        "forum": "yKbprarjc5B",
        "replyto": "yKbprarjc5B",
        "invitation": "ICLR.cc/2023/Conference/Paper6195/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors find that the MCQA ability of LLMs has been underestimated, and they propose multiple choice prompting (MCP), in which a question and its symbol-enumerated candidate answers are all passed to an LLM as a single prompt. Surprisingly, the performance of LLMs equipped with MCP dramatically improves, approaching or even surpassing SOTA. This demonstrates that the power of LLMs can be used broadly in the future.",
            "strength_and_weaknesses": "Strength:\n\n1. This paper is well-written and easy to understand.\n\n2. The authors propose a simple but effective prompting method, which outperforms previous CP methods, approaching or even surpassing SOTA performance.\n\n3. Experimental results show that the MCQA ability of LLMs has been previously underestimated. And there is a better way to prompt a single LLM. The potential of multiple choices prompts can be further tapped. Future work include prompt engineering is still promising. \n\nWeakness:\n\n1. The novelty of this paper is limited. Multiple choices prompting (MCP) has been used in other QA tasks, such as TruthfulQA and RACE.\n\n2. Although the experimental results prove that the proposed multiple choices prompting (MCP) methods can outperform existing cloze prompting (CP) methods, the reasons behind it are still unclear. Since the authors have listed several problems within CP methods, I'm curious about whether these problems are all solved or avoided by their MCP methods. More analysis is needed to show this.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\nGood. But it would be better to include more analysis.\n\nQuality\nGood. A simple but effective prompting method.\n\nNovelty\nNovelty is limited. Details can be found in the weakness part.\n\nReproducibility\nGood. The code is attached by the authors.\n",
            "summary_of_the_review": "Interesting paper but not good enough. It would be better to include more analysis on whether MCP can deal with these several problems that CP faces.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6195/Reviewer_rg4x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6195/Reviewer_rg4x"
        ]
    },
    {
        "id": "SF1Fe9-L9RC",
        "original": null,
        "number": 4,
        "cdate": 1666711856696,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666711856696,
        "tmdate": 1666711856696,
        "tddate": null,
        "forum": "yKbprarjc5B",
        "replyto": "yKbprarjc5B",
        "invitation": "ICLR.cc/2023/Conference/Paper6195/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the difference between multiple-choice prompting and standard prompting (so called cloze prompting), clarifying major reasons why LLM underperforms on Multiple Choice Question Answering (MCQA) problems. First, what LLM tries to predict in terms of \u201cmore likely\u201d does not always mean \u201cmore correctly\u201d. This conflation often happens when the tokens in the answer sequence is less common or less grammatical. Second, LLM must rely on normalization schemes to compare candidate answers with different lengths or different frequencies. But this yields additional dependency on tokenizer. Third, standard prompting compares different options only indirectly via the (normalized) likelihood without direct comparison. Obviously, such standard prompting is expensive comparing to generating one option token.\n\nTo make LLM solve MCQA problems with order-invariance, the authors propose Multiple Choice Symbol Binding (MCSB) capability that could be model-agnostically testable by recording the answer with the highest probability for each ordering of question (so called PPA). The experimental results show that training on code data (especially by multi-staging) is useful for MCSB. Providing more shots as few-shot examples also help boosting the performance.\n",
            "strength_and_weaknesses": "(Strengths)\n1) Reveals problematic ingredients for likelihood-based answering.\n2) Introduce the concept of MCSB and measure it by PPA.\n3) Concentrated results that significantly improves QA performance by using multiple-choice prompting.\n\n\n(Weaknesses)\n1) Individual problematic ingredients are neither being theoretically-proven nor empirically-proven.\n2) No novel/brand new ideas. Mostly empirical analysis based on OpenAI playground.\n3) Some major arguments are less supported.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The submission is an analysis paper rather than finding something new.",
            "summary_of_the_review": "(Major concerns)\nHow to make sure Codex model clearly outperforms Instruct model? This is a critical question as the authors measures the main experiments (Table 2) that compare Multiple Chocie Prompting (MCP) and Cloze Prompting (CP) only with Codex model. \n\n0) The capability to perform MCSB could be due to human feedback alignment by Reinforcement Learning rather than other points indicated by the authors.\n\n1) Are the PPA difference between Codex and Instruct (in Figure 2) statistically significant? While no statistical test has been provided, it seems not easy to decline null hypothesis that says the difference is a random effect.\n\n2) Only Codex tested on OpenBookQA shows strong performance gain when using MCP, whereas Instruct outperforms Codex on the other two tasks in Table 1. More detailed experiments are necessary to convince how Codex achieve such higher accuracy.\n\n\n(Minor concerns)\n1) Any reason to choose OpenBookQA which also matters the performance of retriever?\n\n2) Do you know how Codex model is exactly trained? Codex model that you used could be first based on Instruct, then being further trained on code data. Equally likely, Codex model might perform it's own alignment similar to Instruct but based on the preference of generated codes.\n\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6195/Reviewer_oxYV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6195/Reviewer_oxYV"
        ]
    }
]