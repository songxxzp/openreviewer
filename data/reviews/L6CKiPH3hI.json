[
    {
        "id": "wZJ3AJ8ks9",
        "original": null,
        "number": 1,
        "cdate": 1666607267091,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666607267091,
        "tmdate": 1666607267091,
        "tddate": null,
        "forum": "L6CKiPH3hI",
        "replyto": "L6CKiPH3hI",
        "invitation": "ICLR.cc/2023/Conference/Paper6376/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Towards the ensemble-based KD, this paper first presents a label prior shift to induce evident diversity among the same teachers. Then the authors propose an aggregation strategy that uses post-compensation in specialist outputs and conventional model averaging. Experiments on several datasets validate its effectiveness.",
            "strength_and_weaknesses": "Strength:\n\n(1) The motivation is clear, where this paper mainly aims to solve the issue of diverse teachers' construction.\n\n(2) The organization is good, which makes the paper easy to follow.\n\n(3) The authors also provide detailed proof for theorems and corollaries.\n\n(4) Sufficient experimental analysis is given for better understanding.\n\nWeaknesses:\n\n(1) My main concern lies in the experimental results. Several methods achieve much better results than the proposed method on all these datasets. The authors compared with PCL on ImageNet, and it achieves a 0.05% improvement over PCL. I wonder why the authors do not compare with it on CIFAR10/100. According to the results on PCL, the results on CIFAR under various backbones are better than the proposed method. Moreover, one important reference[1] is missing, which achieves much better results than the proposed method, including ImageNet.\n\n(2) As introduced by the authors, label prior shift has been extensively discussed by existing methods. Please analyze the difference between the adopted strategy and existing methods. Is this a simple combination with KD?\n\n(3) Sampling has also been well studied. Please also compare the importance sampling with existing methods.\n\n(4) Based on the above analysis, I think the novelty could be improved.\n\n[1] MULTI-VIEW CONTRASTIVE LEARNING FOR ONLINE KNOWLEDGE DISTILLATION, ICASSP, 2021\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to my detailed comments above.",
            "summary_of_the_review": "The presentation and writing are good. But the novelty could be improved. And the experimental results are not convincing.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6376/Reviewer_HD42"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6376/Reviewer_HD42"
        ]
    },
    {
        "id": "pZInYIjUAh",
        "original": null,
        "number": 2,
        "cdate": 1666673653961,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673653961,
        "tmdate": 1670231332803,
        "tddate": null,
        "forum": "L6CKiPH3hI",
        "replyto": "L6CKiPH3hI",
        "invitation": "ICLR.cc/2023/Conference/Paper6376/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a online knowledge distillation framework with specialized ensembles. It first uses a label prior shift to generate different teachers for student to learn from. Then it uses PC-Softmax to post-compensate teacher logits and an averaged classifier manner to aggregate teacher predictions. And the experiments part show the improvement over previous method on CIFAR and ImageNet.",
            "strength_and_weaknesses": "The paper is fairly well-written and the high-level problem is interesting. The writing is clear and easy to follow. And the experiment part especially ablation study is well designed to show the improvement over previous methods.\nHowever, there seems lack of comparison on CIFAR with more current works like CGL [1] or PCL [2]. Also there is a most recent one L-MCL [3] that has better performance. And the performance over those method seems not very significant. (It seems more common to report acc than error rate. Using error rate makes it hard to efficiently compare with the unlisted models.)\n\n[1] Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng Yu, Ding Liang, Xiaolin Hu, and Ping Luo. Online\nknowledge distillation via collaborative learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2020\n[2] Guile Wu and Shaogang Gong. Peer collaborative learning for online knowledge distillation. In\nProceedings of the AAAI Conference on Artificial Intelligence, 2021.\n[3] Yang C, An Z, Zhou H, et al. Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition[J]. arXiv preprint arXiv:2207.11518, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear in most parts---some ambiguities are discussed above.",
            "summary_of_the_review": "Overall, the paper proposes a new approach that uses specialized ensembles to do online KD. I would be happy to raise my score if the authors conducted the additional evaluations discussed in my review above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6376/Reviewer_Xcq2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6376/Reviewer_Xcq2"
        ]
    },
    {
        "id": "inLGsDcO1aL",
        "original": null,
        "number": 3,
        "cdate": 1666753002170,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666753002170,
        "tmdate": 1666753002170,
        "tddate": null,
        "forum": "L6CKiPH3hI",
        "replyto": "L6CKiPH3hI",
        "invitation": "ICLR.cc/2023/Conference/Paper6376/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a framework for online knowledge distillation from an intentionally diversified ensemble of teachers. A common hypothesis for the success of distillation from an ensemble is that diverse teacher models increase the efficacy of the ensemble. The paper provides a formal framework to enhance teacher diversity by introducing a prior shift - effectively turning each teacher head into a \u201cspecialist\u201d. The paper includes a robust empirical analysis and demonstrates that the proposed method significantly improves student performance with respect to both accuracy and calibration.",
            "strength_and_weaknesses": "Strengths:\n+ The paper is written and structured well.\n+ While many of the component ideas have been introduced before, incorporating such specialists in a complete framework of online distillation is novel and interesting.\n+ The empirical analysis is thorough.\n\nWeaknesses:\n- The paper has a number of typos that are sometimes distracting. e.g. \n  - On page 3: \u201c... both categories, using the extra modules as ever.\u201d\n  - On page 4: \u201c... target distribution while only with samples \u2026\u201d\n  - On page 18: \u201c... optimal exposure differs from models\u2026\u201d\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear. As mentioned earlier in the review, while some / most of the component ideas such as label shift, PC-softmax, and the principle of enriching diversity in an ensemble for distillation have been observed before - the framework to put these together for online distillation is novel.\nThe experimental section is well-written and provides enough details for reproducibility.\n",
            "summary_of_the_review": "The paper proposes a new framework for online distillation with the intent to increase diversity among the teacher heads and demonstrates significant improvements over prior approaches in benchmark datasets.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6376/Reviewer_D2AU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6376/Reviewer_D2AU"
        ]
    },
    {
        "id": "ndM2QEj-Yr",
        "original": null,
        "number": 4,
        "cdate": 1670790074056,
        "mdate": 1670790074056,
        "ddate": null,
        "tcdate": 1670790074056,
        "tmdate": 1670790074056,
        "tddate": null,
        "forum": "L6CKiPH3hI",
        "replyto": "L6CKiPH3hI",
        "invitation": "ICLR.cc/2023/Conference/Paper6376/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose to diversify the models in an ensemble forming the teachers in distillation. The specific setup the authors consider is  on-line distillation (where the teachers are trained in parallel with the student) and peer-based (where the models share parts of their weights). \n\nThe proposed approach to achieve the aforementioned diversification of the teachers is based on varying the prior distributions of the labels in the train data used for training each teacher. Then, as an efficient approach to this, importance sampling is employed to avoid sampling from the data separately for each teacher. Then, before averaging the teacher outputs for the student supervision, the \"Post-Compensated Softmax\" loss correction is employed from a previous work.\n\nThe comparison against selected baselines shows improvements. Ablations are provided where the authors measure the effect of the teacher's diversity and the number of teachers within the proposed framework. ",
            "strength_and_weaknesses": "Strenghs:\n- The paper is mostly clear. \n- The Figure 1 is a very nice summary of the work.\n\nWeaknesses:\n- One limitation is that the setup seems very specialized (on-line distillation, peer networks). I don\u2019t see why the technique is specific for that setup - diversifying the teachers (which is the main idea behind the authors\u2019 method) could be well applied to off-line distillation or non-peer networks.\n- The baselines are not explained. It is not clear why these are chosen over other options (there are a lot of distillation methods one could compare to).\n- The ablation section is very useful to have, but I would like to see ablations for why using each of the steps suggested by the authors\u2019 is useful. For example, does PC-Softmax  help? Is importance sampling actually good compared to sampling the data instead? (checking this even on a small dataset would be useful) What about the effect of the peer network choice in the light of your method, as opposed to using completely separate networks?\n- When averaging the teachers, why not weigh them differently depending on what labels they were trained on? Since each teacher focuses on a different set of labels, they should be experts with respect to such labels. It seems that averaging the teacher's predictions is naive and ignores useful information about the teachers.\n- As an application of label prior shift for diversifying the teachers, Why not try it in non-on-line distillation? Is there anything special about on-line distillation that diversifying the teachers is particularly interesting compared to off-line distillation? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear and the method is explained well so it should be reproducible. The novelty is somewhat limited as methods from previous works are combined without a clear explanation why using them is the best option.\n\nSome specific questions:\n- What is the motivation for the label prior shift for training models in an ensemble? MoEs are mentioned, but I missed an explanation if that particular technique was employed there and what its success was. \n- \u201cnotably expected calibration error\u201d \u2013 why is this finding notable? \n",
            "summary_of_the_review": "The use of the methods described by the authors looks interesting and promising. I suggest conducting more ablations and adding explanations for the decisions to strengthen the paper before resubmission.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6376/Reviewer_uYCq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6376/Reviewer_uYCq"
        ]
    }
]