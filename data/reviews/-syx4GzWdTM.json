[
    {
        "id": "hewA-xqteKS",
        "original": null,
        "number": 1,
        "cdate": 1666584317654,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584317654,
        "tmdate": 1666658967480,
        "tddate": null,
        "forum": "-syx4GzWdTM",
        "replyto": "-syx4GzWdTM",
        "invitation": "ICLR.cc/2023/Conference/Paper5475/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents two techniques for speeding up convolutional neural network inference in homomorphic encryption settings.\nConceptually, the underlying RNS-CKKS homomorphic encryption provides the functionality to securely evaluate functions over a vector of encrypted data in a SIMD manner. In this specific setting, this paper proposes a novel data encoding scheme and a HE-specific pruning technique (sub-block pruning) that substantially reduce the number of homomorphic operations.",
            "strength_and_weaknesses": "Strength:\nThey considered the problem very common and important in building homomorphic encrypted neural network applications, especially the data encoding one. Also, their solutions are simple and very effective. \n\nWeakness:\n- In the experiment setup, they didn't explicitly mention how they deal with level-zero ciphertexts. I assume that they didn't use bootstrapping and do the re-encryption.\n- Same as above, usually in applications, bootstrapping will dominate in the computational cost. Which will make this work less important than it is justified to be. However, these are two independent directions of improvement.\n- Except for bootstrapping, the choice of activation function is also questionable. Beyond quadratic polynomials, other works already adopt high-precision approximation of the non-linear ReLU function for higher accuracy (e.g. https://arxiv.org/abs/2105.10879 ). Though, it's also independent of the proposed techniques. But it's better to check in experiments if they could be combined.",
            "clarity,_quality,_novelty_and_reproducibility": "The result of this paper looks convincing to me, and their group-interleaved data encoding scheme is very interesting.\n\nFor clarity, I think the data encoding scheme deserves a better ( and more formal ) explanation. It's hard to comprehend from just a figure and a short paragraph.",
            "summary_of_the_review": "Overall, I think the problem they considered is very important, and the proposed solutions are simple and effective and have a very wide application in secure neural network applications. But this paper's contribution is also limited by the lack of consideration of other aspects like bootstrapping and approximation of non-linear activation function.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5475/Reviewer_WT91"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5475/Reviewer_WT91"
        ]
    },
    {
        "id": "hH3cwevQCm",
        "original": null,
        "number": 2,
        "cdate": 1666601004107,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601004107,
        "tmdate": 1666601004107,
        "tddate": null,
        "forum": "-syx4GzWdTM",
        "replyto": "-syx4GzWdTM",
        "invitation": "ICLR.cc/2023/Conference/Paper5475/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a method for privacy-preserving neural network inference using homomorphic encryption. The main contributation is optimizing the convoluational layer to fit the SIMD structure of homomorphic encryption, including reducing the number of non-zero weights and re-training.\n",
            "strength_and_weaknesses": "The core idea is interesting and novel to me, but I see issues with the presentation.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The technical contribution is described solely in figures and examples. Neither the appendix nor the supplemental material provide further insight. Similarly, I don't understand Algorithm 1 because it's so terse.\n\nI'm missing a description of HEFNet.\n\nMinor issues:\np2: missing whitespace after footnote 1\np3: \"A client could upload his/her (...) data (...) for obtaining an online inference service.\" - Maybe \"obtain the inference result\"?\np3: \"semi-honest (e.g. honest but curious)\" - The two terms are synonyms and not an example.\np4: \"pruning(Han\", \"pruning(Wen\" - missing whitespace before brackets\np5: suggest G|M instead of \"G%M=0\"\np6: \"algorithm 1\" - Algorithm 1\np7: \"state-of-the-art method. y We adopt\"\np9: missing and misplaced whitespace in the caption of Table 4\np9: \"There shall exist potential optimizations\" - maybe \"We expect potential optimizations\"?\n",
            "summary_of_the_review": "Interesting idea marred by editorial issues\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5475/Reviewer_CCNN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5475/Reviewer_CCNN"
        ]
    },
    {
        "id": "Bdl_EiSjEYO",
        "original": null,
        "number": 3,
        "cdate": 1667066605423,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667066605423,
        "tmdate": 1670705757393,
        "tddate": null,
        "forum": "-syx4GzWdTM",
        "replyto": "-syx4GzWdTM",
        "invitation": "ICLR.cc/2023/Conference/Paper5475/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper seeks to improve the computational efficiency of convolution layers in FHE. Since homomorphic rotations are the primary computational bottleneck of convolutions in FHE, the paper seeks to reduce the number of rotations. This is achieved in two steps, 1) adopting group convolutions, which reduces out-level rotations, and 2) weight pruning, which reduces inner-level rotations.\n\nThe efficiency of the proposed convolution is evaluated on *shallow* CNNs designed for MNIST and CIFAR-10. The proposed approach shows appreciable speed-up over *naive* implementations of convolutions in FHE.",
            "strength_and_weaknesses": "Strengths:\n- The paper rightly identifies the main bottleneck of *naive* implementations of convolution in FHE, namely rotations. As such, efforts to improve efficiency of convolution is necessary. Leveraging alternative convolutions, such as group convolutions or in the extreme depth-wise convolutions is interesting.\n- Adopting pruning for sparsifying the convolution and optimizing the sparsity pattern for reducing number of homomorphic rotations.\n\nWeaknesses:\n- The main drawback of the paper is the lack of comparisons to prior work that improve efficiency of convolutions. These include multiplexed convolutions [1], mobile networks explored HEMET [2].\n- The main premise of the paper is that rotations in convolutional layers are the main computational bottleneck of networks in FHE. So the paper considers shallow networks only, which are not likely to be practically useful. For instance 85% accuracy on CIFAR-10 is quite poor by the standards of the best plaintext models which achieve ~99% accuracy.\n- As networks become deeper, the main accuracy bottleneck is low-degree polynomial approximations of non-linear functions like ReLU and the main computational bottleneck is the bootstrapping operations required for evaluating high-multiplicative depth circuits. So improving efficiency of convolutional layers does not benefit deeper networks since convolution is not the main bottleneck for such networks.\n\nOther Clarification Questions:\n- The paper does not mention how pooling operations or strided convolutions are handled. Strided convolutions result in wasted slots. How  does that affect the proposed convolutions?\n\n[1] HEMET: A Homomorphic-Encryption-Friendly Privacy-Preserving Mobile Neural Network Architecture, ICML 2021\n[2] Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions, ICML 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clear for the most part. The figures are a bit challenging to understand, but becomes clear with the description in the text.\n\n- The quality of the paper is good for the most part, however there are important baselines that are missing. Such methods have not been cited, discussed, or compared against.\n\n- The proposed method is fairly novel. Most existing CNN implementations in FHE using standard dense convolutional layers. This paper proposes to use group convolutions which are more HE friendly. Sparsity pattern is also optimized for minimizing rotations as opposed to other criterion used in standard networks.\n\n- The proposed approach is not reproducible based on the descriptions in the paper. There is missing information, hyper-parameters etc. And the paper does not provide code, nor do the authors promise to release code publicly later on.",
            "summary_of_the_review": "The paper proposed to use group convolutions and a weight pruning to mitigate the computational bottlenecks of convolutional layers, namely homomorphic rotations. The paper, however, does not compare to or discuss existing attempts toward HE-friendly CNNs. Furthermore, the experiments are conducted on shallow networks. The proposed approach will not provide much computational benefit for deeper networks since convolution is not the main bottleneck for such networks. Reproducibility is also limited.\n\nOverall, the paper has good ideas, but an evaluation, comparison, and discussion of the broader utility of the proposed approach are missing.\n\n**Update After Rebuttal:** The author's rebuttal does not adequately address the comments from the initial review. In theory, the method may have promise in achieving the claims in the rebuttal, but I do not believe it is straightforward and needs to be demonstrated. I will maintain the original rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5475/Reviewer_AXmR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5475/Reviewer_AXmR"
        ]
    }
]