[
    {
        "id": "uCVW-q0z0sp",
        "original": null,
        "number": 1,
        "cdate": 1666378157913,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666378157913,
        "tmdate": 1669570087463,
        "tddate": null,
        "forum": "x-mXzBgCX3a",
        "replyto": "x-mXzBgCX3a",
        "invitation": "ICLR.cc/2023/Conference/Paper6390/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper is focused on ensuring fairness on tabular data. Since Gradient boosted decision trees (GBDT) - based classifiers are found effective on tabular data, the authors propose FairGBM framework for training GBDT under fairness constraints. The experimental result shows improvement in the training time, predictive performance and fairness.",
            "strength_and_weaknesses": "Strength:\n- The paper is well written and well motivated. \n- There are not much existing work that focused on fairness for GBDT models. \n- In depth formalization of the proposed framework\n\nWeakness:\n- It is not clear how the proxy-Lagrangian can be designed/derived for any fairness metrics. Is the cross-entropy-based proxy only applicable for demographic parity, equal opportunity and predictive equality?\n- The proposed constrained optimization method works for GBDT-based classifier. Is it possible to address unfairness in regression task with the current approach? What would be the proxy function on that case? Note that Agarwal et al., 2019 extended their reduction-based approach for regression task as well. It would be great if the authors include discussion or additional experiments on this.  \n- Baseline models EG, GS and RS were trained with 10 iterations for an equal budget for all algorithms. But Agarwal et al., 2018 stated that 5 iterations were enough for their reduction approach in all cases. Additional experiments with varying number of iterations for all models would be helpful to understand if EG were not overfitted with additional iterations.\n- Instead of completely discarding EG from the experiment of real-world data, why not changing the decision threshold and n in EG for feasibility, in case of this dataset.\n- How trade-off \\alpha = 0.5, 0.75, and 0.95 selected? Fairness-accuracy trade-off plots for all models would be helpful to understand the impact of different constraint methods.\n- Fairlearn package is mainly implemented in Python, while FairGBM is implemented in C++. Does FairGBM get additional runtime benefit due to C++? \n ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well-written. It's clear and easy to understand. Key resources (e.g., proofs, code, data) for reproducibility are available with sufficient details. But the novelty of the paper is limited.",
            "summary_of_the_review": "Although the authors claimed to develop a learning framework for fair GBDT, the current approach is only applicable for classification task. Furthermore, constrained optimization is not new in fair ML research. Their formulation of constrained optimization is very similar with  (Agarwal et al., 2018)'s reduction approach. The only difference is using the differentiable proxy functions which was adapted from (Cotter et al. 2019). So, the main ideas of the paper have limited novelty. In addition, the paper lacks adequate experiments to establish author's claim. See above section. \n\n-------------------------------------------------\nI thank the authors for explaining some of my concerns. Although I am not completely convinced regarding the novelty of the paper, after reading other reviewer's comments, I have decided to increase my rating.  \n ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6390/Reviewer_MVBV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6390/Reviewer_MVBV"
        ]
    },
    {
        "id": "Smnd1AxV4ub",
        "original": null,
        "number": 2,
        "cdate": 1666641464794,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641464794,
        "tmdate": 1666643514970,
        "tddate": null,
        "forum": "x-mXzBgCX3a",
        "replyto": "x-mXzBgCX3a",
        "invitation": "ICLR.cc/2023/Conference/Paper6390/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a method for training gradient-boosted trees with fairness constraints via a (proxy) Lagrangian approach. They provide an algorithm, an open-source implementation, and empirical results on two datasets (only one appears to be open-source) showing that the proposed algorithm generally improves over existing methods.",
            "strength_and_weaknesses": "## Major Comments\n\n* The authors note that there are clear differences between the two datasets (e.g. differences in label imbalance, differences in spread over performance metrics). Given this, it is hard to make much sense of the results, since they seem to imply meaningfully different conclusions (i.e. Fairlearn EG achieves excellent tradeoffs on ACS Income and much lower variance than the proposed method; GS and RS achieve near-perfect fairness on AOF, but not on ACSIncome). Given that the authors are already (I assume) using the folktables package to generate ACS Income dataset, why not use additional tasks already available in that package (Public Coverage, etc.)? This would also provide more publicly verifiable results on open datasets.\n\n* The AOF dataset is not clearly described. Is it an open dataset (I do not see it in supplement)? What is the source? What features exist in the dataset, what are their data types, etc.? Particularly if the dataset is *not* open, much more detail would be useful.\n\n* Given the very strong performance of EG on ACSIncome, I wonder why more effort was not made to include it in AOF experiments. If EG gives a randomized binary classifier, why can't the average of many (randomized) predictions be used as a de facto continuous prediction?\n\n* The authors perform 10 experimental iterates, but do not use these iterates to provide measures of statistical variability in comparing their point estimates -- a missed opportunity, and also a critical one given that several similar algorithms are being compared. Please provide, if possible, estimates of variation (i.e. SD over trials, Clopper-Pearson confidence intervals, etc.) of the various metrics being compared e.g. in Table 2, Figure 2. This will help the authors eliminate subjective language in their analysis such as \"excellent trade-offs\", \"only slighting lagging behind\", etc.\n\n* The paper is missing a discussion of the limitations of the proposed approach. Please comment on this.\n\n* It's not clear why equalizing the group FNR make sense for any real-world task on ACS Income. Please comment on the motivation for this, or choose a constraint that actually has some grounding in the income task.\n\n* I think the paper should comment on, and ideally compare to, the approach described in \"Individually Fair Gradient Boosting\", Vargo et al., https://arxiv.org/pdf/2103.16785.pdf ).\n\n## Minor Comments\n\n* Please use some other form of marker to differentiate between models in Fig. 2; it is difficult to see the differenecs between colors.\n\n* Hwo is the alpha=0.75 in Table 2 chosen? If it is arbitrary, how do the comparisons change with \\alpha? Is it possible to instead plot curves over all values of alpha in [0,1]?\n\n## Typos etc\n\nP1 \"no clear winner method\" -> no clear winning method\n\nPlease spell out \"w.r.t\" in the paper\n\nEquation (5): If I understand correctly, the max can be over a,b \\in S (eliminating the need for \\forall b \\in S)\n\nI don't see S_s or L_S_s defined in the paper.\n\nP8: \"significative\"",
            "clarity,_quality,_novelty_and_reproducibility": "AOF dataset needs improved documentation/discussion, if it is not open source. Otherwise, the paper provides an open-source implementation which appears to be an important contribution of the work, and should greatly aid in reproducibility.",
            "summary_of_the_review": "Overall, this paper pursues an important direction for machine learning research -- tree-based models are, as the authors note, SOTA for tabular data yet are incompatible with many gradient-based regularization approaches, so new methods are needed. In general, I think the paper is well-written, but the discussion of the experimental results could be improved. Given the clear differences between the two datasets, I also think that adding additional datasets (e.g. additional tasks from folktables) would considerably improve the paper. See detailed comments above/below.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6390/Reviewer_Cc3g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6390/Reviewer_Cc3g"
        ]
    },
    {
        "id": "_FpMj24uS6",
        "original": null,
        "number": 3,
        "cdate": 1666696422502,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666696422502,
        "tmdate": 1666696422502,
        "tddate": null,
        "forum": "x-mXzBgCX3a",
        "replyto": "x-mXzBgCX3a",
        "invitation": "ICLR.cc/2023/Conference/Paper6390/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a first of its kind in-processing learning framework FairGBM for training GBDT, without affecting its performance, constrained by fairness. The paper tries to advanced the area of FairML, limiting risks of unfair or biased ML systems and aims to establish a gold standard method. The authors provide evidence that FairGBM is an order of magnitude faster compared to existing related work such as LightGBM, RS Reweighing, Fairlearn GS and Fairlearn EG when tested against benchmark datasets and is superior to them in both fairness, performance and runtime. The authors discuss background related work in detail by providing an in-depth analysis of pre-processing, in processing and post processing methods and argue why introducing fairness during the in processing is more beneficial vs the other two. ",
            "strength_and_weaknesses": "Strengths: \n1. Strong summarization of relevant work, including pre processing, in processing and post processing and gap analysis to find area of opportunity for method development that can lead to highest impact i.e. there is a strong lack of a gold standard method that just works across the board and key identification of in-processing step as area of development. \n2. The method proposed is efficient and does not require additional training or storage for keeping intermediate training states. \n3. Method is applied to two diverse datasets to cover a variety of issues, bias, class imbalance, budget constraints on positive predictions etc., one of these datasets is benchmarked\n4. The paper proposed mentions that the work is generalizable to any differentiable constraints, not just fairness constraints. \n5. Comparison to other methods is fairly strong towards assesment of performance and fairness. \n\n\nWeakness: \n1. While the authors explicitly focus their attention to tabular data for evaluation of FairGBM, it would have been thoughtful to discuss potential impact of these methods on other structured/unstructured datasets as well (such as images, natural language etc). Even though the authors discuss there is no gold standard method that works regardless of data format or bias, it is then counterintuitive to focus the attention to just tabular data, given there are several other formats of data available. \n2. Could be worth testing on more benchmark datasets to increase confidence coverage. \n3. While the work is supposedly generalizable to additional constraints, apart from fairness constraints, it is not tested or validated for any other type of constraint. \n4. While authors claim FairGMB can be applied to any GB algorithm, most comparisons to other constrained optimized methods are done by implementing FairGBM on LightGBM, while other methods also use LightGBM as base learners. \n5. The authors can perhaps provide some commentary of how this work can be extended to other ML models, including deep learning, beyond just gradient boosting methods. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity& Quality: The paper is of high quality in terms of arguments made towards choosing the opportunity, discussion of relevant background work, mathematical choices around constrained optimization and choice of L, evaluation setup of experiments and description of results. \n\nNovelty: The paper is novel in proposing differentiable proxy functions for regular fairness metrics on the basis of cross entropy loss. \n\n\nReproducibility: All the materials including the algorithm, datasets, implementation code, experimental setup are clearly provided to ensure high reproducibility. They have followed the reproducibility checklist thoroughly and closely. ",
            "summary_of_the_review": "The authors provide sufficient ground of reasoning why FairML is necessary in terms on risks of biases and discrimination affecting various ML work and how tabular data, an important format of information, important across various applications could be a strong opportunity for evaluation of the method proposed. The authors provide sufficient background around relevant work and argue correctly why the choice of opportunity is in processing and why constrained optimization options with fairness metrics and their non-convex properties lead to the development of the mentioned proxy functions. Further the work put forward in terms of experimental setup and comparison to other methods is fairly strong to justify why this method could be considered as a gold standard method for tabular data and for gradient boosting models. Although the scope is limited, it is still strong and hence I propose that this work should be accepted. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6390/Reviewer_WMdi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6390/Reviewer_WMdi"
        ]
    },
    {
        "id": "dG1zrLiwXVZ",
        "original": null,
        "number": 4,
        "cdate": 1666809011002,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666809011002,
        "tmdate": 1666809011002,
        "tddate": null,
        "forum": "x-mXzBgCX3a",
        "replyto": "x-mXzBgCX3a",
        "invitation": "ICLR.cc/2023/Conference/Paper6390/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a new FairGBM method to train GBDT under fairness constraints that shows little impact to predictive performance but improved fairness metrics as compared to unconstrained GBDT. The major challenge in the Fairness constraint is from the non-differentiable property of fairness notion, which is resolved by the proxy Lagrangian in this paper. Afterwards, the problem is resolve under the two-player game formulation where a descent step optimizes the loss and a ascent step ensures the fairness. Numerical results on ACSIncome-Adult and AOF dataset show that FairGBM proposed from the paper have better trade-off among Fairness, Performance, and Efficiency. ",
            "strength_and_weaknesses": "Strength\n[+] The problem is well motivated. Fairness is an important topics and designing a optimization framework in GBDT to ensure fairness is an interesting and important area.\n\n\nWeakness\n[-] FairGBM is a modified version of LightGBM with fairness constraint. From the experimental results in Table 2, it shows FairGBM is better than lightGBM in both performance and fairness. Could the author helps to clarify the source of improvement of performance? It seems contra intuitive that a fairness constraint algorithm outperforms a unconstraint algorithm that purely optimize for performance.\n\n[-] Could the author helps to confirm fairness metrics used in evaluation and model training (\\tiltle(c)_i in equation 7)? In figure 2, the fairness metrics is different across dataset, FNR for ACSIncome-Adult and FRR for Account Opening Fraud. Is different fairness constraint selected when algorithm is applied on different datasets? ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, paper is good written and well motivated. ",
            "summary_of_the_review": "Overall the paper proposed a fairGBM with a proxy Lagrangian formulation. The problem is well motivated and solution is reasonable sound. There are some doubts on the experimental results, while overall results demonstrate the efficiency of the new proposed algorithm. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6390/Reviewer_f61N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6390/Reviewer_f61N"
        ]
    }
]