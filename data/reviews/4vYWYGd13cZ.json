[
    {
        "id": "j0D2VWO82J",
        "original": null,
        "number": 1,
        "cdate": 1665928935235,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665928935235,
        "tmdate": 1666110282091,
        "tddate": null,
        "forum": "4vYWYGd13cZ",
        "replyto": "4vYWYGd13cZ",
        "invitation": "ICLR.cc/2023/Conference/Paper4246/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper tackles the problem of adversarial attacks against Machine Learning (ML) methods for Time Series Forecasting (TSF). The paper rightly points out that there are few works in this domain: hence, the paper proposes an original formalization of \u201ctargeted attacks\u201d against such methods, which also works for \"regression\" ML problems (typical in TSF, but rare in the 'traditional' computer vision setting envisioned in adversarial ML research). The proposed attacks are then empirically validated on two datasets (representing two diverse real-world settings) and the effectiveness of the attacks is validated via statistical tests.",
            "strength_and_weaknesses": "STRENGTHS:\n+ Trendy topic that has been not very explored\n+ The statistical validation is interesting\n\n\nWEAKNESSES\n- Very poor presentation\n- Lack of scientific rigour (in clarity and quality)\n- Unrealistic assumptions\n- Biased experiments\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the presentation of the paper is below the standards of quality for ICLR.\n\nThe quality of the English text is poor (but the manuscript is readable).\n\nFigures and Tables are (aesthetically) fair.\n\nThe topic addressed by the manuscript is trendy, and important.\n\nThe references are (somewhat) appropriate.\n\nThe contribution is not significant (currently).\n\nThe experiments are partially reproducible (the data is public and the algorithms are described in the text, but the code is not disclosed).\n\nThe novelty is poor: attacks against ML-based methods for TSF have been proposed in the past, and the provided formalization is too vague to determine whether it is truly a novel contribution or not.",
            "summary_of_the_review": "First, I appreciate the line of research tackled by this paper. It is true that there have been few papers that investigated \u201cadversarial attacks against ML methods for time series forecasting\u201d. Furthermore, it is also true that these methods are becoming increasingly popular in the real world, as they are being integrated into production systems. Therefore, it is also crucial to study this subject from a security viewpoint.\n\nUnfortunately I cannot recommend acceptance of this paper \u2013 at least not in its state, and definitely not for ICLR. Indeed, the paper is affected by several and critical weaknesses, which affect both its \u201csecurity\u201d focus and its technical rigour (required for ICLR). I will elaborate below on the many problems that affect the paper, and provide suggestions for improvement.\n\nBefore I do this, however, I must warn the authors that I will gladly engage in a discussion with them, but that I am also very unlikely to change my mind on the submission, as I believe the amount of effort required to put it into an \u201cacceptable\u201d state for ICLR is simply too much to address during the rebuttal phase.\n\n**Limited scope.** \nAccording to Section 3.1, the considered setting assumes the usage of Time Series Forecasting only for predicting the \u201cnext\u201d value of a given time-series. Such a setting can be a bit simplistic as there are several cases in which the forecasting may entail the prediction of \u201cmultiple\u201d future values. Albeit one can argue that predicting multiple values can be done by iteratively predicting a single one, it is unclear whether this scenario also falls within the threat model of the paper.\n\n**Vague definitions.** \nThe following \u201cdefinition\u201d of adversarial time series is not rigorous: \u201cThe adversarial time series $\\hat{X}$ ($X_{adv}$) is intended to significantly worsen the output prediction $\\hat{Y}$ of a TSF model.\u201d Given the scope of the paper (let me quote from the introduction: \u201cWe define and formalize targeted attacks on deep learning time series forecasting.\u201d), such \u201cvagueness\u201d is unacceptable. The same issue affects the definition of the attacker\u2019s \u201cgoal\u201d, which I quote: \u201cThe goal of the attacker is to create a targeted output impact on the time series.\u201d What is a \u201ctargeted output impact\u201d? Furthermore, I do not agree with this statement: \u201cProperties of the perturbation. The performance of the model prediction would reduce as the strength of the perturbation increases.\u201d It may be intuitive to assume that \u201clarger\u201d perturbations have a greater impact, but such a statement is not universal. Do the authors *require* their perturbations to have a stronger impact as their magnitude is increased? Finally, the following sentence is problematic: \u201cThis attributes to the cost of data manipulation. It is notable that it is more expensive to achieve higher input perturbation\u201d: first, there is no mentioning of this \u201ccost\u201d, thereby increasing the vagueness; second, because \u201cgreater perturbation = greater cost\u201d is also not universally true (see [A]). Put simply, the authors attempt to formalize adversarial attacks on TSF, but the provided definitions are vague and the formalization \u2013 which appears to be universal \u2013 is limited to very few specific cases (which are not even properly defined). \n\n**Extremely Powerful Attacker.** \nThe proposed threat model envisions an extremely powerful attacker, who has white-box knowledge of the targeted ML model. This is highly unrealistic in the real world; furthermore, although the authors claim that \u201cthe attacks work even in black-box settings\u201d, the way in which such attacks work is by iteratively querying the targeted ML model---thereby assuming an attacker who has \u201coracle access\u201d to the ML model. Both scenarios are highly unlikely in real world settings (see, e.g., [A]), especially because the attacker \u2013 on top of knowing/being able to query the targeted ML model \u2013 also needs to introduce the perturbations (i.e., they must be able to manipulate the input data stream analyzed by the ML model). Simply put, from a security standpoint, the considered \u201cthreat model\u201d is unrealistic and I am not surprised in the slightest that the attacks \u201cwork\u201d. I invite the authors to provide some specific use cases of the proposed threat model, which should be provided with concrete justifications as to why an attacker would invest so much effort in setting-up, and then launching, a similar offense. Otherwise the authors should remove these considerations and simply state that the proposed \u201ccontribution\u201d is devoted to assessing the robustness of ML models for TSF to adversarial examples (which are not necessarily malicious). I point the authors to a recent work that summarizes all these issues, [A]\n\n\n**The definition of \u201ctemporal targeted attacks\u201d (TTA) is flawed.**\nAccording to Section 4, these attacks are defined as follows: \u201cIn a Temporal Targeted Attack (TTA), the attacker crafts the attack such that a minor perturbation in the input causes a specific time region in the output to be manipulated. In the given region, the attacker would want to change the direction of the output (DTA) or the amplitude of the output (ATA) in a specific time window $(t_1, t_2)$.\u201d. This definition is in stark contrast with the definition of \u201ctime series forecasting\u201d provided in Section 3, which I quote: \u201cGiven a time series X = [$x_1$, $x_2$, \u2026 $x_T$], a time series forecasting task predicts the value of $x_{T+1}$ based on the previous samples [$x_{T\u2212w}, x_{T\u2212w+1}, ..., x_{T}$], where $w$ is the window size under consideration.\u201d Indeed, if the goal of time series forecasting is predicting $x_{T+1}$, then how can the attacker influence the output in a specific time window $(t_1, t_2)$? This is only possible if $t_1=t_2$, which means that the window is not a window in the first place. Therefore, either one of the following is true: the definition of time series forecasting is flawed, or TTA are redundant.\n\n\n**No (realistic) justification for the Experimental Evaluation.**\nThis is a follow-up issue to the \u201cextremely powerful attacker\u201d described above. Let me quote from Section 5: \u201cStock prediction is a high-demand, high-impact and high-risk area in the financial markets. Stock market prediction can have a direct impact on individuals, organizations, and nation as a whole, thus inaccurate predictions are seen as high-impactful and high-risky. However, these models are also vulnerable to attackers, where one can launch an adversarial attack to make the necessary predictions.\u201d Specifically, \u201chow can an attacker launch an adversarial attack against ML models tasked to predict stock market values?\u201d According to the above, the attacker must be able to manipulate the data, and either know the targeted ML model (which is an IP of a company), or be able to query such ML model. How is this realistically feasible? Reality is far more complex and my impression is that the paper is being oversold (from a security perspective), thereby increasing the confusion (endemic among practitioners) about the real threat of adversarial attacks against ML models [A]. (Note that the same comment applies also for the \u201cenergy prediction\u201d scenario: why, and how, would an attacker attempt to do so?)\n\n**Biased experiments.**\nFor the evaluation, the authors adopt some parameter settings without properly motivating them. Why was the Google Stock Prices split into train:test with a 4:1 split? Why were the time windows set to 5? Maybe there is a logic, but as it is not described in the paper, I am inclined to believe that the authors arbitrarily \u201ccherry picked\u201d the values that confirmed their attacks to be successful.\n\n**Unclear statistical validation.**\nIt is unclear how the KS test was carried out. Table 1 shows the resulting p-value of the many comparisons performed in the paper. However, it is not clear what was actually being compared: did the authors simply aggregate the overall results obtained by a (single) ML model on the the (single) train:test split with a (single) parameter configuration? In other words, how large is the size of the \u201cpopulations\u201d that are being compared? My guess is that the authors performed such tests by considering a single \u201cconfiguration\u201d, which is not fair and definitely not enough to support any claim about the effectiveness of the attacks. I invite the authors to consider diverse configurations (e.g., different splits, different window sizes), and then use all such diverse configurations to perform a (more transparent) statistical test by also adopting appropriate \u201ccorrection\u201d techniques (e.g., Bonferroni Correction). Doing so would tremendously help supporting the effectiveness claims made in the paper!\n\nSome additional issues:\n\n\u2022\tThere is no buildup for Section 3.2. \n\n\u2022\tThis sentence makes no sense (Section 3.2): \u201cCorresponding to the above distance, a p-value is calculated. Given a significance value \u03b1. the null hypothesis that the sample follows the reference distribution is acceptable, if the p-value is acceptable if it is greater than the significance value \u03b1.\u201d\n\n\u2022\tThe quality of writing in Section 4 is very low\n\n\u2022\tI am inclined to believe that the proposed \u201cATA\u201d are just a subset of \u201cDTA\u201d. A more rigorous definition is necessary\n\n\u2022\tThe following statement is wrong: \u201cThe FGSM attack Goodfellow et al. (2014a) forms the basis of any known adversarial attack techniques.\u201d There are a plethora of \u201cadversarial attacks\u201d wherein the adversarial examples are created by means of techniques that have nothing to do with FGSM. I invite the authors to avoid making such statements.\n\n\u2022\tThe paper should make an effort to cite some existing works that proposed adversarial attacks against TSF for the two considered use cases (a quick search on Google Scholar would reveal several papers that evaluated their attacks on such data).\n\n\u2022\tThe proposed attack is evaluated only on \"univariate\" time series. Will it work also on multivariate time series? Why has this setting not been considered in the paper?\n\n\nEXTERNAL REFERENCES\n\n[A]: Apruzzese, G., Laskov, P., de Oca, E. M., Mallouli, W., Rapa, L. B., Grammatopoulos, A. V., & Franco, F. D. (2022). The Role of Machine Learning in Cybersecurity. Digital Threats: Research and Practice.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4246/Reviewer_6F4s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4246/Reviewer_6F4s"
        ]
    },
    {
        "id": "GxA_4Z25sA",
        "original": null,
        "number": 2,
        "cdate": 1666348466227,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666348466227,
        "tmdate": 1666521063386,
        "tddate": null,
        "forum": "4vYWYGd13cZ",
        "replyto": "4vYWYGd13cZ",
        "invitation": "ICLR.cc/2023/Conference/Paper4246/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper extends targeted adversarial attacks to time series forecasting, and performs a statistical evaluation to demonstrate the effectiveness of the methods.",
            "strength_and_weaknesses": "The paper is clear and the experiments appear sound. The use of proper statistical tests is appreciated. However, I have serious concerns about the originality and novelty of the contribution, because the paper overlooks directly related work on targeted attacks for time series forecasting. Besides, Section 4 (4 -Targeted Time Series Attacks) looks more like a review of existing attacks rather than a proper contribution, though I acknowledge that the APGD algorithm has been adapted.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The submission completely overlooks the related work of Dang-Nhu et al. [1] which performs adversarial attacks on the well known DeepAR model, applied to stock price prediction and electricity consumption forecasting. Dang-Nhu et al. present:\n- a general setting with arbitrary target function\n- a threat model that is similar to the submission\n- a probabilistic framework that allows to perturb the whole output distribution\n- an iterative setting that permits application to multi-step forecasting\n- a detailed discussion on the attack's gradient estimators\n\nThis seems to be more a much more general approach than what is discussed in the submission, dating from 2020. There are certainly other related papers that are not cited.\n\n[1] Rapha\u00ebl Dang-Nhu, Gagandeep Singh, Pavol Bielik, and Martin Vechev. Adversarial attacks on probabilistic autoregressive forecasting models, ICML 2020.\n",
            "summary_of_the_review": "Because of serious concerns about novelty and originality, I can not recommend acceptance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4246/Reviewer_zM1v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4246/Reviewer_zM1v"
        ]
    },
    {
        "id": "nelIJpjnlHb",
        "original": null,
        "number": 3,
        "cdate": 1666565429239,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666565429239,
        "tmdate": 1666565429239,
        "tddate": null,
        "forum": "4vYWYGd13cZ",
        "replyto": "4vYWYGd13cZ",
        "invitation": "ICLR.cc/2023/Conference/Paper4246/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose to study targeted attacks on deep learning methods for time series forecasting. They adapt existing approaches to attack trained neural networks for image classification and run a number of experiments.",
            "strength_and_weaknesses": "The paper nicely summarizes various popular attack patterns for deep learning methods and uses them in a time series forecasting setup. Notably absent from the paper is a meaningful defense mechanism to a targeted attack.",
            "clarity,_quality,_novelty_and_reproducibility": "The main observation of the paper appears to be that targeted attacks are more effective than untargeted attacks, which must hold quite generally and should not be limited to this particular question. So, while the paper is understandable and clear, it is not clear what the novelty is.",
            "summary_of_the_review": "The paper appears to apply established techniques to a related problem. This contribution appears to be marginal in terms of novelty.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4246/Reviewer_oRvB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4246/Reviewer_oRvB"
        ]
    },
    {
        "id": "JMz94CbTYvR",
        "original": null,
        "number": 4,
        "cdate": 1666634952391,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634952391,
        "tmdate": 1666634952391,
        "tddate": null,
        "forum": "4vYWYGd13cZ",
        "replyto": "4vYWYGd13cZ",
        "invitation": "ICLR.cc/2023/Conference/Paper4246/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the targeted attack on time-series forecasting tasks. Specifically, this paper proposes three different targeted attack goals: directional attack, amplitudinal attack, and temporal attack. This paper then designs the corresponding FGSM, PGD, and APGD attacks for these goals. All attacks are evaluated on the google stock dataset and household electric power dataset, and the evaluation results show that the proposed attacks achieve adversarial attack goals.",
            "strength_and_weaknesses": "Strength: \n\n+ Clean writing: The paper is well-written and easy to read.\n\n+ Novel method: The paper proposes targeted adversarial attacks against time-series forecasting tasks based on three different attack goals: directional attack, amplitudinal attack, and temporal attack. \n\n+ Empirical experiment support: The paper evaluates the proposed methods on two time-series datasets and shows that the proposed attack method achieves the attack goals.\n\nWeakness:\n\n+ Lack of methodology details: I think the most interesting part of the paper is the different targeted attack goals and their corresponding strategies to achieve these goals. In equation 3, amplitudinal and temporal attack needs $lim$ and $att$. Although there is a discussion before Section 4.1 on these two functions, I am still not quite clear about these functions are defined. It will be great to describe these two functions more formally to avoid unnecessary confusion.\n\n+ The effectiveness of the attack: In Figure1, if we compare the delta between the original, predicted, and attacked predicted values, we can find that the delta is always less than $0.1$. Given the $\\epsilon=0.1$, a $0.1$ change in predicted values can be considered expected. For example, for a DTA-up attack, if we simply increase the input value by $0.1$, we may also be able to increase the predicted value. This change makes me feel the adversarial attack is not effective enough. It will be good to visualize the perturbations and the adversarial examples so that readers can better understand how adversarial perturbations cause unexpected changes in the forecasting results.\n\n+ Also, where are the red lines (PGD) and green lines (FGSM) in Figure 1? And why is only the DTA-down attack with $\\epsilon = 0.5$? What\u2019s the performance of the DTA-down attack with $\\epsilon =0.1$? Based on the value on the y-axis, $0.5$ is a very large perturbation.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The paper is well-written and easy to read.\n\n**Quality, Novelty:** This paper proposes a novel targeted attack on the TFC task. However, adding more details to describe the attack methods more clearly will be great. The evaluation cannot fully support the effectiveness of the adversarial attack.\n\n**Reproducibility**: The paper provides some experiment setup but does not provide the code.",
            "summary_of_the_review": "Thanks for the paper, I thoroughly enjoyed reading the paper. My concern about the paper is the effectiveness of the targeted attacks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4246/Reviewer_X1J3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4246/Reviewer_X1J3"
        ]
    }
]