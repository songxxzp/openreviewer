[
    {
        "id": "78dVmc6zqb",
        "original": null,
        "number": 1,
        "cdate": 1666695730032,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695730032,
        "tmdate": 1666695730032,
        "tddate": null,
        "forum": "B-z41MBL_tH",
        "replyto": "B-z41MBL_tH",
        "invitation": "ICLR.cc/2023/Conference/Paper3199/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies imitation learning through the lens of structural causal models. Specifically, this paper proposes graphical conditions to enable imitator to learn an expert-level policy, when unobserved confounders (UCs) occur. Interestingly, the proposed method can extends to existing IRL algorithms when UCs are present. Two showcased algorithms are causal version of multiplicative weights algorithm (MWAL) and generative adversarial imitation learning (GAIL). The proposed methods have been evaluated using real-world and synthetic data, showing supervior performance than the original ones.",
            "strength_and_weaknesses": "## Strength\n\n+ The proposed method is novel. Under the structural causal model of inverse reinforcement learning, this paper extends the graphical condition for causal behavioral cloning to causal inverse reinforcement learning. This was done by formulating the problem as learning to paly a two-player zero-sum game, where the prior knowledge about the latent rewards can be incorporated. With informative knowledge, the imitator can outperform the expert in causal environment. \n\n+ The proposed method is well motivated with theoretical analysis, and naturally extended to two existing inverse reinforcement learning algorithms, i.e. MWAL and GAIL. The proposed method was well supported by the empirical results.\n\n## Weakness\n\n+ More diverse and challenging experiments are expected. \n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n**Clarity**\nThe presentation is clear.\n\n**Quality**\nThis paper is of high quality.\n\n**Novelty**\nThe proposed method in this paper is novel.\n\n**Reproducibility**\nDetails of the algorithms and experiments have been provided.\n",
            "summary_of_the_review": "Overall, this paper provides a novel causal inverse reinforcement learning method. The quality is good.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3199/Reviewer_o8Xw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3199/Reviewer_o8Xw"
        ]
    },
    {
        "id": "eGBm7mAiZo",
        "original": null,
        "number": 2,
        "cdate": 1666799205315,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799205315,
        "tmdate": 1666799205315,
        "tddate": null,
        "forum": "B-z41MBL_tH",
        "replyto": "B-z41MBL_tH",
        "invitation": "ICLR.cc/2023/Conference/Paper3199/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a new inverse reinforcement learning algorithm that's able to provably meet the performance of an expert demonstrator in the presence of uncontrolled confounders, when certain conditions are met (i.e., does the agent have the causal structure of the data generating process right?).",
            "strength_and_weaknesses": "Strengths: The submission does a fantastic job laying the groundwork for causal reinforcement learning, motivating its algorithms, theorems, and results within the formalisms of structured causal models.   I also greatly appreciated the extensive FAQ in the appendix.  The experiments clearly demonstrate the power of the method.\n\nWeakness: While the exposition of the paper is exceptionally high quality, there does not appear to be a code repo associated with the submission.  This method, and its uptake by the community, would greatly benefit from open sourcing of code / experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality: Both extremely high\nNovelty: While these ideas have been \"in the water\" in the community for a while, this work certainly represents a novel synthesis, and provides the first algorithm that can convincingly exceed expert performance in scenarios with UCs.\nReproducibility: Probably possible by an extremely patient researcher, but the authors should strongly consider open sourcing.",
            "summary_of_the_review": "An excellent submission to the study of causal reinforcement learning.  Please open source it :)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3199/Reviewer_Ne7b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3199/Reviewer_Ne7b"
        ]
    },
    {
        "id": "S94ZR-Le0t",
        "original": null,
        "number": 3,
        "cdate": 1666907450224,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666907450224,
        "tmdate": 1668446523870,
        "tddate": null,
        "forum": "B-z41MBL_tH",
        "replyto": "B-z41MBL_tH",
        "invitation": "ICLR.cc/2023/Conference/Paper3199/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a framework for causal imitation learning. The problem assumes that the experts has access to latent features that are not directly observable to imitators. The solution is based on identifying the minimal \\pi-backdoor admissible scope from the causal diagram. Experiments on a few datasets are provided to validate the effectiveness of the proposed approach. ",
            "strength_and_weaknesses": "Strengths:\n\n1. The problem described in the paper is meaningful and challenging. \n2. Theoretical proofs for theorems are provided. \n\nWeaknesses:\n\n1. The paper lacks a clear problem formulation. The input/output and objectives of the problem are not provided. \n2. The paper uses many different variables, where some of them are either undefined, or repeatedly used for different purposes. The presentation makes it hard to follow the details of the paper. \n3. The paper seems to be incomplete. The only algorithm only shows how to identify identifiable policy scope. However, how to use this algorithm with IRL or GAIL is not provided. \n4. The experiments are not convincing. Why would the MNIST dataset be used, and how to learn a policy from this dataset?\n5. How is the problem setting in this paper different from a partially observable MDP (POMDP)? More discussions are needed. ",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed work has its value especially for imitation learning. However, the paper lacks a clear problem formulation. The presentation, in particular, the use of variables, is very confusing and hard to follow. The solution part does not appear to be complete. A number of concepts are not clearly defined. \n\nFor example, variable X and x used to refer to different concepts in Section 1.1 and Section 2. The concept of \"intervention\" is not clearly defined and denoted using different variables (e.g.,  X and do(\\pi)). ",
            "summary_of_the_review": "The paper proposes an imitation learning framework, where the imitator can learn policies from observations with unobserved confounding. The solution is based on identifying identifiable policy scope from a causal diagram of the underlying decision process. Experiments on a few datasets are conducted to validate the effectiveness of the proposed solution. \n\nThe problem discussed in this paper is interesting, important, and technically challenging. However, paper fail to provide a clear formulation of the problem (input/output/objective, MDP formulation). The presentation, in particular the non-rigorous use of  variables and concepts make it hard to follow the details of the paper. The solution lacks the steps to complete the imitation learning loop. Experiments can be strengthened by using more relevant datasets and evaluating on additional measures (e.g., fidelity of learned polices). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3199/Reviewer_iWpt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3199/Reviewer_iWpt"
        ]
    },
    {
        "id": "RRwnhiiDUJM",
        "original": null,
        "number": 4,
        "cdate": 1667771159087,
        "mdate": 1667771159087,
        "ddate": null,
        "tcdate": 1667771159087,
        "tmdate": 1667771159087,
        "tddate": null,
        "forum": "B-z41MBL_tH",
        "replyto": "B-z41MBL_tH",
        "invitation": "ICLR.cc/2023/Conference/Paper3199/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors considered the problem of imitation learning when the underlying causal structure of the environment is given. They provided a causal formulation of the problem of inverse RL (IRL) (equation (2)). They introduced the notion of minimal $\\pi$-backdoor admissible scope and showed that the effect of such policies on the average reward can be computed from the observational distribution over $(O,Y)$, i.e., the observable variables and the reward. Based on this, a canonical equation for IRL is given in (4) where for two settings of MWAL and GAIL, this formulation is simplified. Then, the authors used a method called IDENTIFY which is beyond the graphical condition in $\\pi$-backdoor admissible scope and it is sound and complete for checking whether a policy scope is identifiable from $P(O,Y)$. Finally, they proposed LISTIDSCOPE to enumerate all identifiable policy scopes.",
            "strength_and_weaknesses": "Strength:\n\n- The authors provided a nice formulation of inverse RL in the case that we have access to the causal graph. Moreover, they also gave some examples of this formulation for MWAL and GAIL.\n- They presented a method that can enumerate all identifiable policy scopes from $P(O,Y)$.\n\nWeakness:\n\n- Regarding the gap $\\nu^*$, it is not clear why the authors only considered the cases that $\\nu^*\\leq 0$. This can only happen if the expert has suboptimal performance in the environment (or in causal language, in some SCM $M$) that is acting. \n\n\nIn the following, I give my detailed comments:\n- It would be great if the authors can give some real cases that the performance of an expert is suboptimal even in the environment that is acting in it. The example in the introduction is not convincing enough as it is not clear why the expert acts based on just $X$ which results in poor performance.\n- The authors considered a ``sequential decision-making\" setting however, it seems that $Y$ is not indexed with time steps. How can the results in the paper be extended to the more general setting where we have a sequence of $Y_i$'s? In the FAQ, it is mentioned that $Y$ can be a set of variables but it might be the case that the cumulative reward is identifiable while each $Y_i$ is not identifiable from $P(O,Y)$.\n- Under what conditions, is $\\nu^*$ positive?\n- What is the exact definition of effective actions and covariates in Theorem 1? What happens to other variables $\\mathbf{X}$ and $\\mathbf{Z}$ in Theorem 1?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written but some more explanations can be given in the paper. For instance, it is good to discuss the projection algorithm mentioned in the footnote on page 7 and also mention why it is required to perform such projections. Regarding the novelty of the paper, I think the causal formulation of causal IRL and the algorithm LISTIDSCOPE are somewhat novel but it is not clear how causality can help to get better performance than the expert. The example in the introduction is not convincing. It seems that the results can be reproduced based on the explanation in the appendix.",
            "summary_of_the_review": "The submitted paper shows the advantage of considering the underlying causal diagram in the problem of inverse RL and provides a method to enumerate all identifiable policy scopes. Based on this, the canonical equation of causal IRL in (4) can be computed for such policies. The experiments showed that causal IRL can have better performance than the expert. The main ambiguity in the paper is for the case $\\nu^*>0$ and it is required to justify why the expert is not acting optimally even at least one SCM $M$.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3199/Reviewer_U7KX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3199/Reviewer_U7KX"
        ]
    }
]