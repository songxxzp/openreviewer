[
    {
        "id": "D_mJouO-0k1",
        "original": null,
        "number": 1,
        "cdate": 1665761064094,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665761064094,
        "tmdate": 1665761064094,
        "tddate": null,
        "forum": "MpGP-z07TmM",
        "replyto": "MpGP-z07TmM",
        "invitation": "ICLR.cc/2023/Conference/Paper1042/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes trainable activation functions for physics-informed neural networks (PINNs). The basic idea is to consider a set of base activation functions, and build a trainable one as a convex combination of the base ones. They provide experiments on many problems, showing that PINNs are generally sensitive to the choice of AF, while the trainable one provides good performance across most cases. They also try to motivate the approach from the point of view of the neural tangent kernel (NTK).",
            "strength_and_weaknesses": "Strength: From the point of view of PINNs, the proposed neural networks have better accuracy and performance than the base ones. \n\nDrawbacks: \n- From the methodological point of view, the method is *equivalent* to the method proposed in (Manessi and Rozza, 2019), referenced in the paper, which was also proposed under the name of \"adaptive blending unit\" in (Sutfeld et al., 2020), which is not referenced here. The authors are renominating them as \"physics-informed activation function\" (PIAC), but as far as I can see there is no difference that warrants a change of name (apart from slightly modifying the number of base AFs). They mention that \"previous methods experiment with limit choice of activation functions\", but they use 5 base AFs which is similar to 6 base AFs in (Sutfeld et al., 2020). \n- The method is also compared only with fixed or parametric AFs, and not against similar expressive AFs (e.g., APLs).\n- The NTK derivation is quite shallow and it does not provide any special insight into the AFs themselves.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written well except for a small number of typos (e.g., \"over-parameterizaiton\"). The experimental part is well detailed but with an insufficient number of comparisons. Novelty is extremely small, as described above.",
            "summary_of_the_review": "The method is a rebranding of a known technique, applied on a different domain. While this is potentially interesting for PINNs, I feel this is not a good practice and it is also an artificial construct to increase novelty.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1042/Reviewer_E27E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1042/Reviewer_E27E"
        ]
    },
    {
        "id": "8pBTbDHbWu",
        "original": null,
        "number": 2,
        "cdate": 1666455655039,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666455655039,
        "tmdate": 1670265033497,
        "tddate": null,
        "forum": "MpGP-z07TmM",
        "replyto": "MpGP-z07TmM",
        "invitation": "ICLR.cc/2023/Conference/Paper1042/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present a method for parameterizing activation functions to improve convergence in physics-informed-neural-networks. They demonstrate through empirical experimentation that the proposed method converges to better solutions than non-learned activation functions or adaptive slope methods. Finally, the authors provide an analysis based on Neural Tangent Kernel that sheds light on why the proposed method is effective.\n",
            "strength_and_weaknesses": "The method proposed by the authors addresses a significant issue in PINNs. The approach is demonstrated on 5 different PDEs. The method is compared to several baselines as well as some ablations. \n\nThe NTK based analysis is quite hand-wavy. There is no treatment of the approximation error of the two-phase analysis and the notion that the NTK is learned to suite the underlying PDE is approached empirically with a single example.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and clear. \nOne thing that was not clear to me is: Do all activation functions in the network share the same parameters? or are they all learned independently? \nThe approach is somewhat novel in the PINN literature.\nThe work appears to be reasonably reproducible. I have no concerns there.\n\nThe other reviewers have raised the criticism that the proposed method is very similar to existing work. I was not aware of that work when I wrote my review. I find that the existence of the prior work significantly reduces my view of the novelty of the authors' contribution. I have therefore reduced my score. I think the authors could improve their contribution by formally proving their NTK claims or perhaps by expanding the number of base activation functions to very large numbers (e.g. using a basis). ",
            "summary_of_the_review": "Overall this paper is a solid demonstration of an approach to address the training difficulty empirically observed in PINNs.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1042/Reviewer_J89g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1042/Reviewer_J89g"
        ]
    },
    {
        "id": "emIJpG4HSH",
        "original": null,
        "number": 3,
        "cdate": 1666656083712,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656083712,
        "tmdate": 1668621936663,
        "tddate": null,
        "forum": "MpGP-z07TmM",
        "replyto": "MpGP-z07TmM",
        "invitation": "ICLR.cc/2023/Conference/Paper1042/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors present a formulation to create a trainable activation function based on a convex combination of other activations.\nThis new mixture approach is intended to work on physics-informed neural networks (PINNd) where higher order derivatives play a role, thus causing problems for some of the piecewise linear based common activations.\n\nThe authors the provide an empirical evaluation as well as a link to the neural tangent kernel.",
            "strength_and_weaknesses": "The authors present a technically sound approach for PINNs. Evaluated on datasets that clearly present the difficulties in this domain. \n\nHowever, the main weakness of this paper is in the lack of comparison to other learnable activation functions. The authors present these family of activation functions in the section \"Adaptive activation functions\", yet they are not present in the evaluation.\nSome of them could be relevant as they do provide higher oder derivatives. Maybe there is an issue with those activation functions and that is a good reason not to include them, but this is not clear from the paper.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and technically sound. The gating function and components are presented clearly. The activation function can be reproduced from the information in the paper.",
            "summary_of_the_review": "In this paper, the authors present an activation function that is amenable to PINNs. The authors present the problem clearly and offer a solution for this particular context based on previous work. They however do not compare to other adaptable activation functions that might still behave well in this domain. \nThe empirical evaluation would be much stronger, if you were to compare to that class of activations. In particular, it would be great to show the benefits of your activation function, or explain why the others are not suitable and demonstrate that empirically.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1042/Reviewer_qn6w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1042/Reviewer_qn6w"
        ]
    },
    {
        "id": "pSZh185xr_",
        "original": null,
        "number": 4,
        "cdate": 1666659722836,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659722836,
        "tmdate": 1668920189284,
        "tddate": null,
        "forum": "MpGP-z07TmM",
        "replyto": "MpGP-z07TmM",
        "invitation": "ICLR.cc/2023/Conference/Paper1042/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The goal of this work is to automatically learn the activation functions via the combinations of different candidate activation functions for PINN. Specifically, it adopted gate function with a learnable parameter to identify the coefficients for different candidate activation functions. Extensive experiments are carried out to evaluate the proposed physics-informed activation functions on a variety of partial differential equations (PDEs). The results demonstrate the efficacy of the simple approach. ",
            "strength_and_weaknesses": "* Strength\n1. It is interesting to automatically choose the activation functions for PINN\n2. The proposed method is simple yet effective to solve PDE systems\n3. Conduct extensive experiments on various PDEs to verify the effectiveness of the proposed approach\n\n* Limitations\n1. Technical innovation is somewhat limited. The proposed gate function to learn the coefficients of activation functions is the same as that in the literature [Qian et al. 2018]\n2. Compare to more baselines. I am curious if you could compare the prior works that learn combinations of activation functions?\n3. A few grammatical errors. For example, \"by minimize the following objective function\" in the bottom of page 2; and \"we consider a extreme case of insufficient collocation points\", a-> an\n\nReferences:\\\n[Qian et al. 2018] Adaptive activation functions in convolutional neural networks, 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and organized. However, the technical innovation is limited.",
            "summary_of_the_review": "It is interesting to automatically learn the activation functions for PINNs. The proposed method is simple yet effective for PDEs systems. However, this work adopted the same gate function method from prior work and apply it to PINNs. Hence, the technical innovation is somewhat limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1042/Reviewer_5qLY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1042/Reviewer_5qLY"
        ]
    }
]