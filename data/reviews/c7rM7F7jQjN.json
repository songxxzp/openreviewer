[
    {
        "id": "yVGSVX0erh",
        "original": null,
        "number": 1,
        "cdate": 1666465555361,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666465555361,
        "tmdate": 1669405393123,
        "tddate": null,
        "forum": "c7rM7F7jQjN",
        "replyto": "c7rM7F7jQjN",
        "invitation": "ICLR.cc/2023/Conference/Paper2136/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the problem of offline learning from play data and presents a goal-conditioned version of behavior transformer (BeT) and demonstrates it outperforms several existing algorithms in selected simulation environments and a real robot setup.",
            "strength_and_weaknesses": "Strength:\n- Goal-conditioned learning is a clean and simple formulation, compared to Q learning.\n- C-BeT addresses the shortcomings of BeT by conditioning on goals to resolve ambiguities.\n- This paper presents results in both simulated and real-world datasets and shows that C-BeT outperforms baselines in simulated benchmarks.\n\nWeakness: \n- The core idea is doing GCBC with BeT as the BC backbone, which is only incrementally novel. \n- The criteria of choosing baselines is not super clear. A concurrent work of Play-LMP, RIL (Gupta et al.), that outperforms GCBC in the Kitchen domain isn\u2019t included while GCBC is in. A concurrent work of GoFar, PLATO (Belkhale et al.), that outperforms Play-LMP isn\u2019t included while WGCSL (which is shown to be not as good as GoFar) is included.\n- The result metric for simulated tasks is confusing, what is the unit? Is it the number of successes over how many total tasks? Why not just use the standard success rate? Did you run multiple different seeds?\n- The reason why BeT would outperform goal-conditioned methods isn\u2019t well explained. It is a little counterintuitive and leads to the suspicion that the baseline models are not sufficiently tuned to work with the selected domains. Why not reuse the Play-LMP dataset/tasks? How would C-BeT and BeT perform in tasks without artificially introduced multimodal data?\n- The type of generalization is very limited. \n- The setup is fixed and the policy theoretically could have learned the door and knob tasks without the visual observation and be more robust to visual perturbations. \n- The generalization test of \u201cfresh\u201d demonstrations are still of the same tasks.\n\n\n\nReferences:\n\nGupta, Abhishek, et al. \"Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning.\" Conference on Robot Learning. PMLR, 2020.\n\nBelkhale, Suneel, and Dorsa Sadigh. \"PLATO: Predicting Latent Affordances Through Object-Centric Play.\" arXiv preprint arXiv:2203.05630 (2022).\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with clarity and high quality. The code (conditional portion of BeT) is not included in the submission but the hyperparameters are specified in the appendix. \n\n",
            "summary_of_the_review": "This paper presents a conditional version of BeT and demonstrates that it performs well on a set of simulated tasks as well as a real robot kitchen environment. The novelty of the method is limited and the selection of baselines is not satisfying but I do appreciate the simplicity of this algorithm. I don\u2019t recommend accepting this work in its current form but won\u2019t argue strongly against it.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2136/Reviewer_nm8H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2136/Reviewer_nm8H"
        ]
    },
    {
        "id": "jAPoNzDnOC",
        "original": null,
        "number": 2,
        "cdate": 1666568952879,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666568952879,
        "tmdate": 1669733266477,
        "tddate": null,
        "forum": "c7rM7F7jQjN",
        "replyto": "c7rM7F7jQjN",
        "invitation": "ICLR.cc/2023/Conference/Paper2136/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper presents a method to learn from \u201cplay data\u201d in a robot manipulation domain. The \u201cplay data\u201d is first popularized by Lynch et al., 2018 and is defined as robot experience data collected through human teleoperation where the human teleoperator controls the robot to achieve a variety of goals in a domain. The difference between such data and, e.g., experience data from reinforcement learning is that neither the underlying goal nor reward signals is given. The proposed method aims to train a policy that (1) captures the multimodal behavior exhibited in such data (2) generate behaviors shown in the training data by conditioning on a desired goal state and (3) generalize to new behaviors not shown in the data. The method combines two prior works: Shafiullah et al. learns multimodal behaviors and is based on a transformer architecture. Lynch et al. (2019) makes the policy goal-conditional. The method is evaluated on a number of simulated domains, including CARLA and a table-top block pushing domain, and a real-world robotics setup where the robot is trained to manipulate objects in a toy kitchen. The paper shows that the proposed method outperforms a few baseline methods, including a prior works without using transformer architecture (Lynch 2019) and a transformer-based model that is not conditioned on the goal (Shafiullah et al., 2022). ",
            "strength_and_weaknesses": "Strengths:\n+ I like the spirit of developing a unified algorithm for imitation learning from a variety of domains. \n+ I like the real-robot experiment setup and appreciate the effort put into collecting the data and evaluating the policy on a physical robot platform.\n\nWeaknesses:\n\n- My first main concern about the paper is its technical novelty. As stated by the authors themselves, the paper is a somewhat straightforward combination of two prior works, Shafiullah et al., 2021 for the transformer architecture and the multi-modal action learning and Lynch et al., 2019 for goal conditioning. As much as I appreciate the comprehensive empirical study, I had a hard time justifying the technical novelty of this paper given that the main focus of the ICLR conference is on learning methods themselves. \n- My second main concern is the baselines used in the paper. The strongest baselines are BET and Play-GCBC, but they are \u201cset to fail\u201d since the proposed method is the combination of the two methods. Given the baseline choices, I\u2019m not exactly sure the point that the authors hope to convey through the experiment. Is the main contribution of the paper the transformer architecture and the multi-modal behavior policy proposed by BET? Because 5 out of the 6 baselines do not have these components. Is the main contribution of the paper goal-conditioning? Because the strongest baseline BET does not have goal conditioning and obviously will not be able to achieve the specified goal.\n- There are also other missing baselines. For example, IRIS [1] and Mandlekar and Xu 2020 [2] are both goal-conditional imitation methods that focus on disentangling multimodal behaviors. Neither is mentioned nor compared. ImplicitBC (Florence et al., 2021) that aims to learn multi-modal behaviors can also be trivially adapted to be conditioned on goals, similar to how the authors adapted BET to be conditioned on goals. \n\n[1] IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data\n[2] Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good writing. \nQuality: it's a solid execution of a conceptually simple method, although the choice of baselines for empirical evaluation is questionable.\nOriginality: questionable, as the method is a somewhat straightforward combination of two prior works.",
            "summary_of_the_review": "As mentioned in the main comments, although I like the real robot evaluation setup, I have strong doubts about the novelty of the proposed method and the baseline choices in evaluation.\n\n========================\nPost-rebuttal review: I thank the reviewer for adding additional baseline results. At the same time, I still believe the research described in this manuscript has limited technical novelty. As much as I'm impressed by the empirical result on real robot experiment, I must consider this factor due to the nature of the venue. I have updated the review score to reflect this.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2136/Reviewer_dNx6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2136/Reviewer_dNx6"
        ]
    },
    {
        "id": "32ukiA9IDs",
        "original": null,
        "number": 3,
        "cdate": 1666677441654,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677441654,
        "tmdate": 1666677441654,
        "tddate": null,
        "forum": "c7rM7F7jQjN",
        "replyto": "c7rM7F7jQjN",
        "invitation": "ICLR.cc/2023/Conference/Paper2136/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a goal-conditioned transformer for learning from play data. The approach is a pretty straightforward combination of the Behavior Transformer (BeT) (Shafiullah et al) and goal-conditioning. BeT trains a transformer for imitation learning with a hybrid discrete-continuous action space and has been shown to be effective at capturing multimodal data. Learning from play data is known to be multi-modal, so it makes sense that a goal-conditioned BeT would be an effective way to learn from it. \n\nThe results are pretty strong, outperforming the relevant imitation and offline RL goal-conditioned baselines in a set of 3 simulation environments. Additionally, it has good results on a challenging real robot manipulation task.",
            "strength_and_weaknesses": "*Strengths*\n\n- Simple method building on BeT. \n- Intuitive and well-motivated approach for the problem statement (BeT handles multimodality well, makes sense to use it for learning from play data)\n- Strong results in simulation and on real hardware.\n\n*Weaknesses*\n\n- Limited novelty beyond BeT, basically just adding goal conditioning. But it's largely an empirical paper and the results are strong so not a big issue.  ",
            "clarity,_quality,_novelty_and_reproducibility": "Good. ",
            "summary_of_the_review": "Overall the paper extends the Behavior Transformer to be goal conditioned. Doing so enables effective goal-conditioned learning from play, outperforming relevant baselines on simulated and real robotics tasks. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2136/Reviewer_DhxW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2136/Reviewer_DhxW"
        ]
    },
    {
        "id": "CNPjvq5JPX",
        "original": null,
        "number": 4,
        "cdate": 1667266774044,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667266774044,
        "tmdate": 1667266774044,
        "tddate": null,
        "forum": "c7rM7F7jQjN",
        "replyto": "c7rM7F7jQjN",
        "invitation": "ICLR.cc/2023/Conference/Paper2136/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work presents a straightforward, but impressive extension of prior work on \u201cBehavior Transformers\u201d for learning policies from offline, reward-free data, by introducing the ability to learn goal-conditioned policies from undirected play data. Crucially, the play data used to learn policies in this work in inherently multimodal (and in some cases suboptimal); yet, using the goal conditioned behavior transformer (C-BeT) approach, we\u2019re able to overcome such problems \u2014 *without any online exploration*.\n\nThe proposed approach builds on a history of prior work in goal-conditioned behavior cloning from visual observations, and makes a simple tweak to the underlying Behavior Transformer (think a causal transformer that eats states, and outputs actions) architecture, by additionally conditioning on target goal observations (images) of where we\u2019d like the environment to end up. This is a powerful objective, and across three complex simulated environments (CARLA self-driving, multimodal block pushing, and the Franka Relay Kitchen environment) \u2014 each with varying levels of multimodality/suboptimality, C-BeT shows itself to be extremely strong.\n\nHowever, the truly impressive part of this work is the real-robot evaluation; from just 4.5 hours of real user \u201cplay\u201d on a Franka robot, C-BeT is able to learn 5 complex manipulation tasks in a \u201ctoy kitchen\u201d to a meaningful level of competency \u2014 far better than other approaches from the offline RL literature like GO-FAR. In general, I found the evaluation to be thorough, ablations comprehensive, and comparisons to prior art meaningful!",
            "strength_and_weaknesses": "I think this is a very strong paper, showing that existing Behavior Transformers can scale to multimodal, reward free play data. In general, I really believe in the results in this paper, and am excited for the possibilities of using such an approach for general offline policy learning on real (and simulated) robotic tasks.\n\nThe weaknesses of this work are generally just weaknesses around the feasibility of visual goal-conditioning in the wild. In the real world, it\u2019s not clear whether (given a new task, or a combinatorial explosion of objects/states) we\u2019ll be able to provide robots with meaningful \u201cgoal images\u201d that capture what we want to happen; for example, across all the tasks in this work, only a handful of object positions change from start state to goal state; in heavily cluttered environments with dynamic objects, it feels like such an approach would really suffer (in sample efficiency and maybe in just general applicability). This is somewhat shown in the videos, where we add random objects (though that\u2019s also just because the environment is so out of distribution).\n",
            "clarity,_quality,_novelty_and_reproducibility": "I found this paper incredibly clear and easy to read; the quality and thoroughness of the experiments (and attention to baselines) is commendable, and I really buy these results. I like that the authors chose to evaluate on open-source simulated environments as well as a real-robot; it speaks a lot to the reproducibility of this work!\n\nMinor: Seems like Claim 2 in the introduction \u201cC-BeT represents the first work to demonstrate that competent visual policies for real-world tasks can be learned from reward-free play data\u201d isn\u2019t quite right; Implicit BC learns from play-esque multimodal data without rewards, as does follow-up work like Implicit Kinematic Policies (but I could be wrong; might just be a wording thing around the distinction between \u201cplay\u201d and \u201cmultimodal demos\u201d).\n",
            "summary_of_the_review": "I think this is a straightforward and impactful extension of prior work on behavior transformers. I think that being able to learn robust goal-conditioned policies from uncurated reward-free play data in a manner that is 1) simple and 2) completely offline is very meaningful, and a clear win for the field.\n\nI really like this work!\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2136/Reviewer_ySFW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2136/Reviewer_ySFW"
        ]
    }
]