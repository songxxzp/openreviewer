[
    {
        "id": "hd-TMjgvrH9",
        "original": null,
        "number": 1,
        "cdate": 1666586106477,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666586106477,
        "tmdate": 1670004794932,
        "tddate": null,
        "forum": "ScEfNWshH3B",
        "replyto": "ScEfNWshH3B",
        "invitation": "ICLR.cc/2023/Conference/Paper3493/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes to use an adaptive weight decay (AWD) as a regularization, inspired by the observation that the ratio between gradient and weight in weight decay training is a more fundamental parameter than $\\lambda_{WD}$. The paper shows that AWD has a benefit in clean training, adversarial training and learning with noisy labels with rigourous experiment evaluation. Finally, AWD is shown to alleviate the overfitting phenomenon in CIFAR10 since the misclassified samples are mostly mis-labeled. ",
            "strength_and_weaknesses": "Strength:\n\nThe paper is clearly written and the experiment on CIFAR10/100 is extensive. The paper evaluates the AWD not only in the clean training setting, but in adversarial and noisy label setting, which all show the effectiveness of AWD over weight decay. I kind of like the style of this paper: proposing a simple but clever method and showing its effectiveness in a wide range of scenarios. \n\n\nWeaknesses:\n\n1. The first concern is about the hyperparameter in the AWD regularization. The DoG=0.016 is determined based on empirical results on CIFAR100 and used in other settings (only CIFAR10) in this paper. I wonder if the DoG can be determined by simple hyperparameter tuning like grid search. If the hyperparameter can only be determined by first training with weight decay, then I cannot say the AWD is a general regularization. I would recomment to show the performance of AWD with a simple hyperparameter tuning on other tasks such as object detectioc, sementic segmentation or at least other image classification datasets such as ImageNet to demonstrate the its general effectiveness. \n\n2. My second concern is about the motivation. The AWD is proposed to keep the ratio between gradient and weight panelty in the SGD update. But the AWD does not have the same SGD update form as in Equation (1), since the gradient norm is also backpropagated if my understanding is correct (based on the discussion following Equation (5)). In other words, if the motivation is to keep the DoG, one should compute the $\\lambda_{WD}$ based on the weight norm and its gradient norm and keep the original weight decay regularization. Please correct me if you do not take the gradient of the gradient norm, since there is no code as a reference.\n\nMinor Issue:\n\n1. The proposed method is not compared with other adversarial defense method, but the evaluation is kind of standard so it is easy to have a comparison. I suggest adding some baselines as a reference to better position the proposed method.  \n\n2. The regularization approach to adversarial robustness is not fully discussed. For example, the effective margin regularization [1] penalizes the weight gradient norm to boost the adversarial robustness, which looks similar to this paper. Could the author provide a discussion on the diffierence and connection? \n\n[1] Ziquan Liu and Antoni B. Chan, \u201cBoosting Adversarial Robustness From The Perspective of Effective Margin Regularization.\u201d British Machine Vision Conference (BMVC), 2022",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality are good. The proposed method is simple, but its novelty is not a major concern to me. ",
            "summary_of_the_review": "Overall, I would like to give a borderline score at this phase and I am happy to increase it if my concerns can be addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3493/Reviewer_4bPN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3493/Reviewer_4bPN"
        ]
    },
    {
        "id": "ylI-BOlrX1I",
        "original": null,
        "number": 2,
        "cdate": 1666611047844,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611047844,
        "tmdate": 1666611047844,
        "tddate": null,
        "forum": "ScEfNWshH3B",
        "replyto": "ScEfNWshH3B",
        "invitation": "ICLR.cc/2023/Conference/Paper3493/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors propose adaptive weight decay.  This strategy replaces the hyper-parameter of weight decay by the product of a hyper-parameter and the ratio of gradient norm and parameter norm. In this way, it can automatically tune the hyperparameter for weight decay during each training iteration. Then the authors show the effectiveness of the proposed method on adversarial robustness. ",
            "strength_and_weaknesses": "Strengths: \nThere are main two contributions in this work. \n1)The proposed  method is quite simple.   \n\n\n2)The experimental improvement is notable compared with the vanilla regularization.  \n\n\nWeaknesses:\n1)The proposed method is slightly incremental, and brings extra computational cost. The only contribution is that it uses the ratio of gradient norms of the loss and the regularization as a metric to trade-off the updating, which is not novel. Moreover, the gradient norm occurs in the loss, which means that it needs to further compute hessian matrix for backpropagation. But in practice, it is really hard to compute Hessian matrix, and even be inhibitively expensive to compute for large networks.  So this method cannot be used in the modern networks. \n\n\n2)In the experimental section, the authors only compare few baselines which cannot actually reflect the superiority of the proposed method. Moreover,  the experiments also lack of large experiments, such as some experiments on ImageNet which is a standard dataset to test the performance nowadays.  Finally, as discussed above, this method needs to compute Hessian which is very expensive. So it is better to compare the training time of all baselines. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "For writing, most parts of this work are well written and clear.  \n\nFor novelty, the novelty of this work is not high since its only contribution is to use the ratio of gradient norms of the loss and the regularization as a metric to trade-off the updating. Moreover, this method needs to compute Hessian which is very expensive and limits the method to use in large networks. \n",
            "summary_of_the_review": "Overall, this work provides some good empirical results. But it fails to provide good new insights and practical techniques.    ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3493/Reviewer_d8cT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3493/Reviewer_d8cT"
        ]
    },
    {
        "id": "idcX0Kxmkg",
        "original": null,
        "number": 3,
        "cdate": 1666675979145,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675979145,
        "tmdate": 1666675979145,
        "tddate": null,
        "forum": "ScEfNWshH3B",
        "replyto": "ScEfNWshH3B",
        "invitation": "ICLR.cc/2023/Conference/Paper3493/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a dynamic scheduling strategy for the value of the weight decay hyperparameter. During the training procedure, the authors keep the ratio of the value of weight decay value to the gradient of cross-entropy loss (dubbed as DoG) at a constant. Empirically, this adaptive weight decay strategy can help improve the adversarial robustness and gain a smaller drop in performance in the noisy-label setting. ",
            "strength_and_weaknesses": "Strength\n+ The motivation for dynamically finetuning weight decay is clear. By showing the results by grid search, it is difficult and also important for us to find an optimal weight decay during training.\n+ The empirical results support the claim that adaptive weight decay can improve robustness.\n\nWeaknesses\n- I am a bit confused about the functionality of the value of DoG at the epoch when the loss is at the plateau in Section 2. The DoG value is found based on a specific value of the weight decay hyperparameter. Why does keeping the value of DoG constantly in every optimization help training if the DoG is the significant underlying difference between the performance of different cells in the same diagonal? Also, I am confused about the meaning of \u201csignificant underlying difference\u201d and the reason for that \u201csignificant underlying difference\u201d can be evaluated by DoG.\n- Although the authors provide some intuitive reasons for why keeping DoG constantly can help training, it still lacks some theoretical analyses to make the claim solid enough.\n- I am concerned that whether the results of AutoAttack in Table 3 are obtained at the last epoch. It would be better to provide and compare the best AutoAttack accuracy that is selected based on a validation set between the proposed method and the baseline. \n\nMinor comments\n+ The label of the blue line in Figure 2(b) should be \u201cLog(loss)\u201d? \n+ It seems that the loss plateaus is at Epoch 400 in Figure 2(b)? The loss value at Epoch 350 is higher than that at Epoch 400. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well organized. However, some claims or sentences are confusing (I pointed out some in the \u201cWeaknesses\u201d section).  The empirical contribution may be significant according to the adversarial robustness gain. The authors provide the training details and pseudocode of adaptive weight decay, which provides good reproducibility.",
            "summary_of_the_review": "This paper proposes to adaptively finetune the value of weight decay hyperparameter according to DoG. Although the empirical results could be significant, I have several concerns that are stated in the \u201cWeaknesses\u201d section. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3493/Reviewer_N7iq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3493/Reviewer_N7iq"
        ]
    },
    {
        "id": "RKITx0Dul8Z",
        "original": null,
        "number": 4,
        "cdate": 1666839660786,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666839660786,
        "tmdate": 1666843103645,
        "tddate": null,
        "forum": "ScEfNWshH3B",
        "replyto": "ScEfNWshH3B",
        "invitation": "ICLR.cc/2023/Conference/Paper3493/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a simple technique to adaptively choose the weight decay parameter over the course of training, instead of having a fixed $\\lambda$ which is currently the common practice, and call it adaptive weight decay (AWD). The motivation for AWD comes from the need to avoid a full expensive grid search of learning rates and weight decays. At the same time, the authors show that AWD helps with \"adversarial training\" and the downstream accuracy metrics. \n\nThe experimentation is on CIFAR-10 and 100 alongside baselines corresponding to adversarial training. \n\nNote that I am not well versed in this sub-field of machine learning and I am genuinely confused about a lot of things in the paper with the exposition and experimentation. I will be providing the review from a general ML and scientific rigor perspective and would defer to other reviewers for expert opinions. Please excuse the brevity of the review owing to these reasons. ",
            "strength_and_weaknesses": "Strengths:\n1) The problem that is solved by AWD is real and is well demonstrated by the initial experimentation of CIFAR with 2D grid search. \n2) The idea of AWD is extremely clean and empirically informed. \n3) The initial experimentation to show the value of AWD is clean. \n4) The explanation of potential use cases of AWD along with adversarial robustness is very helpful for a new reader. \n5) The gains on \"robust accuracy\" with adversarial training compared to vanilla WD and other baselines are empirically strong. \n6) Rest of the analysis is a good step toward verifying the use of AWD\n\nWeaknesses:\n1) The paper at times is hard to parse. \n2) While AWD is empirically informed, the initial choice of 0.016 opens more avenues of grid search as shown in the rest of the experiments. \n3) The authors have considered a very specific setting where the learning rate doesn't decay. I am not sure if the learning rate decay is a huge factor as it often is for ImageNet scale training.\n4) While \"adversarial training\" is an interesting thing to try out in the context of a white box attack, it often is not the robustness we care about. We care about OOD robustness akin to \"do cifar-10 classifiers generalize to cifar-10?\" and \"do imagenet classifiers generalize to imagenet?\". I think this would be one of my biggest questions if AWD makes things robust in general or is it due to the fact that AWD has a notion of running gradient that approximates to hessian? Because the use of hessian approximations for optimizations could assist in adversarial robustness when trained for PGD attacks. I would like the author's thoughts on this. \n5) The figures and tables of the paper are extremely hard to understand and are often not accessible for ease of reading. It would be great to fix that for a better dissemination of results and motivation. \n6) Lastly, probably my biggest qualm is the lack of results at scale. While CIFAR-10/100 are great datasets to prototype on, it is often rare to see full generalization to ImageNet -- I would like to see at least one comparison at the ImageNet scale -- even if not for the adversiral robustness. \n\nWith all these factors in mind and considering my lack of expertise -- I am happy to chat with the authors during rebuttal and with revisions.  And consider increasing the score after the overall process. While the idea is simple, that does not take away novelty from it. However, the ad-hoc choice of factors like DoG does not sit well without some explanation or understanding. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Well reproducible. See above section for rest. ",
            "summary_of_the_review": "Simple and empirically driven idea to adaptively choose weight decay during the course of training. However, has issues with writing, presentation and some experimentation to be ready for being published.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3493/Reviewer_ksvx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3493/Reviewer_ksvx"
        ]
    }
]