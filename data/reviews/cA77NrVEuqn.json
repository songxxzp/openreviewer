[
    {
        "id": "oTeUTfmBfV",
        "original": null,
        "number": 1,
        "cdate": 1666556761700,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556761700,
        "tmdate": 1666556761700,
        "tddate": null,
        "forum": "cA77NrVEuqn",
        "replyto": "cA77NrVEuqn",
        "invitation": "ICLR.cc/2023/Conference/Paper4344/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper describes a model-based reinforcement learning method designed to learn from offline datasets (generated by an external behavior policy). The method attempts to address two key challenges in model-based RL. First, for common continuous control problems such as locomotion and manipulation, the action space is high-dimensional and fine-grained, resulting in an expensive search problem. Second, learned environment models are often riddled with \u201choles\u201d (regions with poor dynamics & cost estimates) due to uneven data coverage. Uniform trajectory sampling strategies may easily fall into these holes and yield poor performance. The proposed method addresses the first problem by learning a latent action space, where each discrete latent action maps to a short snippet of trajectory. The computational cost of searching in such action space can be adjusted using the trajectory snippet length. The latent action space is discrete and finite and is learned end-to-end with a VQ-VAE (VAE with discrete priors) trajectory reconstruction objective. Such data-informed action space also partially addresses the second problem (holes), as the search is naturally confined to trajectory patterns that exist in the training set. To further tighten the search space, the method proposes to learn the likelihood of the next latent action conditioning on the first state and the latent actions in the plan so far. The method also employs beam search to further improve the planning efficiency. The method is evaluated on a simulated offline RL benchmark (D4RL), which includes continuous control problems such as half-cheetah, and tasks with higher dimensional action space (dexterous manipulation). The result shows that the proposed method achieves comparable performances relative to a number of model-based and model-free RL baselines for tasks with low-dimension actions, and compares favorably in tasks with higher-dimensional action space. ",
            "strength_and_weaknesses": "Overall I quite like the overall premise of the paper: if learning and planning with an accurate and comprehensive model for a control problem is hopelessly complex and expensive, why not focus on the part of the problem that is relevant to the task? I think learning a latent action space and building a constrained dynamics model on top is a nice way to approach this problem. And the proposed method is conceptually simple and easy to dissect. I also like how comprehensive the ablation study is. \n\nHowever,  I believe there are important issues with this paper that need to be addressed. The issues are (1) missing references that are highly relevant and (2) conceptual and empirical differences between learning models and learning behaviors.\n\n1. Missing references\nThere are a number of important references that are missing. First, there is an entire set of work in hierarchical imitation learning [1][2] that tries to learn temporally-abstracted behavior primitives and use them for planning / more efficient imitation. For example, Mandlekar et al., [1] uses VAE to learn a latent space of short-horizon goal-conditional policies and use that for value-based planning. Similarly, [2] learns a latent space of short behaviors and uses them for one-shot imitation. From a practical point of view, these behavior primitives and how they are used are no different than the latent action proposed in this work. In the context of more RL-ish works, Allshire and Martin-Martin et al. [3] is highly relevant. Similar to the premise and motivation of this paper, they learn a latent action space based on offline experience and use it for more efficient RL. There is also [4], which has similar line of argument. Although they do not study temporal abstraction, it is worth discussing and contextualizing the proposed method with this work. Finally, in a slightly far away domain, a number of works [5][6] proposed to learn temporally-abstract action space for better teleoperation. I hope the authors can select a subset of these works and their predecessors / followup works and provide a more thorough account of the idea of learning task-relevant action space, instead of just focusing on offline & model-based RL. In addition, some of these works may be directly comparable and may serve as strong empirical baselines, e.g., [1]. \n\n[1] IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data, Mandlekar et al., 2019\n[2] Learning latent plans from play data, Lynch et al., 2019\n[3] LASER: Learning a Latent Action Space for Efficient Reinforcement Learning, Allshire and Martin-Martin et al, 2021\n[4] PLAS: Latent Action Space for Offline Reinforcement Learning, Zhou et al., 2020\n[5] Learning latent actions to control assistive robots, Losey et al., 2022\n[6] Learning visually guided latent actions for assistive teleoperation, Karamcheti et al., 2021\n\n2. Learning models vs. learning behaviors.\nAs mentioned in my synopsis of this work, I believe the proposed method is more or less limited to learning from offline datasets that is generated by an external behavior policy, since it assumes there exists such a task-relevant state-action space to learn the latent action. So how does one differentiate the proposed \u201cmodel-based offline RL\u201d methods from conditional imitation learning? Based on mainstream understanding, the main advantage of model-based approach is that the model can be shared among many different tasks, as the per-step dynamics are agnostic to a task goal. And one may use the same model to synthesize behaviors that can achieve new goals in a zero-shot manner. It\u2019d be interesting to see how much of the \u201cmodel-based\u201d quality the proposed method still retains. Concretely, I\u2019d like to see experiments that focus on generalization to new goals and behaviors.\n\nMy final critique that is not as important as (1) and (2) is the lack of more complex evaluation task. Since one of the main argument that the paper makes is computational efficiency, I'd really wish to see some real-robot experiment to substantiate this claim. Along the same line, it'd be great if the authors could at least evaluate on simulated benchmark such as robomimic (Mandlekar et al., 2021) for which the tasks are directly modeled after setup that can be replicated in the physical world. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: very clear. great writing! \n\nQuality: solid execution of an intuitive idea, although the evaluation task complexity can be improved. \n\nOriginality: somewhat questionable. See the weakness comments on missing references.",
            "summary_of_the_review": "Again, I like the overall premise and the high-level idea of the work. And I think the method is conceptually simple and would be easy to work with. I'd like to see an in-depth discussion of missing references listed above and also addressing the gap between learning environment models and learning behaviors. More complex evaluation tasks can also substantially improve the overall quality of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4344/Reviewer_Ykqw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4344/Reviewer_Ykqw"
        ]
    },
    {
        "id": "LCQ9j-PFA9",
        "original": null,
        "number": 2,
        "cdate": 1666669989936,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669989936,
        "tmdate": 1666669989936,
        "tddate": null,
        "forum": "cA77NrVEuqn",
        "replyto": "cA77NrVEuqn",
        "invitation": "ICLR.cc/2023/Conference/Paper4344/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "To alleviate the computational overhead of the planning-based RL algorithms, this paper proposes an Offline RL method Trajectory Autoencoding Planner (TAP), which learns and plans in the compact latent action space. \nThe key points are: \n  * (1) TAP achieves both *spatial abstraction* and *temporal abstraction* by applying a Transformer based VQ-VAE to encode each transition tuples $(s_t, a_t, r_t, R_t)$ in a trajectory $\\tau$ into a latent space and then merging every $L$ adjacent encoded transtions into one latent action in the discrete codebook of VQ-VAE.\n  * (2)  TAP uses a Transformer to autoregressively model the prior distribution of the latent action codes. \n  * (3) The planning is executed in the discrete latent action space by applying the trained prior latent action sampler and beam search. As multiple environmental actions are aggregated into one latent action, the planning horizon is reduced to $1/L$.\n",
            "strength_and_weaknesses": "**Strength:**\n  *  Using Transformer based VQ-VAE to achieve both *spatial abstraction* and *temporal abstraction* can significantly reduced the search space of the planning based RL method.\n  * Compared with Trajectory Transformer, considering each transition tuple $(s_t, a_t, r_t, R_t)$ as a single token for the Transformer is much more reasonable, which could significantly reduce the computational cost.\n  * The paper is written clearly and easy to follow.\n  \n\n**Weaknesses:**\n  * While using compact action representation could reduce the search space, the model also losses the ability of doing more fine-grained control and can potentially lead to sub-optimal action selection. How to balance the two?\n  * How to apply TAP to the online RL setting where we do not have lots of and diverse existing data to train the action encoder?\n * As the authors also mentioned in the limitation part, TAP currently cannot not distinguish between uncertainty stemming from a stochastic behaviour policy and uncertainty from the environment dynamics. \n  * It's better to provide the overall training loss function as well.\n  * [1] may be a missing related work.\n  * Dreamer V2 [2] also use discrete latent representations. It's better to discuss connection between the two.\n  * Minors: \n    * \"After adding the positional embedding, the encoder then reconstructs the trajectory\",  the 'encoder'  should be 'decoder'.\n    * \"As such, the planning process only needs to optimize in a 5 or 8-dimensional discrete space with a learned prior\", the 'discrete space' should be 'planning horizon'.\n\n\n\n**Reference**\n\n[1] Continuous Control with Action Quantization from Demonstrations\n\n[2] Mastering Atari with Discrete World Models",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is written clearly and easy to follow.\n* The idea of learning and planning in the temporal abstracted latent action space is novel. Applying the overall method to Offline RL setting is reasonable.\n* The code is also attached in the supplementary material. So, I guess the experimental results can be reproduced properly.",
            "summary_of_the_review": "Overall, this paper proposes an interesting idea of learning and planning in the temporal abstracted latent action space, which significantly reduces the computational cost compared to its counterpart Trajectory Transformer. I recommend an accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4344/Reviewer_Brj7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4344/Reviewer_Brj7"
        ]
    },
    {
        "id": "OC8Jrr5WTQd",
        "original": null,
        "number": 3,
        "cdate": 1666700143004,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666700143004,
        "tmdate": 1666702762578,
        "tddate": null,
        "forum": "cA77NrVEuqn",
        "replyto": "cA77NrVEuqn",
        "invitation": "ICLR.cc/2023/Conference/Paper4344/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors extend the Trajectory Transformer (TT) by learning a latent discrete action-space consisting of trajectory segments, which is used to speed up (beam-search) planning for offline RL. The authors argue that planning in this latent space scales better with dimensionality of the action space. Additionally, the proposed architecture is computationally cheaper than the TT.",
            "strength_and_weaknesses": "Strengths:\n- The idea of learning a latent discrete action space is very interesting and highly relevant\n- The approach appears sound and novel\n- The paper is well-written with a thorough ablation study\n\nWeaknesses:\n- The experiment tasks, while common in this type of paper, are rather simple. The locomotion problems are not complex enough to show improvement, and for the robotic hand experiments, the optmimal length L of the latent trajectory segments seems rather small (3). This makes it a bit unclear how much the learned latent representation helps with planning compared to just 1) reducing the time discretization of the TT by a factor of 1/L (i.e. executing the same action for L=3 steps in a row), and 2) manually discretizing the action space (e.g. clustering it using k-means), although for a fair comparison one would need to sample from a learned conditional sequence prior (or ignore it for both) 3) both of these in combination. I do not think this was covered by the existing baselines? Would 1) be feasible to add?\n- It would have been interesting with some introspection and qualitative examples of the learned latent action representation (trajectory segments). This approach should be more suitable for tasks that have a natural discrete structure. It would have been interesting to see how well it could recover that.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly well written, novel and appears reproducible.\n\nSome things that could be improved:\n- Framing: The intro implies planning-based RL has only shown strong performance on low-dim problems, but as e.g. Go, Chess are quite high-dimensional, this claim seems questionable. I would consider rephrasing that, I think the paper can make its point about improved efficiency in high-dim planning without it. \n\n- How much data was actually used to train the offline model in the experiments? Has to be quite a bit?\n\n- The paper is simultaneously also talking about RL success and compute, but unless the model is actually better (and not just more compact) RL success for earlier approaches should also depend on the compute spent on sampling the (less compact) space? I feel these aspects could have been better examined separately, but I realize space is at a premium.  \n\nSome minor language issues:\n- \"future trajectories estimated by a dynamics model\" -> sampled from?\n- The second paragraph of the intro is a bit wordy, and many arguments seem to overlap. I think it could be condensed a bit.\n- I didn't see the VQ-VAE paper cited. The explanation of VQ-VAE was also terse to the point of being mysterious.\n- Typo on p.4: \"probabiilty\"\n",
            "summary_of_the_review": "This seems like a good paper, presenting novel and relevant ideas for learning a discrete action space in offline RL. I think their analysis of the learned representations (and in particular how much model accuracy vs. compactness contributes the improved success rate) could have gone a bit further, but it does appear sound, results in improved or similar results compared to earlier work and has a large ablation study already.\n\nEDIT: I just noticed that some seemingly relevant but unlisted related work surfaced in the other reviews. I will adjust my score based on the outcome of that discussion.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4344/Reviewer_rxeb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4344/Reviewer_rxeb"
        ]
    }
]