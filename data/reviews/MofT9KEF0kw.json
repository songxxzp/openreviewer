[
    {
        "id": "xpGGqlHHxTS",
        "original": null,
        "number": 1,
        "cdate": 1666425443080,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666425443080,
        "tmdate": 1666425443080,
        "tddate": null,
        "forum": "MofT9KEF0kw",
        "replyto": "MofT9KEF0kw",
        "invitation": "ICLR.cc/2023/Conference/Paper3775/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a method called introspective self-play that uses multitask techniques (add an auxiliary task that predicts bias for each data point) to make the resulting model more accurate and fair. ",
            "strength_and_weaknesses": "Strengths:\n- Interesting idea of adding another task to predict if a sample is underrepresented to tackle the accuracy-fairness tradeoff problem\n- Empirical evaluation included state-of-the-art baselines (e.g. RWT, JTT) and the method outperformed them.\n\nWeaknesses:\n- Results should be accompanied by standard deviations so that we can judge if the results are significantly different\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think this was a clearly-written, well-motivated, nicely-executed paper. The selected baselines for comparison against were reasonable, and the theoretical results were convincing to me. \n\nSome questions for the authors:\n- It would still be interesting to see results on DRO given that DRO is still a very popular method.\n- What happens when batch sizes are very small, say the batch size is 24 and the underrepresented group is only 1% of the data, so the batch only has 2 or 3 samples?\n- Have the authors tried out the training objective with other losses besides CE? Apologies if I missed this in the paper. ",
            "summary_of_the_review": "I found the paper very interesting and well-executed. I think it could be improved even further with some tweaks, like adding confidence intervals, and some investigation of what happens when batch sizes are very, very small.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3775/Reviewer_dJid"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3775/Reviewer_dJid"
        ]
    },
    {
        "id": "wAQOwKty6x",
        "original": null,
        "number": 2,
        "cdate": 1666538151202,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666538151202,
        "tmdate": 1666538151202,
        "tddate": null,
        "forum": "MofT9KEF0kw",
        "replyto": "MofT9KEF0kw",
        "invitation": "ICLR.cc/2023/Conference/Paper3775/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "To Improve the accuracy-fairness frontier of DNN models, This paper presents a technique called Introspective Self-play, which estimates\nDNN uncertainty by adding an auxiliary introspection task requiring DNN to predict the bias for each data point in addition to the label. ",
            "strength_and_weaknesses": "Strength\n\n+The research of fairness is critical to the field of deep learning.\n\n+ Experiment results show reasonable improvement.\n\nWeaknesses\n\n- On page 2, \"Our key observation is that given a sampling model with well-calibrated uncertainty (i.e., the model uncertainty is\nwell-correlated with generalization error), active learning (AL) can preferentially acquire tail-group examples from unlabelled data without needing group annotations, and add them to the training data to reach a more balanced data distribution\" needs to be discussed with more details, especially its relationship to fairness.\n\n- Please include a color legend in Figure 2, 6, 7\n\n- In Table 2, if combined accuracy is defined as (accuracy + worst-group accuracy)/2, why is worst-group accuracy higher than combined accuracy and accuracy (?)?\n\n-In Table 2, ISP-Gap achieves almost identical Tail Sampling Rate as JTT, why does it perform better? More discussion and details are needed.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThis paper is well organized.\n\nQuality\n\nThe paper is technically solid.\n\nNovelty\n\nThe novelty of this paper is reasonable.\n\nReproducibility\n\nGood.",
            "summary_of_the_review": "This paper is technically solid, and its novelty is reasonable.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3775/Reviewer_zGVn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3775/Reviewer_zGVn"
        ]
    },
    {
        "id": "73K6JQprez",
        "original": null,
        "number": 3,
        "cdate": 1666726903944,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666726903944,
        "tmdate": 1669128538231,
        "tddate": null,
        "forum": "MofT9KEF0kw",
        "replyto": "MofT9KEF0kw",
        "invitation": "ICLR.cc/2023/Conference/Paper3775/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studied the problem of improving accuracy (performance of average subgroups) and fairness (performance in worst case subgroups). Then this paper further proposed an active learning strategy by querying the worst-subgroup to improve the accuracy-fairness boundary. Besides, this paper assumes that  subgroup information is hidden and designs a novel ensemble based approach to detect/estimate the subgroup information. The corresponding theoretical understanding is further derived. Finally empirical results verify the proposed framework. \n\n-----------------\n### Post-Rebuttal\n\n**I would appreciate author responses. My concerns about related work, fair notions, and assumptions have been addressed.\nIn light of these results, I support acceptance.**\n\nP.s Both the overall and correctness scores have been updated.",
            "strength_and_weaknesses": "### Pros\n\n- In general, this paper proposed a quite meaningful solution in addressing the worst group performance: active learning by querying the additional minority group samples. Moreover, the scenario is much more difficult than the conventional setting since we did not observe the subgroup information.  Overall I would think the problem is well-motivated and quite meaningful in real-world practice. \n- The paper in general is well-written and clearly illustrated. The related work part is quite comprehensive and detailed. (I really like them!) \n- The theoretical analysis is done and improved results in both toy and real-world language dataset.\n\n### Cons:\n- [Related work] Despite the detailed discussions on related works, I would feel some parts are seemingly incorrect and not clear. \n- [About the title/main contribution] I would think the fair-accuracy is NOT a proper name in the fairness literature. The fairness is defined as worst-case group performance, which is generally different from the conventional fair notations such as demographic parity, equalized odd or sufficiency. That means this paper did not address the inherent trade-off in traditional fairness accuracy trade-off. \n- [About subgroup detection] Despite the theoretical analysis being done, there are still many theoretical/fundamental concerns related to the identifiability of subgroup estimation.\n- [About the practical scalability] I have doubts on the scalability of the proposed approach, because the bootstrapping approach would be quite long in a large scale dataset.\n\n### Detailed Comments on cons\n\n1. [Related work] Despite comprehensive analysis, I would still have several remarks on the related work. Indeed this part is for the purpose of discussion.\n\n(1) In appendix D.1-D.3 learning representation under data bias. I would say the discussion about IRM and EIIL are seemingly inaccurate. In fact, IRM ensures a representation such that for all subgroups P(Y|Z,g) being invariant.  IRM v1 is further proposed to approximate the original objective through gradient penalty. Further EIIL adopted IRM_v1 to detect the unobserved subgroups. \n\nBack to the assumptions within the paper (footnote in page 2), the data generation across different groups should be the same, which seems a bit strong assumption for me. Indeed, in the feature level (e.g, pixels in images), the environment distribution P(Y|X,g) could be completely different (e.g, classifying cat in different backgrounds). However, a more reasonable solution is by assume the existence of a representation function $f(x)=z$ such that P(Y|z,g) are the same. Intuitively, this assumes the semantic(or high-level) information such as cat/dog being invariant.  This is also related to fairness w.r.t sufficiency (see IRM[1], EIIL[2] and recent paper [3] for detailed discussions). Therefore I would think the data generation assumption could be better refined.  \n\n(2) About learning fair representation D3. I should say related work here requires better refinement. Indeed, learning fair representation generally is based on DP/EO/sufficiency (such as EIIL [2] and Paper[1,3,4]). I would say the current discussion does not make sufficient sense in fair learning. Since fair representation learning depends on the predefined fair notions, it could not be simplified as adversarial learning, regularization, etc..\n\n(3) About related work in active learning under distribution shift in D4. Well, as far as I know, in active learning literature, the concept of distribution has been well developed recently such as paper [5,6,7]. They essentially model active learning between labelled and unlabeled dataset as a distribution shift problem.\n\n(4) Related work. D5 Deep uncertainty methods in active learning. I would think the proposed approach are essentially improve the **diversity** rather than uncertainty in the paper (see paper [5-7] for details). Essentially, the reason for the unfair predictions is due to the limited samples within some subgroup. Thus diversity aims to search these samples by making the empirical distribution similar to the ground truth distribution. I would think this is well-aligned with your motivations.   \n\n2. [About the title/main contribution] I would think the fair-accuracy is NOT a proper name in the fairness literature. In this paper, the fairness is defined as worst-case group performance, which is generally different from conventional fair notions such as **demographic parity, equalized odd or sufficiency**. That means this paper did not address the inherent trade-off in traditional fairness accuracy trade-off. \n\nIn contrast, this paper essentially studied the prediction variance of learning from multiple subgroups. If the accuracy/loss variance of all subgroups are zero, the fairness (within your paper) is achieved. However, this notion is completely **DIFFERENT** from EO/DP/sufficiency, where they are directly defined on the predictor behaviors and could induce an inherent trade-off. From this viewpoint, I do think the title is over-claiming and could be potentially misleading for the traditional fairness community. It should clearly reformulate the real contribution by considering the worst-case group performance (such as the related paper in Distribution Robust optimization..) \n\n3. [About subgroup detection] Despite the theoretical analysis being done, there are still many theoretical/fundamental concerns related to the identifiability of subgroup estimation. From the observational data, when could we identify the subgroup? Please note this is the key point within your paper. If we could not identify the correct subgroup, the whole learning procedure is incorrect. From the paper, the assumption seems like \n$P(Y|X,g)$ being invariant as the underlying assumption. While if the data generation distribution is identical, there is no subgroup, right? I agree the empirical counterpart could be slightly different but this is quite difficult to detect the correct version, right? From this perspective, I do think there are sort of issues in the identifiability. \n\nIf we consider a mild assumption such as paper [3] by assuming data distribution $P(y|X,g)$ being different and representation distribution $P(y|Z,g)$ being invariant. This could be a better assumption and it seems that we could identify the subgroup information. \n\nOverall, I do think the paper requires clear theoretical assumptions and illustrates why the cross-validation based approach could identify the subgroup (I am not sure the proposed approach is also ad-hoc or not)\n\n\n4. [About the practical scalability] I have doubts on the scalability of the proposed approach, because the bootstrapping approach would be quite long in a large scale dataset such as a language dataset. What is the time complexity of that? It seems that we need multiple fine-tunings? \n\nRef\n[1] Environment Inference for Invariant Learning. Icml 2021\n\n[2] Invariant risk minimization, 2019\n\n[3] Fair Representation Learning through Implicit Path Alignment. ICML 2022\n\n[4] Out of Distribution Generalization in Machine Learning. Martin Arjovsky. 2020\n\n[5] Deep active learning: Unified and principled method for query and training. Aistat 2020\n\n[6] Discrepancy-Based Active Learning for Domain Adaptation. ICLR 2022\n\n[7] Low-Budget Active Learning via Wasserstein Distance: An Integer Programming Approach. ICLR 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general this paper is clearly written and easy to follow. The reproducibility seems high. \n",
            "summary_of_the_review": "This paper studied the problem of improving accuracy (performance of average subgroups) and fairness (performance in worst case subgroups). \n\nIn general, I would think this paper is solid in the presentation and related work.  There are still several important concerns within the paper (see cons in the review), I would recommend a borderline paper. \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3775/Reviewer_1jcC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3775/Reviewer_1jcC"
        ]
    }
]