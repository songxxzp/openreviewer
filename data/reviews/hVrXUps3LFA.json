[
    {
        "id": "CXNJteTfjr",
        "original": null,
        "number": 1,
        "cdate": 1666372958210,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666372958210,
        "tmdate": 1668764697963,
        "tddate": null,
        "forum": "hVrXUps3LFA",
        "replyto": "hVrXUps3LFA",
        "invitation": "ICLR.cc/2023/Conference/Paper912/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work address the Domain Adaptation of Black-box Predictors (DABP) by using a \"dive to adapt\" strategy.\n\nThe contribution are:\n- a new divide to adapt strategy\n- Theoritical analysis that demonstrates the boundary on the error on the target domain\n- experiments to compare with state of the art benchmarks.",
            "strength_and_weaknesses": "Strength:\n- the method is well introduced\n- experiments are sounding\n\nWeaknesses:\n- in the ablation studies more experiment could have been done on the splitting hyper parameter \\tau especially on more than one dataset.\nthis experiment could also look  qualitatively where the splitting occurs in the dataset.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written but table contents are quite small hopefully I'm reading it from a PDF that I can zoom....",
            "summary_of_the_review": "This work proposed a simple yet effective framework for DABP.\n\nFew analysis (quantitative and qualitative) are done on the influence of the \\tau parameter of the splitting step.\nIt appears not to be influent on one dataset but that is the easiest dataset...",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper912/Reviewer_jpbu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper912/Reviewer_jpbu"
        ]
    },
    {
        "id": "LghdpguJTXj",
        "original": null,
        "number": 2,
        "cdate": 1666677569851,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677569851,
        "tmdate": 1668742876561,
        "tddate": null,
        "forum": "hVrXUps3LFA",
        "replyto": "hVrXUps3LFA",
        "invitation": "ICLR.cc/2023/Conference/Paper912/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Based on a simple observation that intra-domain target samples have different levels of domain discrepancy, this paper proposed a new approach, divide-to-adapt, to address the problem of black-box domain adaptation. The main contributions are two parts: dynamically mitigating the confirmation bias for black-box domain adaptation and providing some theoretical justifications. Experiments on image classification benchmarks demonstrate the effectiveness of the proposed approach.",
            "strength_and_weaknesses": "Strength:\n\n1. The paper is nicely written and the methodology is well described.\n\n2. Theoretical validation and thorough experiments indicate that the proposed strategy, divide-to-adapt, is a good solution for black-box domain adaptation.\n\n3. It is also impressive that the proposed method outperforms the sota approach by 7.0% on VisDA-17 and even surpasses standard domain adaptation using source data.\n\nWeaknesses:\n\n1. The idea of considering domain adaptation as the semi-supervised setting has been studied in AdaMatch [a] and dividing the target domain into easy and hard subdomains for adaptation has been explored in IntraDA [b].  However, the above relevant works have not been included in this manuscript. The authors should provide more discussion.\n\n2. The technical contribution feels a bit scattered. As the framework can be decomposed into a set of smaller contributions, each improving the performance by a smaller amount. Though authors have provided an ablation study to analyze the individual contribution of each component. However, I find it is quite hard to line the contributions to the motivations.\n\n3. It seems that the proposed method performs better with fewer categories (7.0% gain on VisDA-17 with 12 categories while 1.8% gain on Office-31 with 31 categories and 2.0% gain on DomainNet with 365 categories). Also, I'd rather like to see the ablation studies of each component on VisDA-17 in Table 5.\n\n4. Currently the target application seems to be a bit narrow, as it is only image classification on three benchmarks. I think it would be very interesting to see the applications to other settings e.g., semi-supervised learning, and semi-supervised domain adaptation like AdaMatch [a].\n\n5. As the proposed domain division relies on threshold \\tau, it would be better to provide the mean and std of the numerical results under different random seed for each experiment.\n\nRefs:\n\n[a] David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, Alexey Kurakin: AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation. ICLR 2022.\n\n[b] Fei Pan, Inkyu Shin, Fran\u00e7ois Rameau, Seokju Lee, In So Kweon: Unsupervised Intra-Domain Adaptation for Semantic Segmentation Through Self-Supervision. CVPR 2020: 3763-3772.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is written nicely and is easy to follow.\n\nQuality & Novelty: As discussed above, a bit of novelty is contained in the proposed method. And I would suggest comparing more relevant works to understand the contribution of this paper.\n\nReproducibility lacks sometimes.",
            "summary_of_the_review": "To sum up, I find this paper tackles a practical problem for domain adaptation without source data and model parameters. Yet, it still requires clarification and some solid empirical support before warranting acceptance of this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper912/Reviewer_uuuU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper912/Reviewer_uuuU"
        ]
    },
    {
        "id": "-HgoFyrijgM",
        "original": null,
        "number": 3,
        "cdate": 1666947659276,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666947659276,
        "tmdate": 1666947659276,
        "tddate": null,
        "forum": "hVrXUps3LFA",
        "replyto": "hVrXUps3LFA",
        "invitation": "ICLR.cc/2023/Conference/Paper912/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors propose a method to increase the accuracy of classification in the task Domain Adaptation of Black-box Predictors. It aims to suppress the confirmation bias efficiently, and extends this work with a learning framework called BETA. Specifically, inspired by the phenomena that deep models tend to learn easy-to-adapt samples faster than hard-to-adapt samples, the authors divide the target domain into an easy-to-adapt subdomain and a hard-to-adapt subdomain. With treating the former as a labeled set and the latter as an unlabeled set, the authors solve DABP via semi-supervised learning methods. Additionally, the authors propose mutually-distilled twin networks with weak-strong augmentation method to mitigate error accumulation.",
            "strength_and_weaknesses": "Strength:\nThis paper is overall easy to read. The authors describe the task clearly and the work is well organized. The motivation behind this work is clearly presented, i.e., to alleviate the confirmation bias. It is novel to solve DABP problem with semi-supervised learning method. The authors present a comprehensive comparison with previous works. According to the provided result of experiment, the proposed framework outperforms the competing algorithms.\n\nWeakness:\n- I am confused that whenever we get a batch of new test data, is it necessary to do extra studies on threshold value to find a suitable value according to different data distributions? \n- More discussion to DivideMix should be added. This work shares similar spirits to dividemix and the authors should clarify it.",
            "clarity,_quality,_novelty_and_reproducibility": "This work is overall interesting and presents a promising solution.",
            "summary_of_the_review": "I hope the authors can address my concerns in Section II.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper912/Reviewer_zK5K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper912/Reviewer_zK5K"
        ]
    },
    {
        "id": "bsyS5wLMTZD",
        "original": null,
        "number": 4,
        "cdate": 1667284079100,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667284079100,
        "tmdate": 1668746921936,
        "tddate": null,
        "forum": "hVrXUps3LFA",
        "replyto": "hVrXUps3LFA",
        "invitation": "ICLR.cc/2023/Conference/Paper912/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the domain adaptation when the target domain is unlabeled. A black-box predictor trained on the source domain is employed to assign noisy/pseudo labels to the unlabeled target domain data. In this way, the problem is transformed to learning with noisy labels. The noisy label problem is further solved by adapting semi-supervised learning techniques.",
            "strength_and_weaknesses": "**Strength**\n1. The paper is well motivated and the the considered problem is meaningful. \n2. The combination of ideas from LNL and semi-supervised learning are organic, which handle the confirmation bias well.\n3. The derived bound helps explain the effectiveness of the proposed method.\n\n\n**Weakness**\n1. The most concerned point is that the proposed framework is very similar to DivideMix [36]. It looks like implementing DivideMix on another topic. The authors need to highlight the novelty compared with DivideMix.\n2. Theorem 1 needs to be further explained. It would improve the paper if the conformation bias is proved to be lower than the naive method with the result from Theorem 1.\n3. Intuitively, the conformation bias may not always be mitigated by semi-supervised learning (SSL). The benefit of the proposed method may jointly depend on the number of samples in the target domain and the noise rate of hard-domain samples. In algorithms such as MixMatch, it has beed proved that SSL algorithm may hurt some sub-populations if the corresponding pseudo noise rate is high and the unlabeled sample size is small [R1]. It is interesting to see whether there is similar observations in this paper. If so, the conformation bias is not mitigated in this case.\n4. Minor: Does $\\rho_h$ indicate the noise/error rate of the hard-domain samples? \n\n[R1] Zhu, Z., Luo, T. and Liu, Y., 2021. The rich get richer: Disparate impact of semi-supervised learning. ICLR 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity & Quality**\nThe paper is well written and presented clearly.\n\n**Novelty**\nThe novelty compared with DivideMix needs to be highlighted.\n\n**Reproducibility**\nGood.\n\n",
            "summary_of_the_review": "The paper considers an interesting problem and proposes an effective solution. Although it seems to be similar to DivideMix, it successfully implement this idea on a different problem. They also have theoretical results to support their method. The paper would be improved if the authors further discuss the relationship between Theorem 1 and the mitigation of conformation bias.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper912/Reviewer_JUAJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper912/Reviewer_JUAJ"
        ]
    },
    {
        "id": "Y1aNBtmPAZZ",
        "original": null,
        "number": 5,
        "cdate": 1667363827590,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667363827590,
        "tmdate": 1669227967322,
        "tddate": null,
        "forum": "hVrXUps3LFA",
        "replyto": "hVrXUps3LFA",
        "invitation": "ICLR.cc/2023/Conference/Paper912/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work studies domain adaptation of black-box predictors (DABP) where the adaptation procedure on target domain has no access to source model and data. To solve this task, this paper proposes a BETA method which divides target samples into easy-to-adapt instances and hard-to-adapt ones and utilizes augmentation manner to obtain more clean samples for model training. This work achieves better performance than the mentioned baselines.",
            "strength_and_weaknesses": "Strengths:\n\n1. The experimental results on some DABP tasks are better than the mentioned works.\n\nWeaknesses:\n\n1. The current paper lacks some necessary references.\n2. The proposed method is not novel in domain adaptation field.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of the proposed method is limited.",
            "summary_of_the_review": "This work studies domain adaptation of black-box predictors (DABP) where the adaptation procedure on target domain has no access to source model and data. To solve this task, this paper proposes a BETA method which divides target samples into easy-to-adapt instances and hard-to-adapt ones and utilizes augmentation manner to obtain more clean samples for model training. This work achieves better performance than the mentioned baselines.\n\nFor the current version, I have several concerns.\n\n1. The abstract claims that \u201can observation we make for the first time \u2026 target domain usually contains easy-to-adapt and hard-to-adapt samples\u2026\u201d. However, this observation has been well-observed and studied by many domain adaptation works [1, 2, 3, 4]. These works typically use the probability output of model to determine the easy or hard samples. Thus, this concept is not novel, and the sample division strategy used in Eq. (3) and Eq. (4) is basically same with the existing works.\n\n2. The proposed method is not novel for domain adaptation field. Using the linear combination of samples to augment the high-confident instances has been explored by [5]. The mutual information maximization is also used by many domain adaptation works such as SHOT [6]. Thus, the current method seems to be the combination of many existing methods. In addition, [5] and other recent UDA works have achieved higher performance than the mentioned baselines. However, these works are missing in Table 1-3.\n\n3. When dividing target samples into two sub-domains, the error is naturally divided into two parts to form the inequality as Eq. (14). Thus, Theorem 1 does not illustrate that the proposed method can achieve better sub-domain alignment.\n\n[1] Cui, Shuhao, et al. \"Gradually vanishing bridge for adversarial domain adaptation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\n[2] Shu, Yang, et al. \"Transferable curriculum for weakly-supervised domain adaptation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.\n\n[3] Shin, Inkyu, et al. \"Two-phase pseudo label densification for self-training based domain adaptation.\" European conference on computer vision. Springer, Cham, 2020.\n\n[4] Chen, Chaoqi, et al. \"Progressive feature alignment for unsupervised domain adaptation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n\n[5] Na, Jaemin, et al. \"Fixbi: Bridging domain spaces for unsupervised domain adaptation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[6] Liang, Jian, Dapeng Hu, and Jiashi Feng. \"Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation.\" International Conference on Machine Learning. PMLR, 2020.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper912/Reviewer_yM8j"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper912/Reviewer_yM8j"
        ]
    }
]