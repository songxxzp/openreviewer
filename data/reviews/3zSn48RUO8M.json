[
    {
        "id": "nbwcKnmP_d",
        "original": null,
        "number": 1,
        "cdate": 1666458686600,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666458686600,
        "tmdate": 1666458686600,
        "tddate": null,
        "forum": "3zSn48RUO8M",
        "replyto": "3zSn48RUO8M",
        "invitation": "ICLR.cc/2023/Conference/Paper518/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the landscape causes of collapse in self-supervised learning.\n",
            "strength_and_weaknesses": "Strength\n\n1. Existng work have conflict opinion about the collapse in SSL. This work try to fill this gap by studying the geometry of the SSL.\n\n2. This work shows that the interplay between data variation and data augmentation determines the geometry of the loss.\n\n3. They found the geometry of the loss explains when dimensional collapse can be helpful and why certain SSL\nlosses are robust against data imbalance, but not the other.\n\nSuch findings may help design a better loss in SSL.\n\n\nWeakness:\n1. The work may rely on the assumption that the augementation is x = x +e, and all the data and noise must be Gaussian also, the nerual is linear. This assumption may be strong, since they are many non-noise based augementation. It is not clear if other data augumentation still has similar results as in this work.\n\n2. If the data is imbalance as shown in 4.3 How the entire analysis appiled to this case, like if data is not Gaussian?\n\n3. The code is not available, the reproducibility is unclear.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is clear with strong claim about the collapse in SSL.\n\nQuality: The quanlity of this paper is good. \n\nNovelty: neural collapse is not new, but this paper tries to fill the gap about conflict claim about collapse.\n\nReproducibility: It not clear since the code is not available.",
            "summary_of_the_review": "In general, Although the paper has some strong assumption, it still gives some insight about how the collapse happens in SSL. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper518/Reviewer_Q5CS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper518/Reviewer_Q5CS"
        ]
    },
    {
        "id": "OrzxDmPFFyr",
        "original": null,
        "number": 2,
        "cdate": 1666657284426,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657284426,
        "tmdate": 1666657284426,
        "tddate": null,
        "forum": "3zSn48RUO8M",
        "replyto": "3zSn48RUO8M",
        "invitation": "ICLR.cc/2023/Conference/Paper518/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors analyze the problem of collapses in SSL from a landscape perspective. They solved a landscape that can be extended to understand the effect of normalization. Their result suggests that dimensional collapse can be explained in the minimal setting and is something neutral to learning on its own. They showed that when task-irrelevant dimensions are targeted, dimensional collapse can result in dramatically improved performance, whereas an uninformative noise will lead to collapses in the dimensions that are relevant to the task. The authors believe that It is thus important for practitioners to devise targeted data augmentation mechanisms that incorporate the correct domain knowledge. The proposed theory can serve as a theoretical foundation and baseline of any advanced theory of collapses because a correct theory should agree with our results when restricting to the case of a linear model. The authors advocated the thesis that the local geometry of the loss landscape around the origin is an essential component for understanding collapses, and this should invite more future work to understand the landscape around the origin.\n\n",
            "strength_and_weaknesses": "Pros: The authors conduct a solid and comprehensive theoretical analysis of the SSL landscape. For example, in Section 3.1, the authors show that the landscape for a class of situations in self-supervised contrastive learning can be reduced to an effective form in Eq. (3). In Proposition 1, the authors show that the variance term of the loss takes a specific form when the data is Gaussian. Moreover, in Section 4 and Appendix A, the authors illustrate some theoretical and practical implications of their analytic results.\n\nCons: Although focused on analyzing the reasons for the collapse in SSL, this work cannot account for all the SSL collapses but only identifies the results that can be directly attributed to the low-rank structure of the local minima of the landscape. The paper's numeric analysis is limited. The language and organization of the draft have space to be improved. The paper is not easy to understand due to the lack of intuitive explanation.\n\nThe authors are suggested to improve the draft by improving the points mentioned in the Cons to improve the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has moderate clarity, quality, novelty, and poor reproducibility. There is no source code provided or detailed experimental settings available to reproduce the complex analysis in the paper.",
            "summary_of_the_review": "Please refer to the above sections to find the points that can be revised to improve the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper518/Reviewer_m5m1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper518/Reviewer_m5m1"
        ]
    },
    {
        "id": "wEEBNCbTLYT",
        "original": null,
        "number": 3,
        "cdate": 1667043008149,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667043008149,
        "tmdate": 1667043008149,
        "tddate": null,
        "forum": "3zSn48RUO8M",
        "replyto": "3zSn48RUO8M",
        "invitation": "ICLR.cc/2023/Conference/Paper518/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Self-Supervised Learning (SSL) algorithms like BarlowTwins and BYOL (both contrastive, non-contrastive) are known to suffer from $\\textit{dimension collapse}$, where the effective rank of the representations is much smaller than the dimensionality of the representation. Empirical analysis of such algorithms presents conflicting evidence on the role of $\\textit{dimensionality collapse}$ in downstream generalization. The authors give a different perspective, where they study the loss-landscape of SSL with simplified linear models. In particular, this analysis highlights the role of data augmentation & data-variation in inducing dimension collapse in the representations that could be beneficial for generalization by tuning out task-irrelevant dimensions. Building on these insights, the authors perform experiments w/ Resnet-18 on CIFAR datasets to demonstrate qualitative agreement with the theory.",
            "strength_and_weaknesses": "Some strengths of the paper are outlined below:\n+ The article is well-written and presents a coherent argument for disentangling the role of data augmentation in self-supervised learning via the lens of loss-landscape analysis. \n+ The authors provide empirical evidence that suggests some qualitative alignment in the behavior of the SSL learning objectives on realistic models/datasets.\n\nSome weaknesses of the current paper include the following:\n- The analysis focuses almost entirely on linear models, with little discussion on the validity of the nonlinear regime. \n- Though not essential, it would be helpful to have similar qualitative/quantitive experiments for more architectures (in particular transformers) to ablate the role of architecture design from a choice of the loss function.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and critical information relevant to reproducibility is communicated extensively in the experiments section and appendix. Moreover, the authors situate their contributions well in the context of recent progress in understanding the role of geometry in SSL generalization. Further, I believe the loss-landscape analysis of contrastive/non-contrastive SSL is novel and presents a great starting point for designing augmentation strategies that lead to desirable representation characteristics.",
            "summary_of_the_review": "In summary, the authors study the local geometry of the loss landscape with self-supervised learning objectives to understand the dimension-collapse in feature space. Using simplified analytic and linear models, the authors demonstrate that dimension collapse emerges by controlling the strength of data augmentation and data variation at train time and characterize conditions that lead to generalization-friendly dimension collapse. By extending experiments to non-linear ResNet-18 models on CIFAR datasets, the authors show qualitative similarities in loss landscape under different strengths of data augmentation. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper518/Reviewer_ffca"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper518/Reviewer_ffca"
        ]
    }
]