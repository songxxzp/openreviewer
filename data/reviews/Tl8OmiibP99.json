[
    {
        "id": "zoJs2aeeyN",
        "original": null,
        "number": 1,
        "cdate": 1666697418553,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697418553,
        "tmdate": 1670689371125,
        "tddate": null,
        "forum": "Tl8OmiibP99",
        "replyto": "Tl8OmiibP99",
        "invitation": "ICLR.cc/2023/Conference/Paper6154/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a method to improve the stability of differentiable NAS. It can improve stability and performance without introducing too much cost.\n\nThe major contributions of this work include:\n+ We propose a transferability-encouraging tri-level optimization framework to improve\nthe generalizability and stability of differentiable NAS methods.\n+ We propose a new knowledge transfer approach based on matching quadruple relative similarities.\n+ We perform various experiments which demonstrate the effectiveness of our method.\n",
            "strength_and_weaknesses": "## Strengths\n1. Although the content of the article is a bit large, it is generally organized and the article is easy to understand. The formulas are also deduced in detail.\n2. The experimental part is solid and has achieved good performance.\n3. The way to solve differentiable NAS by introducing an auxiliary task is innovative.\n\n## Weaknesses\nThere are two points I don't quite understand:\n1. Why use a method based on matching quadruple relative similarities for knowledge transfer? What other options are there to do this, and why is this way better than others? I don't think the insight behind this is sufficient.\n2. In this paper, the stability of the method is proved by the experiments in Table 8. The stability I understand should be the frequency of network structure degradation in a large number of experiments, not the number of skip-connections in the network. I don't know if there is a more reasonable way to prove this.",
            "clarity,_quality,_novelty_and_reproducibility": "1. **Clarity**: The idea of the article is relatively clear, easy to read, and also provides detailed derivation and various experimental results.\n2. **Quality**: The completion quality of this paper is relatively high, the insight is relatively reasonable, the experiment is valid, and the results obtained are relatively good.\n3. **Novelty**: I think it is somewhat innovative.\n4. **Reproducibility**: From the details provided in the article, I feel that it will be a little difficult to reproduce.",
            "summary_of_the_review": "I think this is an informative article. The solution is more reasonable, and a large number of experiments are used to support the conclusions in the text.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6154/Reviewer_3zEW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6154/Reviewer_3zEW"
        ]
    },
    {
        "id": "6QfB3ekkWt",
        "original": null,
        "number": 2,
        "cdate": 1666771009339,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666771009339,
        "tmdate": 1666772105424,
        "tddate": null,
        "forum": "Tl8OmiibP99",
        "replyto": "Tl8OmiibP99",
        "invitation": "ICLR.cc/2023/Conference/Paper6154/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to improve the generalizability and stability of differentiable NAS methods. Specifically, the authors first train a main model and then transfer knowledge from the main model to an auxiliary model. Finally, the authors optimize the architecture of the main model by maximizing its transferability to the auxiliary model. Experiments on several datasets show the effectiveness of the proposed model. ",
            "strength_and_weaknesses": "Strengths:\n\nThis paper develops a transferability-encouraging tri-level optimization framework in differentiable NAS and demonstrates that they can achieve higher performance on Imagenet and CIFAR compared to similar methods.\n\nThe authors introduce a new knowledge transfer approach based on matching quadruple relative similarities (QRS) to improve the auxiliary model and demonstrate its effects with ablations.\n\nThe paper is well written and easy to understand.\n\nWeaknesses:\n\n1. The method consists of training the main model, training the auxiliary model with the help of the main model (to determine similar data pair), and optimization. The main contribution seems to be the auxiliary model using the data pairs to learn the weights. In other words, the fixed main model provides the supervisory signal for the auxiliary model. The main methodology does not seem to be novel.\n\n2. Most of the baseline methods in Table 3&4 are at least 2 years ago. The work lacks comparison with more latest methods in experiments.\n\n3. Are the two data pairs (x,y) and (z,w) randomly sampled?\n\n4. In the case of cosine similarity c(x,y) > c(z,w), it appears that x,y,z and w examples are 4 class examples. In this case, how to optimize them?\n\n5. What is the difference between the data D_a and D_m?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has a good clarity. But some important details need more description, as listed in the weaknesses section. ",
            "summary_of_the_review": "Although the proposed method is simple, the solution is somewhat trivial (e.g., quadruple relative similarity relation using cosine similarity strategy). The experiments need comparison with more latest methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6154/Reviewer_8tqH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6154/Reviewer_8tqH"
        ]
    },
    {
        "id": "Teyzo0cRFe",
        "original": null,
        "number": 3,
        "cdate": 1666935990950,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666935990950,
        "tmdate": 1669557594754,
        "tddate": null,
        "forum": "Tl8OmiibP99",
        "replyto": "Tl8OmiibP99",
        "invitation": "ICLR.cc/2023/Conference/Paper6154/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The\u00a0author\u00a0adopts\u00a0knowledge\u00a0transfer\u00a0to\u00a0solve\u00a0the\u00a0inhabited\u00a0stability and generalization issue of differential NAS. The method first trains the weights of the main model with alpha fixed, then transfer the knowledge from main to auxiliary by quadruple relative similarities, and finally optimizes the alpha of the main model by optimizing the transferability to the auxiliary.\nThe intuition behind this is that, once the architecture equips with a good architecture that can extract useful knowledge, the obtained knowledge is accurate and good enough to transfer to the auxiliary one, and the auxiliary reaches high quality.",
            "strength_and_weaknesses": "# Strength\n- S1.The paper solves the stability issues of DARTS from a  different aspect, transfer learning view, which is somehow novel and interesting. The motivation is reasonable for me.\n- S2.The paper is well-written and easy to follow. \n- S3.The evaluations are sufficient to prove the validity of the proposed methods.\n- S4.The results are promising on C10, C100, and ImageNet.\n\n# Weaknesses\n- Q1.What are the computation complexity and extra cost when applying QRS compared with second-            order or third-order? Intuitively, when increasing the order, the training cost may increase either. Ablations about this should be added.\n- Q2.In Page 5, the author claims that the auxiliary model ( encoder) is the same as the main model which is a DARTS. It is clear that the main model weights are trained in the first stage and the alpha is trained in the third stage. But it is unclear in the second stage. how to optimize the auxiliary model. Whether to fix the alpha and only train the weights or alternatively train the weights and alpha just like the original DARTS[1], with the head changed to predict the correlations between two pairs of examples. The author should clarify this procedure.\n- Q3.As shown in Page 5, the author ties the architectures and weights of main and auxiliary models.  In what stage they are tied? And my concern is, if the main and the auxilary are the same model both in weights and alpha, actually there are no knowledge trainsfer but only model copy. Why a copied model can take effect in the selection of the alpha?\n- Q4.Is it necessary that auxiliary model have to be the DARTS? Does the author try any other auxiliary mode, such as ResNet50? \n- Q5.Further, maybe the choice of the auxiliary model would be the bottelneck of the method. Maybe more ablations about how the size of the auxilary model affect the main model is needed. Such as, choose ResNet18, ResNet50 and ResNet101 or DARTS with 1,3,5,8 choice blocks as auxiliary model.\n- Q6. Just curious, what is the results when applying on DARTS 1st order? \n- Q7.The method may be too complex and hand-crafted\n- Q8.What is the generalization performance when search a model from C100 and evaluate it on C10 and search from C10 and evaluate on C100.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is of good writing quality.All the ideas and points are easy to follow. \nThe idea is somehow novel. \nThe evaluation is good and enough to prove the effacy of the method.",
            "summary_of_the_review": "Overall, this is a good paper. The writing is clear and the evaluation is good enough. The results show its effacy. \n\n# Update\n\nSince my questions are well answered, I have no further comments and raise my score to 8.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6154/Reviewer_5nMq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6154/Reviewer_5nMq"
        ]
    },
    {
        "id": "o1DGV49jgS9",
        "original": null,
        "number": 4,
        "cdate": 1667318664995,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667318664995,
        "tmdate": 1670392608084,
        "tddate": null,
        "forum": "Tl8OmiibP99",
        "replyto": "Tl8OmiibP99",
        "invitation": "ICLR.cc/2023/Conference/Paper6154/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors tackle the current limitation that most existing NAS methods suffer from, which are unsatisfactory generalizability and stability, such as generating a dominant number of skip connections or poor test performance. To address this, the authors propose a transferability-encouraging tri-level optimization framework for improving the main model and auxiliary models. They demonstrate the effectiveness of their methods. ",
            "strength_and_weaknesses": "**Strength**\n- The paper is easy to read.\n- They tackle the practical limitations of neural architecture search.\n- Experiments are extensive.\n\n**Weaknesses**\n- My main concern for this paper is the quality of the validation sets, which play a key role in measuring \u201ctransferability\u201d from the main model to the auxiliary models. The poor representativeness of validation sets (class-imbalanced, noise, etc) for the main model and auxiliary model may affect the optimization process (step 3) and this should be clearly analyzed. What if we cannot get the class-balanced and non-noisy validation set in a practical scenario? The proposed method with a poor validation set still outperforms the baseline models without any degeneration? If not, are there any possible methods to handle this issue?\n- For one of the baseline models, OFA-large (Table 4) seems to show the best performance, but I was not able to find any explanation for it. As far as I know, OFA can sample subnets from the supernet while keeping (or transferring) the supernet\u2019s pretrained knowledge, which is similar to the proposed method. It would be good to compare the proposed method with OFA more closely. \n- (minor) It would be helpful if the overview illustration for the proposed method is provided. \n",
            "clarity,_quality,_novelty_and_reproducibility": "There exist few points to be clarified for improving the quality and originality, as mentioned above.",
            "summary_of_the_review": "I enjoyed reading the paper, but several improvements seem to be required.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6154/Reviewer_8AHX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6154/Reviewer_8AHX"
        ]
    }
]