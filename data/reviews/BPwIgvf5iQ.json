[
    {
        "id": "EF2WmaKKGg",
        "original": null,
        "number": 1,
        "cdate": 1666666977269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666977269,
        "tmdate": 1666670021875,
        "tddate": null,
        "forum": "BPwIgvf5iQ",
        "replyto": "BPwIgvf5iQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2736/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The focus of this work is learning representations of auditory and visual speech from the underlying raw signals, as opposed to learning a representation from hand-crafted features as is typically done.  The experiments include an ablation over design choices.  The combination of within-modality modeling (for audio) and cross-modality modeling (for audio and vision) provide a powerful representation, as demonstrated by applying to automatic speech recognition and visual speech recognition.  The results are impressive and set a new state of the art.",
            "strength_and_weaknesses": "+The paper is well written.\n\n+Thorough evaluation of the approach.\n\n+Impressive results beating existing state of the art.\n\n-The main text refers the reader to Appendix A.3 for details of the language model, but there is no discussion of what the language model actually is.  There is a term in the overall loss,  but it is not clear how L_lm is calculated.\n\n-Are numbers available about run-time complexity?\n\n-The learned representations are applied only to A/V speech recognition -- it would be useful to show their utility in a wider range of AV tasks (although this is listed as future work),\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and well structured.  The experiments are thorough, and the results are impressive.  Visual-only speech recognition is challenging, so results with percentages in the low twenties beats (expert) human performance \u2014 humans require other context other than just a view of the lips to understand speech (actual recognition rates are incredible low).",
            "summary_of_the_review": "This paper proposes new representations of auditory and visual speech that lead to very low WERs in each domain.  The results are impressive and the experiments clearly test the contribution of the different components of the system.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2736/Reviewer_CL4k"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2736/Reviewer_CL4k"
        ]
    },
    {
        "id": "YQ8MghV7HLi",
        "original": null,
        "number": 2,
        "cdate": 1666667165287,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667165287,
        "tmdate": 1669688701194,
        "tddate": null,
        "forum": "BPwIgvf5iQ",
        "replyto": "BPwIgvf5iQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2736/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a merthod called RAVEn, which is a self-supervised multi-modal approach to jointly learn visual and auditory speech representations. The paper uses masked inputs and momentum encoders. The proposed approach is asymmetric in terms of the two modalities and their pretext tasks. Strong results are obtained. ",
            "strength_and_weaknesses": "Strengths:\n\n1- The paper tackles an important and timely problem, and the solution is relatively novel.\n\n2- The paper is well-written and structured.\n\n3- It achieves strong results.\n\n4- Implementation details and hyperparameters are well-presented, and the authors aim to make the code public.\n\nWeaknesses:\n\n1- Some editing issues are still present in the paper. For example, the heading \"Introduction\" is missing after the abstract.\n\n2- The datasets that have been used could have been expanded \n\n3- As the paper mentions, the setup is asymetric in terms of the two modalities, i.e., the auditory stream predicts both the visual and auditory targets, but the visual one predicts only the auditory targets. It is perfectly fine to have a final solution thagt is asymetric. But it would have been nice to include experiments, e.g. cross-modal + video within modal is missing.\n\n4- I understand the novelty, but it would have been nice to include a bit more intuition regarding the results and experiments to show why the performance is obtained. E.g., how does the cross-modal loss result in better representations?\n\n5- From what I understand, the downstream is done on multimodal. How is the performance on unimodal? I.e., does the teacher-student approach result in better unimodal learning too?\n\n* Update: after the rebuttal, I have now increases my score.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, and has a good quality. It also seems quite reproducible.",
            "summary_of_the_review": "Please see my reviews above",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2736/Reviewer_XgXU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2736/Reviewer_XgXU"
        ]
    },
    {
        "id": "CTeHxpqK54",
        "original": null,
        "number": 3,
        "cdate": 1666740062525,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666740062525,
        "tmdate": 1668556197473,
        "tddate": null,
        "forum": "BPwIgvf5iQ",
        "replyto": "BPwIgvf5iQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2736/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a self-supervised multi-modal approach: RAVEn to jointly learn visual and speech representations. The key components in RAVEn take good practices (e.g., masked prediction, Transformer, and Knowledge Distillation) in existing self-supervised learning approaches. But differently, it leverages audio-visual cross-modal correspondence to train the teacher-student learning model. Extensive experiments show that RAVEn can help to learn powerful visual and auditory speech representations for visual and auditory speech recognition. ",
            "strength_and_weaknesses": "Pros:\n\n\\+  The proposed cross-modal learning mechanism is well-motivated. Considering that audio contains more information relevant to speech than visual, the authors propose a learning strategy that accounts for the modality difference. Its audio student predicts outputs from both audio and video teacher, whereas the video student predicts only audio teacher outputs. \n\n\\+ Extensive experiments and ablation studies in terms of ASR and VSR tasks can validate the effectiveness of the proposed multi-modal self-supervised approach.\n\n\nCons:\n\n\\- The technical novelty is limited. I did not find any novel and significant technical components in the proposed approach. The key designs in the proposed approach, such as SSL with Knowledge Distillation, masked prediction, and momentum-based teacher, can be found in previous works. To me, the proposed RAVEn is like an audio-visual version of the Self-Supervised Vision Transformer: DINO [1]. \n\n[1] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the 18th IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9650\u20139660, 2021.\n\n\\-  In real-world videos, audio and visual content are not always matched. For example, a speaking person can be out of the screen. How can the proposed method defend against the issue?\n\n\\- Since the proposed method can jointly learn audio and visual representations, it is straightforward to perform audio-visual speech recognition using the learned representations. But, in the paper, only ASR and VSR are explored. In addition, we only see very small improvements in the ASR task (LRS3 high-resource setting).  ",
            "clarity,_quality,_novelty_and_reproducibility": "\nTaking good practices in existing SSL approaches, the proposed method achieves competitive performance on ASR and VSR tasks using the learned representations. But, the paper novelty on the technical side is very marginal. \n\nThe authors provided implementation details, which can help reproduce the proposed method. They promised the code and pre-trained models would be publically available.",
            "summary_of_the_review": "Most of the key components in the proposed method, such as SSL with Knowledge Distillation, masked prediction, momentum-based teacher, and audio-visual SSL, can be found in previous approaches. Due to the limited technical novelty, my current rating for this paper is reject. \n\n***Post-Rebuttal***\n\nThank the authors for responding to my questions! Most of my concerns have been clarified by the rebuttal. Although the technical contributions are incremental, the proposed RAVEn can be a strong baseline for joint visual-speech representation learning. \n\nMy rating is upgraded to 6 from 3.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2736/Reviewer_Hokd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2736/Reviewer_Hokd"
        ]
    },
    {
        "id": "zDjRVdGgep",
        "original": null,
        "number": 4,
        "cdate": 1666749051097,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666749051097,
        "tmdate": 1666749051097,
        "tddate": null,
        "forum": "BPwIgvf5iQ",
        "replyto": "BPwIgvf5iQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2736/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper explores a method for pretraining an Audio-Visual speech recognition model directly from raw video with both audio and visual signals. It does so by asymmetrically applying two student-teacher networks 1. the audio student learns to predict both audio and visual targets generated by respective teachers, 2. the video student only learns to predict the audio features. The rationale for this is that the audio provides a much stronger signal than the video. The results are competitive with state-of-the-art for self-supervised learning with small amount of fine-tuning data, e.g. for VASR on TED-LRS3 24.4% better vs 26.9% (Shi ICLR 2022), and for AVASR on TED-LRS3 1.9% is worse than the 1.3% in Shi ICLR 2022.",
            "strength_and_weaknesses": "Strengths:\n- the paper presents a different approach to previous work on learning audio-visual representations in an unsupervised manner compared to the work in Shi ICLR 2022. In Shi's AV-HUBERT, they required a complicated multistage process of first using MFCC features to cluster, before learning an AV representation from raw data. In this work, a one-stage training regime is achieved. \n- the provides extensive analyses on the various aspects of their approach. of note, the ablations in Table 3 nicely motivate the use of asymmetric student-teacher training for best results.\n- the paper achieves state-of-the-art results for unsupervised training for visual-speech-recognition on TED-LRS3\n\nWeaknesses\n- from a certain point of view, the paper is incremental over Shi's ICLR 2022 work. it achieves similar results, however in a simpler manner. \n- the difference of 1.9% in this paper for TED-LRS is a significantly off from the 1.3% found in Shi 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and of good quality. The novelty of the task is low since similar work has been done, but the chosen method is novel in that previous work in this area required multi stages of clustering with hand crafted features before learning representations that can use the raw audio and video features. This work introduces an elegant one stage approach, and provides many experimental details that would aid in the reproducibility in the work. Overall, the technique and provided details should yield results that others can replicate, but not having published scripts and code yet, hurts reproducibility at the moment (the author's promise to release these upon publication).\n\nOne claim of the paper is not correct. \"surpassing... recent fully-supervised method trained on 90,000 hours of non-public labelled data.\" These works from Deepmind/Google Shillingford et al. (2019), Makino et al. (2019), Serdyuk et al. (2021) and Serdyuk et al. (2022) all use found data for *semi-supervised* training ie. a supervised trained recognizer, trained on limited data, is used to produce labels from user uploaded captions as described in \"Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription\", H. Liao et al, ASRU 2013. These is not hand labeled data in the supervised sense---user uploaded captions can be errorful and so the supervised model is used to validate them for use as labels.\n\n",
            "summary_of_the_review": "Overall the paper makes a nice contribution to the growing area of unsupervised training on Audio-Visual data. They provide an incremental way of training an AV model in a simpler model than in the past and achieve results comparable to the state-of-the-art so I support accepting this paper for publication at ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
            ],
            "details_of_ethics_concerns": "I am unsure of this, but I believe that this work used YouTube videos as training data and thus requires downloading them which is against YouTube's term of service. There has been a lot of published work though that has used YouTube as a data source such as AudioSet [1] and VoxCeleb [2]. [1] is even from Google, YouTube's parent.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2736/Reviewer_ykA5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2736/Reviewer_ykA5"
        ]
    }
]