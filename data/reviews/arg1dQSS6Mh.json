[
    {
        "id": "0I8dv4MPInH",
        "original": null,
        "number": 1,
        "cdate": 1666511275223,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666511275223,
        "tmdate": 1666511275223,
        "tddate": null,
        "forum": "arg1dQSS6Mh",
        "replyto": "arg1dQSS6Mh",
        "invitation": "ICLR.cc/2023/Conference/Paper2484/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an attribute alignment and enhancement (A3E) network for zero-shot learning. It uses the attribute location model to align attributes and utilized the attribute relation graph to enhance the attribute. Experiments on three datasets show the effectiveness of the proposed approach.",
            "strength_and_weaknesses": "Strength:\n1. The paper is well-organized, which is easy to read. However, some details are unclear.\n2. The performance seems good.\n3. The combination of attribute alignment and enhancement seems novel, but the novelty is limited.\n\nWeakness:\n1. The novelty of this paper is limited. It seems to be a combination of AL [Yang et al. (2021)] and attribute relation graph[Hu et al. (2022)]. As can be seen in the approach, most parts introduce the workflow without detailed formulation.\n2. Eq 2 aims to learn projection from the visual space to the word vector space. However, there is no supervision loss. How to guarantee the projections are properly learned?\n3. How does attribute alignment conducted? The supervision seems to be classification losses and no explicit alignment is conducted.\n4. What is the difference between Eq 8 and Eq 6? They use the same representations.\n5. How does the \u2018triangle\u2019 in Eq 10 obtained? It is strange to know whether a sample is seen or unseen.\n6. The results in Table 1 are unclear. Why the input size on AWA is different from the other two datasets? Do the authors use different networks?\n7. The authors only describe the phenomenon in Section 4.4 without detailed analysis. Moreover, no detailed qualitative analysis to show the effectiveness of the proposed approach.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The novelty of this paper is limited, which seems a combination of existing ZSL approaches.\n2. The workflow of the proposed approach is clear. However, some details of the proposed approach are missing.",
            "summary_of_the_review": "The novelty of this paper is limited and the detailed implementations are unclear. The experiments are not sufficient, which lacks experiment analysis and qualitative results to show the effectiveness.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2484/Reviewer_hgQc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2484/Reviewer_hgQc"
        ]
    },
    {
        "id": "_KKz0wMQ-wS",
        "original": null,
        "number": 2,
        "cdate": 1666558801072,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558801072,
        "tmdate": 1666558801072,
        "tddate": null,
        "forum": "arg1dQSS6Mh",
        "replyto": "arg1dQSS6Mh",
        "invitation": "ICLR.cc/2023/Conference/Paper2484/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper addresses Zero-Shot Learning problems by localisation and GAT based attribute enhancement. The method is evaluated on the three datasets and did not achieve state-of-the-art results.",
            "strength_and_weaknesses": "The paper is well-presented and well-designed.\n- Localisation, GAT are not new to ZSL. No clear motivation or novel insights for these designs are provided.\n- Ablation study is very rough and superficial.\n- Experimental results are not competitive.\n- Lack of theoretical contributions.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is presented with a high-quality standard. The idea is clear and easy to follow. However, it is difficult to give credit to the originality since the essential modules are from other work and no significant modification or theoretical analysis can be found.",
            "summary_of_the_review": "Overall, the paper is lack of theoretical contribution and the performance is not promising. Although the paper is well written and presented, it is still below the standard of ICLR expected.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2484/Reviewer_a3kk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2484/Reviewer_a3kk"
        ]
    },
    {
        "id": "_-BBqIWi0xi",
        "original": null,
        "number": 3,
        "cdate": 1666697897717,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666697897717,
        "tmdate": 1666698062288,
        "tddate": null,
        "forum": "arg1dQSS6Mh",
        "replyto": "arg1dQSS6Mh",
        "invitation": "ICLR.cc/2023/Conference/Paper2484/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes to fully utilize attributes information for GZSL for simultaneously exploring attribute alignment and enhancement (A3E). A3E consists of an attribute localization (AL) module and enhanced attribute scoring (EAS) module. AL is used for localizing attribute regions, and EAS further enhances these features by GAT. Experiments are conducted on golden GZSL datasets.",
            "strength_and_weaknesses": "The organization is fine. Experiments are sufficient.\n\nHowever,\n\n1.Many statements in this paper are not well motivated, e.g., ``implicit localization capability within the feature extractor\u2019\u2019 is not clear; ``exciting achievements for common categories\u2019\u2019 is also not clear. The story on comparisons among human and ZSL is commonly used in the community; you cannot say this because others have done this. The resulting facts of ZSL can not guarantee the reason of using auxiliary information for knowledge transferring. In fact, auxiliary information is one choice of conducting ZSL, there much exist other manners for performing ZSL,e.g., matching with external dataset. By saying embedding methods are inherently inferior in GZSL to generative methods, why? Also, it is fine to use unseen semantics for overcoming bias issue, yet not prone to unexpected effects. By saying unexpected effects, what do you mean? Overall, the challenges are not well defined. The writing is also not accurate.\n\n2.Terms of attribute semantics, class attribute vectors, attribute prototypes and attribute-visual features are confusing to the beginner of ZSL field. More explanation should be given. Generative adversarial network (GAN) or variational autoencoder (VAE) miss references. Typos exists: so that convert ?\n\n3.The motivation for the usage of GAT for improving the generalization is not clear. In fact, using graph neural network to model attribute relationships have been explored in previous works (e.g., LsrGAN in ECCV20). The overall compared methods are limited, and more methods should be surveyed and compared.\n\n4.The framework is actually incremental improvements on existing methods, e.g., improved on Yang et al. in CVPR 2021 by seeing each channel as attention mask. From this point, the novelty is limited to the community, since this kind of attention has been explored in the community (e.g., in AREN of CVPR2019). Furthermore, the idea of mapping AVFs into attribute semantic space by DAZLE is also not new. The idea of attribute scoring is just the combination of DAZLE with AREN for solving the same task in this paper. All these aspects reduced the contributions of this paper to the community. \n\n5.The overall writing is poor. The formulas are also confusing. Please amend them accordingly.\n\n6.How to initialize the network weights, by pre-trained ImageNet weights or from scratch? What\u2019s the specific network architecture? \n\n7.Since additional operation such as attention is used for the proposed method, I am doubted about the tradeoff between the running time and accuracy.\n\n8.The parameter analysis is shown, however, what\u2019s the specific parameters for achieving these results? E.g. \\beta. It seems the authors have reported adjusted results by varying many \\beta, this is thus not fair to compare with other methods without CS adjustments. Also, the authors used additional attribute features for model training which is also not fair compared with counterparts.",
            "clarity,_quality,_novelty_and_reproducibility": "Limited novelty and unfair comparisons with counterparts.",
            "summary_of_the_review": "Combination of existing works, and limited novelty to the community. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2484/Reviewer_2RdH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2484/Reviewer_2RdH"
        ]
    }
]