[
    {
        "id": "DjyZaMKmqPW",
        "original": null,
        "number": 1,
        "cdate": 1666620388807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620388807,
        "tmdate": 1666620388807,
        "tddate": null,
        "forum": "V5NFgHyNBI8",
        "replyto": "V5NFgHyNBI8",
        "invitation": "ICLR.cc/2023/Conference/Paper3069/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tries to use a large batch size to benefit Offline RL training. The authors try to scale up batch size and square root scaling for the learning rate to accelerate the Offline RL training. The experimental results illustrate that Large-Batch Training can efficiently accelerate offline rl training and achieve about 2x speed up. ",
            "strength_and_weaknesses": "Strength:\n\nThis paper focuses on an interesting area. Offline RL has achieved great attention in the last few years. \nThis paper verifies the performance of proposed method in various tasks, such as the subsets of D4RL.  \n\n\nWeakness:\n\nThe proposed method is not novel. Actually, this paper does not propose a novel method. The main contribution is to scale up the batch size to accelerate the offline SAC training. And I think the paper is more like a technical report.\nIn my opinion, the comparison is not fair for many experiments. For example, the x-axis represents steps in Figure 5, 6, 7. That means the number of epochs is increased for the same number of steps when you scale up the batch size. When the agent can be trained with more data, the convergence will be faster. So I think the s-axis should be the number of epochs. \nThis paper mainly focuses on the SAC-based method and proposes LR-SAC. To verify the generality of your proposed method, maybe you can provide more experimental results about other methods. \nSensitivity analysis about hyper-parameters, such as learning rate. We know RL is more sensitive to hyper-parameters. ",
            "clarity,_quality,_novelty_and_reproducibility": "NA",
            "summary_of_the_review": "NA",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3069/Reviewer_mkHX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3069/Reviewer_mkHX"
        ]
    },
    {
        "id": "zYoc_3aTc0T",
        "original": null,
        "number": 2,
        "cdate": 1666710522335,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666710522335,
        "tmdate": 1666710522335,
        "tddate": null,
        "forum": "V5NFgHyNBI8",
        "replyto": "V5NFgHyNBI8",
        "invitation": "ICLR.cc/2023/Conference/Paper3069/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies if large batch size and learning rate could improve the convergence speed of ensemble-based offline RL methods. Based on SAC-N, the authors found that both large batch size and large learning rate contribute to a faster training. ",
            "strength_and_weaknesses": "### Strength\nResults show that increasing the batch size and the learning rate can lead to a faster training.\n\n### Weaknesses\n* The novelty of this paper seems limited. \n* Clarity needs improvement. ",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\n1. SAC-N algorithm is not formally desribed before being mentioned in the introduction. I cannot find a reference for SAC-N after reading the entire paper. \n2. The motivation of this work needed further clarification. See my comments below.\n\n### Quality and Novelty\nThis paper presents a good empirical evaluation on the impact of a large batch size and a large learning rate for ensemble-based offline RL. However, it seems a straightforward adoption of well-known large-scale deep learning training methods. Could the authors explain why this study is needed and why it's not obvious that previously established fast training methods may not work in the offline RL setting? Before understanding this motivation, I'm not convinced that this paper meets the bar of ICLR. \n\n### Reproducibility\n\nThe authors provided the code to reproduce the work.\n\nMinor issues:\n\n1. The equations in FIgure 2 are clipped with missing right parenthesis.",
            "summary_of_the_review": "I'm inclined to reject the paper because the it seems to be a straightforward adoption of well-established and generic training speedup approaches for training ensemble-based offline RL methods. I'm not convinced that this paper solves a problem unique to offline RL or generates any novel insights on how to better design efficient offline RL methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3069/Reviewer_Z3P6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3069/Reviewer_Z3P6"
        ]
    },
    {
        "id": "QcK7RaQzt3U",
        "original": null,
        "number": 3,
        "cdate": 1666720645035,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666720645035,
        "tmdate": 1666720645035,
        "tddate": null,
        "forum": "V5NFgHyNBI8",
        "replyto": "V5NFgHyNBI8",
        "invitation": "ICLR.cc/2023/Conference/Paper3069/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors investigate the effect of large batch size in training the SAC algorithm. The result shows that a large batch size allows the algorithm to converge faster and introduces a form of conservatism (shown through the standard deviation ratio between ID vs. OOD state/action). This effect cannot be achieved by layer-wise optimization or learning rate alone.",
            "strength_and_weaknesses": "Strength:\nThe overall quality of the paper is high. The paper is well-written, and there are some ablation experiments to support the main argument of the paper.\n\nWeakness:\nMy main criticism of this paper, which ultimately led me to a borderline reject decision, is that this paper treats RL as if it were supervised learning. If I replace SAC with ResNet in every part of this paper, and D4RL with ImageNet or CIFAR-10, only section 4.2.1 (Bigger Batch Size Penalize OOD Actions) would not make sense, but all the rest would still be fine. For example, 4.2.2 (batch size ablation), 4.2.3. (learning rate), 4.3 (a better optimizer) all offer no explanation from the RL perspective.\n\nSection 4.2.1 offers an explanation of why the large batch size would help, and it says:\n> One possible explanation for why earlier penalization might be helpful is for prevention of the iterative error exploitation (which is prominent in iterative algorithms and extensively discussed in Brandfonbrener et al. (2021)), as it allows to soften the overestimation bias early in the training process making its propagation less severe.\n\nIn the TD3/SAC algorithm, critics are updated using the bellman-backup operator:\n\n$$L = \\mathbb{E}\\_{\\tau \\sim D}[(r\\_{t+1} + \\gamma \\min_j Q_{\\theta_j}(s_{t+1}, \\tilde a + \\epsilon) - Q_{\\theta_i}(s_t, a_t))^2]$$\n\nThis is a moving target, which is influenced by how well the Q-function is trained.\nOne can argue, with larger batch size, bellman value propagation is more random at the start -- because you essentially fit $r$ with a random noise outputted by an untrained Q function. So it's possible that large batch size helps reduce the \"early commitment\" issue.\nTherefore, a simple experiment to understand why large batch size helps is to just add Gaussian noise to the critics' target during early training and anneal this noise to see if you can recreate this effect of large batch size.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is very clear. Figures and tables are well-explained, and effects well-explained.\nNovelty: the direction of large batch size's effect on RL is relatively novel and under-investigated.\nReproducibility: the reviewer is fairly confident that the result can be reproduced.\nQuality: The overall writing quality is high.",
            "summary_of_the_review": "The paper shows that large batch size empirically helps with conservatism but offers no explanation of why this phenomenon occurs, and how it impacts the RL algorithm (except for some simple statistics that are used to describe the phenomenon). All the observations/experiments are superficial. If the authors fail to offer more significant insight and cannot design an experiment that reveals why larger batch size leads to more conservatism, this paper does not offer enough contribution to the research community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3069/Reviewer_Fv4L"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3069/Reviewer_Fv4L"
        ]
    },
    {
        "id": "gx5ru2cf_kz",
        "original": null,
        "number": 4,
        "cdate": 1667120057889,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667120057889,
        "tmdate": 1667120486663,
        "tddate": null,
        "forum": "V5NFgHyNBI8",
        "replyto": "V5NFgHyNBI8",
        "invitation": "ICLR.cc/2023/Conference/Paper3069/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies how increasing batch size and adjusting learning in SAC- N [1] can improve the training time in terms of \"convergence\" time. To adjust learning rate, they use square root scaling formula which is mostly based on previous works, To evaluate their method, they only consider a small subset of environments (i.e halfcheetah-medium-*, hopper-medium-*, and walker2d-medium-*). Finally, their studies show that their proposed approach helps with \"convergence\" time but doesn't improve final performance.  \n\n\n\n[1] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. Advances in neural information processing systems, 34:7436\u20137447, 2021",
            "strength_and_weaknesses": "While studying different supervised learning training techniques in RL is an interesting direction and can bring in better training practices into reinforcement learning, there are various concerns with this work.\n\nStrength:\n- This method helps to decrease training time of SAC-N.\n- This work applies multiple supervised learning techniques to SAC-N.\n\nWeaknesses:\n- This paper only focuses on SAC-N and doesn't study any other offline methods. Thus this work is tightly coupled to a very specific method and it is hard to draw general conclusions from these results.\n\n- The definition of convergence time seems arbitrary to me. This is an important issue as the main claim of this paper is improving training time based on the \"convergence\" time. I understand that this paper follows [2] to define this metric. But the setting of this paper seems different from them as [2] works with a large language model and having a metric like that might make sense [?].\n\n- This paper only considers a small subset of D4RL tasks which doesn't help the paper either. Also, halfcheetah, walker2d, and hopper all have very small state and action spaces whereas environments like FrankaKitchen and Adroit in D4RL have larger state and action space. Hence increasing batch sizes like in MuJoCo experiment might not be possible in those other environments as the network can't fit in a single gpu.\n   \n- The fact that results does't change that much by reducing the number of Qs in SAC-N from 500 to 50 tells that Qs are not that diverse. I believe that is what EDAC studies. Plotting means and standard deviation of Qs during training can help to better understand this. \n\n- EDAC results are better in both terms of training time and final results than SAC-N. Also it has better memory consumption per table 2 results in this paper. Thus I am not sure why one should still consider SAC-N. \n\nminor comments:\nDiamond and white squares are not defined in figure 1. \n\n\n[2] Can Wikipedia Help Offline Reinforcement Learning? Machel Reid, Yutaro Yamada and Shixiang Shane Gu\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written very well and it is easy to follow. The proposed method (increasing batch size and adjusting learning rate) is not novel and importantly results from this paper only applicable to SAC-N.",
            "summary_of_the_review": "I appreciate the authors for studying the effect of batch size and learning rate in batch reinforcement learning.That being said,the scope of this paper is very limited and only applicable to a particular method (SAC-N). In addition, they use a limited number of environments to study their proposed approaches. Finally, the proposed method does't lead to significant improvement in the results and method like EDAC already shows better results in the similar setting. Hence, the paper doesn't meet ICLR acceptance bar. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3069/Reviewer_D2xN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3069/Reviewer_D2xN"
        ]
    }
]