[
    {
        "id": "dV4U8aioD2J",
        "original": null,
        "number": 1,
        "cdate": 1666595861036,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666595861036,
        "tmdate": 1666595861036,
        "tddate": null,
        "forum": "V_06QV-kZX",
        "replyto": "V_06QV-kZX",
        "invitation": "ICLR.cc/2023/Conference/Paper1375/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper deals with ensemble learning where the weak learners are soft trees. More precisely, it aims to study from a theoretical perspective soft tree ensembles when the number of weak learners goes to infinity. To this end, the authors investigate the NTK (Neural Tangent Kernel) induced by them and prove several non-trivial results, which seems to me of great interest, without any assumption on the topology of the soft trees. They are in addition validated from numerical experiments. The paper is an extension of an article published in ICLR 2022 by Kanoh and Sugiyama hereafter referred to as [Kanoh and Sugiyama, 2022].",
            "strength_and_weaknesses": "This paper extends in a non-trivial way the understanding of infinite soft tree ensembles that was previously limited to binary trees. In particular, it is shown that the tree topology has an effect only through the number of leaves at each depth. The conclusion on the degeneracy effect observed for binary trees but not for decision list is also highly informative. Finally, the numerical results support the theoretical findings.\n\nI have the two following questions only:\n- If I understood correctly, all the soft trees in Theorem 3 share the same topology. Would it not be relevant to study the case where not all trees have the same topology? I think this could encompass some interesting practical cases.\n- Theorem 3 generalizes Theorem 1 of [Kanoh and Sugiyama, 2022] established for binary trees to any tree architecture. The idea of the proof is to see trees as combinations of independent rule sets, which is quite different from the proof developed in [Kanoh and Sugiyama, 2022]. Can the authors comment on the differences between the two proofs, both their advantages and drawbacks?\n",
            "clarity,_quality,_novelty_and_reproducibility": "In my opinion, the paper is very well-written. In particular, section 2 is very valuable and provides a good introduction to the problem and the tools used. However, I think that the comments on Figure 1 should be extended and clarified. The very short paragraph on the computational complexity of the kernel should also be improved.\n\nThe results are new and extend a very recent paper published in ICLR 2022. The numerical experiments are in accordance with the theoretical analysis.\n\nIf we have to find a flaw, I think the subject is quite specific since it deals with infinite soft tree ensembles in the NTK framework. However, recent literature shows that ensemble learning is a major topic.\n\nTypos:\n- the structure is symmetry (page 1)\n- when the number $M$ of tress gets larger (page 7)",
            "summary_of_the_review": "My opinion on this paper is very positive: it is well written and organized, improves recent results, and the theoretical and numerical results are non-trivial and in agreement with each other.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1375/Reviewer_ck6w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1375/Reviewer_ck6w"
        ]
    },
    {
        "id": "0J8RnES-mI",
        "original": null,
        "number": 2,
        "cdate": 1666613483647,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613483647,
        "tmdate": 1666613483647,
        "tddate": null,
        "forum": "V_06QV-kZX",
        "replyto": "V_06QV-kZX",
        "invitation": "ICLR.cc/2023/Conference/Paper1375/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper analyses tree architectures for neural trees ensembles via NTK. Authors provide important insights on how the structure of trees affects NTK and consider depth limit of NTK. Experimental results are based on kernel regression with arising NTK.",
            "strength_and_weaknesses": "Strength:\n-- rigorous theoretical analysis\n-- comparison of different architectures\n-- discussion of connection with residual networks\n\nWeaknesses:\n-- experimental validation is mostly absent, since authors claim that kernel is non degenerate seeing how stochastic gradient optimization is able to overfit on arbitrary function and it's generalisation gaps (especially compared to classical gradient boosting) are desired to support theoretical claims in the situation of limited novelty.\n-- experiments that are present are done on top of Kernel Regression with kernel defined by NTK, it is interesting to see how stochastic gradient descent results are aligned with Kernel Regression results as in practical setting applications of Kernel Regression is limited due to necessity to invert kernel matrix.\n-- No link with classical gradient boosting",
            "clarity,_quality,_novelty_and_reproducibility": "The theoretical results extend work of R Kanoh et al (ICLR 2022) to the case of non-perfect binary trees, so it is limited novelty. \nThe overall clarity of the work is high.\nThe work seems to be reproducible as the code is provided.",
            "summary_of_the_review": "Theoretical results are novel for literature but the novelty is limited by the fact that results slightly extend analysis of R Kanoh et al (ICLR 2022). Paper has rigorous theoretical analysis but lacks more detailed experimentations. Nevertheless, results are interesting and worth presenting as provide interesting insight on the kernel structure.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1375/Reviewer_RPPM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1375/Reviewer_RPPM"
        ]
    },
    {
        "id": "kvOg8bkt-B",
        "original": null,
        "number": 3,
        "cdate": 1666665606180,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665606180,
        "tmdate": 1666665606180,
        "tddate": null,
        "forum": "V_06QV-kZX",
        "replyto": "V_06QV-kZX",
        "invitation": "ICLR.cc/2023/Conference/Paper1375/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the TNTK, i.e., neural tangent kernel induced by soft tree ensembles. The authors first provide a closed-form expression of the TNTK with arbitrary architecture. This is a generalization of the previous work (Kanoh and Sugiyama, 2022), which only studies the perfect binary tree. The underlying idea is to decompose a tree structure to rule sets, and to utilize their superposition property. Hence, the TNTK is the weighted summation of that of each rule set. This also implies that only the number of leaves at each depth affects the TNTK. Hence, different tree structures can have identical training behaviors and generalization performance under the infinite ensemble limit. Second, they deeply analyze a corner case, i.e., a tree growing in only one direction called a decision list. In particular, the TNTK of the decision tree does not degenerate while that of the perfect binary tree does. Such non-degeneracy property makes the performance of the TNTK not decrease when the depth becomes large. Experimental results also well support their result. \n",
            "strength_and_weaknesses": "Strength:\n\nThis paper generalizes the previous result on the perfect binary tree to the arbitrary ones. This result gives us that only the number of leaves at each depth affects the TNTK. As the authors suggested, this is useful for the neural architect search (NAS) where the candidate models are infinite ensemble of soft trees. \n\n\nWeakness:\n\nAlthough the results of this paper are concrete, a justification of the TNTK is weak and it is still unclear which tasks/applications require the TNTK. The contribution would be much stronger if it shows some practical applications, e.g., Neural Architecture Search (NAS).\n\nA performance gap between the perfect binary tree and the decision list comes from the depth degeneracy. The authors give a simple intuition on the behavior of region split, but this intuition is not sufficient to explain the performance gap. It would be better if more rigorous reasoning is provided.\n\nThe performance of the TNTK in section 4.3 depends on the choice of scaling factor ``alpha\u2019\u2019. Does this alpha control degeneracy of the TNTK? If so, it would be great to compare the degeneracy to practical performance (with different choices of alpha). And it would be better to compare the TNTK with other (sigmoid-like) activations.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written but it would be better if more concrete results of TNTK (e.g., usages, generalization or optimization) are given. The novelty is incremental.",
            "summary_of_the_review": "This paper well generalizes the previous results on the perfect binary tree, but the novelty is weak, and more concrete justification on the TNTK would be needed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1375/Reviewer_gydX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1375/Reviewer_gydX"
        ]
    },
    {
        "id": "rAP54Bjx8H_",
        "original": null,
        "number": 4,
        "cdate": 1666786519376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666786519376,
        "tmdate": 1666786519376,
        "tddate": null,
        "forum": "V_06QV-kZX",
        "replyto": "V_06QV-kZX",
        "invitation": "ICLR.cc/2023/Conference/Paper1375/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "While existing work (Kanoh & Sugiyama, 2022) has introduced NTK to analyze the ensemble behavior of an infinite number of soft trees, such a study has mainly targeted the perfect binary tree, leaving the theoretical understanding of other types of widely used soft trees (e.g., decision list, rule set) unknown yet. To this end, this paper follows this line of work to provide theoretical analyses for other types of widely used soft trees via NTK and provides many interesting insights.",
            "strength_and_weaknesses": "Strengths:\n1. This paper extends the theoretical study of (Kanoh & Sugiyama, 2022) on the perfect binary trees to many other types of soft trees in the literature, which is important to provide a more practical theoretical understanding in this field.\n2. The theoretical results and insights of this paper are quite interesting and usually have been validated using empirical experiments. These results also provide certain implications for the application of these tree ensembles, which can be helpful in practice.\n3. The whole paper is well-written and easy to follow.\n\nweaknesses:\n1. If I understand correctly, the theoretical analysis of this paper is based on the ensemble of the same type of soft trees with the same depth. So, will the theoretical results in this paper be also applicable to the ensemble of the different types of soft trees with various depths (even only empirical verification will be enough)? Because such an ensemble seems to be common in practice.\n2. More interpretation for theorem 3 (e.g., how \"theorem 3 tells us that the limiting NTK depends on only the number of leaves at each depth with respect to tree architecture\" specifically) can make the importance of theorem 3 clearer as theorem 3 is the first and also the most fundamental contribution of this paper.\n3. For the interpretation of Figure 3, it's unclear why NTK can be connected with the training behavior and generalization performance of tree ensembles. Any theoretical support for this, like the ones in (Jacot et al., 2018) and [1,2], but with respect to the soft tree ensemble models? I think more explanations need to be provided here since it's an important insight drawn from theorem 3, which also serves as the second contribution shown in the introduction. \n4. Mentioning \"ensemble\" in the caption of figure 3 can make this result easier to be understood. There are many lines (w/ different colors) without labels in figures 4 and 6. I hope the authors can further polish these results to make them more readable.\n\n[1] Arora, Sanjeev, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. 2019. \u201cFine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks.\u201d\n[2] Cao, Yuan, and Quanquan Gu. 2019. \u201cGeneralization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks.\u201d",
            "clarity,_quality,_novelty_and_reproducibility": "In general, this paper is of good quality with high novelty and I really enjoy reading this paper.",
            "summary_of_the_review": "Overall, I am quite interested in this work and excited to see that NTK has inspired the theoretical analysis in other fields, e.g., the soft tree ensembles studied in this paper. However, as I am not professional in this area, currently I can only deliver a score of 6 for this work with small confidence. But I believe that this paper is already of good quality.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1375/Reviewer_AJRY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1375/Reviewer_AJRY"
        ]
    }
]