[
    {
        "id": "UFzunj_b9b7",
        "original": null,
        "number": 1,
        "cdate": 1666448044956,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666448044956,
        "tmdate": 1666448044956,
        "tddate": null,
        "forum": "TfBHFLgv77",
        "replyto": "TfBHFLgv77",
        "invitation": "ICLR.cc/2023/Conference/Paper3244/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to model the latent state representation in RL with hyperbolic space. They find that naively applying existing hyperbolic deep learning methods is not helpful, and introduce two techniques (spectral normalization and feature rescaling) to address the issues. The integrated method (S-RYM) is evaluated on Procgen and Atari-100K benchmarks, effectively boosting the performance compared to baselines.",
            "strength_and_weaknesses": "Strength:\n* The paper is clearly written and easy to follow.\n* The idea of learning latent representations using hyperbolic space for RL is novel, and successfully adapting hyperbolic deep learning methods to RL is non-trivial.\n* The empirical performance improvements on Procgen and Atari are significant.\n* Visualizations of 2-dimensional hyperbolic embeddings (Figure 10 and 12) are very intriguing.\n\nQuestions:\n* In general, the state relationship is more like a directed graph than a tree. I am curious how this would fit into the hyperbolic latent learning framework in this paper?\n* Can the authors provide more details on how \"Euclidean PPO + S-RYM\" in Figure 7 is implemented?\n* It would be better if the authors can make more visualization (like Figure 10) for other games in Procgen.\n\nMinor issues:\n* Typo: below Table 4, \"common practice win on-policy methods\", win -> in?",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: Good.\n* Quality and Novelty: The quality is pretty good and the proposed method is novel.\n* Reproducibility: The source code is provided, while I have not checked it.",
            "summary_of_the_review": "In summary, this paper identifies and resolves the issues when applying hyperbolic deep learning methods to RL. Modelling latent representations with hyperbolic space naturally fits the hierarchical relationship between states in RL. Extensive experiments on Procgen and Atari-100K demonstrate the effectiveness of hyperbolic RL.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3244/Reviewer_56hh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3244/Reviewer_56hh"
        ]
    },
    {
        "id": "-YsgQyMN4nW",
        "original": null,
        "number": 2,
        "cdate": 1666747721169,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666747721169,
        "tmdate": 1669071484227,
        "tddate": null,
        "forum": "TfBHFLgv77",
        "replyto": "TfBHFLgv77",
        "invitation": "ICLR.cc/2023/Conference/Paper3244/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces hyperbolic representations to deep reinforcement learning, with the intuition that hyperbolic space should be better for representing the sorts of hierarchical relationships between subsequent states found in deep RL. However, a straightforward implementation built on top of PPO does not seem to improve performance, and in fact results in worse performance on some environments. The paper hypothesizes that this is due to optimization difficulties introduced by the hyperbolic layer, and proposes using spectral normalization and rescaling of the hyperbolic layer's outputs to alleviate the optimization difficulties. The resulting module improves performance when combined with both PPO and Rainbow DQN on a broad set of benchmarks. Extensive empirical investigations suggest that hyperbolic representations could be a good basis for building future deep RL algorithms.",
            "strength_and_weaknesses": "**Strengths:**\n+ Very well-written and clear; the analogy between hyperbolic spaces and tree structures was very helpful for gaining intuition, as were the examples provided throughout (Figures 2 and 3 in particular).\n+ The introduction does a good job of arguing for hyperbolic representations for a variety of reasons.\n+ An ablation study performed on the two components (spectral normalization of the euclidean sub-network and rescaling of its output) of the proposed method S-RYM showing they are both necessary.\n+ Broad empirical demonstrations that result in fairly consistent performance improvements.\n\n**Potential weaknesses/questions:**\n1. In Figure 4, the relationship between $\\delta$-hyperbolicity and the train-test gap seems very questionable. In all of the plots, the train-test gap grows while $\\delta$-hyperbolicity is shrinking. Additionally, in the fruitbot plot $\\delta$-hyperbolicity increases several times with no apparent change in the train-test gap. Furthermore, the scale of the $\\delta$-hyperbolicity is the same between representations that learned to successfully solve the task (fruitbot and starpilot) and those that failed to generalize (bigfish and dodgeball), suggesting all of the representations are of a similar level of hyperbolicity. Doesn't that suggest that something else is the cause of the failure to generalize? It definitely does not validate the hypothesis.\n1. Much of the reasoning in the first part of Section 3.3 seems very speculative. It would be good to adjust the language to better convey that this explanation is a hypothesis.\n1. How do standard neural net layers yield Euclidean velocity vectors? This detail seemed to be glossed over.\n1. Some of the figures and most of the plots are too small to read easily. Consider removing the depictions of the environments, and moving the legends to be horizontally laid out underneath the plots. That should allow for larger plots without too much vertical space being used.\n1. The colour scheme is not accessible to people with colourblindness, and the size of the lines in the legends are too small to easily see their colour.\n1. What are the shaded regions in the plots? Confidence intervals? I wasn't able to find this information.\n1. \"our instabilities appear to occur in the gradients from the hyperbolic representations\". This was not clear to me from Figure 6. Is there some reason to not use spectral normalization on the hyperbolic representations? Did it hurt performance?\n1. Do the representations learned using S-RYM actually exhibit increased $\\delta$-hyperbolicity over the base algorithms? I'm very curious about this, but couldn't find this information in the appendices.\n1. What are some of the shortcomings of the proposed algorithm? I could not find any discussion of limitations.\n\n**Minor comments/questions:**\n- Bottom of page 2: duplicate \"upon\".\n- Bottom of page 3: \"of the form in Equation 6\" would be clearer.\n- Figure 6 appears before Figure 5.\n- What does the dashed line represent in Figure 5?\n- Start of related work section: \"objectives appears has been shown prone to overfitting\"\n- The legend in Figure 8 seems not quite right. The legend indicates dashed lines represent performance on the 200 training levels, but the plots involve a varying number of training levels.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\nThe paper is very clear and well-written. Several helpful figures are included that make it easier to gain intuition about the concepts introduced. Thank you, authors!\n\n**Quality:**\nI have a few concerns about some of the claims made in the paper (detailed above), but overall the work seems well-done to me.\n\n**Novelty:**\nAs far as I am aware, hyperbolic representations have not been used in *deep* RL before, but I am also not a specialist in this area.\n\n**Reproducibility:**\nEnough details are included in the paper and appendices to make reproducing the results possible.",
            "summary_of_the_review": "I recommend accepting the paper for publication; it is very clearly written, proposes a new approach, shows a general way to get it to work with existing algorithms, and demonstrates the benefits on multiple benchmarks. It would be nice if my concerns were addressed, but even without doing so the paper should probably be published anyways.\n\n**Update:** The authors have addressed all of my concerns satisfactorily, and I can now strongly recommend accepting the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3244/Reviewer_4ZkJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3244/Reviewer_4ZkJ"
        ]
    },
    {
        "id": "lYPiI5By6jz",
        "original": null,
        "number": 3,
        "cdate": 1667518924395,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667518924395,
        "tmdate": 1668410397794,
        "tddate": null,
        "forum": "TfBHFLgv77",
        "replyto": "TfBHFLgv77",
        "invitation": "ICLR.cc/2023/Conference/Paper3244/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes spectrally-regularized hyperbolic mappings (S-RYM) to learn a hyperbolic representation for deep RL, by applying a spectral normalization for learning the hyperbolic representation. ",
            "strength_and_weaknesses": "Strength:\n1. As an empirical paper, it demonstrates superior performance on many benchmarks (Atari and Procgen).\n2. It illustrates the connection between the hierarchical structure in hyperbolic representation and reinforcement learning\n\nWeakness/Questions:\n1. To the best of the reviewer\u2019s understanding, the implementation details of this paper are not clearly stated:\n(a) If the reviewer understands the implementation clearly, it seems that the implementation adopts the spectral normalization to the hydra package (https://github.com/facebookresearch/hydra)? The reviewer would appreciate it if the author can clarify the novelty in the implementation for better readability! (b) Regardless of whether the novelty in implementation is purely \u201cspectral normalize+hydra\u201d, the author should at least cite the Hydra Package if they are using it as an implementation backbone. (c) The reviewer guesses the last paragraph of section 2.2 \u201cIn line with recent use of hyperbolic geometry in supervised \u2026\u201d characterizes the main contents of the main implementation details of the hyperbolic embedding, but perhaps the authors can elaborate more on this part so that the main contribution of this work is much better than  \u201cspectral normalization + some hyperbolic representation learning method other people proposed\u201d.\n2. The motivation from $\\delta$-hyperbolicity to the pursuit of hyperbolic representation is very inspiring. The reviewer is wondering whether the authors can reproduce Figure 4 using PPO + S-RYM. If the authors can demonstrate the $\\delta$-hyperbolicity decreases with PPO + S-RYM, it would also improve the results of this work.\n3. How does the implementation of the hyperbolic embeddings different from other online packages (e.g., https://github.com/nalexai/hyperlib)? ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is nicely written for motivation and stating the results. However, the reviewer cannot grasp the implementation details of this paper which hinders the reviewer\u2019s evaluation of the novelties of this paper. The reviewer believes the empirical results of this work are reproducible (although the reviewer did not run the code). ",
            "summary_of_the_review": "In summary, since the empirical results of this work beat the SOTA, the reviewer believes it definitely reaches the bar for acceptance. However, the reviewer believes there is a large room for the writing part (for clarifying the methods and addressing how the proposed method is different from SN + Hydra) so that other readers to better appreciate the merits of this work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3244/Reviewer_7pBh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3244/Reviewer_7pBh"
        ]
    }
]