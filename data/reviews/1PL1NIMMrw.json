[
    {
        "id": "Xmahv9YzhQX",
        "original": null,
        "number": 1,
        "cdate": 1666538558317,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666538558317,
        "tmdate": 1666538558317,
        "tddate": null,
        "forum": "1PL1NIMMrw",
        "replyto": "1PL1NIMMrw",
        "invitation": "ICLR.cc/2023/Conference/Paper2176/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a new decoding and answer generation method for LMs to achieve multi-step math problems in a \u201cstep-by-step\u201d way. Different from the standard \u201cchain-of-thought\u201d prompting which uses greedy decoding, this method samples several reasoning paths generated by LM, and then generates the final answer by finding the most frequent answer. Author conducted a series of experiments varying in LM type and LM size to assess the self-consistency method\u2019s performance.",
            "strength_and_weaknesses": "Strength:\n1 Motivation is straightforward.\n2 The method is simple and effective.\n3 The author conducted a variety of experiments to assess the method\u2019s performance.\n\nWeak:\nSelf-consistency is an interesting point. However, my main consideration is that this paper does not go deep enough into the study of self-consistency. This lack of depth is mainly reflected in two perspectives (1) only the majority vote of multiple randomly generated answers is considered. There is a lack of in-depth observation on how to combine different answers; (2) different answers are obtained by controlling the temperature and k. For text generation, the effect of these two hyperparameters is unclear. In particular, how these two hyperparameters affect answer consistency is unclear.\n\nOther weak points:\n1 Experiment results suggest that LM cannot distinguish well between correct and incorrect answers (at least from the aspect of token probability), but why taking majority vote over answers generated from different reasoning paths works? More in-depth studies should be done.\n2 Since \u201cdiversity of reasoning paths is the key to better performance\u201d, why other ensemble-based approaches (e.g. the multiple prompts setting) didn\u2019t get larger gain for their diversity?\n3 Figure 2 is confusing: why greedy decode has different number of sampled paths?\n4 It would be better to show full sampled paths in table 4 and other examples if the purpose is to claim self-consistency can repair errored reasoning process.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer to Strength And Weaknesses.",
            "summary_of_the_review": "Overall, I suggest that the authors do a deeper study of the problem, e.g., for LMs, how does the problem of consistency arise and what is its fundamental nature? What exactly does improving self-consistency mean? Why does majority vote improve the model effect?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2176/Reviewer_Tj1M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2176/Reviewer_Tj1M"
        ]
    },
    {
        "id": "b9ovzdXUBVu",
        "original": null,
        "number": 2,
        "cdate": 1666589401165,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589401165,
        "tmdate": 1666590165525,
        "tddate": null,
        "forum": "1PL1NIMMrw",
        "replyto": "1PL1NIMMrw",
        "invitation": "ICLR.cc/2023/Conference/Paper2176/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a decoding strategy to improve chain-of-thought (CoT) prompting in pre-trained language models. The proposed strategy uses nucleus sampling to sample diverse outputs as opposed to a single one in CoT (based on greedy decoding) and selects the answer that is most consistent through marginalization. Evaluation performed on arithmetic and commonsense reasoning shows large improvements compared to CoT. Results are consistent across different language models and sizes.",
            "strength_and_weaknesses": "**Strengths**\n\nProposes a simple and effective decoding strategy to improve LMs with CoT prompting. \n\nContains extensive experiments with different LM types and sizes that demonstrate large gains compared to vanilla CoT without requiring additional training. \n\nSelf-consistency strategy surfaces diverse outputs that correspond to rationales in CoT-prompted LMs and improves robustness to imperfect inputs. Also, it can provide uncertainty estimates for calibration.\n\n**Weaknesses** \nThe decoding strategy itself has limited novelty since sampling strategies (including nucleus sampling) and ranking according to model scores have been previously used for pre-trained LMs albeit not with CoT prompting. \n\nEven though it generally leads to improvements, the proposed decoding strategy creates a significant overhead proportional to the number of outputs sampled e.g 5x latency for 5 samples assuming that samples have equal lengths.\n\nThe marginalization process of self-consistency assumes that answers are coming from a fixed answer set, which decreases the scope of applicability. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written overall and its motivation is quite clear i.e. improving recently proposed CoT. The claims are supported by extensive experiments with a variety of model configurations. I also do not have any major concerns about reproducibility since the authors provide sufficient descriptions and artifacts (code, prompts, model outputs, etc). \n\nIn terms of originality, sampling different model outputs and ranking them according to a metric has been largely explored in the past (e.g. MT, text generation). In the context of pre-trained LMs, the GPT3 paper used beam search results for free-form completion, Adiwardana et al., (2020) used a very similar sample-and-rank method based on regular sampling, and more recently Jung et al. (2022) used nucleus sampling to sample diverse generated outputs to improve reasoning [1]. The last one is arguably parallel work but I am surprised it was not cited.\n\nHaving said that, the approach itself is not that novel but it is a straightforward direction to explore for CoT LMs and the community can benefit from such a simple method and large-scale empirical findings. \n\nQuestions:\n- It looks like that self-consistency is an instance of the sample-and-rank method that uses nucleus sampling instead of temperature sampling and majority vote (simple sum) instead of log probability. Do the authors share this view? I'd suggest making the connection clear to avoid misinterpretations regarding novelty.\n- In table 1, could the authors specify on which set the accuracy scores are computed? It's likely not very crucial since the majority vote was eventually selected, but it would be useful to know. \n\n[1] https://arxiv.org/pdf/2205.11822.pdf",
            "summary_of_the_review": "Overall, this paper conducts a large empirical study of a simple decoding strategy for pre-trained LMs that use CoT prompting. The decoding strategy itself has limited novelty but the empirical findings are quite interesting/useful. The experiments show strong improvements over vanilla CoT prompting, which should be of interest to the community. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2176/Reviewer_Z6Ub"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2176/Reviewer_Z6Ub"
        ]
    },
    {
        "id": "0k-CSaozoC",
        "original": null,
        "number": 3,
        "cdate": 1666627494748,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627494748,
        "tmdate": 1666627494748,
        "tddate": null,
        "forum": "1PL1NIMMrw",
        "replyto": "1PL1NIMMrw",
        "invitation": "ICLR.cc/2023/Conference/Paper2176/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nThis paper investigates an improved version of Chain-of-thought prompting by leveraging self consistency and shows empirical evidence that their self consistency prompting indeed improve reasoning tasks performance compared with the original chain-of-thoguht prompting.\n",
            "strength_and_weaknesses": "Strength: \n1.\tThe self-consistency method is very well motivated, and authors did a great job presenting this motivation. The method is very principled.\n2.\tThe empirical evidence of the superiority of the method is extensive and solid.\n\nWeakness: authors\u2019 notation of r_i is a bit unclear to me, my understanding is that they are just the generated text before the final answer (what authors call reasoning path). It is not clear to me what is the difference between the generated text before the final answer and what authors call the reasoning paths. Did authors do something specific to exclude some potentially irrelevant text that were generated but did not contribute to reasoning? If not, perhaps calling them reasoning paths is a bit confusing? It seems r_i and a_i always appear together, which gives me the impression that the only distinction is that a_i is the final answer and r_i is everything before that. \n\nAnother confusion is the presentation of the weighted sum, is it correct to understand the probability weight is just the logsumexp of the logprobs of the generation? That makes sense to be, equations 1 is a bit misleading to me because it seems to place an emphasis on r_i as being the reasoning path when it\u2019s just the probability of a specific generation. I think it is okay to mention the generation is supposed to be reasoning paths, but it might be more clear to just note them as generated text in those formal expressions.\n",
            "clarity,_quality,_novelty_and_reproducibility": "For clarity and quality see the previous section.\nIn terms of novelty, I think though authors self consistency method is indeed novel when applied on chain of thought prompting, the idea itself is somewhat an obvious next step and something that should work well. So even though the empirical results are extensive and solid, they are not as surprising. So I think the novelty of this paper is only marginal. However the empirical results are nevertheless significant. \n",
            "summary_of_the_review": "I\u2019d recommend marginal acceptance, as I think the self consistency method is very principled and well motivated and empirical evidence is extensive and solid. However the notation and the presentation could be improved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2176/Reviewer_s5hD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2176/Reviewer_s5hD"
        ]
    },
    {
        "id": "wpzePxnSHFM",
        "original": null,
        "number": 4,
        "cdate": 1666699511231,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699511231,
        "tmdate": 1666699511231,
        "tddate": null,
        "forum": "1PL1NIMMrw",
        "replyto": "1PL1NIMMrw",
        "invitation": "ICLR.cc/2023/Conference/Paper2176/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a new way to improve the reasoning power of language models by decoding several sequences of CoT/Scratchpad reasoning and then using a voting-based scheme to choose an answer. At the cost of having to sample multiple times from the model, the authors show a consistent improvement across tasks and models. Through thorough analysis, the authors show the utility of their method, and its advantages over other sampling techniques and highlight the best practices that might be useful in practical applications.",
            "strength_and_weaknesses": "- **Clarity:** The paper is very clearly written and is easy to understand. The takeaways from each section are clear and flow from one to the other.\n- The tasks, baseline sampling techniques, models and various tradeoffs while using different hyperparameters have been explored well! They give a clear picture of what is useful in a practical setting.\n- **********************************Reproducibility:********************************** The method is easy to implement, but not all the models are publicly available or are accessible only through a paid API.\n- The method itself is quite general and can be used across several task types and models.\n- Further Discussion: CoT/ Scratchpads are useful in understanding the output of a model; how would the authors suggest choosing a reasoning path to provide an explanation for the answer given by the model?\n- ******************************Novelty:****************************** The paper is novel in its approach.\n- ******************Quality:****************** The paper, analysis and writing are of a very high quality",
            "clarity,_quality,_novelty_and_reproducibility": "Refer to main review.",
            "summary_of_the_review": "The paper is extremely strong in almost exhaustively testing different settings and tasks. It has very limited weaknesses and will probably have a large impact on various practical applications.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2176/Reviewer_avbc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2176/Reviewer_avbc"
        ]
    }
]