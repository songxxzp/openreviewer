[
    {
        "id": "FDpylNZlgK",
        "original": null,
        "number": 1,
        "cdate": 1666585641187,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666585641187,
        "tmdate": 1668880608410,
        "tddate": null,
        "forum": "KyxJ9Yfxo2",
        "replyto": "KyxJ9Yfxo2",
        "invitation": "ICLR.cc/2023/Conference/Paper4132/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a variational autoencoder (VAE) framework for encoding natural language a natural language sentence into a sequence of nodes in a global latent graph. The author claims two major contributions:\n1. The model enhances the interpretability of traditional pretrained language models by interpreting nodes in the latent graph.\n2. The model reaches SOTA performance in language modeling.",
            "strength_and_weaknesses": "## Strengths\n\n1. This work studies VAE for natural language. This is an important open problem in NLP and VAE community.\n\n2. They conduct experiments using modern pretrained language models such as BERT and GPT-2, and compared with these strong baselines in experiments.\n\n3. This work presents an interesting analysis on the structure of the latent graph (Figure 3 and Table 3), showing possibility of interpreting latent representations of big language models.\n\n## Weaknesses\n\n1. There is a major soundness problem in the LM evaluation. According to Section 3 and Figure 1, the proposed method first encodes the *full* input sentence using a modified Transformer encoder and get $\\mathbf{h}^\\text{enc}$, which is used to compute $\\mathbf{q}$ and then the node sequence $\\mathbf{e}$. In the decoding stage, the modified Transformer decoder model has access to $\\mathbf{e}$ via attention. As a result, although indirectly, the **decoder has access to the full input sentence**. An ordinary Transformer encoder-decoder model having access to the full input sentence can easily achieve a perplexity as low as 1 if trained properly. Therefore, the empirical result that the proposed method outperforms GPT in language modeling makes little sense in showing the strength of the proposed method.\n\n2. The formulation of the algorithm in training and generation is still not very clear. It would be better if there are some pseudo-codes to explain the method step by step.",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n\nThe formulation of the algorithm in training and generation is still not very clear. It would be better if there are some pseudo-codes to explain the method step by step.\n\n## Novelty\n\nThe proposed method is novel.\n\n## Reproducibility\n\nThe proposed method has good reproducibility because all resources required for reproduction is already provided in the supplementary material.",
            "summary_of_the_review": "The work researches VAE in NLP. It is an open problem of major significance. It proposes HSN to auto-encode text sentences into a trajectory in a latent graph. However, the author claims that HSN outperforms traditional LMs in language modeling task, but I consider this claim questionable because concerns about the soundness of evaluation (see weakness 1 for details). Therefore, I propose to reject this paper unless my concerns are addressed.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4132/Reviewer_Dats"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4132/Reviewer_Dats"
        ]
    },
    {
        "id": "tjbZ88Ty3-",
        "original": null,
        "number": 2,
        "cdate": 1666635704281,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635704281,
        "tmdate": 1666635704281,
        "tddate": null,
        "forum": "KyxJ9Yfxo2",
        "replyto": "KyxJ9Yfxo2",
        "invitation": "ICLR.cc/2023/Conference/Paper4132/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a graphical model parameterized by BERT and GPT2 to infer the hidden schema network within the language data. Experiments show that this model is effective in recovering the underlying graphs (to an extend) and effective in language modeling. ",
            "strength_and_weaknesses": "**Strength**\n\n**Interesting generative model.** The generative model proposed in this paper, which discovers a latent schema network within the contextualized representations, is interesting and worth attention. \n\n**Experiments validate the effectiveness of the method.** The experiments is performed in a principled way, starting from a synthetic setting then the real language. The language modeling experiments (though the field have moved from language modeling) shows the advantages of the method\n\n**Weakness**\n\n**Biased critics on language models.** Unfortunately, critics made in this paper towards language models is very biased, or even wrong sometimes. Specifically:\n\n- \u201clack both compositionality\u201d (abstract and introduction): recent developments in language models, especially when language models are large, show extraordinary ability in generating novel and meaningful sentences compositionally (for example: Brown et. al. 2020, Wei et. al. 2022). The general belief of the community is that the content generated by large language models are quite novel and impressive. The authors can also validate this themselves simply by trying to let GPT3 to generate and play with it (registration is free).\n- \u201clack \u2026 semantic interpretability\u201d (abstract and introduction): there is the whole \u201cBertology\u201d field studying the interpretability, either lexically, syntactically, or semantically (Rogers et. al. 2020, Liu et. al. 2019, Hewitt and Manning 2019). The general belief of the community is that the representations produced by the language models contain very rich semantic interpretations.\n- \u201csemantic content is intrinsically relational\u201d (first paragraph in introduction): this statement is wrong. The semantics of certain words may not need to be relational, for example:\n    - Proper nouns, like name of countries (the United States). They have semantic meaning on their own, and one does not necessarily need their relations to other words to better understand their meaning.\n    - New words that emerges as the society develops. For example, when \u201celectricity\u201d was first discovered. The understanding of these words require grounding to the physical, actual world, but may be hard to understand from their relations to other words.\n- \u201cdifficult to manipulate\u201d (abstract and introduction): there are multiple works controlling the generation from language models, for example Miao et. al. 2019, Qin et. al. 2022.\n\n**Limited impact of the result:** the field is now moved from language modeling, as it does not necessarily lead to improvements on the actual tasks. So emphasizing on language modeling results may not be a significant enough contribution. \n\nReferences:\n\nBrown et. al. 2020. Language Models are Few-Shot Learners\n\nWei et. al. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models\n\nRogers et. al. 2020. A Primer in BERTology: What we know about how BERT works\n\nLiu et. al. 2019. Linguistic Knowledge and Transferability of Contextual Representations\n\nHewitt and Manning 2019. A Structural Probe for Finding Syntax in Word Representations\n\nMiao et. al. 2019. CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling\n\nQin et. al. 2022. COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics",
            "clarity,_quality,_novelty_and_reproducibility": "The critics on the language models should be substantially improved to reflect a better understanding and acknowledge of the field, see above discussions. ",
            "summary_of_the_review": "At the first sight of this paper, in the introduction and related work, I was surprised by the critics in this paper made towards language models (\u201dlack both compositionally and semantic interpretability\u201d; \u201cdifficult to manipulate\u201d) which is simply wrong (at least from my understanding of the field and years as an NLP researcher). Yet when I continue to read the modeling part, the inference method, and the experiment setting, I find these content seem to be OK (from a modeling perspective). I would refrain myself from rejecting a paper because lack of commonsense of the area (which in this case is somehow independent of its tech aspects). Yet I would also refrain myself from accepting a paper that has a wrong understanding of its own filed. \n\nOverall, I think the critics in the paper (abstract, introduction, and related work) requires substantial revision to show, understand and acknowledge the development of language models. Yet apart from that, the modeling seems to be OK but requires stronger results, more than language modeling. Combining these aspects, I rate this paper as a reject.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4132/Reviewer_nMiy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4132/Reviewer_nMiy"
        ]
    },
    {
        "id": "ic1Aj5v7CmM",
        "original": null,
        "number": 3,
        "cdate": 1666984453815,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666984453815,
        "tmdate": 1667253883588,
        "tddate": null,
        "forum": "KyxJ9Yfxo2",
        "replyto": "KyxJ9Yfxo2",
        "invitation": "ICLR.cc/2023/Conference/Paper4132/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes building a generative LM (HSN) using a VAE framework to encode sentences into symbols which encode different aspects of language (like topics and sentiments). Using these discrete symbols, the authors claim that we can achieve compositionality of sentence encodings.\n\nThe proof of concept is demonstrated by using HSN to generate a sequence of symbols which correspond to a known ground truth latent graph (nodes corresponding to symbols). A significant overlap is seen between the ground truth graph and the generated graph, implying that the model is successful in uncovering the latent graph.\n\nThe authors then show the performance of HSN while using BERT and GPT2 to train the model for LM and Commonsense Generation task. The performance (perplexity) of HSN is better than other baselines which is impressive. Different topics of text sentences are represented by different \"hot\" symbols in the latent graph.",
            "strength_and_weaknesses": "**Strengths:**\n1. Authors present a generative model which is better than GPT2 on the Language modelling task.\n2. Presents a scientifically sound proof of concept which shows that the network indeed learns the latent graph representation, and that the results are not simply due to randomness.\n3. Impactful problem addressed: Capturing relational and compositional features of semantic contents means that these representations can prove to be better for use in common NLP tasks which right now rely on contextual encodings (like BERT encodings).\n\n**Weaknesses:**\n1. Authors claim that the contextual sentence embeddings lack semantic interpretability. Authors claim is that since different sets of \"hot\" symbols represent different topics (e.g. Science and Math), HSN provides semantic interpretability. However, can't we say similar things for continuous contextual features? It's highly possible that in a similar way \"Science And Math\" topic has high value for some specific continuous features, whereas \"Health\" has high values for some other set of features. In this way, the significance of the work (of using discrete symbols) is not clear.\n2. The authors make claims about compositionality, but they do not compare the compositionality using existing metrics which have been studied in the literature. https://aclanthology.org/2020.repl4nlp-1.22.pdf. Since compositionality is a major part of the paper, readers would be interested in how the compositionality compares to BERT.\n3. It is not clear why enforcing a semantic understanding for the (hidden representation) symbols significantly helps with the language modelling performance. At the end of the day, the hidden representation is an internal detail. More text is needed on this aspect.",
            "clarity,_quality,_novelty_and_reproducibility": "* Reproducibility - Unclear if the perplexity calculation in Table2 is using the same evaluation code for all models considered. It is possible that there are minor differences (like removal of stop words) in the evaluation technique which are causing the errors.\n* Clarity -  Sections of the paper could be more clear. It would be better if authors can provide a better qualitative analysis and a description of Figure1 which is aligned with the math in Section3. It would be better if it is made clear which component is producing which probability distribution.\n* Quality - The paper is well written with a good flow and very few grammatical issues.\n* Novelty - The work is novel in the sense that it develops a discrete symbol based approach for learning a representation for text, which allows for compositionality.",
            "summary_of_the_review": "The paper proposes a novel approach for representation learning which allows for compositionality. This approach leads to significant improvements in LM performance over other predominant models like GPT. The paper is mostly thorough in its analysis, but some of the claims regarding compositionality should be better justified using existing mathematical metrics comparing different representations. The paper could however explain qualitative aspects in a better manner. Apart from a few minor things, it is well written.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4132/Reviewer_5yft"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4132/Reviewer_5yft"
        ]
    },
    {
        "id": "RPUODCj0vg",
        "original": null,
        "number": 4,
        "cdate": 1667069600209,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667069600209,
        "tmdate": 1670991790358,
        "tddate": null,
        "forum": "KyxJ9Yfxo2",
        "replyto": "KyxJ9Yfxo2",
        "invitation": "ICLR.cc/2023/Conference/Paper4132/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a method for learning discrete sequential representations of sequence data. The model demonstrates surprisingly strong performance in language modeling, and preliminary results on commonsense reasoning seem promising. Analysis of the schema networks also demonstrates interpretability.",
            "strength_and_weaknesses": "*Strengths*\n\nThe proposed method has the potential to be more interpretable and controllable than single-vector representations and achieves strong performance on ELBO evaluations. The perplexity bound evaluations in this paper are standard in the related sentence VAE literature [1,2].\nBefore reading this paper, my prior belief was that such a latent variable model would not result in perplexity improvements. Those beliefs have now changed.\n\n*Weaknesses*\n\nThe paper is convincing but could be improved with further experiments on controllable text generation (following Li et. al 2020) and further development of the commonsense reasoning experiment.\n\nEdit: I also agree with reviewer 5yft's position that compositionality claims are not supported. Reducing the claim to a state-of-the-art sentence VAE with discrete representations is one solution.\n\n*Questions and comments*\n* To check my understanding, is the following alternative interpretation of the model correct: the model is a (discrete, first-order) hidden Markov model with an autoregressive emission model (GPT-2) and global sparsity distribution over the transition matrix (schema network)?\n* What is the reason for using LSTMs in the commonsense reasoning schema completion model (Appendix E)? BART or GPT2 might give better results.\n\n[1] Li, Chunyuan et. al. \u201cOptimus: Organizing Sentences via Pre-trained Modeling of a Latent Space.\u201d EMNLP (2020).\n\n[2] Kim, Yoon et al. \u201cSemi-Amortized Variational Autoencoders.\u201d ICML (2018).",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and the paper provides all details for reproducibility. The method is novel and the results are surprising.",
            "summary_of_the_review": "I recommend acceptance. The paper delivers on the surprising claim of a state-of-the-art sentence VAE with discrete latent representations.\n\nEdit on 12/5: After further discussion with the authors, the perplexity comparisons do not use a valid bound. Without updated and fair numbers, I am changing my score to reject until the evaluation numbers are finalized. I think this is interesting work and encourage the authors to (re-)submit with updated numbers and claims.\n\nEdit on 12/13: After receiving the updated evaluation metrics, I am changing my score back to accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4132/Reviewer_tUks"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4132/Reviewer_tUks"
        ]
    }
]