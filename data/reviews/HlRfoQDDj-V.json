[
    {
        "id": "Yj173rFvOE",
        "original": null,
        "number": 1,
        "cdate": 1666692597771,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692597771,
        "tmdate": 1666693031843,
        "tddate": null,
        "forum": "HlRfoQDDj-V",
        "replyto": "HlRfoQDDj-V",
        "invitation": "ICLR.cc/2023/Conference/Paper6282/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authors propose a new construction of validation set from training set without splitting the training data.  Authors construct the validation set in two steps:\n\n- Data augmentation: Apply different data augmentation techniques, e.g. for image data, authors use GridMask, Invert, Solarize and Brightness, to create data generating procedure from the source dataset. Use this augmentation as a new data generator to consturct an auxiliary data source.\n- Data selection from the auxiliary data: Authors select the data by choosing points that are close to the source data. Closeness is measured by using feature extractors, e.g. for image data authors use a pre-trained ResNet18 and text data authors use BERT.\n\nAuthors then empirically show that this validation procedure leads to higher accuracy and lower test-validation gap and smaller variance on different tasks.",
            "strength_and_weaknesses": "Strengths:\n\n- Motivation for constructing new dataset is clearly presented.\n- Method to construct a validation set is easy to understand\n\nWeaknesses:\n\n- Approach lacks theoretical basis. There is no reason to believe why the validation set is representative of the data generating distribution\n- The metrics used for evaluation are not convincing to show the validation set is representative of test distribution",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. \n\nQuality of the evaluation is lacking. Test-Validation gap metric measures the difference of risk of validation and test set. One could, in principle, construct a validation set by merely corrupting the labels by the \u201cright\u201d amount and obtain a validation set that performs good on all the metrics considered. It would not mean that we should use this procedure as a procedure for constructing the validation set. Original construction of validation set is useful mainly because of the iid assumption of the data.",
            "summary_of_the_review": "I recommend rejecting the paper. I think there\u2019s no reason to believe that the proximal set constructed is actually representative of the data generating distribution and thus should not be used as a validation set. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6282/Reviewer_cmsD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6282/Reviewer_cmsD"
        ]
    },
    {
        "id": "tOXn1j_fH0",
        "original": null,
        "number": 2,
        "cdate": 1667009170110,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667009170110,
        "tmdate": 1667111717667,
        "tddate": null,
        "forum": "HlRfoQDDj-V",
        "replyto": "HlRfoQDDj-V",
        "invitation": "ICLR.cc/2023/Conference/Paper6282/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is a new validation protocol for classification tasks to address the fundamental problem of conventional training/valiation approaches by separating validation data from training data, called proximal validation protocol (PVP). For achieving this, the authors present a method to construct proximal valiation set by employing data augmentation and a sampling strategy basd on angular distribution. \nTo show the robustness of the PVP, they introduce two evaluation metrics: variance and T-V gap. \nThe authors validate their protocol on three data domains such as text, image, and tabular data, comparing with the standard valiation protocols such as hold-out and k-fold CV. The results seem meaningful. \n ",
            "strength_and_weaknesses": "### Strength\n- The problem to be addressed is both practical and fundamental for ML community. When ML model is developed for real-world application in industry, it is quite challenging to make a validation set proper, fair, and effective. \n- The proposed idea looks simple but effective using data augmentation and angular-based sampling. \n- The experimental results present the efficacy of the proposed PVP against the conventional validation protocols in terms of three metrics.\n- The paper is easy to read and understand.\n\n### Weakness\nI agree that this paper addresses a significant, practical and fundamental problem, so I think this paper can contribute to ML community. But, there are some room for improvements, in particular, of experiments. \n- [Major] Considering that the method for proximal validation set leverages augmentation methods, classification tasks can benefit from this protocol. that is, multiple tasks such object detection, semantic segmentation in computer vision and QA in NLP might be diffult to be applied. I think the authors need to discuss or clarify this point. \n- [Major] I think more experiments on industrial or real-world classification setups are required because the authors emphasize their motivation is from the debt of ML in industry. For example,\n  - Most image recognition tasks leverage fine-tuning from pretrained models. Even if it is meaningful to the results on CIFAR-LT that the authors presented, more results on small-scale data such as Oxford-car/flower using ImageNet-pretrained will improve the contribution. Also, noisy label scenario experiments will be helpful.\n  - Why did the authors use Distill-BERT instead of BERT for a classification model?\n- [Major] More and intensive abation studies are requried for important hyperparameters to support their hypothesis because the authors do not present theoretical justification.\n  - The size or sampling ratio of proximal valiation set from auxiliary set. It is not clear how to decide the size. \n  - Because the authors argue that PVP is independent on model and optimizers, they need to present the evidence such as more experiments on one or two different models and an addtional optimizer. \n- [Major] The authors should present statistical significance results in Table 1 and 2 at least, considering that this paper address evaluation protocol. \n- [Minor] How is the performance under the below setup? This is also a alternative to address the lack of training data caused by validation set. Of coures, this cannot address the promblem of how to make a good validation set. \n   - make a training (T) and valiation (V) set by spliting validation data  from source training set\n   - make a new training set (T') merging training set (excluding valiation set) and the proximal validation set\n   - Train T' and validate (V)\n- [Minor] Tables 3 and 4 can be improved for better readability. For example, the authors use colored arrows (up-red, down-blue).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n- The address problem is clear and the method is also clear and simple.\n- The paper is easy to read\n\n### Novelty\n- Make a validation set using data augmentation and angular-based sampling seems novel and interesting.\n\n### Reproducibilty\n- Althought the authors did not describe Reproducibility section, they submitted the source code.\n\n",
            "summary_of_the_review": "Overall, I like this paper and agree the importance of the topics this paper addresses. However, some limitations exist in experiments. \nSo I decided my score as borderline . If the authors address my major concerns, I am willing to raise my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6282/Reviewer_sKMx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6282/Reviewer_sKMx"
        ]
    },
    {
        "id": "dqJYb7DJRdm",
        "original": null,
        "number": 3,
        "cdate": 1667200583140,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667200583140,
        "tmdate": 1667200583140,
        "tddate": null,
        "forum": "HlRfoQDDj-V",
        "replyto": "HlRfoQDDj-V",
        "invitation": "ICLR.cc/2023/Conference/Paper6282/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In order to maximize the usage of the training set, this paper provides an alternative validation set construction strategy. Specifically, they construct an auxiliary dataset by augmenting the original training set. Then, they pick examples from the constructed auxiliary dataset. The new strategy is evaluated on several datasets.",
            "strength_and_weaknesses": "In general, this paper looks interesting to me. However, I have concerns about the empirical evaluation and theoretical guarantees. \n\nEmpirically, the proposed strategy is evaluated on several tabular datasets, image datasets, and one text dataset. However, most of these datasets are very small, which can not fully support the claims. For example, the authors use CIFAR10-LT and CIFAR-100-LT, I am wondering why the authors don't use the original CIFAR dataset or ImageNet. A similar issue also happens in the text data. \n\nTheoretically, though the authors mention the theoretical analysis in the limitation. I still would like to see some theoretical results to support the claims in this paper, esp. when they only evaluate the proposed strategy on tiny datasets.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed strategy is somewhat novel to me and the results seem reproducible. ",
            "summary_of_the_review": "This paper proposes and interesting validation set construction strategy, but the current version lacks enough empirical and theoretical supports.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6282/Reviewer_wMBi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6282/Reviewer_wMBi"
        ]
    },
    {
        "id": "qbmKjVa3vTy",
        "original": null,
        "number": 4,
        "cdate": 1667409735940,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667409735940,
        "tmdate": 1667409735940,
        "tddate": null,
        "forum": "HlRfoQDDj-V",
        "replyto": "HlRfoQDDj-V",
        "invitation": "ICLR.cc/2023/Conference/Paper6282/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper identifies the problem of 'split tradeoff', where common train/validation splits for data result in either less data to train on or poorer model evaluation. Instead, the paper introduces Proximal Validation Protocol (PVP), which, instead of splitting the validation set from a train set, creates a validation set through a combination of: 1) data augmentation on the train data and 2) a devised sampling scheme to preserve the distribution of the augmented validation set to be close to the train set. PVP allows one to use the whole training data in order to train models while still having a useful validation set for model evaluation, thereby avoiding the split tradeoff problem. \n\nExperiments on three data modalities: tabular, image and natural language seem to demonstrate that PVP improves both the test performance of trained models, as well as reducing the gap between test and validation performance (i.e. validation is a better evaluation of model performance), whilst maintaining a competitive variance across different runs of their method (with different validation sets).",
            "strength_and_weaknesses": "Strengths:\n1. The paper tackles a clear issue (how best to choose a validation set) which has received little attention in the ML literature.\n2. The proposed scheme, using data augmentation to generate a validation set, appears to be novel and goes some way to solving the issue of 'split tradeoff' problem.\n\nWeaknesses:\n1. The paper seems to only use compare validation sets in terms of how well they evaluate the performance of a *fixed model* (the hyperparameters/architectures in section A.4 all appear to be fixed and pre-defined), whereas the purpose of validation sets for model evaluation in ML, to me, is to be able to select between different models/hyperparameters. I think it is very important for the contribution of this paper to demonstrate having better model evaluation via PVP's validation set does actually lead to better model selection e.g. better hyperparameter tuning. If this is already shown in the experiments then it should be made clearer/explicit.\n\n2. More generally, the paper is not presented clearly, at least to me. This is particularly the case in the experiments section, which is particularly problematic given that the paper focuses on showing the empirical strengths of PVP, rather than theoretical justification. This makes it hard to give a proper evaluation of the paper's contributions.\n\nTo start with, the 'F1' score is not properly defined, there is some alluding to it being the (top-1?) test accuracy in definition 3.1, which is what I take the definition to be (correct me if wrong). In any case, it is not clear what the values in table 3 and 4 represent: the word 'deterioration' alludes (but not in a clear way, and should be properly defined in any case) to the *difference* in each metric between using a proximal validation set vs. standard validation set (in table 3), and using random sampling vs. distributional-consistent sampling (in table 4). The reason why this lack of clarity is confusing is that if I understand correctly, there should be no difference between the F1 scores (test accuracies) in the settings presented in tables 3/4. This is because the choice of validation set shouldn't affect the test performance (for a fixed model trained on the same training data, see point 1 above), and hence that column should read as 0.\n\n3. I'd also like to see more thorough empirical evaluation, e.g. to what extent does PVP depend on the choice of data augmentation. What happens to performance if we do just say additive Gaussian noise as our data augmentation? There are many choices that one can make for the data augmentation: is the empirical performance of PVP robust to this choice? Likewise, there seems to be a non-standard softmax-like weighting using for the class center in equation 6, is there a reason for this? Can the authors provide an ablation to this choice of weighting (say to just a standard uniform weighting)?\n\n4. PVP seems to be designed for classification settings (particularly the distribution-preserving sampling), does it also extend to other settings say regression, or non-supervised learning regimes like self-supervised or unsupervised?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is discussed in weakness above. The propsed PVP idea appears original to me. \n\nThe paper is generally of decent quality, though the writing could be improved (suggestions below):\n\n- Some quite unusual language is used for a scientific paper e.g.  'plague', 'entertained', 'frustratingly', 'profoundly'. I don't think this contributes much to the message of the paper and would reword.\n- 'combo' -> 'combination'\n- deterioation -> deterioration\n- 'The split-relied framework and suffer the train/validation split tradeoff': doesn't make sense to me.\n\n",
            "summary_of_the_review": "I am recommending weak reject. The problem identified is important and the proposed solution appears novel, but the experiments are both unclear and (as far as I understand) lacking in thoroughness, as detailed in my weakness section. I'm happy to be corrected though.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6282/Reviewer_iGVu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6282/Reviewer_iGVu"
        ]
    }
]