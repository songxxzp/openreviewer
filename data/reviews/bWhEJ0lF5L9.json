[
    {
        "id": "y39I1lg11p",
        "original": null,
        "number": 1,
        "cdate": 1666466635787,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666466635787,
        "tmdate": 1666466635787,
        "tddate": null,
        "forum": "bWhEJ0lF5L9",
        "replyto": "bWhEJ0lF5L9",
        "invitation": "ICLR.cc/2023/Conference/Paper3435/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes to use reinforcement learning to design graph augmentation mechanism used in graph contrastive learning. The benefit of using RL is that the way of graph augmentation can be designed via a markov decision process where a long-term reward and the dependence between different parts of the graph are better captured. The paper adopts an actor-critic model. The actor part is to find the perturbation to enlarge the gap between the representations of different augmentations of the same graph. The critical model is to learn to measure the gap between the representations of different augmentations. Extensive experiments have been adopted to show the benefit. ",
            "strength_and_weaknesses": "Strengthes:\n1. To my knowledge, it is a new idea to learn to design graph augmentation via reinforcement learning. \n\n2. Extensive experiments have been done to justify the advantage of the proposed approach. \n\n3. The motivating example to show the issue of AD-GCL is interesting. \n\nWeaknesses:\n1. The biggest concern about this work is its training complexity. The adversarial training in AD-GCL is already hard. Learning rates should be tuned properly to make AD-GCL converge. If the adversarial part is replaced by reinforcement learning, I can see the pipeline is really hard to tune and the training complexity could be high. I have concern on the practical applicability of this method. I suggest the authors to compare the wall-clock time of the training dynamics of different methods (x-axis: training time, y-axis: validation performance). \n\nThere are quite a few notation issues which make me question the correctness of the algorithm.\n\n2. I do not understand when a critic model is needed. The idea to learn a critic model is because the long-term reward is unknown. However, it seem to me that current formulation has a clear reward defined in (8). Also, why is it needed to be learned? Also, I believe the notation in (8) is wrong: LHS depends on t while RHS sums over t. \n\n3. Also, in alg 1, the steps 12 and 13 are wrong, where both adopt gradient ascent, which I do not believe is correct. At least, 13 should be gradient descent. \n\n4. How softmax in (10) is computed? Do you only remove one edge at each step t as the normalization of softmax in (10) is across all edges? If so, when action space in (A) is not one hot encoding but denotes the set of edges. \n\n5. In 4.3, what is standard actor-critic model? It is a common sense to build an AC model for graph contrastive learning. Please clarify the standard AC model and explain your difference. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The language description is okay. However, the notations cause a lot of confusion as said above. \n\nQuality: There are a few technical issues as introduced above. Also, I have concern on the training complexity and the applicability of the proposed approach. \n\nNovelty: I think it is new to consider RL used in graph contrastive learning. \n\nReproducibility: I also have a few concerns on this aspect. As the proposed method is based on RL and many details behind the algorithm are not well explained. \n",
            "summary_of_the_review": "The work studies a new angle to learn graph augmentation for graph contrastive learning. However, due to many notation issues, incorrect technical descriptions, and my concerns on RL training, I cannot give a recommendation on this work. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3435/Reviewer_BDxC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3435/Reviewer_BDxC"
        ]
    },
    {
        "id": "zvzllQ-mYUJ",
        "original": null,
        "number": 2,
        "cdate": 1666562387834,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666562387834,
        "tmdate": 1666562387834,
        "tddate": null,
        "forum": "bWhEJ0lF5L9",
        "replyto": "bWhEJ0lF5L9",
        "invitation": "ICLR.cc/2023/Conference/Paper3435/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of graph data augmentation for improved effectiveness in graph contrastive learning. The key argument is that all existing graph augmentation methods ignore the connection between consecutively augmented graphs; and the authors proposed to use reinforcement learning to plan the trajectory of augmentations so as to find the most effective ones for contrastive learning. The RL-based optimization of graph augmentation step is embedded into the iterative process of standard graph contrastive learning procedure. Extensive experimentations on a wide collection of graph benchmark datasets were performed to confirm the effectiveness of the proposed solution. ",
            "strength_and_weaknesses": "Strength:\n+ The idea of using RL to find the most effective graph augmentations sounds reasonable. \n+ The empirical performance on a variety of evaluation datasets supports the effectiveness of the developed solution. \n\nWeakness:\n- The description of the paper is generally very hard to follow, with numerous and complex notations. This unfortunately prevents the review to completely follow the proposed solution.\n- There is no hint on the time complexity of this proposed solution. However, training of DeepRL is known to be expensive; and hence, the overhead of using RL to find the most effective graph augmentation should not be neglectable. \n- It is very unclear how the edge removing actions were realized in Eq (3) for the actor. I would assume $\\omega_i (t)$ is a $N_i \\times N_i$ vector, but how do we obtain its value (presumably binary values) from a batch normalization layer? In addition, as we are supposed to obtain two views at each round of RL, are we learning one RL agent but taking different states (i.e., previous view from the same branch) or two different RL agents? The description in the paper only discusses one branch. \n- Eq (5) almost shared the same structure as in Eq (3), but now it only outputs a scalar for each element in the Q-function. Eq (3) and (5) together really confused me.\n\nQuestions:\n- There is no explicitly discussion about the key insight in the proposed solution. Based on my understanding, the RL is to find two views that lead to the most different embeddings, which in turn help the graph encoder to better estimate the invariants in the given graph. However, if it is the same, why shall the RL agent care about the cumulative difference over T rounds? What matters should be the final T-th round difference?   \n- What\u2019s the running comparison between the proposed solution and baselines? \n- Eq (3) and (5) can be understood as learning node embeddings; why do not we use the learnt graph encoder for this purpose? Or in other words, can the learnt node embeddings from Eq (3) and (5) be useful for downstream tasks?\n- As the RL agent keeps removing edges to generate the final augmentation, it will encounter less and less choices in the later steps. Why should we restrict ourselves to edge removing only? Will allowing both edge removal and addition provide more flexibility to the RL-based solution? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The notations used in the paper are very complex and the description about the algorithm procedures is also hard to follow, which unfortunately caused difficulty in fully understanding the proposed solution. It is preferred if the authors could provide more explanations about the key insight of their solution, e.g., what the RL-based augmentation is looking for and why it is important for graph contrastive learning.   ",
            "summary_of_the_review": "The lack of clarity, both procedure-wise and purpose-wise, prevents the reviewer to fully appreciate the submission, though the extensive empirical evaluations suggested the effectiveness of the proposed solution. I expect more clarification from the authors during the discussion phase.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3435/Reviewer_bA3w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3435/Reviewer_bA3w"
        ]
    },
    {
        "id": "NrhF9Vp9pp",
        "original": null,
        "number": 3,
        "cdate": 1666644290790,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644290790,
        "tmdate": 1666644290790,
        "tddate": null,
        "forum": "bWhEJ0lF5L9",
        "replyto": "bWhEJ0lF5L9",
        "invitation": "ICLR.cc/2023/Conference/Paper3435/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Traditional graph data augmentation (GDA) strategies regard augmentations as independent processes. This paper considers the GDA as a Markov decision process and develops a reinforced method, named GA2C, to achieve continuous and learnable GDA. Experiments on 17 datasets verify the effectiveness of GA2C.",
            "strength_and_weaknesses": "Strength:  \n(1)\tThis paper reformulates the graph data augmentation as a Markov decision process, which can be considered as the fourth type of GDA methods.\n(2)\tThe authors provide a detail design of a reinforced augmentation framework to achieve the Markov-decision-process-based GDA. \n(3)\tExperiments for the graph classification task show a huge progress than other baselines on totally 17 datasets. Ablation studies, sensitivity analysis, and visualization prove the sufficiency of detail designs.\n\nWeakness:\n(1)\tThe idea of introducing Reward module for \u201cencouraging the two views to be different as far as possible\u201d may not be solid. If the pair of views is too different, it is impossible to encourage the model to generate similar embeddings for them (Tian, Yonglong, et al. 2020). The authors should guarantee that the two augmented views would not lose the essential topologies.\n(2)\tThe design of Model architecture in Section 3.2 hasn\u2019t been well explained. What is the Critic model, and what is the purpose to introduce this model?\n(3)\tExperiment results on node classification, link prediction, and sentiment analysis should be reported in tables with clear numbers. \n(4)\tMany typo errors appear in the paper. For example, references in the first paragraph of Section 1 miss brackets. \u201cbath normalization layer\u201d in the Section 3.2 should be corrected as \u201cbatch normalization layer\u201d. \n\nDiscussion:\n(1)\tWe notice a novel graph data augmentation method, called RGCL, achieve a new SOTA on graph classification tasks. Although this paper is formally published on July 2022, we still expect some discussion on it from this paper. (Li, Sihang, et al. \"Let invariant rationale discovery inspire graph contrastive learning.\" International Conference on Machine Learning. PMLR, 2022.)\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality: This paper is well organized and the writing is OK, but the model architecture should be detailed introduced and there are some minor typo-errors raised. \n\nNovelty: Considering graph data augmentation as a Markov decision process is novel.\n\nReproducibility: The author did NOT provide any code about their experiments.\n",
            "summary_of_the_review": "This paper illustrates a novel understanding of graph data augmentation for graph contrastive learning. Although the experiments show considerable progress in classification tasks, the statement that \u201cencouraging the two views to be different as far as possible\u201d may not be solid. Overall, this paper is marginal below the acceptance borderline.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3435/Reviewer_mgJu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3435/Reviewer_mgJu"
        ]
    },
    {
        "id": "kwOPnlc-7Yo",
        "original": null,
        "number": 4,
        "cdate": 1666765275210,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666765275210,
        "tmdate": 1666765275210,
        "tddate": null,
        "forum": "bWhEJ0lF5L9",
        "replyto": "bWhEJ0lF5L9",
        "invitation": "ICLR.cc/2023/Conference/Paper3435/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper mainly focuses on how to improve the current learnable graph data augmentation strategy (GDA). The authors are motivated by the empirical observation that the previous GDA method only works in an early stage of the training process. Therefore, the authors design a new data augmentation strategy using reinforcement learning. To verify the superiority of the proposed method GA2C, the authors show GA2C GA2C outperforms the SOTA GCL models on a series of downstream in 23 datasets.",
            "strength_and_weaknesses": "**Strength**\nThe author introduces reinforcement learning to learn graph augmentation, which is interesting and novel. To verify the effectiveness of the proposed method, the author conducts a comprehensive empirical study.\n\n**Weaknesses**\n1. The motivation for introducing reinforcement learning to refine the learnable GDA is vague. although the author presents a section with empirical examples to elaborate on that, the interpretation is less informative. \n\n- First, the key question that the author raises and hight in the intro. is how a good augmented view evolves to energize graph contrastive learning. using Fig.3 authors conclude this as the evolution of the previous GDA does not being energized in the training. What does the author mean by\"energize\"? if it refers to the augmentation not being activated during the training. what are the underline problems that lead to this and why non-energized versions are bad for GCL needs to be further verified and explained\n\n- Second, the author also criticizes the current GDA has bottlenecks. But doesn't explain what is the bottleneck.  Why does the author need to make GDA can evolve progressively?  The reviewer believes that current end-to-end learnable GDA can also evolve in training. Thus, what is the difference or significance of the proposed methods?\n\n- More importantly, since the problem of the current GDA is not well explained, the necessity of introducing RL is unclear. What does the RL fix in GDA?\n\n2. Why use the alignment of two augmented views as the reward? It seems unrelated to what the author claim (energize the GDA). The reviewer cannot see any connection and require more explanation on this choice. Is there any other choice for the reward to guide the learnable GDA?\n\n3. Some experiment are reported in AUC and some are reported in Acc. the reviewer think given the prediction and label it is easy to obtain both metrics. \n\n4. It is kind of weird to see the RL algorithm is robust to learning rate and regularization. Especially, the model is still working when the \\lambda is 7 which is an uncommon setting in most machine learning. Can authors give some explanation on why GA2C can achieve this rather than just give the reviewer results?\n\n5. The visualization seems to be unnecessary. It is still hard to say the proposed method is better via Fig.8. If the authors want to show GA2C produces better embedding for clustering. It is better to conduct clustering tasks and there are many metrics (e.g., NMI, ARI, and Acc.) to quantitatively verify the results.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good. It is easy to follow the paper.\n\nQuality: More justification is needed to better show the author's motivation and the necessity of the proposed method.\n\nNovelty: It is novel to introduce RL to GDA\n\nReproducibility: Not Sure about it.",
            "summary_of_the_review": "Overall, it is good try to introduce RL to GCL. However, more justification is needed to better show the author's motivation and the necessity of the proposed method. I would like to increase my evaluation if the author can resolve all of my concerns in the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3435/Reviewer_wbKR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3435/Reviewer_wbKR"
        ]
    }
]