[
    {
        "id": "SjcQvtDrTo",
        "original": null,
        "number": 1,
        "cdate": 1666794026412,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666794026412,
        "tmdate": 1666794026412,
        "tddate": null,
        "forum": "1tXzHPdOJGZ",
        "replyto": "1tXzHPdOJGZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1291/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies universal approximation of invariant functions, using fully convolutional networks with small filter size, small number of channels, but large depth. The theory is based on dynamical systems, using flow maps to construct deep residual networks approximating the target function, and follows previous work by Li et al, which studied such techniques on non-convolutional architectures.",
            "strength_and_weaknesses": "The paper makes interesting contributions to the line of work on approximation via dynamical systems, in particular, it extends previous work by Li et al to showing universal approximation results with convolutional networks with small filters, while Li et al. was restricted to full-width filters.\n\nWhile this contributions can be of interest, it is nonetheless quite marginal and incremental over previous work, and may not be sufficient by itself for publication.\n\nHere are some suggestions on what could strengthen the paper:\n- approximation and statistical rates: the architecture used here is presumably more parameter-efficient than in other works -- can this be used to establish improved statistical rates?\n\n- benefits of this architecture: are there specific function classes where such an architecture is particularly beneficial compared to full-width convolutions, or even fully-connected models?\n\n- better comparison to related work on approximation with convolutional networks, e.g. [this](https://arxiv.org/abs/1611.00740), [this](https://openreview.net/pdf?id=dgxFTxuJ50e), [this](https://arxiv.org/abs/1708.06633), [this](https://arxiv.org/abs/2106.08619), [this](https://arxiv.org/abs/2102.10032), [this](https://arxiv.org/abs/2112.05611) (the last three are for convolutional kernels / infinite-width networks)\n",
            "clarity,_quality,_novelty_and_reproducibility": "ok",
            "summary_of_the_review": "Interesting result, but not useful enough by itself.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1291/Reviewer_iJgd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1291/Reviewer_iJgd"
        ]
    },
    {
        "id": "ijnIumF0_yU",
        "original": null,
        "number": 2,
        "cdate": 1667183708887,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667183708887,
        "tmdate": 1667183708887,
        "tddate": null,
        "forum": "1tXzHPdOJGZ",
        "replyto": "1tXzHPdOJGZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1291/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Universal approximation results in deep learning stated that certain class of neural networks can approximate any function up to arbitrary low error. These results were originally proven for sufficiently wide networks but have been extended in 2 directions:\n1. Proving them for small width networks (with arbitrary depth). For example, the work of Lu et al. 2017.\n2. Proving them for invariant (equivariant) functions and for corresponding  invariant (equivariant) networks. For example, the work of Yarotsky 2018.\n\nThis paper combines these two extensions for shift invariant (equivariant) functions and FCNNs (or residual FCNNs). They prove that FCNNs of constant width can approximate any shift invariant (equivariant) function. They further show that the constant width they show this result for is tight.\n\n\n",
            "strength_and_weaknesses": "The authors do not provide intuition for the universal approximation result in the main body of the paper or in the Appendix. The authors should also describe what are the new ideas that go in the proof and what techniques are borrowed from earlier works, this will make the proof easier to verify. For example, in appendix B.5 (Proof of proposition 3) the authors use the approach of dividing the input space into small cubes which has been used in earlier works such as Lu et al. 2017. Same holds for the \u2018coordinate zooming function\u2019 which was used in Li et al. 2022.\n\n\n\nOther comments:\n1. \u2018Deep, rather than wide, neural networks are thus a promising solution to this dilemma, using layer composition in the depth direction to build complexity, whereas each layer can be kept sufficiently simple to respect symmetry constraints.\u2019 I do not see why wideness of the layers would have any effect on the symmetry constraints.",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned earlier the paper does not provide intuition for the proof and does not compare the proof techniques with related works. Other than that the paper is well-written.",
            "summary_of_the_review": "As the authors do not provide a comparison of techniques with prior works and a description of new ideas that go into their main result I cannot recommend acceptance.\n\nDue to the lack of intuition provided in the paper and my unfamiliarity with the techniques used I was unable to verify the correctness of the proof (given in the appendix) of the main universal approximation result. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1291/Reviewer_RUpT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1291/Reviewer_RUpT"
        ]
    },
    {
        "id": "0c2vBkmmJ8",
        "original": null,
        "number": 3,
        "cdate": 1667496831346,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667496831346,
        "tmdate": 1669049696383,
        "tddate": null,
        "forum": "1tXzHPdOJGZ",
        "replyto": "1tXzHPdOJGZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1291/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies approximation power of (residual) vanilla convolutional networks. The authors show that any translational equivalent/invariant mapping/functions can approximated by such networks, in the $L^p$ sense, if \n- the number of channels is at least 2. \n- the kernel size is at least 2 (in each coordinates)\n- the network is sufficiently deep (essentially, infinitely many of layers)   \n\nThe authors approach this problem from a dynamical viewpoint by viewing CNNs as a discretization of a certain equivalent flow. Although the results is not surprising, the proof is quite technical and non-trivial owing to the number of channels is small (>=2.) ",
            "strength_and_weaknesses": "# Strength \n\n- The overall mathematical clarity of the paper is far above typical ML conference papers. \n- Both the results and proofs are non-trivial. \n\n# Weaknesses: \n- the results are not surprising (as what was expected). The problem can be much simpler if allowing more channels (shallower networks) vs. the current setup ( == narrow + deeper). \n- The contribution mainly lies in theory and is not very relevant to practice, as it essentially requires the depth of the network to be infinite. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "A few minor remarks.\n\n1. I would like to know more about the technical difference between the current paper (with group equivalent) and Li et al., 2022a;b. Is it possible to `reformulate` the current problem in the quotient space $\\mathbb R^k /\\mathcal T$, where $\\mathcal T$ is the group generated by translations and consider flows defined in this quotient space? \n2. Another related question. Is the problem much simpler if the kernel can have full support? Is it hard to show that any one-layer convolution kernel (with full support) can be approximated multi-layer with Conv + activation with kernel size 2?  \n3. Can the problem be reduced to $\\mathbb R$ from $\\mathbb R^k$, namely, is the problem essentially a 1-D problem? \n\n",
            "summary_of_the_review": "update\n\nThanks a lot for the detailed and insightful reply. I will keep my score. \n\n------\n\nAlthough the results of the paper is not surprising, it resolves an important theoretical question regarding approximation power of shallow-and-deep CNNs. I think the result and its proof technique are valuable to the ICLR community.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1291/Reviewer_gqo7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1291/Reviewer_gqo7"
        ]
    }
]