[
    {
        "id": "X5sV81zbIJ",
        "original": null,
        "number": 1,
        "cdate": 1666540821584,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666540821584,
        "tmdate": 1666540821584,
        "tddate": null,
        "forum": "mdECGh-qlK",
        "replyto": "mdECGh-qlK",
        "invitation": "ICLR.cc/2023/Conference/Paper3255/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a direct constraint optimization method to solve optimal transport maps using the original Monge's formulation. Three differential algorithms are studied:  the Langrangian multiplier method, the augmented Lagrangian method, and the alternating direction method of multipliers (ADMM). The proposed method improve the accuracy of learned optimal transport maps on high dimensional benchmarks, reduce the regularization effects and better learn the target distributions at a lower transport cost.\n\nThe contributions are \n1. integrate three constraint optimization algorithms including the Standard Lagrangian (SL), the Augmented Lagrangian method (AL) and the Alternating Direction Method of Multipliers (ADMM) with neural networks to solve the Monge problem of optimal transport\nwith provable guarantees (Theorem 1-3).\n2. show that the proposed  method is able to find an accurate optimal transport map between Gaussian distributions, both theoretically (Theorem 2) and experimentally. Moreover, the method is applied to WGAN and show that our method can find a generative map with lower transport cost while not sacrificing the quality of outputs.\n3. Three algorithms are compared and it is found that  the SL algorithm introduces errors but is simple and easy to implement, while AL and ADMM algorithms can find exact results and are more robust, and ADMM gives a lower transport cost in general.",
            "strength_and_weaknesses": "The strengths of the current work \n1. Propose novel algorithm to solve optimal transport map method directly using Monge's formulation with constraint optimization.\n2. Three different optimization algorithms are formulated, evaluated and compared.\n3. Theoretic proofs for the existence of the solution (Theorem 1 and Theorem 2), and the convergence of ADMM (Theorem 3)\nThe mathematical formulation and proofs are clean, rigorous and thorough.\n\nThe weakness of the current work is that the experiments are conducted on simple datasets (MNIST), it is desirable to test the theoretic results on real world datasets. Another weakness is that by using Yau and Gu's geometric variational algorithm, the problem of finding Monge Optimal transport map is reduced to a convex optimization problem, so the theoretic setup is easier and the optimization can be carried out using Newton's method. It is helpful to compare the proposed method with this convex optimization approach. Another weakness is that the proposed method can not find the singularity of the OT map precisely, since it use soft constraints (mollifiers) which will obscure the boundary of the supports.",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of using constraint optimization to solve optimal transport problem has been existed for a well. By adding the penalty terms to the objective function with Lagrange multiplier is conventional. The ADMM algorithm is relatively novel than the other two methods. The theoretic proofs are convincing, especially theorem 3 guarantees the convergence of the proposed algorithm, this rigorous result has high practical value as well. But the scope of theorem 2 is limited, only for normal distributions. The paper is well written, all the mathematical formulations, key concepts, proofs are very clean and succinct. The algorithms are given in details and the experimental results are easy to reproduce.",
            "summary_of_the_review": "This work proposes a direct constraint optimization method to solve optimal transport maps using the original Monge's formulation. Three differential algorithms are studied:  the Langrangian multiplier method, the augmented Lagrangian method, and the alternating direction method of multipliers (ADMM). The proposed method improve the accuracy of learned optimal transport maps on high dimensional benchmarks, reduce the regularization effects and better learn the target distributions at a lower transport cost.\n\nThe paper is well written, the mathematical formulations are rigorous and elegant, the theoretic proofs are convincing. The preliminary numerical experiments support the claims. The work can be further improved : enlarge the scope of theorem 2, add more real world testing experiments. Furthermore, comparisons with other approaches which also solve the problem directly, such as the geometric variational method by Yau et al, which reduce the problem to convex optimization. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "This research is fundamental and mainly focuses on theoretical aspects.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3255/Reviewer_rwsz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3255/Reviewer_rwsz"
        ]
    },
    {
        "id": "dfbmPD2cth6",
        "original": null,
        "number": 2,
        "cdate": 1666622244108,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622244108,
        "tmdate": 1666622244108,
        "tddate": null,
        "forum": "mdECGh-qlK",
        "replyto": "mdECGh-qlK",
        "invitation": "ICLR.cc/2023/Conference/Paper3255/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Combining traditional numerical methods for solving the Monge equation with deep learning, thus compensating for the limitations of numerical methods for high-dimensional data and the need for deep learning networks based on Kantorovich duality and complex network structures.",
            "strength_and_weaknesses": "Strength:\n(1) The article has sufficient theoretical derivations and proofs for the three algorithms. \nWeaknesses:\n(1)  Although it is illustrated that traditional numerical methods are not applicable to high-dimensional problems, relevant comparative experiments and graphs are missing.\n(2) Lack of comparative experiments with existing learning-based methods. ",
            "clarity,_quality,_novelty_and_reproducibility": "In this paper, the authors combine traditional numerical methods with deep learning methods so as to learn the optimal transmission on a high-dimensional benchmark. The theoretical derivation is relatively sound, but the relevant experiments are missing.",
            "summary_of_the_review": "In general the article would be more valuable if some comparative experiments with traditional numerical algorithms were added, as well as some visual schematics.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3255/Reviewer_ZQmE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3255/Reviewer_ZQmE"
        ]
    },
    {
        "id": "0EG_XQuO8jb",
        "original": null,
        "number": 3,
        "cdate": 1666640355305,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640355305,
        "tmdate": 1666640355305,
        "tddate": null,
        "forum": "mdECGh-qlK",
        "replyto": "mdECGh-qlK",
        "invitation": "ICLR.cc/2023/Conference/Paper3255/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper is considering the problem of learning the optimal transport maps with quadratic cost. In order to do so, they propose to apply a constrained optimization algorithm directly on the Monge formulation. In particular, the marginal constraint T#\\mu = \\nu is formulated as a penalty that involves a distance between T#\\mu and \\nu.  Then, constrained optimization algorithms, such as ADMM, are proposed to obtain the parameters of the map T. The proposed algorithm is explained and illustrated on Gaussian, mixture of Gaussians, and MNISST dataset.  ",
            "strength_and_weaknesses": "Strength:\n- the paper is well-written\n- the discussion on the Gaussian example is very helpful in understanding the proposed approach, its advantages and disadvantages\n\nWeakness: \n- the paper is motivated by shortcomings of the existing approaches in terms of special neural network structure or regularization, but the proposed approach seems to suffer from the same issues. \n- the most challenging step of the approach is how to compute the penalty and perform the ADMM penalty step. It is not clear how to compute the KL divergence or any other type of distance (W1, IPM,..) in general. Minimizing the penalty term is the subject of all GAN papers and the way to do it exactly suffers from the issues that the paper is trying to avoid. \n   ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the results seem reproducible. ",
            "summary_of_the_review": "Although it is a nicely written paper, the proposed approach suffers from the same issues that the paper is trying to avoid. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3255/Reviewer_5X5o"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3255/Reviewer_5X5o"
        ]
    },
    {
        "id": "6489DmaqxkT",
        "original": null,
        "number": 4,
        "cdate": 1667077643139,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667077643139,
        "tmdate": 1667079426944,
        "tddate": null,
        "forum": "mdECGh-qlK",
        "replyto": "mdECGh-qlK",
        "invitation": "ICLR.cc/2023/Conference/Paper3255/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose to directly solve the Monge's optimal transport problem under $L^2$ cost function, where the existence and uniqueness of the solution is guaranteed. Specifically, they use a neural network to parameterize the OT map, and try to solve the KL-divergence regularized OT problem proposed in equation (3). Furthermore, different Lagrangian multiplier methods are used to handle the constraints. Finally, the problems are solved by ADMM method. Some toy examples are used to illustrate the performance of the propose method.",
            "strength_and_weaknesses": "Strength: \n- It is interesting to solve the KL regularized OT problem. Combining them together should give us more understanding of the both problems.\n\nWeakness:\n- The method can only solve continuous OT problem supported on the whole Euclidean space, or it will encounter severe convergence problem since the KL divergence can easily be $\\infty$.\n- If the target distribution is discrete like MNIST, how to compute the KL divergence?\n- For the Alg 1,2,3, it is unclear how to conduct the optimization, by sampling?\n- From the current experimental results, either Fig. 4 or Fig. 5, it is hard to say that the proposed methods work well.\n- No implement details.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarify: The paper is generally not very clear and hard to follow. It makes a lot of hidden assumptions that need to be proven to be true. There is also no implement details, which make it difficult to evaluate its real performance and hard to reproduce.",
            "summary_of_the_review": "Generally, the authors need to prove the hidden assumptions and do more experiments to show the performance. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3255/Reviewer_3wmi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3255/Reviewer_3wmi"
        ]
    }
]