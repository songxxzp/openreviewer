[
    {
        "id": "by2hOdJcnFt",
        "original": null,
        "number": 1,
        "cdate": 1666479700117,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666479700117,
        "tmdate": 1666479700117,
        "tddate": null,
        "forum": "9_VrvV7d-FK",
        "replyto": "9_VrvV7d-FK",
        "invitation": "ICLR.cc/2023/Conference/Paper5025/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considered the problem of unsupervised test adaptation under covariate shift to achieve good fairness-accuracy trade-offs when a small amount of unlabeled data is available. The authors proposed a new weighted entropy based loss function to account for covariate shift, in combination with a representation matching term to address fairness. A set of empirical experiments were conducted on four popular datasets to show the performance of their proposed method in achieving non trivial accuracy-fairness trade-offs.\n",
            "strength_and_weaknesses": "Overall this is a well-written paper with solid theoretical results and comprehensive empirical findings. \n\nStrength:\n1. Novel formulation to address covariate shift along with fairness\n2. Solid theoretical results to support the formulation\n3. Comprehensive empirical results to further support the formulation\n4. Thorough literature review for relevant works\n\nWeakness:\nI have several questions that I hope the authors can further clarify.\n1. The new proposed method utilizes the test set information to address covariate shift. In some scenarios the test set is not known a priori. I am wondering in the experiments section, whether the baselines that the authors compared to, use the test set information or not. If they did not use the test set information, can we call it a fair comparison?\n2. Furthermore, when considering the baseline choices, have you considered any of the methods mentioned in Section 2: Fairness under Distribution shift? I feel like those methods are more relevant to what you studied in this paper. I am interested in seeing the performance comparison with those methods if possible.\n3. The representation matching bears a similar spirit with the paper: Shalit, U., Johansson, F.D. and Sontag, D., 2017, July. Estimating individual treatment effect: generalization bounds and algorithms. In International Conference on Machine Learning (pp. 3076-3085). PMLR, where they also tried to minimize a distributional distance between representations in two groups. I am curious why you chose the Wasserstein metric, even though Theorem 1 suggests the total variation distance. \n4. In Section 5.1, why do we maximize (instead of minimize) over \\theta in Eq. (6)?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was written in a clear way for others to reproduce. The formulation is also novel in the sense that they combine a weighted entropy objective (to address covariate shift) with representation matching (to address fairness). Overall I am satisfied with the quality of the paper.",
            "summary_of_the_review": "As mentioned above, I saw several strengths of this paper:\n\n1. Novel formulation to address covariate shift along with fairness\n2. Solid theoretical results to support the formulation\n3. Comprehensive empirical results to further support the formulation\n4. Thorough literature review for relevant works\n\nI also like the connection they made between a weighted entropy and importance sampling. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5025/Reviewer_iDgW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5025/Reviewer_iDgW"
        ]
    },
    {
        "id": "1wuukfFqxX",
        "original": null,
        "number": 2,
        "cdate": 1666684880612,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684880612,
        "tmdate": 1670534836641,
        "tddate": null,
        "forum": "9_VrvV7d-FK",
        "replyto": "9_VrvV7d-FK",
        "invitation": "ICLR.cc/2023/Conference/Paper5025/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the fair learning problem under the covariate shift. The main contribution is the weighted entropy loss component of the regularized objective function. In the experiments, the proposed method achieves a better tradeoff between accuracy and fairness. ",
            "strength_and_weaknesses": "Strength\n\nFairness under the covariate shift is an important under-investigated topic and very relevant to the community.\n\nThe empirical results are significant.\n\nWeakness\n\nThe lack of the target labels means the proposed method cannot be applied to all the fairness metrics that requires the knowledge of the true labels. So the metric considered in the paper is only an accuracy disparity. This will significantly affect the applicability of the method. Also, in the experimental results, I think the x-axis should be changed as the metric is not the real equalized odds. \n\nThe main contribution of the proposed formulation is actually in the first term, which is the covariate shift correction term, not the fairness regularization term. So it seems the resulting formulation is a weighted entropy loss for covariate shift correction (ratio estimation) in a mini-max end-to-end training. The novelty seems to be limited. For example, without fairness regularization, we can also apply the first term to learning problems under covariate shift. The application of weighted entropy under covariate shift is also not new. How are the fairness constraints connected to the covariate shift?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clearly written. Codes are not attached but there are enough details.\n\nThe technical sections are solid however the novelty is limited. Also how fairness is really related to the new weighted entropy is not clear.",
            "summary_of_the_review": "I am leaning towards rejection as the novel contribution to fair learning is limited, even though the weighted entropy is an effective method under the covariate shift. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5025/Reviewer_DiLH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5025/Reviewer_DiLH"
        ]
    },
    {
        "id": "0rUXLa8hZ-",
        "original": null,
        "number": 3,
        "cdate": 1666960260884,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666960260884,
        "tmdate": 1666980350942,
        "tddate": null,
        "forum": "9_VrvV7d-FK",
        "replyto": "9_VrvV7d-FK",
        "invitation": "ICLR.cc/2023/Conference/Paper5025/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors study the fairness issue under the covariate shift setting. To address this problem, they first show that the previous bound in (Zhao & Gordan (2019)) can not handle the asymmetric covariate shift setting. Afterward, they propose a novel bound on the performance in the test distribution. They further develop a min-max optimization framework to calculate the proposed bound. Finally, they conduct experiments on several datasets to prove the effectiveness of their method.",
            "strength_and_weaknesses": "The authors study an important question, which generalizes standard fairness issues to distribution shift scenarios (in particular, covariate shift). I value the importance of this setting. However, I think the methods have several unclear parts and the experiments should also be improved.\n\nFor the method part:\n1. The first main issue is the unclear fairness notion throughout the paper. Equalized odds and accuracy parity correspond to two different fairness targets and there are different methods to deal with them. The authors should highlight the main fairness notion they target and show why their method could guarantee or approximate the fairness notion.\n2. Theorem 2 needs further explanation. Firstly, the assumption on the constant $\\epsilon$ is unclear. The value of $\\epsilon$ depends on the model parameter and I think it can easily tend to infinity if a model outputs a small $P(\\hat{Y}=y|X)$. Secondly, the superiority of the proposed theorem compared with Theorem 1 (Zhao & Gordon (2019)) is unclear. As I mentioned before, I think the value of $\\epsilon$ may be large in general settings and the second term in the RHS of Theorem 2 may also be large, making the bound loose. The authors should discuss how Theorem 2 could guarantee handling the asymmetric covariate shift setting.\n3. The comparison with density ratio estimation methods is unclear, such as the methods proposed in [1]. The authors claim that \"the typical way of density estimation in high dimensions is particularly hard\". However, Equation (5) requires the estimation of density ratio only and several methods could deal with the problem (such as the KLIEP and LSIF loss mentioned in [1]). As a result, it would be better if the authors can compare their algorithm with these methods.\n4. I am unclear why we need the max step to optimize the density ratio function. The $F_w(X)$ should be the density ratio between the training and test distribution and the authors are encouraged to demonstrate why the max step could lead to the estimation of the true density ratio.\n\nFor the experimental part:\n1. As mentioned before, the authors should compare the methods that use typical density ratio estimation methods [1] to estimate the function $F_w(X)$ instead of the max step.\n2. The fairness-accuracy trade-off curves are encouraged to be plotted. See examples in [2].\n\nOther minor typos:\n1. Throughout the paper: upto -> up to\n2. Second paragraph in the introduction: Similarly, (missing a comma here)\n3. The cross-entropy loss in Equation (1) is wrong. The equations miss the $Y_i$ term.\n\n[1] Menon, Aditya, and Cheng Soon Ong. \"Linking losses for density ratio and class-probability estimation.\" International Conference on Machine Learning. PMLR, 2016.\n\n[2] Agarwal, Alekh, et al. \"A reductions approach to fair classification.\" International Conference on Machine Learning. PMLR, 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "Details are mentioned in the Strength And Weaknesses section.",
            "summary_of_the_review": "Although the authors study an important problem, the methods have several unclear parts and the experiments should also be improved. As a result, I vote for the rejection in this round.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5025/Reviewer_HTwH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5025/Reviewer_HTwH"
        ]
    }
]