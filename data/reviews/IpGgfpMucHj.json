[
    {
        "id": "K9jDGR13gym",
        "original": null,
        "number": 1,
        "cdate": 1666572192366,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666572192366,
        "tmdate": 1666572192366,
        "tddate": null,
        "forum": "IpGgfpMucHj",
        "replyto": "IpGgfpMucHj",
        "invitation": "ICLR.cc/2023/Conference/Paper1689/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose the use of image based features coupled with 3D points to train a network to perform various 3D perception tasks, such as classification, retrieval or segmentation, on point clouds.  The authors demonstrate state of the art performance on synthetic several datasets.",
            "strength_and_weaknesses": "The paper is well written. The approach is well evaluated and the results are strong. The proposed idea is a well-executed derivation of existing art.\n\nWeaknesses:\n\n1) The novelty of the paper is limited. Essentially, the authors create am image based viewing track of a 3D point (common in SFM and SLAM) and modify pointNet to work in a higher dimensionality space.\n\n2) There is too much effort from the authors to establish the term Voint (It is not clear if there is a fundamental difference between max pooling and convolution with VointMax and VointConv other than applying the operation on higher dimension. )\n\n3) The method is assuming that the render is able to project 3D points to 2D with perfect occlusion detection. Have the authors considered how VointCloud can be extended to more complex point-clouds where mesh reconstruction from a renderer may not guarantee high occlusion detection accuracy?\n\n4) How efficient is VointCloud and VointNet in terms of computational and space complexity? \nMinor comments \n1) Un-projection: The correct term is back-projection. \n2) Eq2 M,N are undifined symbols \n3) Table 3 ShapeNet ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and all ideas are well explained \nThe novelty of the paper is limited but the method is well executed \nThe authors provide their code for reproducibility ",
            "summary_of_the_review": "Although I have some concerns about the overall terminology and limited novelty of the paper, I think that the method is well executed and the results support the direction. I recommend the paper for publication. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1689/Reviewer_aJdJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1689/Reviewer_aJdJ"
        ]
    },
    {
        "id": "aT0dXpfu-qO",
        "original": null,
        "number": 2,
        "cdate": 1666677181807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677181807,
        "tmdate": 1666677181807,
        "tddate": null,
        "forum": "IpGgfpMucHj",
        "replyto": "IpGgfpMucHj",
        "invitation": "ICLR.cc/2023/Conference/Paper1689/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a novel point cloud backbone. The backbone renders point clouds into 2D multi-view renderings and extracts 2D features with 2D backbones. These features are then unprojected back onto 3D point clouds where they are fused into final point features. \nThe backbone is very compact and efficient.\nThe authors have performed extensive experiments including 3D point cloud classification, shape retrieval, and 3D part segmentation, and shown good performance on all benchmarks.",
            "strength_and_weaknesses": "Strength\n- Though counter-intuitive, the idea of extracting features in 2D and fusing in 3D is interesting, clean, and neat. The method achieves very impressive performance on all tasks while keeping a compact model size and fast inference efficiency.\n- Experiments have shown that the proposed backbone is less prone to partial observation and change of orientation.\n\nWeaknesses\n- The authors have not studied how noise will affect the backbone performance. It could be much harder to extract useful information in 2D than in 3D for noisy point clouds. This could potentially be a case where 3D methods take advantage.\n- The authors seem to deliberately use different view numbers (8 vs. 12) and sampling methods (random vs. spherical) during training and testing. This is not explained and studied in the paper.\n- The authors should include more details about the renderer e.g. speed, color?, point size, etc.\n\n\n- If it is possible to render point clouds from different distances (say 0.5, 1.0, and 2.0) and use them as multi-scale features? Many 2D and 3D backbones employ similar ideas explicitly. Any discussion will be appreciated.\n\nMinor\n- Table 3: SahpeNet -> ShapeNet\n- Figure 14: Distnace -> Distance\n- Figure 12-15 appear to be very large and not nicely rendered (14,15)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clear and well-written, with some typos to fix. The idea is interesting and novel. The authors have provided their code.",
            "summary_of_the_review": "I'm leaning toward acceptance due to the impressive performance and efficiency of the method. Adding more discussions and completing the details suggested above could further strengthen my recommendation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1689/Reviewer_14aj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1689/Reviewer_14aj"
        ]
    },
    {
        "id": "drRMk-gq0Tq",
        "original": null,
        "number": 3,
        "cdate": 1666745651898,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666745651898,
        "tmdate": 1666745651898,
        "tddate": null,
        "forum": "IpGgfpMucHj",
        "replyto": "IpGgfpMucHj",
        "invitation": "ICLR.cc/2023/Conference/Paper1689/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a multi-view 3D point cloud representation called Voint cloud. The multi-view features are extracted from 2D networks and then aggregated into 3D point clouds using correspondence. Visibility of the points in each view is also attached. The proposed method reaches state-of-the-art-performance on several 3D understanding tasks, including 3D shape classification, retrieval, and robust part segmentation.",
            "strength_and_weaknesses": "Strength:\n- The idea of extracting low-level features from multi-view 2D images first and then aggregating back into 3D is novel. This takes advantage of mature 2D networks and still maintain the 3D geometry.\n- The paper is well-written with extensive experiments and ablation studies.\n\nWeakness:\nI would like to see latency analysis of each component of proposed method and also in comparison with other methods. Looks like unprojection requires a lot of irregular memory access.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, idea is novel.\n\nTypo: table 3 SahpeNet->ShapeNet",
            "summary_of_the_review": "The idea is somewhat novel and results show the effectiveness of proposed method. Incremental change compared to previous method. It'd be nice to see the latency analysis.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1689/Reviewer_LsWo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1689/Reviewer_LsWo"
        ]
    },
    {
        "id": "uFgRgVX6POE",
        "original": null,
        "number": 4,
        "cdate": 1667180805923,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667180805923,
        "tmdate": 1667180805923,
        "tddate": null,
        "forum": "IpGgfpMucHj",
        "replyto": "IpGgfpMucHj",
        "invitation": "ICLR.cc/2023/Conference/Paper1689/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a novel 3D representation called \"Voint Cloud.\" The authors argue against existing heuristic-based aggregation, which could misleadingly incorporate representation from an arbitrary viewpoint. Then they attempted to address the research question of how to aggregate the per-view representation (in a multi-view setting) for a given 3D point cloud. In the process, they proposed their 3D representation, Voint, which is a view-dependent representation associated with each 3D point in a multi-view representation learning setting. The paper derived and described how their Voint Cloud (the proposed multi-view point cloud) could be used to learn representation end-to-end fashion to solve various 3D scene understanding tasks. Experimentally the authors demonstrated the usefulness of the proposed representation on three different 3D understanding tasks: i) 3D point cloud classification (on ScanObjectNN dataset), ii) 3D shape retrieval (on ScanNet Core55), and iii) 3D part segmentation (on ScanNet). The proposed method attained state-of-the-art performance in 3D point cloud classification and 3D object retrieval.",
            "strength_and_weaknesses": "Strengths:\n+ New 3D representation which seamlessly integrates view-dependent representation for each 3D point\n+ Proposed new pooling and convolution operation on the Voint Cloud leading to a new deep neural network called VointNet\n+ state-of-the-art performance on 3D point cloud classification and 3D shape retrieval tasks using the proposed Voint Cloud representation\n\nWeaknesses:\n+ Generalizability to outdoor 3D scene understanding task: All experiments were done on indoor 3D scene understanding tasks (ScanObjectNN, ScanNet). To make a more generalizable claim for Voint Cloud representation, at least one experiment should have been done on an outdoor 3D scene understanding task, e.g., the KITTI dataset. Is there any specific reason for not doing that? \n\n\n+ 3D part segmentation: The proposed method achieved better results against other baselines on a more challenging setup (rotated); however, CurveNet still retained the best performance 84.9 % mIoU on the \"Unrotated setup.\" The paper did not discuss what could be the possible reason for this slight discrepancy. \u00a0The ablation study in Table 6 shows different backbones and different variations of VointNet. It is unclear which result from that table corresponds to the one reported in Table 4. I also acknowledge the fact that the paper mentions that unless otherwise stated, the default method for VointNet is shared MLP. \n\n+ Backbone in 3D part segmentation: The backbones used in the 3D part segmentation task are widely used methods (FCN, DeepLabv3). The newer backbone might be more effective and possibly could improve the results. Please refer to the model \"HRNet-W48\" which was used in the following work: \"MSeg: A Composite Dataset for Multi-domain Semantic Segmentation - Lambert, John, et al. CVPR'20\"\n\n+ Number of views: The authors showed an ablation study for the effect of the number of views on the performance of the Voint Cloud. However, it was shown only for the 3D part segmentation task. How about the 3D point cloud classification and 3D object retrieval tasks? \n\n+ 3D point cloud classification and 3D shape retrieval experiment: The proposed Voint Cloud-based representation achieved state-of-the-art results. It would be interesting to show the comparison against methods with deformable convolution on Point Cloud, such as \n\"KPConv: Flexible and Deformable Convolution for Point Clouds - Hugues Thomas et al. ICCV'19\"\n\n\n+ VointNet variations: The paper shows three different variations of VointNet, the simplest of which is a shared MLP-based architecture. Figure 6 (in the appendix) could be included in the main paper to describe the method clearly. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written for the most part. I would recommend incorporating one item or two from the appendix to the main paper.\nIt brings a new idea -- new 3D representation VointCloud and also has methodological novelty. The experiments seem to be reproducible.\n\n",
            "summary_of_the_review": "The paper has some weaknesses that have been discussed in the limitation section; however, it introduced a new idea (novel 3D representation) and a new methodology (VointNet and its corresponding operations).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1689/Reviewer_KkFB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1689/Reviewer_KkFB"
        ]
    }
]