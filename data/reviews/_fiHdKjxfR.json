[
    {
        "id": "RbCnbzqNmZ6",
        "original": null,
        "number": 1,
        "cdate": 1666397670709,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666397670709,
        "tmdate": 1666397670709,
        "tddate": null,
        "forum": "_fiHdKjxfR",
        "replyto": "_fiHdKjxfR",
        "invitation": "ICLR.cc/2023/Conference/Paper3112/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a learned measure of representation similarity, ContraSim, as opposed to analytical ones like centered kernel alignment (CKA) and canonical correlation analysis (CCA). ContraSim is specific to a task (training distribution), and uses a simple MLP to project the representation to a different subspace. Experiments show that contrastive loss is a key component to achieving good results. \n\nPaper also introduces two additional benchmarks based on multi-lingual similarity and image-caption similarity.  ContraSim performs well on these tasks compared to baselines.",
            "strength_and_weaknesses": "Strengths:\n- Paper is well-written, easy to follow. \n- Introduction of two benchmarks adds a different perspective to this research area compared to literature\n- Proposed method outperforms baselines by a significant margin\n\nWeaknesses:\n- The analyses in the paper can dive deeper into the problem space. How much does ContraSim generalize? Can you use the same  ContraSim model across all benchmarks instead of training a separate one for each?\n- It would be good to discuss the trade-offs introduced by ContraSim. E.g. we need to access to labeled training data. We need different models for each domain. \n- The paper does not discuss the implications of this research. Why is learning based measure useful? What did we learn that's new? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is excellent in clarity, and reproducibility. \n\nThe novelty is above par, but a simple extension of existing ideas.\nThe quality can be improved with deeper analysis, as mentioned in weaknesses.",
            "summary_of_the_review": "I think the paper is above the acceptance bar for ICLR. I would like to see some additional analyses and discussion in the final version of the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3112/Reviewer_1R2q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3112/Reviewer_1R2q"
        ]
    },
    {
        "id": "_q2OpLCN7Q",
        "original": null,
        "number": 2,
        "cdate": 1666588541269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666588541269,
        "tmdate": 1666588541269,
        "tddate": null,
        "forum": "_fiHdKjxfR",
        "replyto": "_fiHdKjxfR",
        "invitation": "ICLR.cc/2023/Conference/Paper3112/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a parameterized similarity measure that shows significant improvement over existing closed-form similarity measures. ",
            "strength_and_weaknesses": "Strength:\n  - The paper is well-written and easy to follow. \n  - The method is simple yet effective.\n  - The empirical analysis is comprehensive. \n  - The provided benchmark can benefit future research.\n  - This paper provides a novel perspective in providing similarity measures compared to the existing closed-form ones.\n\nWeakness:\n  - As a learnable similarity measure, due to the randomness of model training, this metric cannot guarantee consistent results, which may limit its use cases and reliability to some extent.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good.\n\nQuality: Good.\n\nNovelty: Good.\n\nReproducibility: Fully reproducable.",
            "summary_of_the_review": "This paper provides a novel similarity measure based on contrastive learning, which greatly outperform existing closed-form measures in different scenarios.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3112/Reviewer_m4jx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3112/Reviewer_m4jx"
        ]
    },
    {
        "id": "O1gJqY1tfXd",
        "original": null,
        "number": 3,
        "cdate": 1666605214478,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605214478,
        "tmdate": 1666605214478,
        "tddate": null,
        "forum": "_fiHdKjxfR",
        "replyto": "_fiHdKjxfR",
        "invitation": "ICLR.cc/2023/Conference/Paper3112/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a similarity metric learned from similar and dissimilar examples using a loss inspired by contrastive learning. The metric is tested on three benchmarks (i.e., layer prediction, multilingual language modeling, image captioning), among which are two new benchmarks proposed by the authors on which they test the metric. \n",
            "strength_and_weaknesses": "Strengths:\n- Evaluation of the created metric on different types of data. \n\nWeaknesses:\n- Clarity of the work and its goals. \n- Limited technical contribution.\n- Evaluation only on three benchmarks, two of which are created. \n- Comparison with existing similarity metrics is limited.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and the writing could be improved. Some spelling errors should be corrected. \nThe technical novelty is limited as the proposed model makes use of existing methods of contrastive learning. \nReproducibility is constrained by the use of retrieved examples, but when train data are provided by the authors, this problem is mitigated.",
            "summary_of_the_review": "\nWhen defining a new similarity metric, one should first define its theoretical properties and evaluate these properties empirically with data. These important parts are lacking in the paper. \n\nThe motivation of the work is not clear. Is it the purpose to learn an encoder based on a contrastive loss or a similarity metric that can be used in and transferred to all kinds of applications?\n\nWhat would be the effect if another similarity metric than cosine similarity would be used to compare representations during training?\n\nThe novelty of the work seems limited. Learning representations from positive and negative examples based on a contrastive loss and using these to create a semantic space where similarity is measured has been done for almost a decade. The number of baselines and comparisons with other similarity metrics seem limited. \n\nThe authors rely on the FAISS library for similarity search of train examples. What is the influence of this initial search and the underlying methods that FAISS uses on the quality of the examples used in the training and consequently on the quality of the learned metric?\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3112/Reviewer_rucx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3112/Reviewer_rucx"
        ]
    },
    {
        "id": "4y7bkV5Ub-",
        "original": null,
        "number": 4,
        "cdate": 1666649555380,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649555380,
        "tmdate": 1669848100975,
        "tddate": null,
        "forum": "_fiHdKjxfR",
        "replyto": "_fiHdKjxfR",
        "invitation": "ICLR.cc/2023/Conference/Paper3112/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThis paper proposes a new contrastive learning objective for learning representations called Contrasim. Contrasim aims to learn representations that maximize inner-products between \"should-be-similar\" inputs and minimize them between \"should-be-dissimilar\" inputs. Contrasim's claim to novelty is that it adds the additional minimization objective for the \"should-be-dissimilar\" representation pairs whereas other representation learning objectives like project-weighted CCA (PWCCA) and centered kernel alignment (CKA) do not.\n\nThe authors test Contrasim on 3 representation learning tasks:\n1) Determining which encodings of sentences from two different languages are translations of each other.\n2) Determining which sentence and image encodings are (caption,image) pairs.\n3) For neural networks with the same architecture and training data, but different initialization, determine which layers of the neural networks correspond.\n\nExperimental results show that Contrasim dramatically outperforms the chosen baselines: CKA, PWCCA, deep dot-product and deep CKA; and the performance gap increases when using hard negative mining with the FAISS library.\n\nLast, low dimensional projection plots show that Contrasim improves the proximity of should-be-similar inputs relative to the original representations and relative to the aforementioned baselines.",
            "strength_and_weaknesses": "\nThe strength of this paper is that it emphasizes the importance of negative examples in training similarity functions and shows that they are critical in contrastic learning. I also liked that the authors experimented on embeddings produced by SOTA models like RoBerta and ViT.\n\nHowever, the weakness of the paper is that using negative pairs is very common in contrastive learning, despite the paper's claiming the opposite. For instance, in SimCLR (which this paper cites), the objective maximizes the dot products of should-be positive pairs and minimizes the dot products of negative pairs (see equation (1) in the SimCLR paper).\n\nLikewise, well known contrastive learning criteria like noise contrastive estimation (NCE), InfoNCE and triplet loss seek to minimize dot products of negative pairs relative to positive pairs.\n\nThese contrastive learning objectives need to be compared against because they are all highly similar to the proposed Contrasim.\n\nIn particular, equation (3) Contrasim can be seen as maximizing the log-odds probability of positive versus negative pairs if we assume there is a normalizing factor \"Z\" which has been divided out from both the numerator and denominator:\n\n  eq (3) = \\sum_i log [ \\sum_p\\~i [ exp(ai . ap)/Z ] / [ \\sum_n!\\~i exp(ai . an)/Z ].\n\nwhere \"\\~\" denotes the similarity relation and \"!\\~\" denotes the not-similar relation.\n\nSimCLR can be seen as maximizing the very similar odds ratio:\n\n  \\sum_i \\sum_p\\~i log { [ exp(ai . ap)/Z ] / [ \\sum_n!\\~i exp(ai . an)/Z ] },\n\nso it is not clear if Contrasim presents a notable advance, especially in absence of empirical comparisons and/or theoretical justifications.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to read. And it should be easily reproducible once the authors' code is made public, as they have promised to make it upon publication.\n\nAs described above, the lack of novelty is the major shortcoming of this work. And the missing empirical evaluations against highly similar contrastive learning methods critically impact the quality of the work as well.\n",
            "summary_of_the_review": "While Contrasim is a moderately intriguing variant of existing contrastive learning losses, it is too similar to existing methods like SimCLR and NCE and not well enough distinguished from them to warrant acceptance at ICLR.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3112/Reviewer_xBqQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3112/Reviewer_xBqQ"
        ]
    }
]