[
    {
        "id": "21pDJPvfW9",
        "original": null,
        "number": 1,
        "cdate": 1666623080845,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623080845,
        "tmdate": 1666623080845,
        "tddate": null,
        "forum": "Uuw51xL-ZHd",
        "replyto": "Uuw51xL-ZHd",
        "invitation": "ICLR.cc/2023/Conference/Paper2783/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes to enforce $\\pi(a|s)$ close $\\beta(a|s)$ to incorporate uncertainty at each state-action pair $(s,a)$ in enforcing $\\pi(a|s)$ close the estimated behavior $\\hat{\\beta}(a|s)$ where the enforcement weight is controlled the uncertainty estimate $\\hat{u}(s,a)$ (eq. 7). To compute uncertainty estimate $\\hat{u}(s,a)$, they also incorporate estimated behavior policy as $\\hat{u}(s,a) = \\hat{u}(s) / \\hat{\\beta}(a|s)$ where $\\hat{u}(s)$ is the ensemble-based estimator at state $s$. They claim that this proposed method is \u201cmore performant and robust than the pessimism mechanism [they meant LCB] of incorportating uncertainty as a penalty in the evaluation step\u201d.",
            "strength_and_weaknesses": "**Strength**\n\n- Good classification of recent offline RL methods\n\n- the paper is easy to read \n\n**Weaknesses**\n\n- Incremental methods with weak empirical results: the most interesting part is perhaps using the estimated behavior in the uncertainty quantifier. However, simple experiments in RL Unplugged cannot support the benefit of this modification. \n\n- Justification and comparison with LCB is unfair: the extreme hyperparameter analysis is not useful and the paper missed literature of ensemble-based implementation of LCB. The paper also does not clearly explain how LCB is implemented in their baseline\n\n- Significance of the work is low: Given the above reasons. \n\nSee my questions below for details.",
            "clarity,_quality,_novelty_and_reproducibility": "- How can you implement pessimism (Buckman et al., 2020; Jin et al., 2021) in your experimental setting? If you meant \u201cpessimism\u201d in (Buckman et al., 2020; Jin et al., 2021) by LCB, its ensemble-based implementation of LCB in fact has good empirical performance, see \u201cPESSIMISTIC BOOTSTRAPPING FOR UNCERTAINTYDRIVEN OFFLINE REINFORCEMENT LEARNING\u201d.   \n\n- SPIBB has never been explained and what it stands for. \n\n- the claim \u201cour proposed method is more performant and robust than the pessimism mechanism of incorportating uncertainty as a penalty in the evaluation step\u201d is not useful your method is in some sense also pessimism mechanism as it constraints the learned policy to the behavior policy weighting by the state-action uncertain quantifier\n\n- in the experiment section, you say that the purpose of the experiment is to test \u201chow well deep-SPIBB incorporates uncertainty estimates\u201d then \u201cOur main finding is that deepSPIBB consistently outperforms pessimism, suggesting that it does a better job of incorporating explicit uncertainty estimates\u201d. There is a questionable causal relationship between the performance of offline RL and incorporating uncertainty, that is, higher performance does not necessarily suggest better uncertainty corporation. \n\n- the justification of pessimism at extreme hyperparameters is unfair and not useful. There is no good reason someone would set the uncertainty quantifier weight $\\alpha$ to infinity in LCB. LCB would also implicitly implement behavior cloning with proper value of $\\alpha$. \n\n- \u201dSecond, compared to all baselines, deep-SPIBB generally performs slightly better than BCQ and is competitive with CQL\u201d. I don\u2019t think it is competitive with CQL, Figure 2 shows CQL is clearly better than the proposed method.\n\n- Figure 4 and its relevant claim are not clear. What is the y-axis of figure 4? how can we tell one uncertainty estimator is better than another by looking at Figure 4?",
            "summary_of_the_review": "See the weaknesses and question sections. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2783/Reviewer_uHxA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2783/Reviewer_uHxA"
        ]
    },
    {
        "id": "uVhyHO6SDth",
        "original": null,
        "number": 2,
        "cdate": 1666684760756,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684760756,
        "tmdate": 1667405300107,
        "tddate": null,
        "forum": "Uuw51xL-ZHd",
        "replyto": "Uuw51xL-ZHd",
        "invitation": "ICLR.cc/2023/Conference/Paper2783/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new algorithm for offline RL,deep-SPIBB, that uses uncertainty estimation instead of pessimism for regularizing the behavior to prevent undesirable behaviors. The method uses an ensemble based uncertainty estimation on the state and MLE behavior policy estimate to estimate the uncertainty for a given state-action pair and using the uncertainty, the algorithm performs standard offline RL procedure with constraints that force the learned policy to not deviate from the MLE policy when there is high uncertainty. The algorithm also introduces different deviation thresholds for the policy improvement and policy evaluation step. Experimentally, the algorithm performs prior work on two simple environments and outperforms pessimism and is competitive with CQL on  Atari.",
            "strength_and_weaknesses": "### Strength\n1. The proposed algorithm is very sensible and using uncertainty in offline RL seems very promising\n2. The analysis of how different offline RL algorithms use uncertainty is very nice as it unifies different perspective\n3. The paper overall is relatively easy to follow\n\n### Weakness\n1. The performance of the algorithm is not extremely compelling as it underperforms CQL in most settings.\n2. The previous point would normally not be a big problem but the idea of using uncertainty for offline RL has already been proposed in the literature [1]. This work does not discuss [1] and furthermore, [1] seems to outperform CQL on a range of D4RL tasks. This casts doubt on the additive value of this work.\n3. The ablation and analysis of different types of uncertainty could be improved. Currently, only two short paragraphs are dedicated to them, but it seems like uncertainty is extremely important for the proposed method and the proposed way of computing is new, so I believe more thorough analysis is warranted. For example, if we really believe that the quality of uncertainty estimation is the problem for the performance of the algorithm, then one could do an ablation on increasing the size of the ensemble and check if the performance improves.\n\n**Reference**\n\n[1] Why so pessimistic? Estimating uncertainties for offline RL through ensembles, and why their independence matters. Ghasemipour, et al.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity** the paper is clearly written\n\n**Quality** the paper\u2019s quality is somewhat below average\n\n**Novelty** the paper\u2019s novelty is limited due to prior works in similar area\n\n**Reproducibility** detailed hyperparameters are provided in the appendix but no source code so not sure\n",
            "summary_of_the_review": "The paper is well written but is limited in significance and novelty. I am leaning towards reject.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2783/Reviewer_hNm1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2783/Reviewer_hNm1"
        ]
    },
    {
        "id": "7QAig9u_vZ_",
        "original": null,
        "number": 3,
        "cdate": 1667360312046,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667360312046,
        "tmdate": 1667505717325,
        "tddate": null,
        "forum": "Uuw51xL-ZHd",
        "replyto": "Uuw51xL-ZHd",
        "invitation": "ICLR.cc/2023/Conference/Paper2783/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies how to incorporate uncertainty estimations for offline reinforcement learning to prevent the learner from favoring regions of high uncertainty (which are often over-estimated). The authors extend safe policy improvement with soft baseline bootstrapping (soft-SPIBB) to large state-action space, where count-based uncertainty measures become infeasible, by leveraging a neural uncertainty estimator (using random priors). ",
            "strength_and_weaknesses": "Pros:\n\n- The authors extend soft-SPIBB to large state-action space and the empirical performance seemed comparable to CQL. \n\n- The design of the estimator $\\hat{u}(s, a) := \\hat{u}(s) / \\sqrt{\\hat{\\beta}(a|s)}$ is sensible.\n\n- The discussion about constrained improvement step versus penalized evaluation step is interesting. \n\nCons:\n\nLimited novelty:\n\n- The authors modified soft-SPIBB (Nadjahi et al., 2019)  by replacing (count-based) error functions $e(s, a)$ with neural uncertainty estimates $\\hat{u}(s, a)$.\n\n- Estimating state uncertainty via random priors is not new in reinforcement learning, e.g. RND (Burda et al., 2019).\n\nRelated works: prior works should be more explicitly explained \n\n- RND is only mentioned in \"there is a large literature from the deep learning community on uncertainty quantification that we can leverage for OffRL (... Burda et al., 2019; ...)\", which does not emphasize that random priors methods have already been applied in reinforcement learning literature, although in an online setting.\n\n- It is encouraged to briefly explain the essence of SPIBB and soft-SPIBB in e.g. preliminaries, so that it is easier to see which parts in Eqn (7) are proposed by the authors, especially for the audiences who are not familiar with soft-SPIBB.\n\n\nQuestions/additional comments:\n\n- Introducing $\\epsilon_{eval}$ seemed a bit odd to me. Do the authors mean: (a) first train $\\hat{Q}$ until convergence with $\\epsilon_{train}$ and (b) then do one-step policy improvement with $\\epsilon_{test}$?\n\n- Could the authors plot the training curves of experiments in 5.1 and 5.2? As Eqn (7) is now approximated by a greedy heuristic (if my understanding is correct), it is good to see whether it is requiring more iterations to converge.\n\n- The comparison versus Pessimism is not necessarily fair. Deep-SPIBB keeps most essential parts of soft-SPIBB except using a neural quantifier instead of a count-based one. While Pessimism does not keep all key designs of e.g.  (Buckman et al., 2020; Jin et al., 2021), Pessimism should not recover theoretical guarantees of pessimistic approaches while using any count-based uncertainty quantifier because of its simplification. Therefore it does not fully support the claim made in the abstract, \"we argue that the SPIBB mechanism for incorporating uncertainty is more robust and flexible than pessimistic approaches that incorporate the uncertainty as a value function penalty.\"\n\n- I believe pessimistic approaches should be able to recover behavior cloning with proper choice of uncertainty penalty and also proper algorithmic designs. For example, see [1].\n\n[1] Rashidinejad, Paria, et al. \"Bridging offline reinforcement learning and imitation learning: A tale of pessimism.\" Advances in Neural Information Processing Systems 34 (2021): 11702-11716.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good, the writing is overall clear but could be improved, for examples\n\n-  The abbr. SPIBB appeared without its full name in the main text.\n\n- Page 5, \"we use the approximation technique described in Nadjahi et al. (2019).\" I would recommend including the optimization pseudo code in the appendix. Otherwise one has to go through Nadjahi et al. (2019) to locate it in their Appendix A.8 (Arxiv version).\n\n- A lot of details are given in a quite verbal way, the readability could be improved.\n\nQuality: Fair, some statements are not well supported and the experiments could be more comprehensive.\n\nNovelty: Fair, novelty is limited because of Nadjahi et al. (2019) and  Burda et al. (2019). The discussion about constrained improvement step versus penalized evaluation step could be important. However, the attempts made in this version are not convincing enough.\n\nReproducibility: Good, code and hyper-params are provided.",
            "summary_of_the_review": "Although the proposed algorithm is well-motivated (upon some existing works), its novelty is limited, and the discussions and experiments could use some improvements. I am leaning toward a (weak) reject at this moment.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2783/Reviewer_eF2b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2783/Reviewer_eF2b"
        ]
    }
]