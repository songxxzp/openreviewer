[
    {
        "id": "ayokwWBnwjv",
        "original": null,
        "number": 1,
        "cdate": 1665948743561,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665948743561,
        "tmdate": 1665948743561,
        "tddate": null,
        "forum": "moIlFZfj_1b",
        "replyto": "moIlFZfj_1b",
        "invitation": "ICLR.cc/2023/Conference/Paper4577/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Neural ODEs (NODE) are very powerful to model the dynamical systems. In particular, latent NODE models are great candidate when we deal with very high-dimensional data, as they learn the dynamics in a much lower dimensional space. This paper (along with some other papers they have cited) claims that when the length of trajectories (time span) starts to grow, the loss landscape of the NODE models become very complicated leading to poor estimates and generalization. To solve this problem, they employ a technique from optimal control called \"multiple shooting\" (MS), which works has follows. The authors chop the whole trajectory to multiple blocks such that each block contains fewer observation. This allows learning ODEs for smaller periods of time and also parallelization which makes the training much faster. There is a risk that smaller blocks might be disconnected. In order to tackle this issue, they add a continuity prior in a Bayesian framework which also learns a sparse blocks. Overall, the results show that this method is effective in both improving the test errors and decreasing the training time.",
            "strength_and_weaknesses": "Strength:\n\n- Improved generalization and test error. \n- Improved training time.\n\n- Loss function has a lot of terms which can make finding the optimal wighting of terms more difficult.\n- Lack of evaluation based on RNN encoders.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear to read, understand, and easy to follow. The novelty of the paper might not be high as based on the authors, there have been similar works of using MS in NODEs. However, the quality of the paper is good as they have built a unified probabilistic framework to incorporate the ideas from latent ODEs and MS to tackle the problem of learning in long trajectories. ",
            "summary_of_the_review": "The paper is well written and clear. I am mostly convinced that this method is helpful in learning the dynamics for the long trajectories and appreciate the innovative solutions like MS and using Bayesian framework to enable continuity prior and uncertainty of the dynamics. However, I have some questions:\n\n- I wonder where the complexity of long trajectories come from? Is it because of error accumulation on ODE solvers for long times? If this is the case, can we shrink the time span by preserving the ordering of the observation over time? For example, if the original time span is [0,50] second, can we scale it to [0,5] and then solve the ODEs, or will we still see the same problem?\n\n- The new loss function proposed by the authors in (17-19) has six different terms. Usually, in VAE models, the optimal performance is achieved by some non-uniform weights of these terms. I wonder if the authors have played with weighting these terms? Have the authors found that optimizing the loss as is in (17-19) gives the best results? I feel this complicated loss should be tricky to train without knowing the optimal weights and curious to see how that is the case with the authors' experience.\n\n- Why have the authors chosen to use transformer instead of RNNs in the encoder? The original work of latent ODE uses RNN and also consider the case of irregular time-series. I wonder if the authors have tried RNN as well and not found it satisfying? I would recommend adding it as another comparison so that we can see how much adding transformers help in encoders.\n\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4577/Reviewer_9Vai"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4577/Reviewer_9Vai"
        ]
    },
    {
        "id": "1ms12d6CnY",
        "original": null,
        "number": 2,
        "cdate": 1666203166702,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666203166702,
        "tmdate": 1668526126502,
        "tddate": null,
        "forum": "moIlFZfj_1b",
        "replyto": "moIlFZfj_1b",
        "invitation": "ICLR.cc/2023/Conference/Paper4577/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work tackles the issue of modelling time-series data tractably and efficiently. Modelling long, irregular time sequences can be very challenging to optimise (in terms of the underlying landscape of the loss function), and inefficient (simulation of the time series in serial can be very slow). This work adapts the well-known technique of multiple-shooting, but reframes with a Bayesian lens. This allows modelling a time series as a series of blocks that can be simulated in parallel, which resolves both issues of efficiency and optimisation.\n\nA variational framework is used. This allows factorising the shooting variables so they can be sampled independently (hence in parallel).\n\nFor the encoder, the authors develop a novel attention mechanism and positional encoding.",
            "strength_and_weaknesses": "Overall, the paper is very strong.\n\nThe model is clearly explained, both on a theoretical level, and how to make it work in practice. There are thorough ablations and comparisons to other methods.\n\nI only have one question/concern:\n\nHow continuous are the models in practice? Does better continuity correlate with better performance, or is the model learning to make it discontinuous and then learning that?\nI know there is an experiment showing this for different strengths of the continuity prior, but I\u2019m wondering if you have an experiment that measures continuity itself directly, rather than a parameter that is supposed to encourage it.\nHow correlated is the continuity prior with actual continuity (e.g. MSE between shooting points and trajectory points)?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is extremely clear, and the writing is very high-quality.\n\nThe work is high-quality and novel.\n\nThere are sufficient details provided to reproduce the experiments.\n",
            "summary_of_the_review": "Overall, the paper is very solid. The method is novel and interesting, and thorough ablations and comparisons are done to a high-standard with plenty of information for reproducibility. I have a few questions (listed in Strengths and Weaknesses), but overall I\u2019m happy to recommend this paper for acceptance.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4577/Reviewer_AJZS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4577/Reviewer_AJZS"
        ]
    },
    {
        "id": "D48w_deIcb",
        "original": null,
        "number": 3,
        "cdate": 1666644678181,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644678181,
        "tmdate": 1666645003809,
        "tddate": null,
        "forum": "moIlFZfj_1b",
        "replyto": "moIlFZfj_1b",
        "invitation": "ICLR.cc/2023/Conference/Paper4577/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an amortized inference approach combined with a temporal attention mechanism for parallel training of dynamical models given long irregularly sampled trajectories. Experimental results on three synthetic datasets demonstrate improved mean squared error compared to baselines.",
            "strength_and_weaknesses": "**Strengths**\n- The paper tackles an important challenge of training models given long and irregularly sampled datasets\n- The paper is well-written and easy to follow\n- The proposed approach seems easy to implement\n\n**Weakness** \n\n*Technical Limitations*\n-  The novelty is limited. The proposed approach seems to be a straightforward application of amortized inference and attention\n- The discussion of the proposed temporal attention needs to be expanded. There are several approaches for temporal encoding. Did the paper consider alternative formulations to Eqn. 23? \n-  How is the block size chosen given observational data?\n- The performance gains seem marginal when compared to other models such as ODE2VAE and NODEP\n-  Eqn 17: What is the assumed data likelihood? It's unclear why the data likelihood depends on $s_1$ only.\n- Overall the paper proposes several heuristics for training without proper justification, *e.g.*, the paper claims constraining posterior variance is crucial without providing specific details of the actual approach. Hence, it's unclear where the performance benefits come from.\n   \n*Experimental Evaluation*\n- The paper needs to discuss and compare with other latent-ODEs approaches for irregularly sampled observations, *e.g.*, [1, 2] and temporal attention mechanisms [3]\n- The paper evaluates relatively clean synthetic datasets. The evaluation should also consider challenging real-world datasets such as Physionet\n\n**References**\n- [1]  De Brouwer et al., \"Gru-ode-bayes: Continuous modeling of sporadically-observed time series\",  NeurIPS, 2019\n- [2] Rubanova et al.,  \"Latent odes for irregularly-sampled time series\", NeurIPS, 2019\n- [3] Zhang et al., \"Self-attentive Hawkes process\", ICML, 2020",
            "clarity,_quality,_novelty_and_reproducibility": "See above.",
            "summary_of_the_review": "The technical novelty is limited. Additionally, experimental evaluations are missing several baselines and rely on clean synthetic datasets. Also, the paper proposes several training heuristics with little justification. Hence the source of performance gain is unclear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4577/Reviewer_Lhfw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4577/Reviewer_Lhfw"
        ]
    },
    {
        "id": "KP0huWZi9i",
        "original": null,
        "number": 4,
        "cdate": 1666713813950,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666713813950,
        "tmdate": 1666713813950,
        "tddate": null,
        "forum": "moIlFZfj_1b",
        "replyto": "moIlFZfj_1b",
        "invitation": "ICLR.cc/2023/Conference/Paper4577/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper focuses on the problem of improving neural ODEs for learning from long trajectories. It shows that the loss landscape of latent neural ODEs is adversely affected by the length of the time interval and  complexity of loss can grow dramatically with increase in the length of the trajectory. The paper proposes bayesian multiple shooting technique to address this issue. Multiple shooting technique splits the trajectory into multiple short segments which are optimized in parallel while controlling the continuity between multiple segments. Standard multiple shooting approaches enforces continuity of the entire trajectory by applying a hard constraint or penalty term while training. The paper proposes to utilize Bayesian inference and naturally encoded continuity as a prior. The papers also proposes temporal attention and relative positional encodings to build a time-aware attention based encoder to deal with irregularly sampled input data.  Experiments on multiple datasets show that the proposed approach is able to achieve better performance and that multiple shooting clearly helps improve the performance and reduce training times via parallel computations.",
            "strength_and_weaknesses": "### Strengths\n1. The paper presents a novel approach involving Beyesian inference for a quick and stable training of latent ODE models on long trajectories. It also introduces a transformer-based encoder with novel time-attention and relative positional embeddings for irregulalry sampled data.\n2. The motivation of the paper is clear and it focuses on solving an important problem. The experimental results indeed show that multiple shooting is an interesting and effective way to deal with long trajectories while reducing training times.\n3. The paper shows ablations to show the effectiveness of the proposed time-aware attention and relative positional encodings. \n\n### Weaknesses\n1. Different feature in irregularly sampled sequences can have observations at different times. It is not clear how does the proposed encoder takes care of this. The paper is also missing an important related work [1] in this area which proposes an attention based encoder for irregularly sampled time series.\n2. Another concern with the paper is the lack of rigorous experimentation to show the usefulness of the proposed method for irregularly sampled time series datasets. The paper does not include any experiments on real-world irregularly sampled time series datasets (e.g. PhysioNet, MIMIC-III). I would expect to see at least the comparison on the PhysioNet experiments form Latent ODE paper. \n3. The paper defines short trajectory with N = 10 in Figure 1 but the Latent ODE is running in continuous time. N = 10 can constitute of much longer trajectory based on the time difference between the observations or a much shorter if the observations are close to each other in time. \n4. The paper is missing important comparisons with the Latent ODE work with ODERNN encoder. \n5. How does the block size impact the MSE and training time on irregularly sampled time series? Empirical study with PhysioNet or MIMIC-III/IV would be very useful here. \n6. The paper is also missing comparisons with Hedge et al(2022) or Jordana et al(2022) to show the effectiveness of their proposed bayesian multiple shooting approach.\n\n[1] Shukla, Satya Narayan, and Benjamin M. Marlin. \"Multi-time attention networks for irregularly sampled time series.\" arXiv preprint arXiv:2101.10318 (2021).",
            "clarity,_quality,_novelty_and_reproducibility": "**Novelty:** The paper presents a novel approach involving Beyesian inference for a quick and stable training of latent ODE models on long trajectories.  It also introduces a transformer-based encoder with novel time-attention and relative positional embeddings for irregulalry sampled data.\n\n**Reproducibility:** Not reproducible. Although the paper includes the hyperparameters, the code to reproduce the results is missing.\n\n**Clarity:** The paper is well written and easy to follow.",
            "summary_of_the_review": "My recommendation for this paper is weak accept. Although the paper proposes a novel solution and shows clear motivation behind the proposed approach, rigorous experimentation is missing. I would be happy to increase my score if my concerns are addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4577/Reviewer_XZSe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4577/Reviewer_XZSe"
        ]
    }
]