[
    {
        "id": "G92kzT7zLXy",
        "original": null,
        "number": 1,
        "cdate": 1666369311118,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666369311118,
        "tmdate": 1666369311118,
        "tddate": null,
        "forum": "EPUWZhBd9Lb",
        "replyto": "EPUWZhBd9Lb",
        "invitation": "ICLR.cc/2023/Conference/Paper2889/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new finding in subspace by constructing a Compressed Parameter Subspaces (CPS). It is found that a\ngeometric structure representing distance-regularized parameters mapped to a set of train-time distributions can maximize average accuracy over a broad range of distribution shifts.",
            "strength_and_weaknesses": "Strength\n\n+ The study of subspace structure gains more insight to various critical aspects of deep learning.\n\nWeaknesses\n\n- Theorem 2 and its proof in A.1 is confusing. With (9), does it mean \u2202L(\u03b8i; Xi)/\u2202\u03b8i = \u2202L(\u03b8n; Xn)/\u2202\u03b8n? \u03b8i and \u03b8n are trained with different train set from {Di}N, why will SGD update them always in the same direction?\n- The main contribution and focus of this paper is \"Departing from constructing subspaces w.r.t. a single/unperturbed input distribution, we investigate the construction of subspces w.r.t. multiple perturbed distributions\". However, the results in Table 1 show underperforming accuracies comparing with models trained with single input distributions. For example, CPS (backdoor) performs better than Backdoor Adversarial Training, but worse in  Stylization and Rotation. CPS (stylization) and CPS (rotation) perform much worse in Backdoor Attack. The effectiveness of CPS is questionable and inconclusive based on the experiment results.\n-\"strong capacity for multitask solutions and unseen/distant tasks\" is not fully supported by experiment results as Adversarial Attack and Random Permutations get better accuracies, but Stylization and Rotation are worse.\n-\u03b2 is the key parameter to CPS. However, it is not studies sufficiently in the paper. The experiments only include \u03b2=0 and \u03b2=1.0.\n- N + 1 parameters have to be trained, which is computationally expensive even with parallel training.\n- Please make the texts in Figure 1 larger.\n- Please fix \"that can multiple types of distribution shifts concurrently\"",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThis paper is well organized.\n\nQuality\n\nThe paper has come technical concerns that need to be addressed.\n\nNovelty\n\nThe novelty of this paper is reasonable. \n\nReproducibility\n\nGood.",
            "summary_of_the_review": "This paper investigates the interesting filed of subspace and CPS. There are some technical concerns and experiment results are not convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2889/Reviewer_326H"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2889/Reviewer_326H"
        ]
    },
    {
        "id": "_7DyE3Sr8Oy",
        "original": null,
        "number": 2,
        "cdate": 1666681794823,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681794823,
        "tmdate": 1666681794823,
        "tddate": null,
        "forum": "EPUWZhBd9Lb",
        "replyto": "EPUWZhBd9Lb",
        "invitation": "ICLR.cc/2023/Conference/Paper2889/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work propose a method to tackle the train-test inconsistent problem, e.g. distribution/domain/task shift exists at test time. The method tries to find a way to linearly combine multi models (obtained from some samples interpolated via train and test data) so that it can adapt to seen tasks as well as unseen interpolated tasks. It\u2019s not a real domain generalization method because the shift in test data need to be known during training.",
            "strength_and_weaknesses": "strength\uff1aThe work propose to tackle multiple types of distribution shift concurrently, and on some cases the performance is better than augmentation, which may declose a new way for enhance the robustness of a model.\n\nWeakness: the method is not real domain generalization because the shift in test data need to be known during training. The method is complicated, because it needs to train a model for every interpolation type with a specific {alpha}. Typos, especially in equations, makes it difficult to follow the main idea. The performance is much lower than augmentation in some cases. The relationship of this work to \"parameter subspaces\" in the title is not clear.\n\nsome concerns are listed,\n1.It looks more like a model level information combination method rather than the title claimed \u201ccompressed\u201d.\n2.Legends in Figure 1 is too small. \n3.The definition 1 is confusing, it says that x^ is a interpolation between x0 and x_i^det, but the equation contains only  x_i^det, the definition of i is more confusing.\n4.Symbol N has two different meaning is not a wise choice.\n5.Bottom of page2, Where does x_i come from?\n6.Page 6 on task interpolation, the equation for x^ , is it true that alpha is only related to index i? why?\n7.Table1, Why the performance of CPS sometimes are better than augmentation and sometimes much worse than augmentation? Especially for the stylization and rotation cases. For CPS with rotation, the performance on clean test set is destroyed seriously, why? \n\n8.What about CPS when comparing to some domain generalization or domain adaptation methods?\n\nTypos:\n1.Page2 \u201c that can multiple types of\u201d\n2.Algorithm 1, there are two D_i s in row10, what\u2019s the difference?\n3.Algorithm 1, how to compute the loss \u201ccos()\u201d in row 13?\n",
            "clarity,_quality,_novelty_and_reproducibility": "There are many typos and mistakes in the equations. The work is novel in considering the domain shift problem from the point of parameter space. ",
            "summary_of_the_review": "The work needs to be polished more seriously, a lot of mistakes in equations make it difficult to understand the main idea. I think it\u2019s not ready for publishing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2889/Reviewer_jM1a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2889/Reviewer_jM1a"
        ]
    },
    {
        "id": "oCdM_njqpfP",
        "original": null,
        "number": 3,
        "cdate": 1666868632004,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666868632004,
        "tmdate": 1666945910336,
        "tddate": null,
        "forum": "EPUWZhBd9Lb",
        "replyto": "EPUWZhBd9Lb",
        "invitation": "ICLR.cc/2023/Conference/Paper2889/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to learn multiple parameters (with a cosine distance regularization to emphasize the diversity of the learned parameters). During a testing period, one can use the interpolated parameter to increase model robustness. The authors also prove a couple of important theorems that i) a compressed parameter subspace can be learned and ii) the interpolated parameter is a good selection. An extensive set of experimental results are introduced.",
            "strength_and_weaknesses": "- S1) A new method robust to distribution shifts\n- S2) A theoretical framework to have a robust parameter\n- S3) A simple algorithmic design\n\n- W1) During training, N times larger resources are needed.\n- W2) The cosine distance regularization may fail to find the compressed parameter subspace.\n- W3) It is unclear whether or not more advanced optimizers do not break the theorems.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing and technical points are clear, but in my opinion, the overall idea is not that novel although this paper delivers several good points. However, this paper has value in constructing a robust compressed parameter subspace. I cannot access their supplementary material so I cannot judge the reproducibility.",
            "summary_of_the_review": "I think this paper has value since it introduces a new approach to having a robust parameter set. But I have several concerns below:\n\n- Q1) During training, N times larger resources are needed.\n- Q2) The cosine distance regularization may fail to find the compressed parameter subspace.\n- Q3) It is unclear whether or not more advanced optimizers do not break the theorems (due to the momentum of Adam).\n- Q4) Why didn't you compare with other ensemble-based baselines? Since your method also internally build an ensemble of models, it is fair to compare with them.\n- Q5) CIFAR-10/100 are too restrictive environments. Do you have more results in other datasets/tasks?\n- Q6) Why don't we train sequentially to have a better-compressed parameter subspace? Is training in parallel really the best?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2889/Reviewer_haBW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2889/Reviewer_haBW"
        ]
    }
]