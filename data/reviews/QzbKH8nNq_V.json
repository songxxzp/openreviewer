[
    {
        "id": "zfPVShGXf83",
        "original": null,
        "number": 1,
        "cdate": 1666598321471,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666598321471,
        "tmdate": 1666601150560,
        "tddate": null,
        "forum": "QzbKH8nNq_V",
        "replyto": "QzbKH8nNq_V",
        "invitation": "ICLR.cc/2023/Conference/Paper2250/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to learn instance-aware per-layer masks for embedding fusion in DNN layers. Both Single tower and dual tower architecture are compatible with such DNN layer mask fusion. A careful instance normalization trick is introduced to resolve re-scaling issue. Experiments have demonstrated orthogonal performance improvement over various deep CTR models.",
            "strength_and_weaknesses": "Strengths \n\nThe paper is well written and quite readable. Experiment is comprehensive with solid ablation study.\n\nWeaknesses \n\nA strong and recent baseline Wang 2021b is not benchmarked. \n\nThere is no data points in complicated prod setting such as industrial AB testing. The method is more or less complexity scaling up and prod setting is much more sensitive to the complexity-gain ROI tradeoff.  ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good and the paper is easy to follow.\n\nQuality is high given detailed component wise introduction/analysis, comprehensive experiment results and studies.\n\nNovelty is fair given a lot of incremental changes over Zhao et al 2021. \n\nReproducibility is fair and there is no notion of code publicity as I can see.",
            "summary_of_the_review": "The paper introduces a per-instance musk fusion method to boost personalization capacity of DNN layers in CTR prediction. The experiment so far reads solid, although the gain is not surprising given boosted complexity. \n\nOne big concern is its technical contribution beyond empirical gain. Since this is an application domain narrative, this paper could be a good addition if evidence of adoption in industrial environment is given. It's a marginal paper at its current form.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2250/Reviewer_bHvx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2250/Reviewer_bHvx"
        ]
    },
    {
        "id": "5J6BxnrYDM",
        "original": null,
        "number": 2,
        "cdate": 1666683988651,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683988651,
        "tmdate": 1670241655570,
        "tddate": null,
        "forum": "QzbKH8nNq_V",
        "replyto": "QzbKH8nNq_V",
        "invitation": "ICLR.cc/2023/Conference/Paper2250/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method MaskFusion that can be incorporated into existing deep CTR  models for enhancing prediction performance. \n\nThis idea is to multiply input feature embeddings with mask vectors, and then concatenate them with each DNN layer in deep CTR models. The mask vectors are dependent on the input feature embeddings, and are normalized among different layers so that different features will be assigned to different DNN layers. \n\nThey conduct experiments on three benchmark datasets. The empirical results demonstrate that MaskFusion can be incorporated into different deep CTR  models to improve their performance.",
            "strength_and_weaknesses": "S1. The paper is very well-organized and the writing is good. The figures are clear and helpful for understanding the proposed model structure.\n\nS2. The proposed MaskFusion method is simple, intuitive, and effective. It is also general as can be seamlessly applied to existing deep CTR models, and thus may have a broader impact.\n\nS3. The empirical result is promising as the overall improvement is significant among different backbones and datasets.\n\n---\n\nW1. Since the main comparison results are based on sensitive metrics like AUC, it is suggested to provide mean, variance, and statistical significance values to improve the convincing of the results.\n\nW2. It is suggested to provide an analysis of why the benefit of MaskFusion is more significant on some backbones than others, and on some datasets than others in Table 1.\n\nW3. The discussion on the change of inference time in Appendix E.2 is fundamental and is suggested to be put in the main body.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is novel, and the quality and clarity are good.",
            "summary_of_the_review": "The proposed MaskFusion method is novel, general and effective for CTR prediction problem. Thus I would vote for accept.\n\n---\n\nAFTER AUTHOR RESPONSE:\n\nI've read the authors' responses and the comments from other reviewers.  Basically, I like the paper for its clarity and soundness, and the response has properly addressed my concerns. Thus I would like to keep my score. I'm still glad to discuss with other reviewers if there are any different opinions after the rebuttal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2250/Reviewer_aUmA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2250/Reviewer_aUmA"
        ]
    },
    {
        "id": "EPt7z_yKqj",
        "original": null,
        "number": 3,
        "cdate": 1667047501615,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667047501615,
        "tmdate": 1667047501615,
        "tddate": null,
        "forum": "QzbKH8nNq_V",
        "replyto": "QzbKH8nNq_V",
        "invitation": "ICLR.cc/2023/Conference/Paper2250/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a general framework being able to enhance  a wide variety of existing click-through rate (CTR) prediction models. In order to balance memorization and generalization, an instance-wise gating network is utilized to dynamically select the feature embedding which is fused with the deep representation of each layer. Experiments across several benchmarks (existing CTR prediction models) and several CTR prediction datasets demonstrate the effectiveness and generality of the proposed framework.",
            "strength_and_weaknesses": "**Strength**\n\nThe paper is well written and clearly presented.\n\n**Weaknesses**\n\nIt is not clear whether the proposed framework MaskFusion introduce more computational overhead; it lacks necessary discussion and experiments about efficiency. \n\nAlthough several existing CTR prediction models with input-adaptive masks are discussed in the Related Work section, experiments comparing these methods are also necessary. It is not satisfying to summarily claim these previous methods are not verified to be applicable to all existing deep CTR models.\n\nThe discussion on related work should not be limited to the CTR prediction models. Input-adaptive masks are also wildly used in other fields such as CV and NLP.\n\n**Typos**\n\nLine 22 of Page 3: \u201cdifferent\u201d \u2192 \u201cdiffer\u201d",
            "clarity,_quality,_novelty_and_reproducibility": "(Clarity) The paper is well written and clearly presented.\n\nThe code should be made public to validate *reproducibility* owing to the fact that implementation details are missing in the text.\n\n*Quality* and *novelty* is not sufficiently convincing (see Strength and Weaknesses).",
            "summary_of_the_review": "See \u201cStrength And Weaknesses\u201d and \u201cClarity, Quality, Novelty And Reproducibility\u201d.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2250/Reviewer_73YJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2250/Reviewer_73YJ"
        ]
    },
    {
        "id": "LyavNNQ_Oe",
        "original": null,
        "number": 4,
        "cdate": 1667282336262,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667282336262,
        "tmdate": 1667282405117,
        "tddate": null,
        "forum": "QzbKH8nNq_V",
        "replyto": "QzbKH8nNq_V",
        "invitation": "ICLR.cc/2023/Conference/Paper2250/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors focus on the problem of improving the balance between memorization and generalization for CTR prediction, by explicitly incorporating the initial embedding layers into the final layers of typical single and dual tower models.\n\nIn particular, each embedding from the embedding layer has a softmax mask determining its contribution to each subsequent layer, and such embedding is concatenated in each layer reweighted by the corresponding mask.\n\nThe approach is reminiscent of skip-connections in that it facilitates information flow across many layers. However the embeddings are actually concatenated to the existing layers.\n\nThe experimental results are conducted on three realworld dataset and show lift across the board.",
            "strength_and_weaknesses": "Pros:\n- Approach is general and can be applied to several architectures.\n- Experimental results are promising.\n\nCons:\n- Concatenating multiple embeddings at each layer of the MLP might explode the number of parameters. The authors mentioned memory saving for certain cases, but I would suggest with existing models by keeping the number of total parameters comparable for a more complete comparison.\n- While consistent, the experimental lift in ROCAUC are tiny, I would recommend including average precision as well to better gauge these comparisons.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite straightforward to follow.",
            "summary_of_the_review": "While simple, the approach presented show promise in the included experimental results. However, while consistent, the improvements are small and the approach can substantially increase the total number of parameters of the model.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2250/Reviewer_Ug5p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2250/Reviewer_Ug5p"
        ]
    }
]