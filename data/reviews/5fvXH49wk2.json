[
    {
        "id": "tG5FkN-3dKK",
        "original": null,
        "number": 1,
        "cdate": 1666560355305,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666560355305,
        "tmdate": 1666560355305,
        "tddate": null,
        "forum": "5fvXH49wk2",
        "replyto": "5fvXH49wk2",
        "invitation": "ICLR.cc/2023/Conference/Paper1670/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a denoising approach for speech enhancement for use in downstream tasks such as automatic speech recognition (ASR). They augment the typical end-to-end training with an auxiliary loss function that also seeks to minimize distortion with the notion of ASR-independent generalization. The results have been evaluated on 2 noisy ASR evaluation datasets and show a small improvement over a couple of previous systems.",
            "strength_and_weaknesses": "Strengths\n1.Good literature review.\n2. Insightful connection to related efforts in Section 3. \n3. Evaluation across a variety of recognizers and datasets.\n4. Helpful wav and spectrogram files in the supplementary section.\n5. The attached source code promotes reproducibility.\n6. The writing is very clear.\n\nGrowth opportunities\n1. The authors claim that \"tuning parameters in an ASR system is not a feasible solution for practical scenarios.\" With large-scale multi-condition training, it is not clear how much tuning is necessary.\n2. The annotation cost comment on MCT is misleading. Typically, only the clean speech is annotated, incurring no additional cost for MCT.\n3. What is the justification for the learning rate and update period values used? How sensitive is the solution to these parameters?\n4. Why is the SNR range being investigated in the range from -4dB to 6dB?\n5. CHIME-4 and Aurora-4 are medium vocabulary ASR tasks. It would be beneficial to evaluate the proposed approach on LVCSR tasks.\n6. Only small improvement over GCLB is observed. On the Aurora-4 and Google ASR API on CHIME-4 tasks, even SRPR performs competitively.",
            "clarity,_quality,_novelty_and_reproducibility": "I really enjoyed reading the paper. The writing was quite clear, and the language was quite precise. The attached material support reproducibility.",
            "summary_of_the_review": "The paper builds on a host of related work and shows small improvements on medium vocabulary ASR tasks and is computationally more efficient than a grid-search-based approach. Without LVCSR or large-scale MCT comparisons though, the power of the approach might need to come from its applicability to other tasks, e.g., intelligibility improvements of enhanced speech.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1670/Reviewer_jXQS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1670/Reviewer_jXQS"
        ]
    },
    {
        "id": "ewS8mij35uk",
        "original": null,
        "number": 2,
        "cdate": 1666601066358,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601066358,
        "tmdate": 1666601881055,
        "tddate": null,
        "forum": "5fvXH49wk2",
        "replyto": "5fvXH49wk2",
        "invitation": "ICLR.cc/2023/Conference/Paper1670/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a method for Speech Enhancement (SE) applied to Automatic Speech Recognition (ASR). In the proposed approach, the SE model is trained to enhance noisy speech and to keep improving recognition performance of the acoustic model. This is accomplished by training the SE model with multi-task learning, where one of the losses is an estimation of the ASR performance, together with an adjustment scheme to learn the weights directly. The approach is evaluated on several noisy ASR benchmarks, including an unseen ASR system and is shown to improve peformance.",
            "strength_and_weaknesses": "Strengths:\n - The approach presented in the paper is significant as it is designed for unseen ASR systems and yields state-ot-the-art performance.\n - The adjustment scheme is very interesting and makes the approach very practical and appealing to use.\n - The experiments are thorough and clearly show the viability of the proposed approach.\n\nWeaknesses:\n - The paper doesn't situate well the proposed approach with the literature: in the third paragraph of Section 2, several related works are cited which seems to be very similar to the proposed approach as they all use the auxiliary task approach, but it's unclear which tasks and how it is relating to the proposed approach. Please describe the relevant related works and clarify.\n- The clarity of the paper could be improved.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clearly written and easy to follow, but some parts could be improved:\n - In Section 2, the second half is not easy to read as it's not really clear what is related work, what is motivation and what is the proposed approach. Using Subsections should help, as well as are clearly stating which part of the section (typically Equation (1) and (2)) are related work and which part are original contributions.\n- In Section 4, using bold font for the acronyms is quite distracting, using italic (or \\emph) should make is more readable.\n- Section 4.1, 3rd paragraph: \"SRPR, GCLB, and D4AM are associated with Chen et al. (2015); Ma et al. (2021).\": I thought the D4AM is the proposed approach, why is it \"associated\" with other papers? is it a typo?\n\nThe proposed approach seems novel, although it's unclear which part is original (see Strengths and Weaknesses). ",
            "summary_of_the_review": "Overall, the approach seems novel and the findings are significant. The domain is very specific (using SE for ASR) and might not be of huge interest to the whole ICLR community, but the algorithm with the adjustment could be useful and is worth publishing. I thus recommend to accepting the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1670/Reviewer_mD2F"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1670/Reviewer_mD2F"
        ]
    },
    {
        "id": "gmBk7fiYIFa",
        "original": null,
        "number": 3,
        "cdate": 1667080335358,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667080335358,
        "tmdate": 1669739377550,
        "tddate": null,
        "forum": "5fvXH49wk2",
        "replyto": "5fvXH49wk2",
        "invitation": "ICLR.cc/2023/Conference/Paper1670/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a denoising frontend for automatic speech recognition. The method combines speech enhancement (regression) and recognition (classification) loss. The paper presents a scheme for automatically balancing the regression and classification losses. The paper shows the effectiveness of the proposed method in the efficacy of noisy speech recognition experiments.",
            "strength_and_weaknesses": "Strength\n- consistent improvements of the ASR performance even with a black box ASR API. The problem setup of developing universal speech enhancement techniques for various ASR systems is crucial. \n- automatic balancing scheme of regression and classification losses seem to be novel\n- showing the robustness of the balancing scheme experimentally.\n\nWeaknesses\n- optimization of speech enhancement with ASR loss is not novel. Of course, a part of dealing with unseen ASR systems is novel, but it is not a sufficient novelty.\n- the method is specific to speech processing problems and would not attract general machine learning and AI researchers. The automatic balancing scheme seems to be a general scheme, and if this has more critical or different applications, this part will be improved more.\n- Although the balancing scheme is sophisticated, the result is not very different from a conventional grid search. It is not difficult to tune a single hyper-parameter with the dev set.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n- It is difficult to read Section 3, especially Section 3.1, due to many inline equations. It is better to provide more high-level derivations; some details can be moved to the appendix sections.\n\nQuality\n- I cannot fully agree with the \"SE approaches do not consider the generalization ability to unseen ASR systems.\" I think it is valid for weak speech enhancement models, but the latest state-of-the-art speech enhancement shows the generalized performance without changing the ASR system, e.g., the L3DAS challenge https://www.l3das.com/icassp2022/index.html fixes the ASR backend, but the many speech enhancement systems improve the ASR performance without using the ASR backend as a loss function. I understand the claim of \"unseen ASR systems\" to some extent, but some strong speech enhancement may achieve such properties. \n- The effectiveness of the automatic balancing scheme compared with the grid search is marginal. To show the effectiveness of the proposed method, it may find more complex cases for the grid search (e.g., more tuning parameters).\n\nNovelty\n- The optimization of speech enhancement with ASR loss is not novel. Of course, a part of dealing with unseen ASR systems is novel, but it is not sufficiently novel, and the paper requires more clarification of the technical novelty of this paper.\n- I think the automatic balancing scheme seems to be novel.\n\nReproducibility\n- Training data seems to be based on their own simulation methods. It is better to release this setup for reproducibility",
            "summary_of_the_review": "The paper does not have sufficient novelty in terms of the technical/algorithm part. Various researchers have already studied the joint training of both enhancement and recognition losses (as shown in Section 2). The automatic balancing scheme seems novel, but the effectiveness compared with the grid search is marginal.\n\nOther suggestions\n- Please discuss the reverberation cases. This is very critical for actual ASR deployment.\n- Can we use this method without the regression loss? In the practical scenario (real data situations), we cannot prepare the clean data, and we cannot construct the regression loss. I want the authors to discuss such cases and how the proposed method works.\n- Abstract: It is better to provide more concrete descriptions (e.g., with some numbers) to claim the effectiveness of the proposed method experimentally. \n- Can you discuss whether the proposed automatic balancing scheme more generally applies to the other machine learning problems? I think such a discussion makes the potential effectiveness of this scheme strong.\n- The introduction (section 1) and motivation (section 2) have some similar logical flows, and this part can be refined.\n- Why did you use DEMUCS? Can you add some references about the state-of-the-art performance of DEMUCS in some SE tasks?\n- Table 1. Please use the same decimal place for all numbers (2nd decimal place).\n- Section 4.2 \"As seen in Table 2, the result of INIT is comparable to the state-of-the-art results of CHiME-4\" Can you add a reference? Which system are you comparing?\n\n\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1670/Reviewer_DCcQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1670/Reviewer_DCcQ"
        ]
    }
]