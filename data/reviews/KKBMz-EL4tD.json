[
    {
        "id": "zYM_RuVugv",
        "original": null,
        "number": 1,
        "cdate": 1666636023207,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636023207,
        "tmdate": 1666636023207,
        "tddate": null,
        "forum": "KKBMz-EL4tD",
        "replyto": "KKBMz-EL4tD",
        "invitation": "ICLR.cc/2023/Conference/Paper367/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Alternating Differentiation for Optimization Layers (Alt-diff), a method for differentiation of parameterized optimization problems. Prior methods first solve the optimization problem on the forward pass using e.g. interior point methods, and then implicitly differentiate the KKT conditions on the backward pass by solving a linear system of equations.\n\nAlt-diff is based on instead using ADMM to solve the optimization problem on the forward pass by decoupling it into multiple subproblems, which are then solved in an iterative alternating fashion. The jacobian can then be computed simultaneously to the forward pass, by differentiating the subproblems in the same iterative alternating fashion as the forward computation. The main benefit from this new approach is a computational speedup due to the reduced complexity of implicitly differentiating the subproblems with a smaller KKT matrix, compared to directly differentiating the original problem.\n\nThe authors show that the backward pass enjoys quadratic complexity. They also show that the runtime can further be improved by truncating the iterative procedure. This gives a tradeoff between the error on the jacobian and the runtime, which is also theoretically analysed in the paper.\n\nThe proposed method is experimentally tested on multiple experiments, including a runtime analysis of optimization layers with randomly generated parameters, an energy scheduling problem in predict-then-optimize fashion, and an image classification task. The experiments show that the proposed method can often yield results of quality similar to state-of-the-art methods in less time.",
            "strength_and_weaknesses": "### Strengths:\n- The idea of using ADMM instead of competing methods to solve optimization problems in implicit layers is appealing, due to the simplicity, gneral applicability, and high parallelizability of ADMM.\n- The paper is overall well written and well structured.\n- The results are mostly convincing, with runtimes that are highly competitive with prior methods.\n\n### Weaknesses:\nWhile this paper proposes a method that brings clear advantages of ADMM to implicit layers, I believe that the claims of Alt-Diff being strictly better than previous methods are overstated.\n(Examples: \"Alt-Diff substantially decreases the dimensions of the Jacobian matrix and thus significantly increases the computational speed of implicit differentiation\",  \"comprehensive experiments demonstrate that Alt-Diff yields results comparable to the state-of-the-arts in far less time\")\n\nFirstly, the complexity analysis in Table 1 shows that in both prior methods and in Alt-Diff, the computation required for the backward pass is of the same order (or less) as the forward pass.  (Note that OptNet [1] even claims: \"The backward pass gradients can be computed \u201cfor free\u201d after solving the original QP with this primal-dual interior point method, without an additional matrix factorization or solve.\")\n\nTherefore the backward pass computation is not necessarily the computational bottleneck. And for the forward pass, from the complexities in Table 1 it is by no means obvious that using ADMM (Alt-Diff) is always better than using e.g. an Interior Point Method (OptNet), as there is a strong dependence of the practical runtimes on the number of iterations M and T needed for convergence.\n\nThis is also evident in the experiments, as the results for the sparsemax layer in Table 4 show that Cvxpy is much faster than Alt-Diff when disregarding the initialization, which to my understanding only needs to be performed once in the training procedure, so Cvxpy has a clear advantage over Alt-Diff in this setup.\n\nIn my opinion the story of the paper should be adjusted to accurately reflect that Alt-Diff does not make previous methods obsolete, and instead is one promising option among others, and choosing the best one highly depends on the problem at hand.\n\n[1]: Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In International Conference on Machine Learning, pp. 136\u2013145. PMLR, 2017.\n\n### Additional questions:\n- While I also believe that the idea of trading accuracy for runtime by truncation is very interesting, I do not think that this is inherent to ADMM and therefore could also be applied to speed up the competing methods. Have the authors tested increasing the tolerance to see if a similar speedup can be achieved?\n- It would be useful to split the reported computation time into forward and backward pass also for OptNet.\n- The results for OptNet are computed on a CPU, however, to my understanding OptNet was optimized for running on a GPU. This could potentially affect the runtime results. Have the authors attempted to run the experiments on a GPU?\n\n\n### Minor issues:\n- Equation 18, 19 (and text before): The computation misses the dependence on the parameter $q$. However, the final result in Equation 19 appears to be correct. A similar issue is in Equation 22, 23 with a missing dependence on $y$.\n- Equation 32: $A$ and $G$ transposed missing in third line.\n- Equation 35: First $\\rho$ should be removed from second line.\n- Equation 37: \"opt\" subscript is missing/missused.\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n- The clarity of the paper is good, the ideas are clearly presented. \n### Quality\n- There are some weaknesses in quality as reported in strengths and weaknesses.\n### Novelty\n- While using ADMM in an algorithm for implicit differentiation is not novel in itself (e.g. it was used for solving the homogenous self-dual embedding of a cone program in [2], which is used in Cvxpy), the idea of implicitly differentiating the subproblems of ADMM in an iterative fashion to compute the jacobian in parallel to the solution of the optimization problem is novel to the best of my knowledge.\n### Reproducibility\n- The authors provide code for one of the experiments (sparsemax layer). I tested the code and was able to reproduce the results reported in Table 4. The authors promise to release the code for the remaining experiments upon acceptance.\n\n[2]: Akshay Agrawal, Shane Barratt, Stephen Boyd, Enzo Busseti, and Walaa M Moursi. Differentiating through a cone program. arXiv preprint arXiv:1904.09043, 2019b.",
            "summary_of_the_review": "The idea of reducing the computational complexity of implicit differentiation by decomposing the optimization problem in an ADMM style is exciting.\nMy main concern is that the authors overstate the general superiority of the proposed method and do not highlight enough the importance of choosing an appropriate algorithm for the optimization problem at hand.\nIf the remaining issues regarding the presentation of the comparison to previous works are resolved, I believe this paper will be of great interest to the ICLR community.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper367/Reviewer_apdv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper367/Reviewer_apdv"
        ]
    },
    {
        "id": "IutOg9S6PU7",
        "original": null,
        "number": 2,
        "cdate": 1666649685515,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649685515,
        "tmdate": 1666649685515,
        "tddate": null,
        "forum": "KKBMz-EL4tD",
        "replyto": "KKBMz-EL4tD",
        "invitation": "ICLR.cc/2023/Conference/Paper367/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "An optimization layer is a function that assigns to given parameters $\\theta$ the solution $x^*$ of an optimization problem $\\min_x f(x,\\theta)$ subject to constraints $x\\in C(\\theta)$. These layers are also implicit layers as $x^*$ is a solution of a certain implicitly defined function $\\cal F(x^*,\\theta)=0$. Under reasonable assumptions, these layers have analytical gradients $\\partial x^*/\\partial\\theta$, and they can be used in end-to-end trainable NN models. The aim is to study how to calculate the forward and backward passes of these layers efficiently.\n\nThe paper studies the alternating direction method of multipliers (ADMM) for convex optimization layers ($f$ is convex). On the forward pass, at every ADMM iteration, the primal and dual variables and their Jacobians are updated until convergence. The paper contains theoretical justification, complexity analysis and comparison to OptNet and Cvxpylayers (using the interior point method), and the performance is demonstrated and compared on a synthetic and two real-world datasets.",
            "strength_and_weaknesses": "Strengths:\n- Approaches an important and difficult problem with a different viewpoint by using ADMM\n- Convincing evaluation showing performance improvement over SOTA in some cases\n- Well-written related work providing enough background to understand the paper\n\nWeaknesses:\n- As I understood the paper's message, it claims the supremacy of Alt-Diff over OptNet and cvxpylayer in all cases. However, this is overstated:\n\t- OptNet and cvxpylayers never compute the full Jacobian matrix on backward (as claimed in the abstract and also later in section 3). In fact, from how the forward is computed, the backward is \"for free\" (see the first line of 3.1.1. of OptNet paper), i.e. O(1). This is also true for Alt-Diff; there is, in fact, no backward pass. In any case, only the larger of the two (forward/backward) counts.\n\t- Hence, we are comparing $O(T(n+n_c)^3)$ of cvxpylayer (interior point method) to $O(Mn^3)$ of Alt-Diff (ADMM). What ultimately decides is the number of iterations $T$ and $M$. Therefore, one should use the method which is suitable for a given optimization problem: If the ADMM converges faster, Alt-Diff is the right method, and if the interior point method works better, cvxpylayer should be used.\n\t- 'Additional numerical experiments' in Appendix F.1 on sparse quadratic programs actually proves my point: cvxpylayer is 30x faster than Alt-diff since the initialization needs called only once.\n- Convergence analysis. It is evident from the experiments (Fig. 2a) that the gradient is correct. Unfortunately, I do not see how it follows from the proof of Theorem 4.2.\n\t- I do not understand where Lemma 4.1 is used. Also, why from all possible fixed point theorems, this particular one is quoted (for reflexive Banach spaces). The assumption should also be 1-Lipschitz (nonexpansive) mapping from a bounded closed convex set into itself.\n\t- Thm 4.2: The proof contains computations showing that differentiating optimality conditions of ADMM leads to the same analytical derivative as differentiating the KKT conditions (this seems to be more or less correct). Any proof that the convergence of values (guaranteed by the convergence of ADMM in the convex case) yields the convergence of derivatives is missing (referred as 'obvious'). In general, convergence (even uniform) of functions does not imply convergence of derivatives (see eg. https://en.wikipedia.org/wiki/Uniform_convergence#To_differentiability). It is not, in my opinion, obvious, and it should be clarified.\n\t- Theorems 4.3 and 4.4 present expected results under very strong assumptions with elementary estimates. However, in particular, estimate 11 proves the convergence of derivatives in this case.\n\nsugesstions, typos etc.:\n- eq. 6: is $s\\ge 0$ correct here?\n- Thm. 4.2: $\\partial x^*/\\partial\\theta$ denotes the true analytical Jacobian; there is no need to refer to cvxpy.\n- Algo. 1: The elements of the last iteration should be returned.\n- Tab. 1: Is $k$ correct here? ($=M$?)\n- Section 5.1 Matrix $P$ seems to be only defined later in the appendix.",
            "clarity,_quality,_novelty_and_reproducibility": "To my knowledge, using ADMM in optimization layers on both forward and backward passes is a novel and significant contribution. The idea is clearly presented and easy to follow. I have some concerns about the contribution exaggeration and quality of the technical part; see the weaknesses above. I have not run the supplemented code alone, but I do not expect any surprises in reproducing the reported numbers.",
            "summary_of_the_review": "The paper persuasively shows that differentiating ADMM leads to better runtimes in optimization layers where ADMM is the appropriate algorithm. If the authors address the abovementioned issues, the paper will be a valuable asset to the community.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper367/Reviewer_kA5n"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper367/Reviewer_kA5n"
        ]
    },
    {
        "id": "3ZvTe0GzGW",
        "original": null,
        "number": 3,
        "cdate": 1667394516869,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667394516869,
        "tmdate": 1667394516869,
        "tddate": null,
        "forum": "KKBMz-EL4tD",
        "replyto": "KKBMz-EL4tD",
        "invitation": "ICLR.cc/2023/Conference/Paper367/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose Alt-Diff, a framework for differentiation of optimization layers in neural networks. The basic idea is to write down the augmented Lagrangian for the considered class of convex optimization problems with polyhedral constraints, derive associated ADMM updates in which objective and constraints are decoupled, and therefrom gradients of primal, slack and dual variables w.r.t. optimization parameters. The resulting recursive sequence of gradients is theoretically shown to converge (in terms of the number of ADMM iterations) towards the gradient obtainened by implicit differentiation. In addition, the authors provide a complexity analysis of the proposed method, theoretical upper bounds on the error in gradient computation introduced by truncation of the recursive scheme after a finite number of iterations, and numerical experiments.",
            "strength_and_weaknesses": "Strengths:\n\nCompared to existing approaches that use implicit differentiation, namely OptNet and CvxpyLayer, the proposed framework needs no differentiation through KKT conditions which reduces the dimension of the required inverse Hessian by the number of constraints of the optimization problem. As a byproduct, the inverse Hessian for differentiation can be reused from the preceding ADMM update. This reduces the complexity of the backward pass from cubic to quadratic. Compared to unrolling, the proposed method does not require that intermediate iterates from the forward pass are stored for backpropagation as gradients are computed on the fly. The presented numerical results suggest that Alt-Diff yields state-of-the-art accuracy on benchmark problems while saving significant computational budget.\n\nWeaknesses:\n\nThe proposed approach still needs inverse Hessians. Through the decoupling of objective and constraints, the scalability in terms of the number of constraints is improved. On the other hand, if I see it correctly, the scalability in terms of the number of optimization variables is not better than in case of existing approaches. Moreover, the threshold $\\epsilon$ needed for the stopping criterion constitutes an additional tuning parameter which might not be obvious to choose in practice.",
            "clarity,_quality,_novelty_and_reproducibility": "From my point of view, this paper is well-written and comprehensible. The authors provide an appropriate introduction and review of related work so that the paper is more or less self-contained. As far as I can see the results are novel, and the authors have published their source code. ",
            "summary_of_the_review": "All in all, I think that this is a very good paper. The authors provide a tractable approach for the differentiation of optimization layers accompanied by a rigorous theoretical analysis, and the numerical results seem convincing to me. I would say that the above-mentioned weaknesses are rather mild, and the approach stands well for itself in comparison with existing approaches.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper367/Reviewer_mj5g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper367/Reviewer_mj5g"
        ]
    }
]