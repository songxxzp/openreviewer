[
    {
        "id": "h7dgnQPvKN",
        "original": null,
        "number": 1,
        "cdate": 1666487203688,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666487203688,
        "tmdate": 1666487203688,
        "tddate": null,
        "forum": "JVlyfHEEm0k",
        "replyto": "JVlyfHEEm0k",
        "invitation": "ICLR.cc/2023/Conference/Paper4830/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a theoretical results of FOMAML that shows train-train and train-validation methods can achieve a small training loss. It is necessary to perform a train-validation split in the task data to get good generalization results. Compared to previous work that only studies linear models . This paper advances the results to two-layer convolutional networks.\n",
            "strength_and_weaknesses": "**Strength**:\n\n* This paper presents theoretical results of FOMAML and advances previous results to convolutional neural networks. \n\n* The paper is well-written\n\n**Weakness**:\n\n* Does the theoretical results or analysis also applicable for second-order MAML, implicit MAML[1], closed-form solver[2], etc?\n\n\n* If the number of samples is large, how do the theoretical results change with different numbers of samples? In this case, does train-validation still better train-train meta learning?\n\n\n* How does the results compare to Reptile[3], which does not need a train-validation split.\n\nReferences;\n\n[1] Meta-Learning with Implicit Gradients. NeurIPS 2019\n\n[2] Meta-learning with differentiable closed-form solvers. ICLR 2019\n\n[3] On First-Order Meta-Learning Algorithms\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written.\n\nThe novelty is moderate.\n\nResults seem to be reproducible. ",
            "summary_of_the_review": "This paper proposes a theoretical results of FOMAML that shows train-train and train-validation methods can achieve a small training loss. The results advance previous results and provide a deeper understanding of meta-learning. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4830/Reviewer_SqBj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4830/Reviewer_SqBj"
        ]
    },
    {
        "id": "bn-OamvV7d",
        "original": null,
        "number": 2,
        "cdate": 1666732937653,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666732937653,
        "tmdate": 1666732937653,
        "tddate": null,
        "forum": "JVlyfHEEm0k",
        "replyto": "JVlyfHEEm0k",
        "invitation": "ICLR.cc/2023/Conference/Paper4830/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, authors provide a theoretical analysis regarding the usage of training data only during episodic training in a meta-learning setup against train each episode using train and compute the loss on a validation split to propagate back through the network. The second strategy is commonly used by practitioners, however, some existing relevant work argue that it might not be required. In this work, authors show that using a train-val split strategy can achieve lower test loss in a meta-learning setup (when used with a two-layer convolutional network and first-order MAML as the meta-learning algorithm).",
            "strength_and_weaknesses": "**Strengths**\n\n* The idea of training using a train split and back-propagating using the loss on the validation split has been a longstanding practice in the meta-learning literature - however, there has not been a lot of interest in analyzing that, either empirically of theoretically. Only existing work on this is from [Bai et al.](https://arxiv.org/pdf/2010.05843.pdf) which the authors discuss here. Theoretically analyzing whether train-val split is needed for meta-learning is an important topic.\n* Most of the assumptions made in terms of proving the bounds make sense to me. Authors use a Huberized-ReLU instead of a normal ReLU to simplify the proof and show that empirically it does not make much difference. I have not looked at the derivation in details provided in the appendix - the lemmas stated in the main manuscript makes sense to me at a high-level. \n* Authors perform experiments using synthetic datasets and also using a two-layer network on standard datasets like Mini-ImageNet which corroborates their theoretical findings.\n\n**Weakness**\n\nFor me, the biggest weakness or doubt about this paper that I have in mind is how well these results will extrapolate to other type of networks or networks with more layers. There is a discrepancy in terms of findings between this work and Bai et al. where Bai et al. claimed train-train split is enough with a linearized network and this work claims that their hypothesis is not valid when using a two-layer convolutional network. However, the empirical results of Bai et al. speaks a different story - if we see their Table 1 (page 11), they experiment with the same convolutional backbone as this paper on Mini-ImageNet and Tiered-ImageNet and show that train-train works equal or better compared to train-val, which contradicts the claims made in this paper. \n\nAlong the same line, my other question is - is there any guarantee that claims made in this paper will hold true for a different kind of architecture e.g. vision transformers or if we simply increase the number of layers in the convolutional network/add a normalization layer/add a residual connection? If these networks are too complicated to be analyzed theoretically, I think some empirical evaluation using train/train vs train/val strategy will help to justify the claims made in this paper. \n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe paper is written in a succinct way - it unambiguously states the problem it wants to analyze, clearly defines the notations and describes the lemmas. Empirical evaluation setup is also described well.\n\n**Quality and Novelty**\n\nThe proposed work is one of the very few which explores the usefulness of train-train vs train-val strategy used in episodic meta-learning setup. The theoretical framework borrows ideas from Bai et al., however, the results they found are different so there is novelty in that aspect. The quality of the paper is still in doubt - especially with respect to the contradictory empirical vs theoretical results when compared to Bai et al. I would like authors to comment on it before I finalize my thoughts on the quality/novelty aspect.\n\n**Reproducibility**\nThis is mostly a theoretical paper and implementation ideas analyzed in this paper are well-known.",
            "summary_of_the_review": "The paper is well-written, analyzes an important implementation details from the meta-learning literature from a theoretical viewpoint and provides bounds which show that train-val is a better strategy for the model to generalize as opposed to train-train which is the common practice in the literature. However, the empirical and theoretical results are in conflict with the previous work from Bai et al. which casts doubts about the contributions of this paper.\n\nI'll be willing to modify my score if authors can clarify my confusion about the results from Bai et al.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4830/Reviewer_7Hwe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4830/Reviewer_7Hwe"
        ]
    },
    {
        "id": "CqVnr7NDAz",
        "original": null,
        "number": 3,
        "cdate": 1666753004580,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666753004580,
        "tmdate": 1666753004580,
        "tddate": null,
        "forum": "JVlyfHEEm0k",
        "replyto": "JVlyfHEEm0k",
        "invitation": "ICLR.cc/2023/Conference/Paper4830/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors conduct an investigation into the importance of having train-validation sets for episodic few-shot learning, learned by meta-learning. They do as such by both theoretical proofs and empirical evaluation. \n\nWhile the intuitions behind having a train - validation set for meta-learning are quite clear and reasonable, this is a very good complementary study that showcases that the intuitions do in fact measure up to reality. ",
            "strength_and_weaknesses": "Strengths:\n\n- Clear empirical evaluation, with thorough experiments. \n- Inclusion of both theoretical justification and empirical results.\n\nWeaknesses:\n\n- Writing quality is clumsy, and at worst significantly affecting clarity and precision of what is being communicated. ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the work are its main problem, with the writing clarity affecting both the overall clarity of the work as well as its precision. \n\nThe work seems reproducible from what is in the paper.",
            "summary_of_the_review": "I recommended a weak reject due to the writing quality being problematic enough to cause precision and clarity issues in the work. If that is improved, I'd be happy to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4830/Reviewer_YDGh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4830/Reviewer_YDGh"
        ]
    },
    {
        "id": "x6I2JLotoSU",
        "original": null,
        "number": 4,
        "cdate": 1666758703925,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666758703925,
        "tmdate": 1666758703925,
        "tddate": null,
        "forum": "JVlyfHEEm0k",
        "replyto": "JVlyfHEEm0k",
        "invitation": "ICLR.cc/2023/Conference/Paper4830/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper analyzes the train-validation split in meta-learning for classification problems with a two-layer CNN model. It proves that the train-validation split is necessary to learn a good prior model compared with a train-train model. Experiments justify the theoretical claims.",
            "strength_and_weaknesses": "Strength:\n- There is few research work studying train-validation split of meta-learning with neural networks, so it is great to have such work targeting on the theoretical foundations.\n- The theoretical analyses seem to be solid but I have not checked the proof details.\n\nWeakness:\n- What does it mean by \"FOMAML\" (first appears at Page 2)? It seems no where the term is introduced.\n- There is a little gap between the theories and the experiments, namely it would be great to have some synthetic data and experiments to meet the theoretical assumptions and to see how it matches to the theoretical claims. For example, we can generate synthetic data by Definition 3.2, then we can use a two-layer CNN described in the theoretical analysis as the backbones. To verify Thm 4.4 and 4.5, we can set different values of $d$ in synthetic data and to confirm the training loss is at the order of O(1/poly(d)) as claimed in the theorems. Similarly for the test loss, we want to see a figure plotting the test loss evolution, where train-train test loss converges to some constant while train-validation test loss continues to decrease. I think the theories and experiments can be more closely connected in this way.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the quality and clarity are good, and the originality is great since there is few work studying train-validation split of meta-learning with CNN.",
            "summary_of_the_review": "Based on the novelty and strengths discussed above, I am close to an acceptation of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4830/Reviewer_kJcp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4830/Reviewer_kJcp"
        ]
    }
]