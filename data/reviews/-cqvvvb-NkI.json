[
    {
        "id": "1QIXsrDvxJ_",
        "original": null,
        "number": 1,
        "cdate": 1665862213039,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665862213039,
        "tmdate": 1665862213039,
        "tddate": null,
        "forum": "-cqvvvb-NkI",
        "replyto": "-cqvvvb-NkI",
        "invitation": "ICLR.cc/2023/Conference/Paper5181/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors presents RECITE, a novel prompting Language Model (LM) paradigm to improve know knowledge-intensive NLP tasks. The main idea is to first let the language model to generate knowledge via prompting (knowledge-recitation) and then to let the language model solve the task (task-execution). The author evaluate the performance of the proposed approach with three close book QA tasks (Natural Questions, TriviaQA, and HotpotQA) and three LMs (In-house LM (62B), UL2 (20B), and OPT (30B)). The results shows better performance than direct prompting and chain-of-thoughts, and comparable performance to retrieval methods (BM25).",
            "strength_and_weaknesses": "Strength\n- Retrieval free QA model are an important topic to study, and RECITE enable midsize LMs to achieve competitive performance in the close-book QA. \n\nWeaknesses\n- As also mentioned is Limitation, the main drawback is the the updated of the model with new knowledge. This is even more evident when compared to simple DB retrieval in Table 5, where BM25 performs better that LM-Recitation 5-shot with 20-path (the best proposed model). \n\nQuestions/Suggestion:\n- What is the size of the index + docs (compressed) of BM25? is the size larger or smaller of the model parameter? if the size of the DB is larger of the model parameter, why not constrain it (e.g., randomly removing articles) and check if the performance in Table 5 changes? \n- What is the inference speed of LM-Recitation K-shot with t-path? How's compared to BM25? \n- Does scale matter? what's the performance of larger LM (PaLM-500B or GPT-3/Instruct-GPT3) or smaller (GPT-J 6B)?  \n- Figure 4 shows the performance difference by sampling more self-consistency paths, but what append if the model samples more responses for the direct method (or chain-of-thoughts)? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Very clear\nQuality: Very high quality \nNovelty: somehow novel\nReproducibility: 2 out of three LM are available for the research community.\n",
            "summary_of_the_review": "The paper propose a two step approach to improve knowledge-intensive NLP tasks by using only LMs (no external retriever).  However the model performance are a bit lagging compare to simple BM25 + LMs, plus the clear weakness of updated knowledge.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5181/Reviewer_m3Mw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5181/Reviewer_m3Mw"
        ]
    },
    {
        "id": "n2hVWdUjBd6",
        "original": null,
        "number": 2,
        "cdate": 1666383955340,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666383955340,
        "tmdate": 1668778562914,
        "tddate": null,
        "forum": "-cqvvvb-NkI",
        "replyto": "-cqvvvb-NkI",
        "invitation": "ICLR.cc/2023/Conference/Paper5181/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new prompt engineering algorithm for few-shot open-domain question answering with pretrained language models. The key idea is that instead of asking language models to directly generate answers to factoid questions, ask it to first generate a paragraph of text which will contain the answer (\"recitation\"). After recitation is complete, language models are instructed (via prompt examples) to use their recitation to answer the question. The paper additionally adopts several tricks to improve the performance of their system, including \"self consistency\" (generating through multiple recitations and taking majority vote), multiple recitations (for multi-hop questions). Finally, the authors explore fine-tuning a model to generate diverse recitations via passage hints. The authors use synthetic data generated by the frozen LM to achieve this goal.\n\nThe authors conduct experiments on Natural Questions, TriviaQA and HotpotQA, and find performance improvements over standard prompting techniques.",
            "strength_and_weaknesses": "**Strengths**\n\n1. This is an interesting idea, and an exciting way to incorporate the ideas of \"chain-of-thought\" prompting and \"self-consistency\" for open-domain QA tasks where the answer is a factoid.\n\n2. The authors observe 2-6% improvements over standard direct prompting across all 3 tasks / 3 models. This is quite good and surprising for me --- I had thought the model would be able to answer the factoid question directly if it's able to generate a much longer paragraph containing the answer. It is interesting that the method requires multiple recitation paths to work (in Figure 4 performance is lower than standard prompting with just one recitation path). However, I think of this as a strength of the proposed method, since you cannot really use multiple paths if you are generating a direct answer (since the answer is so short, sampling doesn't make sense).\n\n3. The authors perform several insightful analysis experiments discussing robustness to prompts, comparison to BM25 retrieval, and an error analysis.\n\n**Weaknessess**\n\n1. The paper would be much stronger with experiments on GPT3, Instruct-GPT3 (davinci-002), and larger language models (larger in-house LMs?). It's not really clear from the paper whether recitation helps with larger scale, which I think is important for the generalizability of the method [1]. This could work both ways --- I'm suspecting larger LMs will be better at both recitation and directly performing QA. I think experiments on InstructGPT [4], T0 [3] or FLAN [2, 7] will be especially interesting, since it's been fine-tuned on instructions / examples / human preferences.\n\n2. A major advantage of retrieval augmented systems is their applicability on (1) tail distribution information; (2) generalization to information which was not present in the model's training set (like COVID for BERT). I think these are important limitations of the proposed method, and (1) is not really discussed (2 is just mentioned in the conclusion). Are most of the correct recitations cases which were seen often during training?\n\n3. Overall, the performance of closed-book models in this paper seems to significantly lag behind recent few-shot retrieval-augmented systems [5, 6]. For instance, ATLAS [5] gets 42% on NQ with 64 examples and a smaller model, while the best number in this paper is 32% (5-10x larger model). While I agree that setting up retrieval is technically cumbersome, there are very good retrieval APIs available, which were utilized in [6] without any extra LM fine-tuning. Note that I do think it's incredible that closed book LMs are doing so well, but practically (from a performance stand-point) it may be better to just retrieve some text from the web rather than ask an LM to generate it with few-shot examples. Also, retrieval augmented LMs often have lesser parameters [5], so it's unclear which is a better method from an efficiency perspective.\n\n4. I have mixed thoughts about the passage hints fine-tuning experiments, since it requires fine-tuning a large LM on Wikipedia data. Perhaps the performance gains are because of the dedicated fine-tuning on Wikipedia data for the recitation LM model (which makes it overfit to Wikipedia)? Did you remove the passages from the test set questions while doing this fine-tuning? Also I don't think enough experiments are done in the paper to justify its added complexity over vanilla LM-Recitation. I would suggest moving it to the appendix, or performing experiments on all 3 datasets / models to show its benefit.\n\n[1] - https://twitter.com/_jasonwei/status/1526589104758042624  \n[2] - https://arxiv.org/abs/2109.01652  \n[3] - https://arxiv.org/abs/2110.08207  \n[4] - https://arxiv.org/abs/2203.02155  \n[5] - https://arxiv.org/abs/2208.03299  \n[6] - https://arxiv.org/abs/2203.05115  \n[7] - https://arxiv.org/abs/2210.11416\n\n**Minor**\n\nThis paper is relevant to https://arxiv.org/abs/2004.05483 and https://arxiv.org/pdf/2110.08387.pdf, it would be great to cite them.\n\nIn Table 4 (LM-Recitation_5), why is the number for different from Table 1 (Recite and answer)? (16.34 in Table 1 vs 14.16 in Table 4)",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity* - Very clear\n\n*Quality* - Very thorough experiments overall except for the experiments on passage hints. I would have liked other models being tested (weakness #1), but the experiments on the current set of tasks look good to me.\n\n*Novelty* - Good novelty. The idea has similarity to chain-of-thought prompting, self-consistency prompting, and self-talk, but overall I think the idea is pretty new (especially in the context of large LMs and QA).\n\n*Reproducibility* - Should be fully reproducible except the experiments on the in-house LM.",
            "summary_of_the_review": "The paper has interesting ideas and surprising results, but I have two main concerns - (1) the paper does not evaluate the method on larger LMs which are available; (2) I don't think there's justification that this method is a replacement for retrieval in any way (weakness #2, #3).\n\nI am currently leaning reject, but will be happy to move to the accept range if weakness #1 is addressed via experiments on GPT3-170B and InstructGPT3-170B.\n\n------\n\n**After rebuttal**: Thanks to the authors for the very detailed response! I've decided to raise my score to 6 (accept range) due to the improvements shown on Codex. I would still suggest the authors to take a more balanced take in their conclusion, mentioning that while there are improvements over direct generation, there is still a gap behind retrieval-augmentation on NQ.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5181/Reviewer_HxiC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5181/Reviewer_HxiC"
        ]
    },
    {
        "id": "U_DYqQ8Okrn",
        "original": null,
        "number": 3,
        "cdate": 1666631930308,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631930308,
        "tmdate": 1666631930308,
        "tddate": null,
        "forum": "-cqvvvb-NkI",
        "replyto": "-cqvvvb-NkI",
        "invitation": "ICLR.cc/2023/Conference/Paper5181/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors utilize LLM to first retrieve relevant information from LLM\u2019s own memory (by sampling), then accomplishing more accurate factual generation to produce final outcomes. The core idea is motivated by the intuition: recite-step (retrieval) that recollects relevant knowledge pieces helps answer-step (generation) better output to knowledge intensive questions. Two models are provided:\n\nFor the prompt-based model,\n(a) It first performs in-context learning for generating recitation provided with few-shot question-recitation examples.\n(b) Then it performs another in-context learning to generate the right answer provided with the corresponding recitation and question as well as the question-answer pairs\n(c) For a given question, the model samples multiple recitations (by (a)) and greedy-decode the answers (by (b)) for each sampled recitation.\n(d) The model could do multi-hop reasoning by sequentially generating recitation (by simple number prompting) till the special tokens occurs, then finding the answer based on chain-reasoning.\n\nFor the passage-hint model,\n(a) For each question-answer pair, prepare several Wikipedia pages that are ranked at the top (from a search engine) for the question query. \n(b) Create the corresponding passage hint by concatenating hierarchical title orders from the topic and section/subsection titles to the position of the paragraph in the (sub)section. \n(c) Given evidence-question pairs, perform in-context learning to sample a new question given an evidence passage.\n(d) Learn a fine-tuned LM that outputs passage hint (from (b)) as well as evidence passage (from (c)) given a question input.\n(e) Given a question, fine-tuned LM first samples multiple passage hints and passage evidence (by (d)). \n(f) Aggregate multiple passage evidences (in (e)) as a diverse recitation to the given question, then generating the answer with a few-more question-answer demonstrations in a frozen language model.\n\nThe authors verify that the recite-and-answer method is effective for Closed-Book Question-Answering tasks being compatible with other useful approach that improve few-shot in-context learning capability.",
            "strength_and_weaknesses": "(Strengths)\n\n(1) Intuitive ideas\n\n(2) No need to retrieve documents.\n\n(3) Compatible with other approaches like chain-of-thought prompting.\n\n\n\n(Weaknesses)\n \n(1) Major concerns that will be listed below.\n\n(2) Unclear cost for running the proposed methods against the standard-prompting models.\n\n(3) Insufficient rationale to support gains and no gains ",
            "clarity,_quality,_novelty_and_reproducibility": "Mostly clear and novel.",
            "summary_of_the_review": "(1) As the passage-hint model is a corpus-specific treatment that additionally utilizes title hierarchies, the prompt-based model must have great added benefits. The core concern is that the model performs in-context learning based on the model-generated recitations. No matter how you tune the temperature parameter, there must be a pathological trade-off between sacrificing diversity and sacrificing factuality. The current draft does not address impacts of less-accurate or incorrect recitations. \n\n(2) It will be great to have the performance graph (given two metrics) over increasing number of few-shot prompts. If you could approximately measure the correctness of generated recitations, another figure that demonstrate the performance dependency with respect to the number of correct shots will benefit readers.\n\n(3) Another major concern is the computational trade-off from using many shots. Table 1 shows that the proposed recite-and-answer model (with 5 shots) does not have significant impacts against standard-prompting (with more shots). Can you address the claim that using the standard-models with more shots is more expensive than running your prompt-based model with less-shots?\n\n(4) It is less clear that how you decide the number of shots you would use for standard-prompting model and your recite-and-answer model. For example, UL2-20B is tested with 16 shots, whereas the in-house version used 64 shots. Justify the rationale of picking the number of shots and the reason not to test on more shots for OPT-30B.\n\n(5) Excluding UL2-20B (mostly encoder-decoder T5 structure except the mixture of denoiser variations with multiple special tokens), is your In-house model of decoder-only? If so, why In-house has significant performance gain on HotpotQA but no visible boost on TriviaQA, which is the entirely opposite to OPT-30B? Any specific example-based explanation than a conjecture would be beneficial.\n\n(6) The passage-hint model consists of significantly more steps. It will be useful to see overall time-cost trade-off between your model and standard models.\n\n(7) Is the passage-hint model flexible if the evidence documents do not have a clear and fine-grained title/subtitle structures? What if some of title components from this hierarchy is missing? Should we expect performance degradation or performance boost because the passage hints are also generated being possibly incorrectly if the information structure is too granular.\n\n(8) Overall, not much qualitative examples are provided. Having some successful and failure cases by examples will be highly \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5181/Reviewer_rRR1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5181/Reviewer_rRR1"
        ]
    },
    {
        "id": "yVg2-6kjkFI",
        "original": null,
        "number": 4,
        "cdate": 1666666151156,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666151156,
        "tmdate": 1666666151156,
        "tddate": null,
        "forum": "-cqvvvb-NkI",
        "replyto": "-cqvvvb-NkI",
        "invitation": "ICLR.cc/2023/Conference/Paper5181/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a LLM-based method to improve accuracy in factual knowledge generation. Instead of retrieving from external resources, the proposal trains a model to recite relevant knowledge given a query and subsequently answer the query conditioned on the recitation. The proposal is empirically shown to be effective on the closed book question answering task, with some ablation and analyses on various model settings.",
            "strength_and_weaknesses": "Strengths\n- The proposed method is intuitive and well motivated.\n- Evaluation is sensible and yield positive results both wrt performance and data/resource efficiency.\n- Writing is mostly clear.\n\nWeaknesses\n- The analysis section on what works and why is a bit thin; Table 5 overall is quite confusing. I believe answering the following questions  may help add more clarity and strength to this part.\n  - What does Hits mean? What's the difference between @1 and @20? What is Hits@1 (no explanation in text)? What is (EM) following Hits@1? I'm guessing that's the subset of examples that the model answers correctly?\n  - If I'm guessing correctly, the definition of Not Recit, hits@20-recit., and hits@20-path follows a binary decision tree, on whether the ground truth answer appears in any recitation or any QA output. It is rather difficult to figure this out based on the text in 4.3.4. Maybe a logical tree of some sort can help improve readability here.\n  - Are there cases where, e.g., the ground-truth answer is not recited but the model still answers the question correctly? E.g., a more detailed characterization of Hits@1 would be interesting, unless reciting the gt answer is an empirical necessity \u2013 which by itself is interesting to point out if true.\n  - How are the 1024 examples selected?\n  - On a more minor note, the names for categories are very not self-explanatory and could be improved for better readability.\n- There are some missing details that might slightly hurt readability\n  - Judging from Fig 2-(1), <Recitation N> is generated from \u2013 and directly related to \u2013 <Question N>, with other Q-R pairs as in-context examples. Moving on to Fig 2-(2), it seems there are multiple QA pairs (Q0, A0, ...) inserted between <Recitation N> and <Question N>. Are the questions in these QA pairs the same as those in the QR pairs in the top-left block of Fig 2? Are they somehow related to <Question N>? If not, is there going to be some long-distance memory issues between <Recitation N> and <Question N>?\n  - In Fig 2-(3), are the multiple paths (containing Recitations-dots-Answers) repeating Fig 2-(2)? \n  - In the multi-hop setting, do the numbered prompts like \"Recitation 1\" and \"Recitation 2\" naturally give rise to generating recitations of different topics, or does one have to train/update the LM to solicit such behaviour?\n  - \"Since the multiple recited passages are generated in one-pass from the LLM decoding sequentially, ...\" How to use multiple numbered prompts to generate multiple recitations in one-pass sequentially?\n  - I'm personally curious about more details on the generated recitations, e.g., in NQ dataset, do the recitations resemble the \"ground truth\" passages in number of words/sentences/paragraphs? What percentage of recitations contain the correct answers?\n- Minor points on motivations\n  - One motivation mentioned for recitation is to match the form of causal LM pre-training objective (with the decimal of $\\pi$ example). Personally, I tend to believe that recitation helps by making certain knowledge more explicit (value of $\\pi$), and when primed on this explicit piece of information (in the form of recitation), the LM is then more likely to arrive at the correct answer (the 10th decimal of $\\pi$). In other words, I'd argue that with or without recitation, the objective remains the same (i.e., masked or next-word prediction), so this motivation to me feels a bit mis-directed.\n  - \"Human's ability to recite relevant factoid knowledge before answering knowledge-intensive questions.\" Do humans have this ability? Regardless of the answer, having this ability doesn't necessarily help humans answer knowledge-intensive questions more accurately, which I guess should be the true human-inspired motivation for the proposal. In any case, citations are perhaps needed to make either claim.",
            "clarity,_quality,_novelty_and_reproducibility": "Both the core idea and writing are both clear and comprehensible. Missing details may well be compensated by open sourcing the code for reproducibility. The idea itself is well-motivated and novel.",
            "summary_of_the_review": "The paper is of overall good quality, with a sound idea and good execution by putting many moving parts together into a working system with empirical gains. The analysis part is a bit unclear and underwhelming.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5181/Reviewer_9YM6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5181/Reviewer_9YM6"
        ]
    }
]