[
    {
        "id": "i4uO8Suspa-",
        "original": null,
        "number": 1,
        "cdate": 1666557092834,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666557092834,
        "tmdate": 1666557092834,
        "tddate": null,
        "forum": "am22IukDiKf",
        "replyto": "am22IukDiKf",
        "invitation": "ICLR.cc/2023/Conference/Paper2063/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new distillation method, with a teacher model given a long, detailed instruction as well as in-context examples, and a student model given a concise instruction. The motivation is that, while the model performs the task well by reading a detailed instruction and examples, it is often expensive at inference time, because it has a significantly longer context. Therefore, having a student model that operates with shorter instruction but works as well as the model given a long instruction and in-context examples is beneficial. The paper evaluates the proposed model on a range of scenarios, effectively showing that the student model effectively learns from the teacher model with long instruction or explanation, and outperforms baselines such as naive instruction tuning without distillation.",
            "strength_and_weaknesses": "**Strengths**\n* The paper tackles an interesting problem \u2013 building a model that learns from a concise instruction and performs the task as much as the model given a detailed instruction and examples does. The problem is well-motivated \u2013 reading a long instruction and examples is often very expensive and inefficient, and is even strictly bounded by the maximum sequence length of the Transformer.\n* The idea of distilling from the model reading a long instruction to the model reading a shorter instruction is new.\n* The paper specifies a series of hypotheses and shows empirical results that verify each hypothesis very clearly.\n\n**Weaknesses**\n* Overclaiming:\n    * The paper made a strong emphasis that the proposed model is able to \u201cinternalize\u201d the task, and made this claim many times overall in the paper. However, to my point of view, \u201cinternalizing the task\u201d is exactly \u201cfine-tuning\u201d, and in fact, the proposed method is all about \u201cbetter fine-tuning through distillation\u201d. In this regard, I think the paper is overall overclaiming.\n    * Some details that indicate the proposed model is based on more assumptions than the baselines are not explicitly mentioned. For instance, based on my understanding, the proposed model is trained on the unlabeled portion of the test datasets, whereas baselines are not. Is it correct? (I am not super sure about this and asked this question in the \u201cclarity\u201d section.) If it is true, it is a fairly strong assumption to assum the unlabeled data of the test datasets, and should be mentioned very clearly. Not making this clear seems overclaiming.\n* Missing the most critical baseline\n    * I think the most important baseline is a student model that is instruction-tuned on a particular template (either teacher\u2019s template or student\u2019s template) with no distillation. To my understanding, this baseline is not provided, except for one experiment (Table 2). Instead, the paper claims that the proposed model effectively learns from the teacher model because it is better than the pre-distill student model. However, to my understanding, the pre-distill model is not instruct-tuned, thus it is unclear if this gain is coming from instruction tuning, or distillation from a teacher model. For this reason, I strongly believe that the paper should have mainly compared with the model instruction-tuned but without distillation, and should not claim the effectiveness of the distillation method without this baseline. \n    * Side note: I think \u201cDirect Gradient Descent\u201d is basically the model that is instruction-tuned but without distillation. Is this correct? I think the name is not very intuitive. To my understanding, it is using the template for the teacher model (long instruction, in-context examples) \u2013 and if it is correct, I think it\u2019s also good to include the model using the template for the student model (concise instruction). This will support the assumption in the paper that longer instruction is more helpful than concise instruction, which this paper actually never explicitly showed (nor cited relevant prior work).\n* Experiments are not very consistent with no specific reason specified. For instance:\n    * In the experiments with Natural Instruction v2, the result is evaluated only on 5 datasets with the largest gains from instruction tuning and other 5 datasets with the smallest gains from instruction tuning. It is not clear why the paper chooses to do so instead of reporting all datasets, especially because the paper did not report these two sets of 5 datasets separately and discuss how their results differ.\n\nMinor:\n* While the paper claims that prior work in distillation is mostly focusing on reducing the model size, I don\u2019t think it is true \u2013 previous work has shown that distillation helps even when the teacher model and the student model have the same size of the model.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* The core idea of the paper is very clear and well-written.\n* Some experimental details are not super clear from the main paper. For instance, just to clarify:\n    * Is it correct that the teacher model and the student model have the same size?\n    * Is it correct that the training tasks and test tasks are mutually exclusive when instruction-tuned, and all models (including all baselines and the proposed model) are only using the labeled data on training tasks but not the test tasks?\n    * And is it correct that the proposed model performs distillation on the test dataset? In other words, the proposed model uses the unlabeled datasets of the test datasets, whereas the baselines do not?\n",
            "summary_of_the_review": "Overall, the paper is well-written, and the idea of dilstilling from the model reading detailed instruction to the model reading more concise instruction is interesting and new. However, I am concerned with overclaiming in the paper, missing critical baselines and inconsistency in evaluation of the model.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2063/Reviewer_Krod"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2063/Reviewer_Krod"
        ]
    },
    {
        "id": "z9cdAgTM-o",
        "original": null,
        "number": 2,
        "cdate": 1666651167761,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651167761,
        "tmdate": 1666651167761,
        "tddate": null,
        "forum": "am22IukDiKf",
        "replyto": "am22IukDiKf",
        "invitation": "ICLR.cc/2023/Conference/Paper2063/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work addresses some of the shortcomings of few-shot learning paradigms with large language models such as learning from instructions and explanations, scratch-pad/chain-of-though reasoning and learning from demonstrations. The authors point out that the following issues are prevalent with these methods - (i) The context window can be long, which results in inference being computationally expensive (ii) The inability to fit more examples than the context window allows and (iii) The gains disappear after the demonstrations or scratchpad disappear, i.e., the model doesn\u2019t internalize these gains. This work proposes \u2018context distillation\u2019 as a solution. A student model is trained to match the distribution of outputs of a teacher model, where the student model uses a simpler and shorter context, such as only a simple instruction without examples/explanations/scratch-pad. Experiments attempt to show the advantage of the proposed method under the different few-shot learning paradigms described above.",
            "strength_and_weaknesses": "Pros\n* The paper considers an interesting problem - Internalizing the performance gains from methods known to improve few-shot learning performance, but without paying the additional price of having to process a long context.\n* An interesting solution is proposed.\n* Experiments show some evidence of the advantage of the proposed approach.\n\nCons\n* Technical details are missing/not clearly described.\n* Experimental results are sparse and not fully convincing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* The authors do not clearly describe the learning objective. Section 2.4 says \u2018we minimize the token-level KL divergence\u2019, but there is no mathematical description of how they do this. \n* I couldn\u2019t follow the motivation for \u2018Sequential Distillation\u2019 in section 2.3 and the problem addressed as well as the solution described are vague.\n* The evaluation setting in 3.1 is vaguely described. \u2018We select 5 tasks where teacher can most significantly improve\u2026\u2019 - I don\u2019t understand the motivation behind these choices. \n* \u2018We plot the margins for each task in Figure 5\u2019 - The discussion relies on a figure in the Appendix. The main text should be self-contained and the reader should not have to rely on the appendix to understand the details. \n* The main experimental results are discussed in Table 2 and Table 3.\n  * Table 2 shows the advantage of context distillation compared to a fine-tuning baseline on a text-to-SQL task. The student performance is much worse than the teacher in this case.\n  * Table 3 shows the benefit of distilling scratch-pad on an 8 digit addition task where the student performs comparable to the teacher. \nFrom these results alone I am not convinced that this is a general method that can work well across a variety of tasks. \n",
            "summary_of_the_review": "The authors propose a neat idea which is applicable across a variety of learning paradigms with large language models. However, the experimental results are not comprehensive enough to understand the benefit of the proposed approach. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2063/Reviewer_ipLo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2063/Reviewer_ipLo"
        ]
    },
    {
        "id": "fk-zrIXIvFy",
        "original": null,
        "number": 3,
        "cdate": 1666684309373,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684309373,
        "tmdate": 1666684309373,
        "tddate": null,
        "forum": "am22IukDiKf",
        "replyto": "am22IukDiKf",
        "invitation": "ICLR.cc/2023/Conference/Paper2063/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present an alternative paradigm to prompting: context distillation. The idea is that the model, instead of utilizing examples and specific prompts just for predicting without learning anything from these, tries to internalize this information. The authors propose a teacher-student architecture, where both start from the same parameters' state, and the updates of the student network are driven by the differences in x and y, fed and output respectively, by each network. While the teacher network receives a complete input, with examples and explanations, and outputs the target plus scratch-pad text, the student network generally receives some simple task explanation and the raw input, and is trained to output the raw target.\n\nThroughout the paper, the authors come up with several hypotheses, aiming at proving these contextualization capabilities, and perform different tests to assess the correctness of these hypotheses. In all cases the student network, after distillation, outperforms the original student network, proving the potential of this approach for incorporating valid contextual information into the networks. The authors test the presented methodology in three different contexts, which requires different datasets and downstream tasks.",
            "strength_and_weaknesses": "The authors propose a really interesting idea, where distillation is used to infuse a pre-trained network with further context that is associated with diverse downstream tasks, boosting their performance. The approach considered, where different hypotheses are laid out and assessed with different tests, is interesting, and helps grounding step by step the possibilities of the methodology. Most of the results are clearly presented, and successfully justify the hypothesis postulated, even though further tests on other downstream tasks would be necessary to really confirm them. \n\nStill, I believe the authors do not tackle a really important point, critical to assess the full potential of the methodology: how the knowledge distilled in the network vanishes, or not, as it is trained on more and more tasks. Therefore, it would have been interesting seeing results of this phenomena, as another hypothesis laid out on this premise. I believe such hypothesis would have been more relevant to evaluate than some of the ones considered by the authors. The concern is that the knowledge distilled vanishes as the network is trained on more tasks, which could be indicated by the decrease in performance of those tasks. If this is the case, the demanding fine-tune required would be less justifiable, and more ad-hoc networks or other fine-tuning methods might be a better alternative.\n\nBesides, it would be valuable to have confidence intervals in the reported accuracies, in order to understand how the distillation process can be affected by the randomness in the set of provided inputs, as most experiments are just trained on a few epochs. \n\nFinally, I have two other questions:\n- The explanation given in Section 2.4, for the simplification done on the soft-labels, is not really clear to me. Could you further elaborate? \n- In table 1, my intuition tells me that, when training on a single task, i.e. naive post-distill, the performance should be better than when mixing several. Is not that right? Or is it the case that the single task for naive training is the most complicated one, among all the ones used in the Mixed training? However, according to table 6, the performance of all tasks increases in the mixed version. Could you provide some further intuition on this?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper overall is clear, and the quality of writing is quite good. Nevertheless, I consider the location of figures to be confusing. For example, figure 1 is referred after figure 2, while actually both figures could be almost merged into a single one, that explains the idea of template, etc., through the example of number addition. Then, while hypothesis 2 uses figure 5, this has been moved to the appendixes. Besides, figure 5 would benefit from square-axis.\n\nFor the sake of reproducibility and further supporting the statements, the authors provide further results in the appendix, as well as the code. \n\nFinally, I found a couple typos:\n- Cognative: last line 1st page\n- By: in capital letters, in Table 4 caption\n",
            "summary_of_the_review": "Overall, I believe the paper presents really interesting ideas, as well as tests and results to support the main claims. However, I still feel that the concept of knowledge vanishing has not been tackled, and it would be important to understand how it works in order to really provide a methodology that can perform in multi-task environments. I believe the authors should further discuss this, in order to improve the overall quality.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2063/Reviewer_7ryR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2063/Reviewer_7ryR"
        ]
    },
    {
        "id": "g_YagyiEW",
        "original": null,
        "number": 4,
        "cdate": 1666706751056,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666706751056,
        "tmdate": 1670316878757,
        "tddate": null,
        "forum": "am22IukDiKf",
        "replyto": "am22IukDiKf",
        "invitation": "ICLR.cc/2023/Conference/Paper2063/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes \u201ccontext distillation\u201d, a method to \u201cinternalize\u201d the information provided by abstract instructions, concrete demonstration examples, or scratchpad (model intermediate output that helps reasoning). They first use a language model to sample some input (e.g., movie reviews), and then they use the teacher model to generate pseudo answers given inputs as well as demos, instructions and generated scratch-pad. The pseudo input-answer pairs are used to fine-tune the student model (minimizing the KL divergence using teacher logits), without the extra information. The benefit of internalizing the knowledge is that there is no longer a need to use the extra input and output, making the use of LLM more efficient. The difference to traditional distillation is that the teacher and student models are the same one, and it is the \u201cextra information\u201d that gets distilled into the student model. The authors conducted three experiments:\n\nExp#1: learning from instructions. They first fine-tune the model on Natural-Instruction-V2 (a T5-11B). They show that they can significantly improve the student model\u2019s performance without instruction or explanation.\n\nExp#2: learning from examples. The authors use SPIDER text-to-SQL as a test bed. They show that context distillation can effectively internalize the examples in the context and outperform direct fine-tuning. \n\nExp#3: learning from step-by-step reasoning. The authors test the model on 8-digit addition questions. The experiment shows that the student model can even internalize the step-by-step reasoning by fine-tuning on teacher model\u2019s output, without using scratchpad.\n\nEven though the concept of \u201ccontext distillation\u201d was already proposed by Choi et al.; Askell et al., this paper did a good job of systematically testing it on three distinctive aspects, and the experiment results show that context distillation is an effective way to \u201cinternalize\u201d knowledge from the \u201ccontext\u201d.\n",
            "strength_and_weaknesses": "# Strength\n\nThe authors conduct a systematic study of \u201ccontext distillation\u201d in three aspects: instructions, demonstrations, and step-by-step reasoning. The method is intuitive and the results are strong. It is not only a way to improve the efficiency of using in-context learning and instruction prompting, but the fact that the knowledge can be \u201cinternalized\u201d is very interesting. \n\n# Weakness\n\nThe experiment setup can be significantly improved. First, the paper can be improved on how they demonstrate the results. There are very few tables/figures on the results in the main paper and most numbers scatter in the text. Second, the clarity of the experiment setup (what datasets they use for exp#1, how many pseudo input examples they generate, etc. sorry if I missed it) can be improved. For Exp#2, it is very limited to only test context distillation on the SQL dataset. It would be better to show other datasets too, like NLU, QA, etc. ",
            "clarity,_quality,_novelty_and_reproducibility": "Even though the concept of \u201ccontext distillation\u201d is not new, the authors did a good job of conducting comprehensive research on three different perspectives. However, the presentation/clarity of the paper can be significantly improved, as suggested in the \u201cweakness\u201d section. Besides, I have several questions for the paper:\n\n1. (Sorry if I missed it) what model did you use to generate the input and how many inputs did you generate for each task?\n2. I would be curious to see some examples of generated input.\n3. Is it possible to provide more results in exp#2 on datasets other than SQL?\n4. Have you thought about/tried other optimization methods other than fine-tuning? E.g., soft prompt tuning?\n5. For the step-by-step reasoning experiment, have you tried it on models that are not fine-tuned on NatIns? E.g., directly using chain-of-thought on an LLM?\n",
            "summary_of_the_review": "Overall the paper is well written and the conducted research is insightful and the results are strong. The paper should be accepted. The quality of the presentation can be further improved per the weakness section. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2063/Reviewer_fidG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2063/Reviewer_fidG"
        ]
    }
]