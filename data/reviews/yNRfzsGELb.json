[
    {
        "id": "1T1JxuUWdH",
        "original": null,
        "number": 1,
        "cdate": 1666434709405,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666434709405,
        "tmdate": 1666435136462,
        "tddate": null,
        "forum": "yNRfzsGELb",
        "replyto": "yNRfzsGELb",
        "invitation": "ICLR.cc/2023/Conference/Paper4562/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to remove structured noise from noised images using a diffusion model that describes the joint distribution of the clean data samples and the structured noise. Using an additional score model for the noise, an approximate score for the joint distribution (for all time $t$) is derived. Experimental results show that the proposed method outperforms the previous benchmarks based on normalizing flows and GANs.",
            "strength_and_weaknesses": "Strength:\n\n- Removing structured noise is an interesting task and the idea of the paper is fashionable.\n- The experimental results look promising.\n\nWeaknesses:\n\n- Technical soundness & Clarify: I found the technical part of the paper hard to follow and a little bit misleading.\n    * At the beginning of Section 3.1, it seems that this work tries to build a conditional distribution $p(\\mathbf{x},\\mathbf{n}|\\mathbf{y})$ so that the inversion problem can be solved by just sampling from this distribution or taking its MAP estimation. And then to build this sampling process (or to model this distribution), a diffusion process is imposed on the \"joint space\" of $(\\mathbf{x},\\mathbf{n})$, while everything is conditioned on the noise observation $\\mathbf{y}$. However, in Eq.(14), the score is conditioned on $\\mathbf{y}_t$, which is further explained below in Eq.(16) as a corrupted version of $\\mathbf{y}$. What is $\\mathbf{y}_t$ here? Why does the conditioning depend on time $t$? What is the exact definition of $p(\\mathbf{y}_t|\\mathbf{y})$ and why is it related to the diffusion process of $\\mathbf{x}$ and $\\mathbf{n}$?\n    * In Eq.(15&16), why does the second term of RHS use a sample $\\hat{\\mathbf{y}}_t$? Is it a good approximation?\n    * In page 3 it mentioned $s_{\\phi}(\\mathbf{n},t)\\approx\\nabla_{\\mathbf{n}}\\log p_N(\\mathbf{n})$. Is the score model $s_{\\phi}(\\mathbf{n},t)$ actually independent with time $t$?\n    * It is not very clear how are the two score networks trained. Is $s_{\\phi}(\\mathbf{n},t)$ trained jointly with $s_{\\theta}(\\mathbf{x},t)$?\n- Experiments: It is mentioned in Section 4.1 that the baseline results are improved to be more competitive. However, in Whang et al. (2021) the results seem to be better than what is reported in this paper (see Figure1&2 in Whang et al. (2021)). The inconsistency seems to make the results here questionable. Can you please provide more details on this?\n\n\nMinors:\n- In Eq.(13) the \u201cproportional to\u201d is not correct as the RHS uses log probabilities.\n- In Eq.(14) the drift term should be $f(t)(\\mathbf{x}_t, \\mathbf{n}_t)$.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity & Quality: I have raised some concerns. Please see my comments above.\nNovelty: The idea looks novel to me.\nReproducibility: The paper has described the reproducibility statement, but some details, such as how the score net for structured noise is trained, remain unclear.",
            "summary_of_the_review": "In conclusion, I have raised several concerns regarding the technical soundness and empirical results of the proposed method, which prevents me from giving this paper a higher rating at this time. I will consider increasing my rating if the authors address my concerns and clarify my misunderstandings.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4562/Reviewer_o14W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4562/Reviewer_o14W"
        ]
    },
    {
        "id": "5h8wKliWO36",
        "original": null,
        "number": 2,
        "cdate": 1666558839016,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558839016,
        "tmdate": 1666565651571,
        "tddate": null,
        "forum": "yNRfzsGELb",
        "replyto": "yNRfzsGELb",
        "invitation": "ICLR.cc/2023/Conference/Paper4562/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new method to solve a large class of inverse problems using diffusion models. The approach is to jointly learn both a source data diffusion model and a noise diffusion model, from only a training set of noisy observations. It is assumed that the noise is added to the source signal in a known manner, but a highlight here is that the noise is allowed to be structured and not assumed to be i.i.d.\n\nA method to jointly learn the two diffusion models is proposed. The method is then tested by using CelebA as the source signal and then adding MNIST digits as a noise model. In experiments, the diffusion model is shown to outperform alternative solutions that use GANs and and flow models. ",
            "strength_and_weaknesses": "Strengths:\n- The paper is clearly written and well-organized.\n- The paper addresses a broad class of problems. Scientists in other fields (e.g. astronomy, physics) could find this very useful. \n- The method proposed is not obvious, and a significant contribution. \n- The experiments are well done. \n\nWeaknesses:\n- None",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and original. ",
            "summary_of_the_review": "This is exciting work, and an excellent contribution to ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4562/Reviewer_kwKg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4562/Reviewer_kwKg"
        ]
    },
    {
        "id": "4u9xVJVRBXI",
        "original": null,
        "number": 3,
        "cdate": 1666650684085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650684085,
        "tmdate": 1666651004490,
        "tddate": null,
        "forum": "yNRfzsGELb",
        "replyto": "yNRfzsGELb",
        "invitation": "ICLR.cc/2023/Conference/Paper4562/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes an approach for solving the inverse problem via diffusion models. Particularly, the work considers structured noise \nthat contaminates the observations. To solve the problem, another line of the diffusion process that describes noise is used except \nfor the image generative process. The score models in both image generation and noise generation are pretrained, and then applied\nto two reverse stochastic differential equations to sample images and noises. During inference, a data-consistency step is added.\nExperiments about removing MNIST digits and compressive sensing are presented to verify the effectiveness of the proposed\nmethods.\n",
            "strength_and_weaknesses": "Strength: \n1. The work additionally takes the noise structure into consideration and proposes a noise diffusion process together with an image \ndiffusion process to solve the inverse problem. \n2. The method that samples image and noise from joint conditional posterior is rational and interpretable, and also seems easy\nto implement.\n\nWeakness\n1. The right side of Eq. (14) does not contain $n_t$.\n2. In algorithm1, step 7 and step 8 seem to be covered by step 10 and step 15, respectively.\n3. The necessity of using the diffusion model to describe the structural noise is not convincing. Why is it better than the other generative model, such as GAN, VAE and Flow?\n4. How the structural noise is embedded into the diffusion model? This point is not clear.\n5. It is not clear what the training data is when training the second score function $s_{\\phi}(n,t)$. \n6. I wonder how the proposed method behaves compared with non-generative methods for removing MNIST digits and compressive sensing, and what is the superiority of the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: some clarity is not clear.\nQuality: ordinary\nNovelty: limited.\nReproducibility: not sure.\n",
            "summary_of_the_review": "The work is relatively easy to follow and the idea is clear. However, the clarity to embed the structural noise is not clear. Additionally, the necessity of using the diffusion model is not convincing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4562/Reviewer_uwBZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4562/Reviewer_uwBZ"
        ]
    },
    {
        "id": "Vhz3Q_6u5gd",
        "original": null,
        "number": 4,
        "cdate": 1666658600011,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658600011,
        "tmdate": 1666658600011,
        "tddate": null,
        "forum": "yNRfzsGELb",
        "replyto": "yNRfzsGELb",
        "invitation": "ICLR.cc/2023/Conference/Paper4562/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes to use a score-based model for the task of removing structured noise. Specifically, the method uses a pretrained (unconditional) NCSNv2 model as the signal prior and uses modified update rule during sampling that incorporates the score of a separate noise model.  Experimental results show that score-based models achieve superior performance to existing methods based on GAN and normalizing flow.",
            "strength_and_weaknesses": "**Strengths**\n* Clear exposition and motivation.\n* Problem is relevant to the field as inverse problems encompass a large family of problems and is an active area of research.\n\n**Weaknesses**\n* Experimental comparison is done on a single dataset -- especially when compared to the main baseline (Whang et al.), which considered various types of noises.\n* Unclear whether the benefits are due to the difference in the model class, or other factors (e.g. hyperparameters, network size, etc).  While I don't doubt that score-based prior could outperform GAN/flow priors, it would have been nice to see some effort on trying to decouple the effects of model class vs. other architectural/implementation details.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\nThe paper is clearly written, and I had no problem understanding the motivation as well as the proposed method.\n\n**Quality**\n* Pages 2-6 are spent on background and the description of the method.  This seems a bit excessive and could be made more concise to make room for additional experiments.\n\n**Novelty**\nGiven the recent success of score-based models, its application on the existing task of structured noise removal is a relatively straightforward idea.  That said, I do think a comparison between score-based prior vs. other generative priors have a merit.  This is why I hoped to see a more thorough experimental section.",
            "summary_of_the_review": "This paper proposes to combine two score-based models -- signal prior and noise prior -- for the task of removing structured noise.  The proposed method outperforms existing deep generative priors on the task of MNIST digit removal, and while the approach is simple, this discovery has some value.  That said, the experiments and analysis are lacking.   ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4562/Reviewer_Kqyc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4562/Reviewer_Kqyc"
        ]
    }
]