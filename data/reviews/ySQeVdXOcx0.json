[
    {
        "id": "gbij_Paf-ho",
        "original": null,
        "number": 1,
        "cdate": 1666431387291,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666431387291,
        "tmdate": 1666431387291,
        "tddate": null,
        "forum": "ySQeVdXOcx0",
        "replyto": "ySQeVdXOcx0",
        "invitation": "ICLR.cc/2023/Conference/Paper2013/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes three tailored quantum circuits, inspired by the FNO (Fourier Neural Operator), to learn the functional mapping for PDEs (Partial Differential Equations). The authors evaluate the proposed methods on three PDE families, with results showing that the quantum methods are comparable in performance to the classical FNO. This method is further verified on image classification task with comparble performance to CNN, which shows the potential value of their method to other domains.",
            "strength_and_weaknesses": "Strength:\n1) good topic and quamtum AI solver for PDEs is promising and worh study\n2) complete evaluation on three typical kinds of PDEs with additional test on image classification\n3) the technical approach seems convincing with good justification\n\nWeakness:\n1) the AI PDE solvers e.g. ONet and FNO are all open-sourced, this work may also release the code for advancing the research in this intersection area?\n2) some details might be missing in the paper, e.g. the image classification implementation part.\n3) it still gives me an impression as a combiation of existing techniques i.e. FNO and quantum Foruier transformation\n\nOverall, I think this is a good paper, and I vote for acceptance.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is interesting and some implementation details are given, while the authors do not show or plan to release any source code which I personally think is an important issue in existing quantum machine learning literature (not only this paper). I strongly suggest the authors could release their code otherwise the impact of the work can be less prominant.\n\nFor Section 4.2, I am not quite sure how it is implemented for image classifition.",
            "summary_of_the_review": "The paper addresses an interesting and promising area for adapting quantum computing to solve the machine learing-based PDE solvers. Such AI solvers themselves have attracted intensive attention in both computer science and applied math communities and derive two main lines of research: ONet and FNO. This paper presents a quantum version of FNO based the natural quantum-friendly computing of Fourier transformation and its inverse. The experiments over three typical PDEs and it can also be applied in other domains as preliminarily verified on the image classification task. The experiments are all done on simulation based classic computers for the unavailability to high-end quatum computers (with challenges in terms of noise and qubit limites etc.), which I think is understandable.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2013/Reviewer_EV9o"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2013/Reviewer_EV9o"
        ]
    },
    {
        "id": "RLcuuZA6kBW",
        "original": null,
        "number": 2,
        "cdate": 1666725559386,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725559386,
        "tmdate": 1669129053034,
        "tddate": null,
        "forum": "ySQeVdXOcx0",
        "replyto": "ySQeVdXOcx0",
        "invitation": "ICLR.cc/2023/Conference/Paper2013/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Fourier Neural Operator (Li 2020) has become a popular tool in solving PDEs numerically. The main contribution of the paper is to replace the classic Fourier Layer in FNO with the quantum Fourier layer. The authors argue that the proposed algorithm is provably substantially faster (using quantum hardware) than the classic counterpart. The authors also provide several experimental results (solving 3 PDEs and one simple image classification) to show that the proposed method is comparable to the classical FNO. \n\nI am not familiar with quantum computing and won't be able to judge the significance of the theoretical contribution. Nevertheless, the overall quality of the paper seems not to be very high, which will be detailed below. Moreover, the experimental results from the paper do not support the strong claims made by the authors.",
            "strength_and_weaknesses": "- Presentation in Sec 2 (the classical FNO) is confusing, both notational-wisely and mathematically. \n- The experimental results are not convincing enough to support many claims in the paper. \n- The complexity bound, at least for the classical setting, seems not correct \n\nPlease see the details below. ",
            "clarity,_quality,_novelty_and_reproducibility": "- Complexity bound for the classical FNO. Page 3 : \"The Time Complexity of this complete Fourier Layer (F T+linear\ntransform+IF T) is O(K + 2Nslog(Ns)).\" There are $K$ many matrices multiplication with size $N_c \\times N_c$, the complexity should be $O(KN_c^2)$. Am I missing something? When using the big-O notation, there is no need to put a constant inside the `O', namely,  the `2` in front of $N_s\\log (N_s)$  \n\n- Presentation in Sec 2 (the classical FNO) is confusing and seems to contain errors. \n1. Notational-wise. I never see using superscript $f$ to represent Fourier transform, which is quite non-standard. The use of subindices $i, j$ coupled with $f$ makes it very hard to parse the notation. In addition, the same letters $i, j$ seem to be used in both the original and Fourier domain. \n2. Equations (2) and (3) seem wrong to me, compared to equation (5) in the FNO paper (https://arxiv.org/pdf/2010.08895.pdf). It reads the authors take the first $K$ data points of the first transform rather than the first $K$ modes. Isn't there should be a $f$ for all $a_j$ for $j>K$ in equations (2) and (3)? \n\n- Empirically, how can I tell if the proposed method is *substantially faster* than the classical counterpart. I did not see empirical support. \n\n- In all PDEs experiments, the classical FNO is better than the quantum counterpart. \n\n- What are the scales (linear vs. log) used in the $y$-axis in Fig 3? The Burger plot uses a `linear` scale, while the Navier stoke's equaiton uses a `log` scale. I can't tell what scale is used for Darcy's flow. \n\n- In addition, I really think the `blue curve` (CNN architecture) should be removed from the plot so that we could see the high resolution of the (quantum) FNO. The error from CNN is one order of magnitude larger than the others, which makes it hard to see the finer difference between the proposed and the baseline methods. \n\n- The experimental results in image classification are unconvincing for many reasons below. In particular, \"Experimental results further verify that proposed quantum circuits perform efficiently in both solving PDEs and image classification\" is an overstatement.  \n1. I did not see papers using FNO for image classification. If so, is there a common baseline? If not, how can I trust the models in the experiments are well-tuned and near-optimal? \n2. Using MNIST (and siblings) for image classification is unconvincing due to its simplicity. A more complicated dataset, e.g., CIFAR10/100, should be used. \n3. I am not sure if the result from the proposed method is comparable to CNNs. Different CNN architectures will give a very different performance, which also depends on many other factors (e.g., data augmentation, batch-normalization, learning rate, batch size, etc.) To make a claim like \"It can be observed that our proposed algorithms are better than the classical Fourier Layer and comparable to\nCNNs, especially the sequential circuit, thereby proving to be effective in the vision domain as well,\" requires **rigorous** and **comprehensive** empirical study. \n\n\n\n",
            "summary_of_the_review": "Overall, the quality of the paper seems not to be high and I have raised several concerns. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2013/Reviewer_ExUz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2013/Reviewer_ExUz"
        ]
    },
    {
        "id": "u6YxTnO6lc",
        "original": null,
        "number": 3,
        "cdate": 1666774392388,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666774392388,
        "tmdate": 1666774392388,
        "tddate": null,
        "forum": "ySQeVdXOcx0",
        "replyto": "ySQeVdXOcx0",
        "invitation": "ICLR.cc/2023/Conference/Paper2013/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors extend the classical Fourier neural operator with Quantum circuits. It applies butterfly circuits as a model compression. The paper studies Burgers, Darcy, and Navier-Stokes equations. The Quantum FNO achieves similar accuracy with less amount of parameters. It prepares scientific computing for near term quantum device.",
            "strength_and_weaknesses": "While it is interesting to see connections between quantum algorithms and classical PDEs, I am not sure if this extension is well-motivated. Empirically, in classical FNO, the FFT is already quite fast and the matrix multiplication takes the most computation since the channel dimension of FNO can be very large. It's not very sure if quantum FNO can be more efficient in practice.\n\nStrength:\n- nice observation to speed up FFT with quantum algorithms.\n- empirically, the Quantum FNO has comparable performance, with a small number of parameters.\n\nWeakness:\n- The Quantum FFT seems not to be justified. Unnecessary to use quantum language when everything is classical.\n- empirically, the performance is not as good as the original FNO.\n- the Mnist example looks unnecessary. There exists a previous work on Imagenet (https://arxiv.org/pdf/2111.13587.pdf).\n- there exists a previous work that uses matrix butterfly decomposition for neural operators (https://proceedings.mlr.press/v162/dao22a.html). \n",
            "clarity,_quality,_novelty_and_reproducibility": "I find the clarity of the paper can be improved.",
            "summary_of_the_review": "Overall, it's good to see the connection. However, it seems the real application is still quite distant. The review feels the paper is marginally below the threshold of acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2013/Reviewer_URvi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2013/Reviewer_URvi"
        ]
    }
]