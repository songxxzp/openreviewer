[
    {
        "id": "RWwqI0GQO3",
        "original": null,
        "number": 1,
        "cdate": 1666293110441,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666293110441,
        "tmdate": 1666293110441,
        "tddate": null,
        "forum": "5jBBG-zgrwl",
        "replyto": "5jBBG-zgrwl",
        "invitation": "ICLR.cc/2023/Conference/Paper982/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a new notion of privacy, specifically targeted at individuals, rather than the entire training dataset as a whole. The core idea behind the concept of IF is that individuals should get gains (in privacy and performance) out of participation. Most results highlight disturbing trends in individual fairness- specifically for neural networks. Although most of the results (vis-a-vis tree algorithms and MIA defenses) are negative in nature, they raise the essential question of why users should participate in such model training routines, given their performance and privacy gains.",
            "strength_and_weaknesses": "## Strengths\n\n- This work introduces the user's perspective (which is the data owner, and should thus be at the center of decisions in an ideal scenario) to privacy-preserving learning- convincing users to participate in training, and in some way demonstrating benefits to privacy and/or utility. Although the results are negative, they show how MIA defenses and increasingly private learning do not necessarily lead to improved performance for 'all' users, which is what model trainers should strive for.\n\n- Evaluation is extensive, and the notion of $(\\epsilon, \\delta)$-IF (and its connection to IF) is well explained and intuitive to follow and understand. \n\n## Weaknesses\n\n- The authors acknowledge that there \"are differences in privacy risks between users\", and argue that a training algorithm is IF if all users gain gains corresponding to privacy risks. However, this seems to defeat the very purpose of privacy and algorithmic fairness in general. All users' privacy guarantees should be similar, and not unequally biased towards a specific sub-group of users; the same goes for model performance as well. From what I understand, IF says 'if you had poor privacy guarantees, you should be content with this gain, even though it may be less of a gain than some other user who already had better privacy guarantees. I am not convinced that this is the right approach to dealing with privacy, and is certainly not fair.\n\n- The proposed approach seems to be excessively computationally expensive. Even for a relatively small shadow model, re-training for each potential user is not feasible at all (and opens doors to other kinds of adversaries that may craft adversarial data and use the \"expected gains\" to leak sensitive information from the victim's models/training data). If a user is wary of the model trainer in the first place (which is why they want to look at potential gains), how can the same user be trusted with any 'metrics' that the model trainer may report as expected gains? One way could be to provide black-box API access to the trained model as proof, but that itself is a security/privacy risk.\n\n- Although this is a start, true overlook of \"individual\" fairness should really look at \"individuals\", not their individual records. A user, for instance, may have 5-10 images that they might upload to a cloud provider. Although the privacy/performance gains would be low when measured for a single data point (which is expected, given the relative scale of training data), they would be much higher if a collection of data is used instead. With the help of subject-level membership attacks [1, 2], similar gains can be measured for this case as well, which is much better aligned with real-world use cases. \n\n- I would like to see the use of some other defenses already proposed in the literature, in addition to the ones described in Section 4.3. For instance, causal learning is known to reduce inference risk significantly [3].\n\n### References\n\n[1] Hartmann, V., Meynent, L., Peyrard, M., Dimitriadis, D., Tople, S., & West, R. (2022). Distribution inference risks: Identifying and mitigating sources of leakage. arXiv preprint arXiv:2209.08541.\n\n[2] Suri, A., Kanani, P., Marathe, V. J., & Peterson, D. W. (2022). Subject Membership Inference Attacks in Federated Learning. arXiv preprint arXiv:2206.03317.\n\n[3] Tople, S., Sharma, A., & Nori, A. (2020, November). Alleviating privacy attacks via causal learning. In International Conference on Machine Learning (pp. 9537-9547). PMLR.",
            "clarity,_quality,_novelty_and_reproducibility": "__Clarity__: The paper is well-written and easy to follow. Mathematical notation is well described, and connections between sections make sense. Overall, the paper is a good read.\n\n__Quality and Novelty__: The quality of work is unquestionably well, and analyses that bring these privacy-preserving approaches closer to practical scenarios are much needed. Although the approach of measuring utility by simply adding datapoints is not novel (and faster approximations such as influence function-based methods do exist, especially in model poisoning literature), focus of the privacy lens to potential participants is new. \n\n__Reproducibility__: Detailed experimental setups are described in the paper (and in detail in the Appendix). Although minor details (like exact seeds) are not available to perfectly reproduce results, sufficient information is provided to achieve results with expected trends.\n\n### Minor comments\n\n- Page 1: \"...users decide whether to provide their data to ML service.\" Can the authors please provide concrete real-world examples? It is hard to think of any instances where the users have a clear choice like this and is directly impacted by that decision.\n- Figure 1 is not a Figure- please rename the type.\n- Section 3.2: \"...instead of the loss decrease because...\" model users may care about metrics like precision/recall/F-1 which are computed for specific thresholds, the performance of which can be impacted by different loss values indeed. The reasoning here to use accuracy is not very convincing.\n- Section 4.2: \"...changing the strength of privacy protection.\" Please add a more explicit discussion on 'how' this varying strength of privacy protection is achieved.\n- Section 4.3 argues that Topk \"makes MIa difficult when the confidence of the correct class is small\", which is unlikely for a well-performing classifier to begin with.",
            "summary_of_the_review": "The main issue with the paper in its current form is how it convinces the reader about the severity of this issue. Particularly, the approach of retraining models (even if shadow) for every potential participant is far from feasible, and it is unclear if relative gains should be the focus of the study, given the aspect of fairness (eliminating propagation of biases, in the form of privacy or performance gains). I think if the authors can address this main concern (discussed above) and add comparisons with some defenses (like causal learning, or something apart from Differential Privacy that is shown to work well in the literature), the paper would be in much better shape.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper982/Reviewer_bzAv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper982/Reviewer_bzAv"
        ]
    },
    {
        "id": "pm2WWo6WGAA",
        "original": null,
        "number": 2,
        "cdate": 1666680216752,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680216752,
        "tmdate": 1666680216752,
        "tddate": null,
        "forum": "5jBBG-zgrwl",
        "replyto": "5jBBG-zgrwl",
        "invitation": "ICLR.cc/2023/Conference/Paper982/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a new definition of individual fairness (IF), which requires all users\u2019 tradeoffs between gain and privacy risks are similar. Empirical analysis shows different findings of how the fairness of neural networks and tree-based algorithms change under different privacy protections.",
            "strength_and_weaknesses": "Strengths:\n1. The new IF notion is novel in terms of its formulation.\n2. The empirical study is thorough regarding the diversity of model architecture, datasets, etc.\n\nWeaknesses:\n1. The contributions of the paper are limited by the motivation of the proposed IF. First of all, the authors do not justify the proposed IF well, and the requirement that similar gain corresponds to similar privacy is arguable. I suggest the author better justify the proposed IF notions by several practical use cases.\n2. Even though the definition makes sense, the paper does not provide any methods to mitigate the unfairness. It is not surprising that existing privacy protection methods do not provide any useful mitigation to the fairness problem. If the authors could provide some mitigation methods to the proposed fairness notion, even though it is only empirical, \n3. Lastly, the empirical findings do not provide additional insights to the community: for the tree model, the proposed IF notion is unnecessary; For the NN models, it has been well known that the majority of existing DP methods cause significant performance loss. \n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity** The paper is well-written and clearly organized. However, I think some sections can be moved to the appendix. For example, though the synthetic dataset section is interesting, the section does not contribute a lot to the main story.\n\n**Quality** The submission seems incomplete, and there is a large room for improvement. For example, no method is proposed for the new IF notion, and the empirical findings do not provide any valuable new insights.\n\n**Novelty** The proposed new IF is novel but needs more justification.\n\n**Reproducibility** No issues with the reproducibility if the author could release the codes. ",
            "summary_of_the_review": "The contributions of the paper are limited by the motivation of the proposed IF, the lack of mitigation methods for unfairness, and the limited values provided by the empirical findings.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper982/Reviewer_fzWY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper982/Reviewer_fzWY"
        ]
    },
    {
        "id": "cF2BTT2w9fT",
        "original": null,
        "number": 3,
        "cdate": 1666690817754,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690817754,
        "tmdate": 1666690817754,
        "tddate": null,
        "forum": "5jBBG-zgrwl",
        "replyto": "5jBBG-zgrwl",
        "invitation": "ICLR.cc/2023/Conference/Paper982/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers a timely topic: connection between fairness and privacy-preservation of an ML model. The angle is interesting: fairness is considered from the point of view that the model performance gains for user's data should correlate with the privacy leakage. It is well known that e.g. training with differential privacy leads to unequal individual privacy-preservation, see e.g. [1] and [2]. It is also known that the model performance correlates with privacy-preservation in a sense that the most accurate classes have best privacy-protection [2].\n\nThis paper consider empirical epsilons instead of a priori DP epsilons, computed as in [3] using a membership attack guessing game. These empirical epsilons are compared to empirical 'gain values' computed with models that contain and do not contain the particular sample. These (normalized) empirical values are used to determine the fairness and this fairness definition is tested on several experiments.\n\n\n[1] Feldman, V., & Zrnic, T. (2021). Individual privacy accounting via a renyi filter. Advances in Neural Information Processing Systems, 34, 28080-28091.\n\n[2] Yu, D., Kamath, G., Kulkarni, J., Yin, J., Liu, T. Y., & Zhang, H. (2022). Per-Instance Privacy Accounting for Differentially Private Stochastic Gradient Descent. arXiv preprint arXiv:2206.02617.\n\n[3] Nasr, M., Songi, S., Thakurta, A., Papernot, N., & Carlin, N. (2021, May). Adversary instantiation: Lower bounds for differentially private machine learning. In 2021 IEEE Symposium on security and privacy (SP) (pp. 866-882). IEEE.",
            "strength_and_weaknesses": "Pros:\n\n- The problem the paper is studying is very interesting, this is a timely problem.\n\n- The main idea is interesting: \"We consider that a training algorithm is individually fair if all users obtain gains corresponding to their privacy risks.\"\n\n\nCons: \n\n- The content is a bit thin. There are only empirically computable quantities defined, which are tested in experiments. \n\n- The paper contains some errors. For example:\n\n\"A training algorithm is individually fair if the difference in privacy risks between any pair of users is small. This definition can be satisfied by reducing privacy risks with privacy-preserving ML because the differences are small if all users\u2019 privacy risks are small. \"\n\nI would claim this is not true. As shown e.g. in [1] and [2], there can be very large differences in individual DP epsilons between\nusers. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing could be improved a lot. I think perhaps the most important part, where the technical contributions lies,\nare the two dense paragraphs at the end of page 4. These should be expanded, it is difficult to find the definitions\nof $g_i$'s and $r_i$'s from there.\n\n- p.3: \"upper bounds of FPR and FNR calculated with the Clopper-Pearson method...\" \nThis should be elaborated, I don't think it is enough to simply cite here a paper.\n",
            "summary_of_the_review": "All in all I don't think the paper is mature for publication. The novelty and content is not enough and also the paper requires some rewriting in my opinion.\n\nInstead of only comping up with a new definition of fairness, it would be nice if some remedies were suggested. As for example shown already in [2], it happens to be so that there is a correlation between privacy preservation and model performance: the classes with the lowest average epsilons have also the best performance. And this seems to be inherent to DP-SGD trained models (and probably to DP-trained ML models in general). So instead of inventing new tools to study the individual fairness, it might be more interesting to try find remedies. And this, as far as I see, this paper is not able to do.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper982/Reviewer_77T3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper982/Reviewer_77T3"
        ]
    },
    {
        "id": "3obyxeHniO",
        "original": null,
        "number": 4,
        "cdate": 1666799163510,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799163510,
        "tmdate": 1666799163510,
        "tddate": null,
        "forum": "5jBBG-zgrwl",
        "replyto": "5jBBG-zgrwl",
        "invitation": "ICLR.cc/2023/Conference/Paper982/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper works on the problem of individual fairness in terms of privacy risks and improved accuracy. More concretely, differential privacy has been shown to have a disparate impact on model accuracy for minorities, meaning groups with fewer members/outliers tend to lose more accuracy (by use of DP) than other larger groups. What this paper does is study this phenomenon, on an individual level, i.e. how much each individual loses, as opposed to each group. To enable this, the authors propose a new individual fairness metric that takes into account both the privacy risk and utility gain. They basically measure how much accuracy gain a single sample gets by participating in training, versus how much privacy they lose in terms of membership inference power.\nThey show that the shared trend is the more privacy risk, the better accuracy, and that similar to what was priorly shown on a group level, applying DP and other privacy mitigation attempts has a disparate impact on gains on an individual level.",
            "strength_and_weaknesses": "Strengths\n1. The paper is looking at individual fairness which is an understudied problem. They consider the interplay of individual accuracy increase and privacy gain which is new.\n\nWeaknesses\n1. I am not entirely sure about the use of individual training accuracy gain as part of the metric, I think we should technically look at user-level accuracy, where we have at least a single sample held out test for a given training sample thereby measuring generalization accuracy gains. I think measuring the gain on training samples is a bit meaningless as technically the default accuracy on those samples could be 100% for that user, if we do like a KNN classifier.\n\n2. The measurement of privacy expenditure is inconsistent, there should just be 4 privacy budget epsilons and all experiments done on those, as opposed to different epsilons and even different ways of reporting. Like for one method epsilon is reported,for another the standard deviation of the noise added is reported (which could be converted to epsilon and I think should be for presenting results). All this said, I find stacking all these inconsistent guarantees together like that in Figure 3 in appropriate.\n\n3. I am not sure I fully understand the two lines of reasoning for why the naive definitions of IF are bad, in section 3.2:\n\u2022\tA training algorithm is individually fair if users having similar data face similar privacy risks. This definition is inadequate because a difference in privacy risks can be large if users\u2019 data are dissimilar --> I am not sure how this relates to the former sentence. Doesn't really make sense.\n\u2022\tA training algorithm is individually fair if the difference in privacy risks between any pair of users is small. This definition can be satisfied by reducing privacy risks with privacy-preserving ML because the differences are small if all users\u2019 privacy risks are small. However, strong privacy protection with DP is known to degrade classification performance --> There are two issues with this: a) the only way to get similar privacy risks is not by applying privacy preserving methods, there could be other ways out there, so to say that this is achieved by strong DP guarantees is a bit inaccurate, and b) there are recent papers that show DP can be achieved with little loss to accuracy [3-5]\n\n\n4. I am not really sure what the conclusion  \u201cnecessity of the proposed IF for NNs'' really means. If it means we need improvements, there are some group level improvements, why not try them and then test? Improvments like [1-2]. I think the fact that the only conclusion from the paper is that individual level fairness is not good in privacy preserving methods is a bit repetitive gien prior work. \n\n[1]  Tran, Cuong, Ferdinando Fioretto, and Pascal Van Hentenryck. \"Differentially private and fair deep learning: A lagrangian dual approach.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 11. 2021.\n\n[2] Fioretto, Ferdinando, et al. \"Differential Privacy and Fairness in Decisions and Learning Tasks: A Survey.\" arXiv preprint arXiv:2202.08187 (2022).\n\n[3] Li X, Tramer F, Liang P, Hashimoto T. Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679. 2021 Oct 12. (ICLR 2022)\n\n[4] Yu D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J, Lee YT, Manoel A, Wutschitz L, Yekhanin S. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500. 2021 Oct 13. (ICLR 2022)\n\n[5] Tramer, Florian, and Dan Boneh. \"Differentially Private Learning Needs Better Features (or Much More Data).\" International Conference on Learning Representations. 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and reads well, however, the novelty and the new insights provided by it are limited. The results seem reproducible.",
            "summary_of_the_review": "I have provided an extended review in the strengths and weaknesses box. To summarize, I think the problem the authors are studying is important and interesting, and the proposed individual fairness measure seems new. However, what I am skeptical about is the novelty of the findings, as it seems to mostly reflect the disparate impact of privacy-preserving methods which has been shown before on multiple different setups, and there are mitigations provided for it too [1-2]. Given all this, I find the contributions limited. However, there is a chance that there is something I am missing, so if the authors clarify their contributions and I realize I have misunderstood something I am willing to increase my score. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper982/Reviewer_MgQj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper982/Reviewer_MgQj"
        ]
    }
]