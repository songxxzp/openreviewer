[
    {
        "id": "0nTweqB7i_E",
        "original": null,
        "number": 1,
        "cdate": 1666547068376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666547068376,
        "tmdate": 1666547068376,
        "tddate": null,
        "forum": "ZCTvSF_uVM4",
        "replyto": "ZCTvSF_uVM4",
        "invitation": "ICLR.cc/2023/Conference/Paper5300/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "I would like to begin by saying that this paper makes repeated and relentless reference to a given set of papers that induced this reviewer to suspect that it was self reference, and it took barely a minute of google search to verify this fact. I find this very disturbing, since the other papers I have reviewed have by and large played by the rules of double blind.\n\nThat said, this is a very interesting paper. The authors consider a feedforward ANN, and analyze an algorithm called prospective configuration that falls within the perview of predictive coding. Basically, given an data point and its label, the input and output of the network are clamped to their respective input and output values, and each intermediate layer searches thru gradient descent for input values that would minimize the squared difference between its output and the current output. This allows the error at the top layer to propagate downwards. Once equilibiirm has been reached, the input outputs of each layer is clamped and a gradient descent on the weight vectos is initiated, again until convergence. This process is repeated until convergence.\n\nWhat the paper shows is that this this algorithm converges to the same local optima that back propagation converges to. It also show that the algorithm builds a bridge between BP and target propagation (an alternative learning algorithm for deep networks).\nIt builds its results on the intuitive idea that the problem can be rephrased as a missing data problem, that is the activation of the intermediate layers) and can therefore be attacked using EM.",
            "strength_and_weaknesses": "Strength: The analysis is quite comprehensive and connections to BP and TP are shown\nWeekness: Several of the results are for linear networks where the problem becomes trivial. Also, a lot of the results are similar to deep gaussian networks that came out many years back.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written, and the results are novel.",
            "summary_of_the_review": "This is a well written paper that provides theoretical justification for yet another technique (prospective configuration) for training deep networks.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "It would have not hurt the paper if they did not make so many self references. As it stands, this paper stretches what is permissible in a double blind review.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5300/Reviewer_c2Qw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5300/Reviewer_c2Qw"
        ]
    },
    {
        "id": "3XuM7PsL2ls",
        "original": null,
        "number": 2,
        "cdate": 1666669268951,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669268951,
        "tmdate": 1666669268951,
        "tddate": null,
        "forum": "ZCTvSF_uVM4",
        "replyto": "ZCTvSF_uVM4",
        "invitation": "ICLR.cc/2023/Conference/Paper5300/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper attempts to provide a theoretical analysis computation in predictive coding networks (PCNs), relating it to the more commonly used backpropagation-based (BP) training of feedforward artificial neural nets (ANNs).",
            "strength_and_weaknesses": "Strengths: \nUnderstanding the relation between PCN and usual ANN with BP is definitely a worthy goal.\nThe observation that PCN \u2018inference\u2019 is a combination of a forward and a backward signal and that the balance between the two is decided by precision ratios. This insight may not be completely new, but is brought out well in the discussion.\nWeaknesses:\nThe proofs of some theorems seem to be spoiled by what looks like sloppy or confusing arguments. Take for example theorem 3.1. In the proof, equation 3 ignores the difference between $\\epsilon^*_{l+1}$, the equilibrium value, and $\\epsilon_{l+1}$, the backward propagated error, to make the argument.\nIn some cases, it is not clear whether some conditions are non-trivial or not, if taken literally. For example, for theorem 3.5, if one thinks $\\mathcal{L}$ depends only on $x_{L-1}$ and the target, the partial derivative $\\partial \\mathcal{L}/\\partial x_{l}$ would naively be zero for $l<L-1$, making the inequality trivially true.\nCalling the $x_l$ \u2018inference\u2019 via a local mode, not the expectation, the E-step Eq (14), and then invoking theorems about local convergence of expectation maximization seems confusing.\n\nMinor issues:\nIn equation (2), it seems $\\epsilon_l\\cdot f\u2019(..)$ is column vector of the same size as $x_l$. Given that, in equation (1), should the order of $W^T_{l+1}$ and $\\epsilon_{l+1}\\cdot f\u2019(...)$ not be reversed?\n",
            "clarity,_quality,_novelty_and_reproducibility": "While this is a bold attempt at some theory, several claims seem dubious and others are often trivial.",
            "summary_of_the_review": "I wish this paper was what it promises to be. I believe far more careful work is necessary to substantiate the claims made here.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5300/Reviewer_3s9Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5300/Reviewer_3s9Y"
        ]
    },
    {
        "id": "IlUxdtR9hU9",
        "original": null,
        "number": 3,
        "cdate": 1666690750687,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690750687,
        "tmdate": 1666690750687,
        "tddate": null,
        "forum": "ZCTvSF_uVM4",
        "replyto": "ZCTvSF_uVM4",
        "invitation": "ICLR.cc/2023/Conference/Paper5300/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "While backpropagation is the mainstay technique used for artificial neural networks, it is considered biologically implausible, mainly due to the so-called \"weight-transport\" problem, wherein neurons need to know the weights of neurons that lie upstream in order to correctly update their own weights. Two frameworks that have been proposed to be more biologically realizable are predictive coding and target prop.  While both of these methods have been found to be related to backpropagation under certain circumstances, a fuller accounting of their interconnections has remained elusive. Through both mathematical analysis of linear networks and simulations of nonlinear ones, the present work demonstrates how backprop and targetprop approximately represent two endpoints of a continuum of solutions that predictive coding network inferences lie on (i.e., the equilibrium prediction errors lie close to the gradients of backprop on one end, while the equilibrium activities lie close to targetprop target activities on the other). Moreover, the authors show that predictive coding learning results in convergence of the network to the same minima as for backpropagation.",
            "strength_and_weaknesses": "Strengths\n\n1. The authors provide a full mathematical characterization of linear predictive networks, demonstrating analytically their relationships to backprop and targetprop networks\n\n2. Furthermore, simulated nonlinear networks are analyzed and shown to produce the same qualitative results as found for the linear networks\n\n3. The authors' writing is clear and well-organized\n\nWeaknesses\n\nMajor\n\nNo major weaknesses have been \nobserved.\n\nMinor\n\nThe precision ratio is introduced after Theorem 3.4 as $\\Pi_{l+1} / \\Pi_l$; however, I believe the precision ratio comprising the x-axis values in Fig. 3C is instead $\\Pi_{l} / \\Pi_{l+1}$.  Either changing the figure to have the same ratio that is initially introduced or specifying the ratio that is used in the figure would help to avoid readers' confusion.",
            "clarity,_quality,_novelty_and_reproducibility": "The present study builds on extant work that connects predictive coding and backpropagation, further elucidating the relationship of predictive coding to backprop and target prop away from prior limits studied.  The authors describe their analysis clearly and thoroughly, including demonstrating how results with full nonlinear networks are consistent with their linear network findings.  The proofs in the body of the paper are further supplemented with further descriptions and proofs in the appendices, while the numerical experiments are detailed in the appendix and will be included in a code release, allowing others to reproduce the results in a manner that is aided by, but not dependent on, the provided code.",
            "summary_of_the_review": "The authors have analytically demonstrated the connections among predictive coding, targetprop, and backprop in linear networks. They back up the analysis with numerical experiments on nonlinear networks that demonstrate the same qualitative results.  This work represents an important and novel advance in understanding predictive coding networks, which currently represent one main direction within neuroscience for understanding cortical functionality.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5300/Reviewer_G2QQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5300/Reviewer_G2QQ"
        ]
    },
    {
        "id": "xcqjmM2EHhj",
        "original": null,
        "number": 4,
        "cdate": 1666713299374,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666713299374,
        "tmdate": 1666713299374,
        "tddate": null,
        "forum": "ZCTvSF_uVM4",
        "replyto": "ZCTvSF_uVM4",
        "invitation": "ICLR.cc/2023/Conference/Paper5300/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a novel theoretical framework for understanding the inference and the learning in predictive coding networks (PCN). The math derivations and proof are sound. ",
            "strength_and_weaknesses": "This is a paper full of math proof and derivations, and is theoretically sound. It is quite interesting in interpreting the PNC dynamics as performing a constrained EM algorithm, and comparing the PCN with other algorithms including target propagation and back propagation. The theoretical results seem novel based on the author's description but I am not quite sure since I am not an expert on predictive coding networks.",
            "clarity,_quality,_novelty_and_reproducibility": "- Quality and clarity\nThe math proof and derivations are sound and are presented clearly.\n\n- Originality\nThe theoretical results seem novel but I am not fully sure since I am not an expert on PCN.",
            "summary_of_the_review": "I like the deep math analysis of the inference and learning dynamics of PCN and it greatly gain our understanding of this network. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5300/Reviewer_3qX4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5300/Reviewer_3qX4"
        ]
    }
]