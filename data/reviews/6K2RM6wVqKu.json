[
    {
        "id": "RmRZnARRMc",
        "original": null,
        "number": 1,
        "cdate": 1666163840230,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666163840230,
        "tmdate": 1668790174436,
        "tddate": null,
        "forum": "6K2RM6wVqKu",
        "replyto": "6K2RM6wVqKu",
        "invitation": "ICLR.cc/2023/Conference/Paper2600/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a self-supervised or pretrained method for molecular representation learning. It can be summarized as follows,\n\n#### 1. A Transformer-based backbone.\n\n#### 2. Two self-supervised learning or pretraining tasks: 3D position recovery and masked atom prediction.\n\n#### 3. A few finetuning strategies are presented for fine-tuning or downstream tasks.",
            "strength_and_weaknesses": "## Strength\n\n#### 1. This paper motivates using unlabeled data to improve molecular or protein representation learning via self-supervised learning or pertaining tasks, which is a popular topic in the research area.\n\n#### 2. The two pretraining tasks are technically sound.\n\n#### 3. Two large-scale datasets are collected for pretraining.\n\n#### 4. The paper outperforms most existing methods.\n\n#### 5. The code, model, and data will be made publicly available. \n\n\n## Weaknesses\n\n#### 1. I thought the novelty is questionable.  The authors claimed that the proposed Uni-Mol is the first pure 3D molecular pretraining framework. However, there have been already a few similar works. For example, \n\na. The Graph Multi-View Pre-training (GraphMVP)  framework leverages the correspondence and consistency between 2D topological structures and 3D geometric views. \n\nLiu et al., Pre-training Molecular Graph Representation with 3D Geometry, ICLR 2021.\n\nb. The geometry-enhanced molecular representation learning method (GEM)  proposes includes several dedicated geometry-level self-supervised learning strategies to learn molecular geometry knowledge.\n\nFang et al., Geometry-enhanced molecular representation learning for property prediction, nature machine intelligence, 2022.\n\n\nc. Guo et al. proposed a self-supervised pre-training model for learning structure embeddings from protein 3D structures.  \n\nGuo et al., Self-Supervised Pre-training for Protein Embeddings Using Tertiary Structures, AAAI 2022. \n\nd. The GeomEtry-Aware Relational Graph Neural Network (GearNet) framework uses type prediction, distance prediction and angle prediction of masked parts for pretaining. \n\nZhang et al., Protein Representation Learning by Geometric Structure Pretraining, ICML 2022 workshop. \n\n#### 2. The comparison with the SOTA methods may be unfair. The performance of the paper is based on the newly collected 209M dataset. However, the existing methods use smaller datasets. For example, GEM employs only 20M unlabeled data. Because the scale of datasets has a significant impact on the accuracy, the superior of the proposed method may be from the new large-scale datasets. \n\n#### 3. The authors claimed one of the contributions is that the proposed Uni-Mol contains a simple and efficient SE(3)-equivariant Transformer backbone. However, I thought this contribution is too weak. \n\n#### 4. The improvement is not very impressive or convincing. Although with a larger dataset for pretraining,  the improvement is a bit limited, e.g., in Table 1.  \n\n#### 5. It is not clear which part causes the main improvement: Transformer, pretraining or the larger dataset? \n\n#### 6. It could be better to show the  3D position recovery and masked atom prediction accuracy and visualize the results. \n\n#### 7. The visualization of the self-attention map and pair distance map in Appendix H is interesting. However, according to the visualization, the self-attention map is very similar to the pair distance map, as the author explained. In this case, why not directly use pair distance as attention? Or what does self-attention actually learn besides distance in the task? As self-attention is computationally expensive, is it really needed?  ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. The method is clearly presented.  The detailed appendix and promised code release enabled reproducibility. However, the novelty is questionable. ",
            "summary_of_the_review": "The paper presents a  pretraining framework for molecular representation learning. I like the newly collected dataset, which may be potentially useful to the topic.  \n\nHowever, the novelty may be overclaimed. For a pretraining work, the most important thing is the self-learning design. However, predicting the property or attributes of the masked or missing parts is straightforward and has been widely used in existing works.  The experimental comparison is also questionable. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2600/Reviewer_LVDL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2600/Reviewer_LVDL"
        ]
    },
    {
        "id": "2JPRHPBD-T2",
        "original": null,
        "number": 2,
        "cdate": 1666599606246,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599606246,
        "tmdate": 1666599606246,
        "tddate": null,
        "forum": "6K2RM6wVqKu",
        "replyto": "6K2RM6wVqKu",
        "invitation": "ICLR.cc/2023/Conference/Paper2600/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a universal 3D Molecular representation learning (MRL) framework, called Uni-Mol, that significantly enlarges the representation ability and application scope of MRL schemes. Uni-Mol contains two pretrained models with the same SE(3)-equivariant transformer architecture: a molecular model pretrained by 209M molecular conformations; a pocket model pretrained by 3M candidate protein pocket data. Besides, Uni-Mol contains several finetuning strategies to apply the pretrained models to various downstream tasks. By properly incorporating 3D information, Uni-Mol outperforms SOTA in 14/15 molecular property prediction tasks. Moreover, Uni-Mol achieves superior performance in 3D spatial tasks, including protein-ligand binding pose prediction, molecular conformation generation, etc.",
            "strength_and_weaknesses": "##########################################################################\n\nPros:\n\n- Uni-Mol is the first pure 3D molecular pretraining framework, and the first molecular pretraining framework that can be directly used in 3D tasks in the field of drug design. which is a very valuable work for 3D molecular representation learning.\n- Uni-Mol contains a simple and efficient SE(3)-equivariant Transformer backbone, and an effective 3D pretraining strategy. The ablation benchmarks demonstrate their superior performance, and Uni-Mol also outperforms SOTA in various downstream tasks.\n- The paper is well-written and the experiment section is solid.\n##########################################################################\n\nCons:\n\n- I think this paper is a solid work, a small piece of advice is since this paper focuses on 3D molecular data, in addition to quantitative experimental results, it will be more intuitive and convincing if more qualitative results can be displayed. Another concern is how much computational resources and time for 3D molecular pretraining needs to consume, which the paper does not seem to mention.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality, clarity, and originality are good.",
            "summary_of_the_review": "good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2600/Reviewer_uSFd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2600/Reviewer_uSFd"
        ]
    },
    {
        "id": "gWgOGA0u7ki",
        "original": null,
        "number": 3,
        "cdate": 1666695011638,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695011638,
        "tmdate": 1666710940102,
        "tddate": null,
        "forum": "6K2RM6wVqKu",
        "replyto": "6K2RM6wVqKu",
        "invitation": "ICLR.cc/2023/Conference/Paper2600/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to push representation learning for molecule-related task further than before, by mixing different tasks in its per-training step, defining a representation that is generic enough to be applicable to a variety of downstream tasks, on which it performs as well or better (and several times, significantly better) than the reported SOTA.\nThe main contributions are :\n- this original idea of mixing very different pre-training tasks / going further in representation learning\n- the extensive per-training performed, in terms of the strategy: the model used is the transformer, and the strategy is to imitate the guess-a-missing-word strategy: instead, one needs to recover the corrupted position of each atom.\n- the extensive per-training performed, in terms of training data (in my perspective it seems impressive, but I haven't read the related literature)\n\nWhat is not new, as paper admits, is:\n- The transformer architecture itself\n- the various type, position, relative distances encoding (although they are well benchmarked), that are SE(3)-invariant\n- the tasks and datasets (taken individually), except for a regression task on pocket property prediction, which is new (no Baseline there).\n",
            "strength_and_weaknesses": "Strengths:\n- the contribution is both original (new) and strong. \n- performance is extensively review (many downstream tasks, enough detail to follow)\n- the simplicity of the ideas: mostly using and combining already existing conceptual tools, in general rather clearly explained (from someone with a good ML/DL background)\n\nWeaknesses:\n- no major conceptual novelty (may be an asset in a sense)\n- Architecture is a bit overlooked/not explained enough in the main text (page 3)\n- a few unclear parts, or parts that need a bit more discussion\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- overall, paper is clear.\n- the architecture explanation could be more detailed. For instance the node representation initialization is not specified (I guess it includes atom type, and not positions, for equivariance?)\n\nQuality:\n- overall, the statements are sound. A few remarks:\n- I think the embedding of edges (atom pairs) into a (type-aware) Gaussian Kernel function of the norm of the distance of atom-pair, is an invariant representation, not an invariant one. Then, Eq (3) may indeed be equivariant , because c_ij is invariant and x_i is of course equivariant. I think the mention of equivariance should be corrected, as it is confusing for the reader.\n- atom types are a bit hidden. In Fig 2, instead of atom head, it could read \"type head\" (I suggest..)\n- I am from GNNs. I am not very familiar with transformers, I just know the attention mechanism (esp. the standard softmax one used here). \n    It seems to me like your transformer can be seen as an attention-GNN with no aggregation mechanism for the edge features (just a ResNet style addition) and a new node value that solely depends on the neighbor edges (via attention, Q-K-V). In interpret the Value as the node feature.  If I am not too much wrong, then I think it would be a good correction to show how the architecture presented fits into the GNN framework. It seems to me that it does not differ much from it. (also, you use a fully-connected graph).\n\n    The current introduction to the paper is otherwise slightly misleading:\n    > Most previous MRL frameworks used graph neural networks(GNN) [22; 23; 12]  (...) Therefore, we use Transformer as the backbone\nmodel to fully connects nodes/atoms.\n- runtimes of pre-training are missing (or I missed them, not highlighted enough)\n\n\nNovelty:\n- as stated in my summary of the paper, there are several original contributions, but no new theoretical one\n\nReproducibility:\n- for now, the code is not publicly available. It is stated it will be. Given the paper's goal, I believe it will !\n",
            "summary_of_the_review": "Aside from a few points that need further correction/clarification/discussion, the paper is sound and good, and brings a new way of pre-training 3D representations to the community, with proved efficiency.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2600/Reviewer_CPRe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2600/Reviewer_CPRe"
        ]
    },
    {
        "id": "WoDhN98cEj",
        "original": null,
        "number": 4,
        "cdate": 1668429091139,
        "mdate": 1668429091139,
        "ddate": null,
        "tcdate": 1668429091139,
        "tmdate": 1668429091139,
        "tddate": null,
        "forum": "6K2RM6wVqKu",
        "replyto": "6K2RM6wVqKu",
        "invitation": "ICLR.cc/2023/Conference/Paper2600/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a novel universal molecular representation learning framework, called Uni-Mol, that incorporates 3D information. The network consisting of stack of SE(3)-equivariant transformer layers, is pretrained on two different tasks each involving molecular and protein pocket data respectively. This pretraining is established by learning to predict 3D atomic positions and masked atom representation from the noisy input. These pretrained networks when applied for finetuning various downstream tasks, it is shown to achieve superior performance on molecular / pocket property prediction, molecular conformation generation and protein-ligand binding pose prediction. ",
            "strength_and_weaknesses": "Strengths:\n1. The paper is well-written and easy to follow. As per my knowledge, it is one of the first work to have shown to incorporate 3D information to improve representation capacity of molecular processing architecture.\n2. In order to preserve rotation and translation invariant properties, Uni-Mol totally operates only on input atomic representation and pairwise Euclidean distance inputs. \n3. Unlike the previously introduced SE(3) equivariant transformers that uses complex processing blocks, Uni-Mol attention layer is quite simple that transforms to and fro between pairwise and atomic representations.This is possible by relegating the 3D positions update to the final layer of the network.\n4. With efficient backbone, Uni-Mol is able to effectively pretrain on large-scale molecular and protein pockets datasets.\n\nWeaknesses:\nWhen drawing evaluation against various baselines on benchmark tasks, it is quite unfair to make comparison of models trained solely on task-specific data to Uni-Mol which has been pretrained on large datasets. For instance,\na. On molecular conformation tasks, all other baselines generates conformation using only 2D molecular graph information. On the other hand, Uni-Mol leverages the 3D position output from RDKit (that uses ETKGD with MMFF) as input for pairwise representation. This for sure provides better start-point for Uni-Mol compared to other baseline that starts with gaussian random noise as 3D position.  \nb. Moreover, pretrained Uni-Mol architecture is relatively very high capacity when compared to baselines.\n\nGiven the nature of contribution it may be difficult to establish apple to apple comparisons. However, fair comparison the efforts can be put into establishing, \na. similar capacity Uni-Mol baseline network\nb. Uni-Mol baseline that is evaluated using gaussian noise input\nc. evaluating each task using other MRL networks\n",
            "clarity,_quality,_novelty_and_reproducibility": "As pointed earlier, it is a good read and the model is well engineered. The novel SE(3) equivariant that is efficient and simple to implement is central to the SOTA results. Given that whole lot of details are being provided in the appendix it should be possible to reproduce the results.",
            "summary_of_the_review": "Overall, it is a good work and I am inclined towards accepting this work. It establishes two different models that can be put to use for improving accuracy in many downstream tasks. It also provides good tricks for setting up pretraining tasks on molecular / drugs data. For now, I see Uni-Mol strengths outbeats its weaknesses and its contribution may open up avenue of progress in drugs discovery. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2600/Reviewer_6Ybw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2600/Reviewer_6Ybw"
        ]
    }
]