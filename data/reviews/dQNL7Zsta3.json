[
    {
        "id": "6zMfCCPryq",
        "original": null,
        "number": 1,
        "cdate": 1666627515929,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627515929,
        "tmdate": 1666814434728,
        "tddate": null,
        "forum": "dQNL7Zsta3",
        "replyto": "dQNL7Zsta3",
        "invitation": "ICLR.cc/2023/Conference/Paper3156/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper studies a setting with two environments from a specific instance of the model from Rosenfeld et al. and show that overfitting (i.e., interpolation of the training data) and invariance to the spurious features are fundamentally at odds, and that while both are achievable, they cannot be done simultaneously. They then propose an algorithm which learns to make invariant predictions based on these two environments.",
            "strength_and_weaknesses": "**I like this paper, and I think it is well-written. There are some non-insignificant weaknesses, but overall I am in favor of acceptance. But, there are some changes/missing discussion I would like to see.**\n\nThe primary theorem is asking a really interesting question, and I think the result, while not too surprising, is valuable. Actually, what I find more surprising is the fact that the data is linearly separable at all with the chosen parameters (high dimensions are weird! I had to simulate this to convince myself that it was true). Generally, and along the same lines, I think this work would significantly benefit from a high-level description of the proof and giving an overview of the general order of the parameters that are necessary to make the proof work. Seeing \"there exist parameters a, b, c\" without a sense of scale feels extremely unsatisfying and sometimes gives me the impression that the result may not be as interesting **(e.g., we could take $\\mu_c \\to 0$ and the result would be trivial, no?).** It also seems unusual to have the means be separated by a term which varies with N but is constant as a function of the dimension... I would appreciate an indication of how robust this result is to different ranges of the parameters, even if just an intuition. For example, what if we choose $\\theta_2 \\neq 0$? How small would it have to be for the result to still hold? What if $\\theta_1 \\in o(1)$?\n\nUnfortunately I think the second part of the paper is quite a bit weaker. First of all, the model studied here is a specific case of the model from Rosenfeld et al. where the spurious means are collinear. This is pretty unrealistic (I admit this is a bit subjective), but it also immediately suggests a trivial method for invariance: just do PCA on the features to identify the shared spurious subspace, then project it out. However, **this approach has already been suggested and studied for a more general model by two works [1, 2], which both show invariance with only two environments.** So the second half of the paper unfortunately is giving a weaker result than one that is already known. The proposed two-stage algorithm is a reasonable idea, and I appreciate the authors' efforts to acknowledge existing work which does similar things---but it would be helpful to specifically explain how this approach differs rather than just giving a long list of citations.\n\nIn summary:\n\n* The first half of the paper asks an interesting question and gives a very nice result, though it requires a pretty specific parameterization which I think needs to be clarified for an updated draft.\n* The second half of the paper is already known, though the slightly different algorithm and experiments do provide some value showing that this approach is reasonable. \n\nOn the basis of the first half of this work alone I think this paper should be accepted, and I think this work would be much stronger if the results there could be made more general or at least clearer.\n\n**One last note: Given the restricted setting of the theory, I think the title would be much more accurately stated as a possibility, such as \"can/may be/is sometimes fundamentally at odds\". As it is written it feels heavily overclaimed.**\n\n[1] Provable Domain Generalization via Invariant-Feature Subspace Recovery. Wang et al. '22\n\n[2] Iterative Feature Matching: Toward Provable Domain Generalization with Logarithmic Environments. Chen et al. '21\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: High\nClarity: General writing is great, the proof technique and specific parameterizations in the first theorem could be made much clearer.\nNovelty: First half - excellent, Second half - already known\nReproducibility: Good",
            "summary_of_the_review": "See above",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3156/Reviewer_VPKR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3156/Reviewer_VPKR"
        ]
    },
    {
        "id": "mBLlh6DL6W",
        "original": null,
        "number": 2,
        "cdate": 1666712036064,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666712036064,
        "tmdate": 1666712036064,
        "tddate": null,
        "forum": "dQNL7Zsta3",
        "replyto": "dQNL7Zsta3",
        "invitation": "ICLR.cc/2023/Conference/Paper3156/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper demonstrates that interpolation and invariance are fundamentally at odds in overparameterized learning via constructing a linear two environment problem. Specifically, the authors demonstrated that in the constructed learning problem, any interpolating linear model with a positive margin cannot robustly have good out-of-distribution generalization. In contrast, the authors can construct an algorithm (to a certain extent, specifically designed according to the nature of the constructed problem) that is guaranteed to produce robust linear classifiers.\n\n",
            "strength_and_weaknesses": "Strengths: \n\nThis paper points out an interesting trade-off between achieving interpolation and invariance. \n\nThe authors back up their theoretical claims with convincing simulations.\n\nThe paper is well-written and has rigorous proof.\n\nWeaknesses:\n\nTheorem 1 only proves the existence of certain particular values of $r_c,r_s, d, \\sigma, \\theta_1,\\theta_2$. This makes the result relatively weak. In addition, although the idea introduced in the paper is interesting, the constructed learning problem seems \"unfair\" for standard learning methods. For example, if $\\theta_1,\\theta_2$ are both positive, then when faced with this particular linear two environment problem, any reasonable learning algorithm without additional prior knowledge should try to learn both $\\mu_c$ and $\\mu_s$ -- given only the training data set generated based on positive $\\theta_1,\\theta_2$, who would possibly know that in a new test task the corresponding $\\theta$ could be negative? \n\nOn the other hand, it is fairly easy to construct algorithms such as Algorithm 1 by utilizing \"hidden knowledge\" that:\n1. In a new test task the corresponding $\\theta$ could be negative;\n2. $\\mu_c$ is \"robust\" while $\\mu_s$ is not;\n3. $r_c$ and $r_s$ are different.\n\nTherefore, the significance of the result is questionable. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of the paper is clear. The results are original and reproducible.",
            "summary_of_the_review": "By constructing an example problem, this paper introduces an interesting observation about interpolation and invariance in overparameterized linear classification. However, the significance of the result and the impact of the constructed example problem need a further demonstration. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3156/Reviewer_p2tP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3156/Reviewer_p2tP"
        ]
    },
    {
        "id": "SNoaLPFQmD",
        "original": null,
        "number": 3,
        "cdate": 1666963149747,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666963149747,
        "tmdate": 1666963149747,
        "tddate": null,
        "forum": "dQNL7Zsta3",
        "replyto": "dQNL7Zsta3",
        "invitation": "ICLR.cc/2023/Conference/Paper3156/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a neat observation for a simple linear overparameterized classifier for a simple \"linear two environment problem\": interpolating classifier and invariance property can not be achieved at the same time. The paper also presents an algorithm that is provably invariant in this simple setting while being a non-interpolating classifier. ",
            "strength_and_weaknesses": "Strength:\nThe idea of the paper is neat. The claims are in general well-supported.\n\nWeaknesses: \n1. For Theorem 1, why is the probability over the draw S? while the robust error is a deterministic function of w? I guess the S here refers to the space of possible N samples, not the set of data points, but please clarify. \n2. Please also clarify on the drawing of \\mu_s and \\mu_c in Part 3 of Theorem 1. The paper mentioned that both are uniformly distributed on a sphere, but they also need to be orthogonal -- is \\mu_s drawn from the subspace that is orthogonal to \\mu_c? Also, 65 in Theorem 1 seems to be a strange number, please clarify as well. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality: The paper is relatively easy to understand. However, I think adding a figure illustrating the linear two environments in a picture may help reader understand the problem setting much faster. \n\nNovelty: I am not very familiar with all relevant literature and to what extent the model setting here is reasonable / too simplified to all the researchers in the field. The paper does seem self-explanatory though. \n\nReproducibility: My best guess is that the work should be reproducible. ",
            "summary_of_the_review": "I would vote for marginally above the acceptance threshold, given that there are minor issues that need to clarify (see above), and I am not in the best position to comment on the novelty aspect, although I find the paper self-explanatory. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3156/Reviewer_8afp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3156/Reviewer_8afp"
        ]
    },
    {
        "id": "YZZGBArCrY",
        "original": null,
        "number": 4,
        "cdate": 1667084190973,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667084190973,
        "tmdate": 1670285184132,
        "tddate": null,
        "forum": "dQNL7Zsta3",
        "replyto": "dQNL7Zsta3",
        "invitation": "ICLR.cc/2023/Conference/Paper3156/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the generalization of interpolating models under distribution shift by focusing on a linear classification problem.  They consider a source distribution that can be interpreted as a `two environment' distribution, and define a notion of `robust error' that measures generalization on a (family of) target distribution(s).    They show that there exist learning rules which can achieve good robust error, but any interpolating rule will have bad robust error.  They verify their results with experiments.  However, I believe there are issues with their setup that I am not sure can be fixed easily. ",
            "strength_and_weaknesses": "\nUnderstanding the benefits and pitfalls of interpolation is an important research question.  It is worthwhile to try and develop insights by the study of simpler models, like the linear models considered in this work, and to then translate and verify these intuitions into more complex neural network models.  I quite liked the authors' attempt at studying overparameterization by using ResNet baseline features that are then mapped into a higher-dimensional space via random features.  This provides a novel way of introducing overparameterization that is attractive due to its ease of computation.  I'm unsure if this approach had been introduced before (can the authors comment?), but I liked it. \n\nHowever, I found serious problems with the framing and interpretation of the setup and of the results in the paper.  In particular, the claim that \"interpolation is at odds with invariance\" does not seem to be supported by the theoretical setup they showed.  In the proof, they consider theta_1=1 and theta_2=0, so that the source distribution is a mixture of two components: (1) Gaussian mixture with mean y(mu_c + mu_s), and (2) Gaussian mixture with mean y mu_c.  They assume that ||mu_s|| >> ||mu_c||, and show that an interpolator on the source data will rely more upon mu_s than upon mu_c, and that this causes the \"robust error\" to be large because any reliance upon mu_s increases the \"robust error\".  There are a number of problems with this setup:\n\n(W1) If ||mu_s|| >> ||mu_c||, it isn't clear to me why we should think of mu_s as \"spurious\": it is the dominant feature for data that comes from N( y(mu_c + mu_s), sigma^2 I).  And as the authors mention in the second-to-last paragraph of Section 2, the relationship of ||mu_s|| to ||mu_c|| matters much more than the number of samples from each environment in the overparameterized regime, so large ||mu_s|| really is the determining factor.  Because of this, I don't believe mu_s is really a 'spurious' feature: it is the most important feature for the setting considered. \n\n(W2) The definition of \"robust error\" seems odd in light of point (1).  The error gets worse if the learner relies on mu_s at all, but this doesn't make sense if we think of mu_s as an important feature.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The work was fairly clear and, from what I am aware, novel.  There were a few points that could use additional clarity/clarification:\n\n(C1) Theorem statement: presumably this should be gamma >, not gamma <, since it says 'lower bound'.  For part 3 of the theorem, presumably this gamma is not the same as the gamma in the theorem statement, since in the subsequent paragraphs you say that the result holds for classifiers with 'arbitrarily small margin gamma'.  I am assuming this refers to *only part 3 of the theorem*, since the theorem statement presumably requires a lower bound for gamma.    I was also confused if gamma was supposed to play a role in part 2 of the theorem -otherwise, it doesn't seem to appear anywhere?  ",
            "summary_of_the_review": "~~I think some of the central claims in the paper are not supported by the evidence provided by their work.  In particular, the theory setup relies upon a mixture of source distributions and the notion of a \"spurious\" feature, but the setup seems to imply that this feature is not spurious but is actually the dominant, useful feature.  If I have mis-understood something, I am happy to reconsider my opinion.  ~~\n\n\n==== post discussion with other reviewers ====\nI met with other reviewers and the AC to discuss the paper.  \n\nOn the one hand, my fundamental criticisms still stand: if one considers a distribution with a large core feature and a small spurious feature, most natural algorithms will fail to be invariant in the sense the authors describe, and it appears previous work has indeed shown that in similar settings natural ERM-based algorithms will fail to be invariant.  Thus, it is not surprising that interpolating models will exhibit the same failure to invariance, and it isn't clear what insight this provides into the phenomenon of interpolation: previous work on benign overfitting (and classical statistical learning theory) has established how brittle interpolating models can be even for within-distribution generalization.  \n\nOn the other hand, it appears some machine learning practitioners may have taken from the 'benign overfitting' results that interpolation is not only something that can be sometimes ok, but is something that should be encouraged.  This work then clearly shows that, in at least the setting of high-dimensional gaussian class-conditional distribution with large  spurious features, interpolation precludes invariance.  \n\nIn summary, although I believe this work is borderline, I would be fine if the AC/PC accepts the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3156/Reviewer_aRur"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3156/Reviewer_aRur"
        ]
    },
    {
        "id": "BoUkc8PUku5",
        "original": null,
        "number": 5,
        "cdate": 1667540048596,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667540048596,
        "tmdate": 1669740670429,
        "tddate": null,
        "forum": "dQNL7Zsta3",
        "replyto": "dQNL7Zsta3",
        "invitation": "ICLR.cc/2023/Conference/Paper3156/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proves that, in a particular class of two-environment linear models containing both relevant and irrelevant directions, interpolation is incompatible with invariance (having low \"robust error\" as used in out-of-domain generalization, or also corresponding to fairness constraints).\n\nIt also proposes an algorithm, based on learning a fair/robust linear combination of predictors learned independently on each environment, can provably learn a predictor with low robust error, and shows that it works reasonably well both in the setting to which the theoretical bounds apply and also for a linear model on pretrained features for Waterbirds, where \u2013\u00a0unlike previous approaches \u2013\u00a0it does (slightly) reduce the FNR gap past the interpolation threshold.",
            "strength_and_weaknesses": "Strengths:\n\n- The explicit incompatibility between interpolation and robustness in a specific setting is good to have worked out.\n- The fact that Algorithm 1 works on one particular problem setting is also good to know.\n- Based on one experiment, the algorithm seems like it might be somewhat effective in (linear) interpolating regimes, where previous invariance approaches such as IRM totally fail.\n\nWeaknesses:\n\n- The parts of the main theoretical result that aren't about the algorithm seem to be unsurprising; see below.\n- This means Algorithm 1 is relatively more important to the overall quality of the paper. But Algorithm 1 is a little bit underexplored:\n  - It's only defined for two environments (though there's an obvious extension to more)\n  - It's only theoretically analyzed in one quite-restrictive setting\n  - It's only empirically analyzed for:\n    - a small number of instances of that restrictive setting,\n    - and one relatively-unrealistic real-data experiment, where empirical results do show some ability to do _something_ in the interpolating regime, but at quite a high cost (see below).",
            "clarity,_quality,_novelty_and_reproducibility": "## Unsurprising that there exist problems where interpolation and invariance are at odds\n\nConsider, for simplicity, rotating your problem so that $\\mu_c = r_c e_1 = (r_c, 0, \\dots, 0)$ and $\\mu_s = r_s e_2 = (0, r_s, 0, \\dots, 0)$.\nThen your data distribution is\n$$\ny \\sim \\operatorname{Unif}\\\\{-1, 1\\\\}\n\\qquad\nx \\mid y \\sim \\mathcal N\\left( \\begin{bmatrix}y r_c \\\\\\\\\\ \\theta y r_s \\\\\\\\ 0 \\\\\\\\ \\vdots \\\\\\\\ 0 \\end{bmatrix}, \\sigma^2 I_d \\right)\n\\tag{A}\n.$$\nIn the main motivating setup of your paper, you'd like there to exist a large-margin separator for this problem, and moreover for it to exhibit benign overfitting, i.e. to have low generalization error.\n\nIn the main motivating setup, you'd also like for there to _not_ be an invariant large-margin separator, which is exactly the question of there not existing a large-margin separator for the $(d-1)$-dimensional problem\n$$\ny \\sim \\operatorname{Unif}\\\\{-1, 1\\\\}\n\\qquad\n\\tilde x \\mid y \\sim \\mathcal N\\left( \\begin{bmatrix}y r_c \\\\\\\\ 0 \\\\\\\\ \\vdots \\\\\\\\ 0 \\end{bmatrix}, \\sigma^2 I_{d-1} \\right)\n\\tag{B}\n.$$\n\nThat this situation can occur is a large part of the results of your paper (the other part being everything about Algorithm 1).\nIntuitively, though, it's clear when framed like this that this is possible \u2013\u00a0if (A) is \"right on the boundary\" of having a separator of a given margin, and $\\theta r_s$ isn't too small, then (B) won't.\n\n(I don't know offhand of previous results that establish the achievable margin for (A) or (B), but they're _almost_ addressed by the far more general results of section 6 of the (concurrent) paper [Zhou et al. (2022)](https://arxiv.org/abs/2210.12082), who only study Gaussian $x$ but I believe can be extended to handle $x$ being a mixture of a small number of Gaussians fairly straightforwardly, and then show both a margin bound and conditions for benign overfitting in the squared hinge loss.)\n\nAll of this is only about the existence of any high-margin separator. Maybe this was in e.g. Rosenfeld et al. (2020) or Nagarajan et al. (2020) and I don\u2019t remember (in which case you should recap the results here), but it would also be good to understand what particular algorithms \u2013 especially the max-margin separator \u2013\u00a0do in situations like this one: how much weight do they put on $\\mu_s$? We'd expect that even when there exist invariant interpolators, the max-margin separator will still put a lot of weight in the \"bad\" direction $\\mu_s$ (since it doesn't know it's bad).\n\n## Experimental results\n\nFor the simulations, the experiments are fine and show that your algorithm works in the case it was designed for, although it'd be nice to also see maybe the test accuracy vs $d$ for a value of $\\theta$ or two. I'm also especially wondering whether having a $\\theta = 0$ environment is very important to the qualitative nature of your results, since it will presumably put very little weight towards $\\mu_s$, and then the fair learning step will likely mostly just use that predictor since it's naturally going to be nearly fair. Does something similar happen if you have a $\\theta = 1$ and $\\theta = 0.5$ environment?\n\nFor Waterbirds: these results do indeed show that the test FNR gap is decreased by the regularizer even for high $d$, which is good! The amount of improvement there is fairly small, though, and the problem somewhat contrived. I'd be interested to know if this holds up if there are more than two training environments, if the individual-environment predictors are not themselves linear, and so on.\n\nAlso, almost all of the improvement in test FNR gap is achieved by \"Ours ($\\lambda = 0)$\", indicating that the vast majority of the added invariance of your method comes from combining predictors learned on each environment separately. (I'm not aware of this scheme having been used previously in invariant learning, but it's closely related to some schemes in meta-learning-type areas, particularly [CAVIA](https://arxiv.org/abs/1810.03642) if only the \"late-layer\" parameters are task-specific, and also multiple kernel learning where the kernels are themselves learned on related tasks as in [MetaMKL](https://arxiv.org/abs/2106.07636).) To understand this a little more, it'd be good to also see results for just $v = (1, 0)$ and $v = (0, 1)$, i.e. just using one or the other predictor, for test FNR in both environments, compared to the outcome of the fair learning.\n\n## Miscellaneous small questions\n\n- Why use $\\lVert v \\rVert_\\infty = 1$ as the constraint in the second phase of Algorithm 1? Taking a convex combination of the $w$s might would feel more natural, but it also isn't obvious that any constraint at all is needed, other than I suppose inside the analysis.\n\n## Typos, etc\n\n- There are several uses of \\citet that should be \\citep, e.g. at the bottom of the first page.\n- Section 5.1 setup: \"we further fix $r_c =1$ and $r_c = 2$\" \u2013\u00a0presumably the second one should be $r_s$ :)\n- Many papers in your bibliography are cited as arXiv papers but were in fact published at conferences, e.g. I noticed Rosenfeld et al. (2020) and Nagarajan et al. (2020) which were both published at ICLR 2021. A tool like [rebiber](https://github.com/yuchenlin/rebiber) could catch all of those for you.",
            "summary_of_the_review": "This paper has two main contributions: one theoretically establishing a conflict between interpolation and invariance, and one suggesting an algorithm and showing it works in this theoretical setting as well as some very limited empirical results.\n\nThe first contribution is unsurprising; working out the details is valuable, but I'm not sure the way the paper is currently written that casual readers will come away with the right idea about \"how often\" this holds, etc.\n\nThe second is more surprising, but somewhat under-evaluated, as described above.\n\nOverall, I think the point of the paper is important \u2013\u00a0interpolating settings are major flaws in current approaches to invariant learning, as has been noted before e.g. in the discussion of the IRM paper \u2013\u00a0but I'm somewhat unhappy with its framing and the thoroughness of its exploration and evaluation. I'm giving the paper a 6 because I think it matters and I think it's above the minimal bar for ICLR, but there's a hypothetical version of this paper with some more work that I would be far more excited about.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3156/Reviewer_7Zqk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3156/Reviewer_7Zqk"
        ]
    }
]