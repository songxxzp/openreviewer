[
    {
        "id": "Pln3sngx_b",
        "original": null,
        "number": 1,
        "cdate": 1666542099990,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666542099990,
        "tmdate": 1666542099990,
        "tddate": null,
        "forum": "2G-vUJ7XcSB",
        "replyto": "2G-vUJ7XcSB",
        "invitation": "ICLR.cc/2023/Conference/Paper5733/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studied the generalization problem in RL and used Bayesian regret as a performance measure. The authors considered if the agent is allowed to use the data from interaction with the testing environment.  ",
            "strength_and_weaknesses": "I feel this is an interesting paper. Considering testing time interaction is the right way to think about generalization in RL. The lower bound in Section 4.1 is not surprising and intuitive. I am glad the author can make it formal. I have two major concerns about this work:\n \n1. The authors mentioned in the training stage, the agent can sample i.i.d MDP instances from the unknown distribution. I can not understand why the agent has such power. This can only happen in a highly-simulated environment and I am not aware any real-application can satisfy this requirement. Algorithm 1 needs to sample a large number of MDPs. If the state space is infinity, how can the agent do that? For example, if I want to train an agent to play Atari, how Algorithm 1 samples MDPs?\n \n2. Theorem 2 is less satisfactory, especially for C(D) term. This complexity measure is not interpretable and so far it's only better than SA in a very extreme case. I feel this should explicitly depend on some quantities that describe the distribution of D. Usually for a tabular MDP, people put Dirichlet prior on the dynamic, which is continuous. Then C(D) should depend on the Dirichlet parameters. It's unclear how good C(D) is and how it reflects the intermediate regime. \n \nIn the end of Section 4.2.2, the author mentions \"when D is subgaussian or mixtures of subgaussian\". What does it mean? The dimension of a MDP is a SxAxS tensor then how do you define sub-gaussian here?\n \n",
            "clarity,_quality,_novelty_and_reproducibility": "Good. ",
            "summary_of_the_review": "This is an interesting paper but the theory part is less strong. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5733/Reviewer_s13Q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5733/Reviewer_s13Q"
        ]
    },
    {
        "id": "HU_s4ykCiG",
        "original": null,
        "number": 2,
        "cdate": 1666622847240,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622847240,
        "tmdate": 1671111003784,
        "tddate": null,
        "forum": "2G-vUJ7XcSB",
        "replyto": "2G-vUJ7XcSB",
        "invitation": "ICLR.cc/2023/Conference/Paper5733/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper theoretically investigates the implications of pre-training for generalization in RL in terms of possible benefits and limitations. Specifically, the paper shows that in an asymptotic limit (where the agent can interact $K \\rightarrow \\infty$ times with its environment and knows the MDP distribution D) pre-training can improve the regret only up to a constant. In the non-asymptomatic case, the authors propose an algorithm for which pretraining can help, by deriving a regret bound that depends on the complexity of the (MDP) distribution D. Finally, authors propose a second algorithm that pursues optimalitiy in expectation for the case where the agent is not allowed for further test-time interactions after pretraining.",
            "strength_and_weaknesses": "Strength\n- The paper offers a more precise understanding of the best-case benefits of pretraining over no pretraining when fine-tuning is allowed for $K \\rightarrow \\infty$ episodes at test-time.\n- Extensive theoretical investigation of the benefits of pretraining in RL with the help of two proposed algorithms for the setting with finetuning or test-time interaction in the non-asymptotic case and without further test-time interactions.\n- The authors do a good job at contextualizing and explaining the theoretical implications and ramifications of their derived bounds.\n- The paper is well-structured and clearly written.\n- Appropriate literature review to contextualize the contributions\n\nWeakness\n- While I appreciate the extensive analysis and theoretical nature of the proposed algorithm my biggest concern is that the benefits and usefulness of these bounds in practical pretraining settings in RL are not very clear from the paper. I would have hoped the authors could at least implement their algorithm and test how well it actually works in popular RL environment test beds, e.g. the Arcade Learning Environment. Without such experiments or empirical results, my worry is that the algorithms and bounds are of limited use. As an example, how effective and computationally expensive would the algorithm be in practice over naively pretraining an agent on the subset of MDP environments, and then adapting to a new environment of the same distribution?\n- The proposed bounds and algorithms assume the pretraining and test environments are sampled iid while pretraining is especially hoped to help in OoD scenarios.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and the theorems are well-presented and contextualized. The derived regret bounds in the particular pretraining setting with test-time interaction considered by the authors are novel to the best of my knowledge.",
            "summary_of_the_review": "This paper offers a range of novel theoretical insights regarding the benefits (or impossibilities) of leveraging pretraining for generalization in RL where the pretraining and test environments are sampled from the same distribution. While the paper is purely theoretical and I believe such insights to be helpful in general, it is unclear how much those particular insights help in practice where pre-training is already a frequently used technique for RL. I would have expected the authors to at least empirically prove the effectiveness of the suggested algorithms on RL test beds. I, therefore, lean towards rejecting the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5733/Reviewer_tCeD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5733/Reviewer_tCeD"
        ]
    },
    {
        "id": "ntcLNRMEeiw",
        "original": null,
        "number": 3,
        "cdate": 1666908944086,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666908944086,
        "tmdate": 1668798787539,
        "tddate": null,
        "forum": "2G-vUJ7XcSB",
        "replyto": "2G-vUJ7XcSB",
        "invitation": "ICLR.cc/2023/Conference/Paper5733/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses a generalization problem in RL, in which the agent interacts with a set of MDPs taken from an unknown task distribution during training, and subsequently minimizes the (expectation) of the regret in a test MDP taken from the same distribution. The paper first studies an asymptotic setting in which the number of test episodes goes to infinity, showing that the benefit of pre-training is at most a constant factor. Then, it considers a non-asymptotic setting through an algorithm, called PCE, that pre-trains a set of policies to achieve a regret upper bound that depends on the complexity of the task distribution but not on the number of states and actions in the MDPs. Finally, the paper studies an additional setting in which no interactions can be taken at test time, for which it provides an algorithm (OMERM) achieving near-optimal zero-shot adaptation to the test MDP by taking a polynomial number of samples during training.",
            "strength_and_weaknesses": "Strengths\n- (Relevance) The paper addresses the interesting problem of pre-training a set of policies for efficient RL at test time;\n- (Originality) The paper introduces several original ideas, including interesting problem formulations and algorithms to pre-train policy sets.\n\nWeaknesses\n- (Presentation) The paper is not an easy read. The notation is not always sharp, the claims are sometimes sloppy and not clearly supported, and some of the comments on the theoretical results can be confusing;\n- (Odd evaluation choices) The benefit of pre-training is studied either in a bizarre asymptotic formulation of the regret or compared against a RL baseline that learns from scratch neglecting the samples complexity of the pre-training itself;\n- (Unreasonable results) Some of the theoretical statements are so strong that look unreasonable (though I am probably missing something);\n- (Non-asymptotic lower bound) There is a clear mismatch between the provided lower bound (on the asymptotic cumulative regret) and the regret upper bounds of the proposed algorithms, which instead come from non-asymptotic analysis;\n- (Related works) Only a few related works are reported, and the relations with some relevant pieces of the literature, such as multi-task RL (e.g., Brunskill & Li, Sample complexity of multi-task reinforcement learning, 2013), are not discussed well-discussed.\n\nComments\n\n(Lower bound) I am wondering what is the purpose of the asymptotic setting. Regret minimization is usually interesting in finite time, what is the point of comparing the infinite values of the asymptotic cumulative regrets? Moreover, all of the other results are reported on finite-time settings, for which a lower bound is not formally provided. This makes unclear how to evaluate the obtained upper bounds.\n\n(Measure of performance) I am not sure the test-time regret alone is a good measure to uphold the importance of pre-training. It is clear that pre-training can provide some benefits at test time, but what is the point in comparing the $O(\\sqrt{SAHK})$ regret of RL from scratch with the $O(\\sqrt{\\mathcal{C}(\\mathbb{D}) K})$, when the PCE algorithm still suffers $O(|\\Omega| \\sqrt{SAHK})$ regret in the pre-training phase (or, alternatively, sample complexity)?\n\n(Algorithm 1) Is the PCE algorithm always guaranteed to find a suitable set of policies during pre-training? I.e., the condition at line 10 is guaranteed to be verified eventually?\n\n(Theorem 3) I am in all likelihood missing something here, but the result in Theorem 3 seems to be so strong to be unreasonable. In particular, when the covering number on the policy space is finite the output of OMERM can generalize to any unseen task without further interactions. The following thought experiment suggest otherwise: If I sample a test task from $\\mathbb{D}$ and then a policy uniformly at random from the obtained policy set, in all likelihood the policy will not be good for the specific instance. If I repeat this process several times (then converging to the expectation) I cannot see how the average sub-optimality can be small. What is wrong with my thought experiment?\n\n(Non-Markovian policies) The paper seems to be only considering Markovian policies, but I am wondering whether this setting necessitates history-based policies. From my intuition, the test task is akin to a partially observable MDP, where the information on the specific instance is hidden to the agent. Do the authors think that Markovian policies are sufficient instead?",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is overall confusing, and it does not help evaluating the contribution of this paper. Notation is not always sharp or intuitive, some symbols are never defined, there are a few sloppy claims and confusing comments. Presentation issues include:\n- The paper often refers to \"learning/knowing $\\mathbb{D}$\", but the algorithms actually learn set of policies and not the task distribution;\n- The cardinality of the set of MDPs is denoted sometimes with $\\Omega$ and sometimes with $M$ (I am not sure the latter symbol have been introduced actually);\n- Algorithm 1 reports a symbol $\\mathcal{U}$ that is defined in the appendix. The Subroutine 4 is only reported in the appendix though it is crucial to understand the algorithm;\n- It is not clear how many times the loop at line 3 of Algorithm 1 has to be executed.\n- \"We select $(\\pi_l, v_l)$ with the most optimistic value $v_l \\in \\Pi_l$\". It is not clear how the value can be (optimistically) estimated without knowing the true test MDP;\n- \"$\\mathcal{C} (\\mathbb{D})$ is still bounded\". Not clear why.",
            "summary_of_the_review": "*After discussion*\n\nAfter a fruitful discussion with the authors, I have a better understanding of the contributions of the paper. Most of my concerns have been solved, and I am updating my evaluation to accept.\n\n---\n\n\nThis paper is in many ways interesting. Generalization in RL is an extremely relevant problem, while the algorithmic procedures and the problem formulations seem to introduce original ideas. Especially, the idea of achieving guaranteed test-time sub-optimality/regret by covering the space of instance-optimal policies looks very promising to me. \n\nHowever, the presentation is less than ideal, and as a result there is a good chance I am missing crucial parts of this submission. The reported results seem sometimes unreasonable to me (especially Theorem 3), and I cannot see how this problem can be solved without strong structural assumptions on the task distribution and the underlying MDPs. \n\nI am currently providing a borderline evaluation that reflects my skepticism about some of the claims. I will revaluate my score upon a due inspection of the proofs, which I could not check given the limited reviewing window, and clarifications from the authors' response.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5733/Reviewer_SxSR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5733/Reviewer_SxSR"
        ]
    }
]