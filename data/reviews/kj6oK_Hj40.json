[
    {
        "id": "eAw2jNecSQ",
        "original": null,
        "number": 1,
        "cdate": 1666559222060,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666559222060,
        "tmdate": 1666559305183,
        "tddate": null,
        "forum": "kj6oK_Hj40",
        "replyto": "kj6oK_Hj40",
        "invitation": "ICLR.cc/2023/Conference/Paper644/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to use self-distillation as a regularization method to improve transfer learning performance on image classification tasks. The paper first observes that further fine-tuning on a pre-trained model on unlabelled target dataset would result overfitting. And then it proposes the self-distillation method to resolve this issue. The general idea of self-distillation method is to first further pre-train a teacher model on target unlabelled data with a MAE objective, and then guide a student model from the original pre-trained model for the same representation learned from the teacher model and additionally trained with the same MAE objective. And finally to fine-tune the student model with the labelled target data. The paper provides theorical analysis on the generalization bound for the proposed method and showed an improved performance over a range of fine-grained image classification benchmarks. ",
            "strength_and_weaknesses": "Strength.\n+ I found the paper is very well written with a clear structure and motivation.\n+ The design of self-distillation method is very clean and general. I can imagine it can be useful beyond image classification tasks.\n+ The paper also provides theoretical analysis showing self-distillation acts as a regulariser on the distance between the initial pre-trained weights and final fine-tuned weights.\n+ Ablative analysis is very comprehensive showing different design strategies within the self-distillation method.\n\nLimitation.\n- Further pre-training induces overfitting? The paper has highlighted multiple times that further pre-training would hurt transfer learning performance and overfits on the target dataset. However, from the Table 1 and 2, the further pre-training strategy actually performs typically the second right after the proposed method, which is a bit contractor to what the paper has highlighted, and it is always better than direct fine-tuning? I am wondering is this because this further pre-training is well tuned or some other reasons? A bit more information on this baseline would be important.\n- A fairer comparison to the baseline. From Figure 2, we can see that the proposed self-distillation method requires 3 times longer than fine-tuning and 1.5 times longer than the further pre-training method.  Comparing with further pre-training might be okay since the further pre-training on the teacher model did not account to be included in the final performance evaluation. However, we may argue that it could be possible that the improved performance was due to longer training time. I would suggest having one small additional experiment showing that the self-distillation method works was due to the design rather than a longer training time.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear and well-written and should be easily reproducible.",
            "summary_of_the_review": "The paper overall is very good with theoretical analysis and very comprehensive experiment support. There are some minor issues listed in the limitation section and I would consider raising my score if the authors could clarify these issues.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper644/Reviewer_7okZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper644/Reviewer_7okZ"
        ]
    },
    {
        "id": "HpoaH3rJKr",
        "original": null,
        "number": 2,
        "cdate": 1667058234997,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667058234997,
        "tmdate": 1667058234997,
        "tddate": null,
        "forum": "kj6oK_Hj40",
        "replyto": "kj6oK_Hj40",
        "invitation": "ICLR.cc/2023/Conference/Paper644/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a self-distillation approach to transferring a pre-trained model to a new task.  The method first applies further pre-training to arrive at a set of teacher weights that is then used to guide the subsequent student training starting from original pre-trained weights.  The authors show self-distillation with further pre-training to be an effective method for regularizing further pre-training for downstream tasks through a variety of empirical studies on image and text classification tasks.  Finally, theoretical analysis shows how self-distillation acts as a form of distance-based regularization on the initial pre-trained weights. ",
            "strength_and_weaknesses": "Pros:\n- The empirical results for self-distillation relative to other forms of regularized fine-tuning is state-of-the art on both text and image classification tasks.\n- The authors conduct a series of empirical studies and ablation studies to verify the regularization benefits of self-distillation.\n- The writing is clear and well organized.\n\nCons:\n- The approach is limited in novelty but I still think the proposed method is important.\n- Figure 1 shows that even with self-distillation, the model regresses to no benefit from further pre-training after 60k steps.\n\nQuestions:\n- From the text, it appears that self-distillation does not have a tunable parameter to trade off between MAE and Distill loss. Is there any benefit to tuning the weight for the two terms in the training objective? \n- The theoretical analysis shows bounded distance between self-distilled weights and initial weights but does not comment on distance to teacher weights.  Is there a breakdown you can provide for that bound?\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\nThe paper is well written and clear.  I appreciate the prose explaining the main takeaways from the theoretical analysis.\n## Quality\nThe empirical studies is conducted in a very thorough manner with multiple seeds, variance quantification, and extensive ablations.\n## Reproducibility\nI believe sufficient details are provided for the reader to be able to reproduce these experiments.",
            "summary_of_the_review": "The authors introduce a simple yet effective way to transfer a pre-trained model to a new downstream task via self-distillation with further pre-training.  The experimental results are strong and show self-distillation to be state-of-the-art across multiple text and vision classification tasks.  Ablations also show self-distillation to be a critical component of these empirical results. Aside from a couple of questions and some concern regarding limited novelty, I would recommend the paper to be accepted due to its relevance and high potential for impact in practical workloads adapting a pre-trained model to a new task.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper644/Reviewer_RiKW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper644/Reviewer_RiKW"
        ]
    },
    {
        "id": "SheO-zjP3x",
        "original": null,
        "number": 3,
        "cdate": 1667150591957,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667150591957,
        "tmdate": 1667150591957,
        "tddate": null,
        "forum": "kj6oK_Hj40",
        "replyto": "kj6oK_Hj40",
        "invitation": "ICLR.cc/2023/Conference/Paper644/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The manuscript studies the problem of unsupervised fine-tuning on a small target dataset before the actual supervised fine-tuning on it. The insight is that even unsupervised fine-tuning (MAE here) on a small dataset causes biased representation that hurts generalization. So the proposed recipe is to use another unsupervised fine-tuned network as a feature distillation regularization term. The authors also provide a theoretical justification using a single-layer network. Evaluations on a set of vision and language tasks show the effectiveness. The method is easy to implemented and codes are provided.",
            "strength_and_weaknesses": "Strengths:\n+ This is a somewhat interesting but understandable technique as the distillation loss can function as an ensemble regularization as it is also trained with a MAE loss. This kind of things usually work but knowing that it works for an MAE loss is new to me.\n+ The method is well-benchmarked and shows consistent improvements on a set of fine-grained classification tasks (and some language tasks that are out of my scope).\n\nWeaknesses:\n- I believe the method works well and I understand that using MAE is preferable due to its simplicity. But I still wonder how it compares with SupCon [A] in the direct fine-tuning setting.\n- Missing references to a line of highly-related self-distillation methods that works on a single network [B][C][D].\n\n[A] Supervised Contrastive Learning, NeurIPS 2020.\n[B] Deeply-supervised knowledge synergy, CVPR 2019\n[C] Be your own teacher: Improve the performance of convolutional neural networks via self distillation, ICCV 2019\n[D] Contrastive Deep Supervision, ECCV 2022",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: easy to follow.\nQuality: Well-developed and presented.\nNovelty: Marginal but principled.\nReproduciblity: easy to implement with a code release.",
            "summary_of_the_review": "A good empirical study to me.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper644/Reviewer_PVpp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper644/Reviewer_PVpp"
        ]
    },
    {
        "id": "nurdmvEdjP",
        "original": null,
        "number": 4,
        "cdate": 1667188872396,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667188872396,
        "tmdate": 1667190996458,
        "tddate": null,
        "forum": "kj6oK_Hj40",
        "replyto": "kj6oK_Hj40",
        "invitation": "ICLR.cc/2023/Conference/Paper644/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the usage of pretrained vision transformers on downstream tasks. The authors point out the weakness of the current further pretraining strategy about overfitting. To solve that problem, they propose a new pipeline which includes a self-distillation process to distill the knowledge from a further-pretrained model to an initial model.",
            "strength_and_weaknesses": "Strength:\n1. The proposed method is simple yet effective.\n2. The authors provide comprehensive theoretical analysis which clearly shows the role of the proposed algorithm.\n\nWeaknesses:\n1. I wonder if it is safe to say the improvement of self-distillation is solely from solving the overfitting problem. In fact besides the original self-distillation paper which adopts this method for better generalization, many other works also use it for better performance without considering overfitting. The authors present the gap between training and testing loss in the experiments, which is great. It would be better if the authors can show that with similar training loss/training accuracy the model train with the proposed method has lower testing loss.\n2. This experiments in this paper only focuses on MAE-based model. I think the authors may have some discussion about the effect of the proposed method on other types of self-supervised algorithms, for example, the contrastive learning based methods like SimCLR and DINO.\n3. Since the proposed method requires two steps of training, I am not sure if this method is efficient enough.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and in good quality. \n\nAs for novelty, the self-distillation technique has been used and proven to be helpful in many previous works. Therefore, I would rather take this paper as a successful application of self-distillation in the specific tasks. \n\nI assume the proposed method can be easily reproduced based on the authors' description and the attached code.",
            "summary_of_the_review": "As mentioned above, this paper is overall good in my opinion. Although the novelty is to some extent limited by applying commonly used technique, the authors present insightful analysis and comprehensive experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper644/Reviewer_5qJ3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper644/Reviewer_5qJ3"
        ]
    },
    {
        "id": "8V_mIfltlh",
        "original": null,
        "number": 5,
        "cdate": 1667451346001,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667451346001,
        "tmdate": 1667451346001,
        "tddate": null,
        "forum": "kj6oK_Hj40",
        "replyto": "kj6oK_Hj40",
        "invitation": "ICLR.cc/2023/Conference/Paper644/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for finetuning transformers on new datasets. By an additional self-distillation process between the pre-training and finetuning stages, the authors show that transformers can achieve better generalization on downstream datasets. The authors provide a theoretical analysis of the proposed method with a simplified model. Experiments on ten image and text classification datasets show the efficacy of the proposed method.\n",
            "strength_and_weaknesses": "Strength\n1. The writing is clear and the proposed method seems straightforward and easy to implement.\n2. The authors have provided a theoretical analysis of the effect of the self-distillation process.\n3. Experiments are thorough and cover both image classification and text classification with transformers. The experiments are repeated multiple runs to test for statistical significance.\n\nWeaknesses\n1. The results, although significantly different, are not better by a large margin compared to fine-tuning, especially for image classification (1-2 points different). It would be great if the authors also provide an analysis of how much additional computational cost there is compared to the finetuning and further pre-training method, which could be a useful reference for people that want to use the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written. I think the proposed idea is straightforward and novel. The method seems easy to reproduce.\n",
            "summary_of_the_review": "I like the clear and novel approach. Although the performance boost is not huge, I think the paper provides enough useful insights and analysis to be accepted.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper644/Reviewer_yUym"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper644/Reviewer_yUym"
        ]
    }
]