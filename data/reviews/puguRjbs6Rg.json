[
    {
        "id": "RhD6V_bpIA",
        "original": null,
        "number": 1,
        "cdate": 1666641501762,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641501762,
        "tmdate": 1666641501762,
        "tddate": null,
        "forum": "puguRjbs6Rg",
        "replyto": "puguRjbs6Rg",
        "invitation": "ICLR.cc/2023/Conference/Paper714/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper suggested two major drawbacks on pruning algorithms:\n\n- Difficulties in controlling the sparsity level\n- Weights that are pruned away at an early stage do not have a chance to recover\n\nThe proposed scheme defines two symbolic states \u2018to-prune\u2019 and \u2018not-to-prune\u2019, then they generate a soft-mask for values $w$ that sits at the pruning boundaries.",
            "strength_and_weaknesses": "Strengths\n\n- This paper is well-written with many illustrative plots.\n- The design choices are well justified, and the authors have made a detailed comparison to various state-of-the-art fine-grained pruning algorithms.\n\nWeaknesses\n\n- The reported MAC savings are very theoretical, with this pruning granularity, I do not think they transfer directly to run-time performance gains.\n- Using a soft-mask for pruning is an existing idea.  However, this paper did perform a nice  reasoning for the usage of a softmax function on these two symbolic states , the proposed method is still an interesting one.",
            "clarity,_quality,_novelty_and_reproducibility": "- Equation 4 is a little confusing to me, I would imagine you can swap the position between w and t, then you might also get rid of the negative sign? Is there a specific reason why we cannot do this and have to write this equation in this particular manner?\n- You might want to label x and y-xis in your Figure 3b.\n- Algorithm 1, line 5, your W is a matrix, are you applying t element-wise? Or is it to each column of W? I would assume this is element wise, then this notation does not look correct.\n- I do not understand what is happening to your caption system, or maybe you just missed a caption for Table 1? The organization on Page 8 is simply not visually pleasing.\n\nThe rest of the paper is generally clear to me. As I mentioned in weaknesses, soft-mask for pruning is an existing topic, but I do think this paper has provided a new angle to this problem.",
            "summary_of_the_review": "Although I have mentioned about how the evaluation using MACs is not ideal and soft-mask bsed pruning is an existing idea, I do think the proposed symbolic states and its corresponding softmax formulation presents an interesting angle to this pruning problem. So I vote for an accept for this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper714/Reviewer_Dmgj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper714/Reviewer_Dmgj"
        ]
    },
    {
        "id": "RGaTaNXf-3",
        "original": null,
        "number": 2,
        "cdate": 1666694763550,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694763550,
        "tmdate": 1666694763550,
        "tddate": null,
        "forum": "puguRjbs6Rg",
        "replyto": "puguRjbs6Rg",
        "invitation": "ICLR.cc/2023/Conference/Paper714/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This post introduces a differentiable pruning algorithm.\n\nThe major contributions include:\n+ A differentiable and parameter-free pruning algorithm based on attention.\n+ Efficiently pruning to offer a high-quality model for a given pruning target.\n+ The state-of-the-art results on both computer vision and natural language tasks.",
            "strength_and_weaknesses": "## Strengths\n1. The article is easy to understand.\n2. The method is simple and effective and has achieved good performance.\n\n## Weaknesses\n### Content\n1. In essence, the core contribution of this article is how to choose a threshold t. After the threshold t is obtained, the rest is to use t to generate a mask and then apply the mask to the weight for e2e training. This is actually a very common idea. For example, in the field of NAS, DARTS uses a similar idea. As for gradually adjusting the pruning ratio r, in the field of Detection, it is actually warm up. So these two points are technically feasible, but they lack innovation.\n2. I think an experimental result with a low pruning rate r needs to be provided. In this way, the experimental results are complete, proving that this method can work (or at least not deteriorate) under various pruning rates r.\n\n### Writing\n1. z(w) is missing a label.\n2. Symbol |.| has a different meaning in z(w) and equation 1.",
            "clarity,_quality,_novelty_and_reproducibility": "1. **Clarity**: The article is easy to read and the details are relatively clear.\n2. **Quality**: There are some flaws (refer to the Weakness section for details).\n3. **Novelty**: Lack of certain innovation (refer to the Weakness section for details).\n4. **Reproducibility**: I have no actual operation, but from the description, it should be relatively easy to reproduce.",
            "summary_of_the_review": "Overall, this article introduces a pruning method that can achieve better performance. But I think this method lacks a certain innovation (refer to the Weakness section). There is still a certain distance from a paper that can be received.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper714/Reviewer_qrMF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper714/Reviewer_qrMF"
        ]
    },
    {
        "id": "iHrxfG4YL7o",
        "original": null,
        "number": 3,
        "cdate": 1666931903586,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666931903586,
        "tmdate": 1666931903586,
        "tddate": null,
        "forum": "puguRjbs6Rg",
        "replyto": "puguRjbs6Rg",
        "invitation": "ICLR.cc/2023/Conference/Paper714/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The main contribution of the paper is to devise an iterative differentiable and parameter-free pruning algorithm utilizing attention-based soft pruning masks. They show that the iterative learnable pruning results in improved performance on vision and NLP tasks.  ",
            "strength_and_weaknesses": "The differentiable pruning utilizing soft attention masks results into simplicity and efficiency of training. There is no overhead of auxiliary optimization or additional parameters. As opposed to learning a pruning threshold which is difficult for controlling the sparsity, the proposed method IDP provides higher flexibility and recovery from unwanted pruning in early stages. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. Although differentiable pruning isn't new, but the use of soft-mask makes the differentiable training simpler - that's a huge plus. ",
            "summary_of_the_review": "This is a well-motivated and well-written paper. It clearly explains the issues with previous methods, and how they addresses some of those. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper714/Reviewer_XNYa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper714/Reviewer_XNYa"
        ]
    },
    {
        "id": "jqLtG6i6Ca",
        "original": null,
        "number": 4,
        "cdate": 1667146434289,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667146434289,
        "tmdate": 1667146434289,
        "tddate": null,
        "forum": "puguRjbs6Rg",
        "replyto": "puguRjbs6Rg",
        "invitation": "ICLR.cc/2023/Conference/Paper714/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This manuscript studies the problem of network pruning for efficiency. The central technical point is a heuristic function (equation.1-3) that assigns a weight-wise soft mask (this is called attention) to each weight during training. The mask value is related to the target sparsity. Combined with a tailored training algorithm that gradually increases the sparsity ratio, the method is shown to out-perform recent pruning state-of-the-art methods. Code is not promised or provided.",
            "strength_and_weaknesses": "Strengths:\n+ This method is technically clear.\n+ The performance on standard benchmarks are quite good\u3002\n\nWeaknesses:\n- I think the major issue is presentation. As far as I can see, the method is not iterative, not differentiable and not attention-based. I don't understand what the title means. The presentation makes me so confused and doubt whether I truly understand the technical part. \n- By iterative, I thought there are some multi-stage residual formulations like [B] but it turns out not. What does 'iterative' mean in this method? According to algorithm.2, I can only see some standard SGD iterations. If so, shall we call every method 'iterative'? \n- By differentiable, I though there are some tricks that allow us to differentiate through some non-differentiable operators but it turns out not. Equations 1-3 are used in a non-differentiable manner. Which part is made 'differentiable'?\n- By attention, I though there some learnable attention masks but it turns out that the heuristically assigned mask values are named attention. Of course the authors have the freedom to call it attention. But this is the first time I see the term 'attention' used in this way.\n- Finally, since the motivation is to allow dead weights to be non-zero again, an important reference [A] that addresses this old problem is missing.\n\n[A] Dynamic network surgery for efficient dnns, NeurIPS 2016\n[B] Network sketching: Exploiting binary structure in deep cnns, CVPR 2017",
            "clarity,_quality,_novelty_and_reproducibility": "They have been presented in the last box.",
            "summary_of_the_review": "Clear technical description (although heuristic), good benchmark results but substantial presentation issues (maybe only to me).",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper714/Reviewer_gR7e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper714/Reviewer_gR7e"
        ]
    }
]