[
    {
        "id": "L1Gutnb5JQ",
        "original": null,
        "number": 1,
        "cdate": 1666646453528,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646453528,
        "tmdate": 1671137693236,
        "tddate": null,
        "forum": "QVcDQJdFTG",
        "replyto": "QVcDQJdFTG",
        "invitation": "ICLR.cc/2023/Conference/Paper2175/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents an algorithm for producing feasible solutions for families of linearly constrained optimization problems with varying right-hand sides. The algorithm can provide feasibility guarantees under certain circumstances by 1) dilating each of the linear constraint (shrinking the feasible region), 2) training a neural network that maps a right-hand side to a \"good\" point, and then 3) bounding the approximation error of the NN w.r.t. the dilated region to provide a feasibility guarantee w.r.t. the original feasible region. Computational results on optimal power flow problems are provided.",
            "strength_and_weaknesses": "The approach of the paper is a reasonable one, and stitches together results from a number of diverse areas in discrete optimization, deep learning, and power systems. However, I have some concerns about a number of technical errors or confusing mathematical informalities, and also feel that the authors oversell the generality of their proposed method.\n\n* The authors assume that (1-2) admits a unique optimum for each value for theta. Is this a reasonable assumption? If so, why? It seems quite restrictive to me (for example, most \"interesting\" families of linear optimization problems will not satisfy this), and seems very dependent on the choices of script{D} and the objective. Please provide some examples, or some explanation of how you would verify this property a priori, or an explanation of how the algorithm proceeds without this guarantee. Additionally, on p8 the authors state that \"In general, DC-OPF is a quadratic programming problem and admits a unique optimal solution w.r.t. load input\". Can the authors provide a citation or proof to this effect? What conditions are placed on the objective and script{D} to make this result hold? I am skeptical such a result will hold if the quadratic objective is simply the zero function, for example.\n\n* The authors state that \"it suffices to focus on OPLC with inequality constraints\". The justification in Appendix B is that any equations can be projected out to deal only with the original variables. This is true, though the authors should take care to consider implied equations. Note that implied can appear conditionally based on the parameter values: consider a simple example with the constraints x <= 2 - theta and x >= theta, with script{D} = {theta in [0, 1]}.\n\n* I am very confused by Remark (i) on p4: the reference points to a paper proving a time complexity bound for linear programming, but the problem under consideration is a _mixed-integer linear programming problem_. In general these will be NP-hard to solve.\n\n* The abstract is promising too much: The \"guarantee...[of] solution feasibility for optimization problems with linear constraints\" is contingent on the existence and algorithmic discovery of quantities with certain properties that may not exist (and it seems difficult to prove a priori _if_ they will exist). Moreover, the paper focuses on learning over families of related instances with only variation of the constraint right-hand sides (essentially), which is not mentioned until Section 3. These restrictions are all perfectly fine and justifiable, but they should be made front-and-center.\n\n* I am somewhat skeptical that the proposed algorithm will work well for general OPLC problems. There are seemingly a number of places where the approach can break down; for example: a) if there does not exist a delta > 0 (see \"implied equations\" point above), b) if such a value for delta exists but cannot be computed in a reasonable amount if time, c) if optimizing a single calibration value in (4-5) for all constraints is too conservative, d) if the DNN size grows needed for a feasibility guarantee is impractically large, or e) if the best solutions lie near the feasible region boundary and are \"cut off\" by the dilation. The author's open the door for such questions by framing their approach as a general purpose one, and so I think the paper would be more defensible if either 1) the authors present a more diverse set of computational experiments, or 2) tighten the focus of the paper towards DC-OPF. ",
            "clarity,_quality,_novelty_and_reproducibility": "* Are unbounded variables permitted in the model (1-2)?\n* \"We remark that given the value of DNN weights and bias, the value of z^k_i is uniquely determined by the status of each neuron.\" This is not true at the decision boundary (e.g., the pre-activation value is zero into a ReLU activation).\n* In the computational section: What are the preprocessing times required to set up and configure DeepOPF+ (e.g., training time for each DNN architecture tried, MILP solving time to compute delta, solving time to compute the DNN size, etc.). How does it amortize over the whole family of instances used for evaluation?",
            "summary_of_the_review": "While the method proposed by the authors uses some interesting ideas, I have concerns about the rigor, framing, and generality of the contributions claimed by the authors.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "The submission has substantial overlap with the following arxiv submission: << REMOVED BY PCs>>",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2175/Reviewer_crFR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2175/Reviewer_crFR"
        ]
    },
    {
        "id": "fEA80PxG-wH",
        "original": null,
        "number": 2,
        "cdate": 1666665269406,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665269406,
        "tmdate": 1666665269406,
        "tddate": null,
        "forum": "QVcDQJdFTG",
        "replyto": "QVcDQJdFTG",
        "invitation": "ICLR.cc/2023/Conference/Paper2175/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a framework for learning provably feasible approximations to optimization problems with linear constraints. This framework entails\n* Rewriting the original optimization problem, if it has (linear) equality constraints, in a solely inequality-constrained form, by using variable reduction techniques,\n* Constructing a buffer on the inequality constraints of the problem in a way that still ensures feasibility with respect to the input space,\n* Sizing a neural network to be large enough to be able to actually achieve feasibility over this input space (and actually solving for the parameters of such a feasible neural network, in order to initialize the network),\n* Training the neural network, using both input data and additional adversarial samples generated during neural network training to improve performance on the optimization objective.\n \nThe authors demonstrate the efficacy of their approach on DC optimal power flow, a linearly-constrained optimization problem used in power grids to schedule power generators under (approximate) physical constraints.\n",
            "strength_and_weaknesses": "Strengths: \n* The problem studied is one of importance, as providing fast and feasible approximators for optimization problems is widely applicable (including in power grid optimization, which the authors explicitly study).\n* The paper proposes a very interesting framework for addressing this problem, which nicely ties together many tools and pieces from across the literature.\n* The experimental comparison is thorough and well-done.\n* The writing and presentation is generally clear and well-done.\n \nWeaknesses: \n* For the ICLR audience, it is important to explain that DC-OPF is an approximation to the true power grid operation problem - otherwise, they may not be aware (and the applicability to DC-OPF only, rather than also AC-OPF, is a key limitation of the work).\n* Corollary 1 states: \u201cSuppose the DNN width is the same order of number of bus $B$.\u201d This assumption is not justified in the text.\n\nNote: I have previously reviewed this paper, and believe the authors have addressed all my major concerns from last time. I think this paper is deserving of publication at ICLR.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The submission is clearly-written, provides new ideas (including tying together many tools/pieces from across the literature), and is high-quality in both its theoretical justification and its experimental comparisons.",
            "summary_of_the_review": "This paper proposes a framework for learning provably feasible approximations to optimization problems with linear constraints. The submission is clearly-written, provides new ideas (including tying together many tools/pieces from across the literature), and is high-quality in both its theoretical justification and its experimental comparisons.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2175/Reviewer_ek1R"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2175/Reviewer_ek1R"
        ]
    },
    {
        "id": "oVCZOKuyDf7",
        "original": null,
        "number": 3,
        "cdate": 1667192813880,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667192813880,
        "tmdate": 1667196155426,
        "tddate": null,
        "forum": "QVcDQJdFTG",
        "replyto": "QVcDQJdFTG",
        "invitation": "ICLR.cc/2023/Conference/Paper2175/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes the first preventive learning based approach to guarantee DNN solution feasible for optimization problem with linear constrains. The key idea is to obtain an conservative satisfication of constriants and adjusting the size of DNN accordlying to guarantee that the redundant feasible region is large enough to offset the approximation error of DNN. The development of the algorithm is heavily based on bi-level optimization which is difficult to solve exactly and the author proposed several approximation approaches to address these issues.",
            "strength_and_weaknesses": "Strength:\n(1) This paper proposed the first DNN-based approach to solve constrianed problem with linear constraints without post-processing, which could be interesitng to the community.\n(2) Most of the steps in the algorithm is supported by theoretical guarantees.\n(3) The effectiveness of the proposed method is supported by some empirical results.\n\nWeakness:\nI didn't see obvious weakness in this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Since this paper proposed the first provably effective framework for using DNN in linear constraint optimization problem without post-processing, I think the contribution of this work is meaningful and novel.",
            "summary_of_the_review": "Overall I think this paper proposed a very interesting approach and most of the results look solid to me. However, since I am not the expert in this areas so I will leave the decision to other reviewers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2175/Reviewer_Z4Mb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2175/Reviewer_Z4Mb"
        ]
    },
    {
        "id": "lubkuDlmU7",
        "original": null,
        "number": 4,
        "cdate": 1667331153065,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667331153065,
        "tmdate": 1669857290541,
        "tddate": null,
        "forum": "QVcDQJdFTG",
        "replyto": "QVcDQJdFTG",
        "invitation": "ICLR.cc/2023/Conference/Paper2175/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a workflow for using neural networks to solve optimisation problems while guaranteeing the feasibility of the solution.",
            "strength_and_weaknesses": "Strength: \n\n1. The paper tackles an important problem of guaranteeing correctness of DNN-based solution (i.e., does not violate constraints) for the optimization problem, which is a major limitation of end-to-end neural network solutions. \n\n2. The evaluation suggests that the proposed techniques outperform baseline methods either in feasibility rate or optimality loss. \n\nWeakness:\n1. The repeated solving of (9)-(10) seems to be very expensive as it involves repeatedly solving MILPs.  \n\n2. It is unclear from the paper how challenging the CD-OPT problem actually are. If would be helpful if the authors report the concrete runtime of Pypower on these problems. \n\n3. It is unclear how scalable the proposed approach is (e.g., how fast the size of the network grows as the number of constraints increases, how long each component of the procedure takes). It would be helpful to report these numbers. \n\nQuestions:\n1. Could the authors clarify why the branch-and-bound procedure for computing the calibration rate has polynomial time complexity? Isn't MILP solving an NP complete problem?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is overall well-written. It might be helpful to add a small running example of solving a small optimization problem with the proposed techniques.\n\nQuality: the quality could be improved if concrete runtime results could be reported.\n\nOriginality: the contribution is original as far as I can tell.",
            "summary_of_the_review": "I like the direction this paper is taking and the proposed techniques seem promising. However I find the experimental evaluation a little insufficient to evaluate the effectiveness of the approach and I am concerned with the scalability of the approach.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2175/Reviewer_f2t2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2175/Reviewer_f2t2"
        ]
    }
]