[
    {
        "id": "ugMtu04rjMZ",
        "original": null,
        "number": 1,
        "cdate": 1666582538264,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582538264,
        "tmdate": 1666594194200,
        "tddate": null,
        "forum": "pgC3fd-zjw2",
        "replyto": "pgC3fd-zjw2",
        "invitation": "ICLR.cc/2023/Conference/Paper1377/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a novel generative model (CoA-CTRL) that integrates cooperative adversarial\nlearning and closed-loop transcription. Experiments on MNIST, CIFAR-10, STL-10, and Celeb-A demonstrate the robustness and consistency of the proposed model.",
            "strength_and_weaknesses": "Pros\n\n- This paper proposed a novel generative model with cooperative adversarial\nlearning and closed-loop transcription. The idea of integrating cooperative adversarial\nlearning and CTRL to improve the robustness of generative models is interesting. \n- The experiments evaluate the robustness of the model on some datasets.\n\nCons\n\n- Previous works [1,2] have already improved the robustness of the generative models, can you explain the advantage of the CoA-CTRL with these previous works?\n- The datasets used in this paper are somewhat small in comparison. Just wondering about the performance of CoA-CTRL on CIFAR-100 and ImageNet\n\n[1] Regularizing Generative Adversarial Networks under Limited Data \n\n[2] Training generative adversarial networks with limited data",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is well-written\n - Quality: Overall, I think this paper is clear and easy to understand.\n - Novelty: I think the biggest novelty is the use of the integration of cooperative adversarial\nlearning and closed-loop transcription. More experiments regarding this part need to be performed (See weaknesses).\n - Reproducibility: This paper does not release the code in the current version.\n",
            "summary_of_the_review": "This paper has some interesting points, but this version lacks a deeper explanation of the proposed framework. Some theoretical or experimental explanations are encouraged to improve the paper. Also, it lacks comparison to some existing works, this paper needs to compare their performance on a better-implied baseline. Thus I give 5 at the current time. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1377/Reviewer_AoR2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1377/Reviewer_AoR2"
        ]
    },
    {
        "id": "bCo3wN5CfO",
        "original": null,
        "number": 2,
        "cdate": 1666673069338,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673069338,
        "tmdate": 1666673100667,
        "tddate": null,
        "forum": "pgC3fd-zjw2",
        "replyto": "pgC3fd-zjw2",
        "invitation": "ICLR.cc/2023/Conference/Paper1377/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper tries to combine the autoencoding (cooperative learning) and GAN (adversarial learning) in a unified framework such that it proposes to apply the encoder to play two different roles and control different objectives.",
            "strength_and_weaknesses": "This paper has already got published in Entropy 2022 [1]. Why is it submitted here?\n\n[1] https://www.mdpi.com/1099-4300/24/4/456",
            "clarity,_quality,_novelty_and_reproducibility": "see above.",
            "summary_of_the_review": "see above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1377/Reviewer_qmAD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1377/Reviewer_qmAD"
        ]
    },
    {
        "id": "5xnWnCyvfup",
        "original": null,
        "number": 3,
        "cdate": 1666679927514,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679927514,
        "tmdate": 1666679927514,
        "tddate": null,
        "forum": "pgC3fd-zjw2",
        "replyto": "pgC3fd-zjw2",
        "invitation": "ICLR.cc/2023/Conference/Paper1377/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed to extend the Closed-Loop Transcription (CTRL) (Dai et al., 2022). While the original CTRL formulates the task of generative modeling as a minimax game between the encoder and the decoder, the proposed method simply employs an additional cooperate term that minimizes the rate reduction in both of the players. The proposed method is evaluated with several GAN and VAE-GAN baselines including CTRL and demonstrated some improvements on simple datasets like CIFAR10 and MNIST.",
            "strength_and_weaknesses": "Strength:\n- The idea presented in the paper is clear.\n- The paper is easy-to-read in general and reasonably self-contained.\n\n\nWeakness:\n- Based on my understanding, the adversarial and cooperate optimization in Eq.(9) and Eq.(10), respectively, give exactly the same gradient with respect to the encoder parameters but in the opposite direction (i.e., only the sign of the gradient is different) since both optimizations are with respect to the same decoder parameters \\gamma. Considering that the authors proposed to apply different learning rates for the encoder when optimizing Eq.(9) and Eq.(10) (i.e., named CoA-ratio), these two optimizations eventually collapse to the same adversarial optimization in Dai et al., 2022 but applying different learning rates for encoder and decoder. Please clarify it in the rebuttal.\n\n- The experiments are generally not convincing. As discussed in Section 4.2.1, the authors intentionally employed unbalanced encoder and decoder to make training unstable. As a result, all baselines such as GAN and CTRL fail to learn even an extremely simple dataset like CIFAR10. This is an unreasonable result considering the advances in the field and the low complexity of the dataset. The authors instead should present the main comparison results with the balanced setting and use Table 1 as an additional analysis. \n\n- Figure 5 is supposed to be evidence to show that the proposed method learns disentangled representation, yet the results display the opposite. As it shows, changing one factor of variation (each row in Figure 5) leads to changes in multiple attributes, which show evidence of entangled representation.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the novelty of the work is marginal since it is a simple modification to the existing method (Dai et al., 2022), where the correctness of the modification is in doubt. Some figures are also largely from prior works (Dai et al., 2022 and Chan et al., 2022).  ",
            "summary_of_the_review": "I lean toward rejecting this paper due to non-trivial flaws in the method and experiments. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1377/Reviewer_1daZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1377/Reviewer_1daZ"
        ]
    },
    {
        "id": "osY-pn_CS2",
        "original": null,
        "number": 4,
        "cdate": 1666864730844,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666864730844,
        "tmdate": 1666864730844,
        "tddate": null,
        "forum": "pgC3fd-zjw2",
        "replyto": "pgC3fd-zjw2",
        "invitation": "ICLR.cc/2023/Conference/Paper1377/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper designs a generative model for effective cooperative adversarial learning through closed-loop transcription.  In the training, both the encoder and decoder are trained simultaneously for cooperative adversarial learning. The experimental results show effectiveness of the proposed strategy.",
            "strength_and_weaknesses": "Pros:\n+ The idea of cooperative adversarial learning is an important and interesting direction. \n\nCons:\n- The paper does not clearly explain the motivations and the ideas. \n- The details of the proposed framework are not illustrated in a clear way.\n- The experimental results do not fully support the conclusion of the paper.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not clearly written and the novelty is deficient. ",
            "summary_of_the_review": "The techinical novelty is below the bar of ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1377/Reviewer_rrv8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1377/Reviewer_rrv8"
        ]
    },
    {
        "id": "3DLkumIy2Fg",
        "original": null,
        "number": 5,
        "cdate": 1667258925511,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667258925511,
        "tmdate": 1667260753547,
        "tddate": null,
        "forum": "pgC3fd-zjw2",
        "replyto": "pgC3fd-zjw2",
        "invitation": "ICLR.cc/2023/Conference/Paper1377/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an optimizatin technique (CoA-CTRL) to stablize the training of closed-loop transcription framework (CTRL). CTRL doing a minimax game of an encoder model $f(\\theta)$ and a decoder model $g(\\eta)$ over the coding rate reduction in the data feature space $Z$. The encoder in CTRL has two roles, the encoder role in Auto-Encoding and the discriminator role in GAN, while it is only optimized with the adversarial process.  CoA-CTRL thus proposed to optimize the encoder for both roles. This equals to first maximize $\\theta$ over the coding rate reduction and then minimize $\\theta, \\eta$ over the same loss. The experiments suggest that this optmization design gives more stable training than CTRL and achieve better performance.",
            "strength_and_weaknesses": "The good part of this paper lies in that the training process it proposed seems to be more stable than CTRL and achieves better performance.\n\nSeveral concerns and questions I have are:\n1. Limited novelty: Most of the designs (like the minimax game over coding reduction rate, the different roles of encoder, etc.) and equations of this paper are from CTRL, while this paper only slightly change the optimization design, i.e. optimizing both $\\theta$ and $\\phi$ in the minimizing part. So for me, instead of calling it a new generative model, I will only see it as an improvement that stablizes the training of CTRL.\n2. Sensitive to hyperparameter: From Table 2, we can see that the performance of CoA-CTRL is highly sensitive to the CoA-ratio. Thus, although there may exist a better solution of CoA-CTRL than CTRL, carefully parameter tuning may be needed to reach this result, which may limited the usage of this technique.\n3. How can new samples be generated after the model is trained? From figure 2 , it seems that the $z$ and $\\hat{z}$ used in training are all inferred from observations $x$ and $\\hat{x}$ instead of being directly sampled from a simple Gaussain distribution like GAN and no constraint for the prior distribution is applied to $z$ as the VAE. Then how can $z$ or $x$ be sampled after the model is trained? Also the paper only shows the reconstruction results. How are the generated samples look like?\n4. The class-wise accuracy of CoA-CTRL in table 4 seems not be high enough. In fact, besides CTRL and catGAN , CoA-CTRL can not beat any other baselines. Consider the MNIST task is very simple, I would regard the performance differences as obvious and can not agree that CoA-CTRL achieves comparable results as its baseline models. \n5. Besides on the simplest MNIST dataset, the sample reconstruction results shown in figure 4 and 6 are not good enough. Many figures are blurry and details are lost.\n6. This paper proposes to cooperatively learn encoder and decoder, then how is this different/related to [1]. Also, as a model trained under new generative framework, besides GAN and VAE, other generative frameworks (like score-based model, diffusion model, energy-based model, etc.) should also be considered as baselines. \n\n[1] Cooperative learning of descriptor and generator networks. PAMI, 2020",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is in general well written and easy to follow. However, some description in the equation part are a bit confusing. More clarification may be needed for readers who are not familiar with $MCR^2$ amd CTRL.\n1. In equation (3), what is $Z^*$ here? And how is $\\epsilon$ used in the right hand? \n2. In equation (4), should $g(x, \\eta)$ be $g(z, \\eta)$?\n3. In equation (7), should $min_{\\theta}$ $max_{\\eta}$ be $min_{\\eta}$ $max_{\\theta}$ ?",
            "summary_of_the_review": "I think this paper proposes a technique that makes a more stable training under the CTRL framework. But some questions and concerns are there to be solved. Please check the Strength And Weaknesses for more details.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1377/Reviewer_kFbE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1377/Reviewer_kFbE"
        ]
    }
]