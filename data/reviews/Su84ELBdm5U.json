[
    {
        "id": "M4sGRx6pSDc",
        "original": null,
        "number": 1,
        "cdate": 1666587531565,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666587531565,
        "tmdate": 1666587531565,
        "tddate": null,
        "forum": "Su84ELBdm5U",
        "replyto": "Su84ELBdm5U",
        "invitation": "ICLR.cc/2023/Conference/Paper3005/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper develops a simple two-group model to study the effects of overparameterization on groups. It provides a theoretical justification for the empirical results in the existing work and shows that the overparameterization improves or does not harm the minority risk of ERM. The majority group subsampling improves minority group performance in the overparameterization regime. ",
            "strength_and_weaknesses": "It provides a theoretical justification for the effect of overparameterization on minority groups. \nThe paper is easy to follow.\n\nThe application scenario of overparameterization and underparameterization in machine learning is not well explained. \n\nIt mentions that the proposed two-group model has parameters for controlling signal strength, majority group fraction and many terms. I would like to see how the signal strength is used in classification, regression or other machine learning task. \n",
            "clarity,_quality,_novelty_and_reproducibility": "It provides a theoretical justification for the empirical results on the overparameterazation on minority group",
            "summary_of_the_review": "see above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3005/Reviewer_2EGb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3005/Reviewer_2EGb"
        ]
    },
    {
        "id": "1sV3ZbFtbT",
        "original": null,
        "number": 2,
        "cdate": 1666744128907,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666744128907,
        "tmdate": 1666744128907,
        "tddate": null,
        "forum": "Su84ELBdm5U",
        "replyto": "Su84ELBdm5U",
        "invitation": "ICLR.cc/2023/Conference/Paper3005/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Recently, it has been empirically observed that overparameterization helps to improve performance on both the majority and minority subgroups of the data. Few works have also proposed methods to improve the performance on minority subgroups like group distributionally robust optimization and data subsampling. This paper confirms these findings theoretically by studying this using a random features linear regression model which takes into account the difference between the true predictors of the subgroups, the ratio of the data and the signal to noise ratio.",
            "strength_and_weaknesses": "The problem that this work studies of how overparameterization affects subpopulations is important for the fairness and safety of machine learning and has gained widespread interest. \n\nI think that the model that this work studies is different from the practical setting that this work motivates. In particular, this work assumes that the subgroups have same feature distribution and only differ in their true conditional output distribution. I don\u2019t see why that assumption is correct because the different subgroups would have different feature distribution. In fact, they assume that the two groups have different output distribution conditional on the features which is not clear if it is true because we can think of both the populations having the same true predictor. The setting that this work considers there does not exist a single good predictor which is not true in the empirical papers that they cite. This is also what was considered in the original paper [1] which first studied this using a toy model.\n\nThe paper misses important citations and comparisons to previous works which have studied similar problems [2,3,4 ].\n\n[1] An investigation of why overparameterization exacerbates spurious correlations\n[2] Covariate Shift in High-Dimensional Random Feature Regression\n[3] Undersampling is a minimax optimal robustness intervention in nonparametric classification\n[4] Throwing away data improves worst-class error in imbalanced classification",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of this works needs to be improved. Some of the statements are unclear in the paper. \n\nOn page 3, this work says that the minority group error increases with overparameterization in the linear model setting but later in the appendix, it says that even the average error increases in this setting. \n\nOne page 1, this works says that [1] showed that overparameterization hurts minority group accuracy whereas at multiple points, they claim in the paper that confirm [1]\u2019s findings by showing that overparameterization helps minority group accuracy. Can the authors please clarify this?",
            "summary_of_the_review": "The main concern I have with this paper is the concept shift model that this paper assumes for the subpopulation setting and lack of citations and comparison to previous theoretical works. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3005/Reviewer_BTHL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3005/Reviewer_BTHL"
        ]
    },
    {
        "id": "wYSwffom5nT",
        "original": null,
        "number": 3,
        "cdate": 1667139244325,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667139244325,
        "tmdate": 1667139244325,
        "tddate": null,
        "forum": "Su84ELBdm5U",
        "replyto": "Su84ELBdm5U",
        "invitation": "ICLR.cc/2023/Conference/Paper3005/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the effect of overparametrization on minority groups from a theoretical perspective. Building upon existing works, the authors study the asymptotic performance of overparametrized methods in minority groups in a stylized model and show that their results are consistent with empirical observations. ",
            "strength_and_weaknesses": "- **Strength**:\nThe paper is well-written and the simulations are well-designed. The message is clearly conveyed.\n\n- **Major concern**:\nOn page 3, it is remarked that the paper adopts the random feature model instead of the linear model because results in the latter model do not coincide with the empirical findings. I am confused in that (1) what is the model in general used in empirical works (it seems to me should be the latter)? (2) if both models are approximations of the models used in practice, then why are there inconsistencies? Choosing the model that exhibits the desired performance feels a bit like cherry-picking to me---but I could be wrong!\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, with a clear exposition of messages and well-designed experiments. I think the choice of models should be better justified.",
            "summary_of_the_review": "The paper is well-written and the message is clear; more justifications are needed in terms of the choice of model.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3005/Reviewer_y1cp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3005/Reviewer_y1cp"
        ]
    },
    {
        "id": "k5J1NpAtXT",
        "original": null,
        "number": 4,
        "cdate": 1667893627530,
        "mdate": 1667893627530,
        "ddate": null,
        "tcdate": 1667893627530,
        "tmdate": 1667893627530,
        "tddate": null,
        "forum": "Su84ELBdm5U",
        "replyto": "Su84ELBdm5U",
        "invitation": "ICLR.cc/2023/Conference/Paper3005/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the effect of overparameterization under the existence of minority groups. The paper provides theoretical characterizations based on existing work, and ran experiments on the fixed point equations to show that the overparamterization does reflect recent empirical findings in this high dimensional setup.",
            "strength_and_weaknesses": "The paper is easy to follow and well-written. The numerical experiments are abundant.\n\nHowever, I find the paper lacks theoretical novelty as the main results are direct applications of previous work. In the abstract the authors claim that they show overparameterization always improves minority group performance, but I found no such theorem is stated instead of numerical solutions to the fixed-point equations characterizing the asymptotic behavior. I would expect a theorem that says the effective bias and variance on the minority group is monotone if it is claimed that this is shown.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear but lacks theoretical novelty.",
            "summary_of_the_review": "See above.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3005/Reviewer_1vd9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3005/Reviewer_1vd9"
        ]
    },
    {
        "id": "GDFkWcakdS",
        "original": null,
        "number": 5,
        "cdate": 1668840026569,
        "mdate": null,
        "ddate": null,
        "tcdate": 1668840026569,
        "tmdate": 1668840102447,
        "tddate": null,
        "forum": "Su84ELBdm5U",
        "replyto": "Su84ELBdm5U",
        "invitation": "ICLR.cc/2023/Conference/Paper3005/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of how overparametrization affects the ERM's performance on minority groups theoretically. To be more specific, this paper theoretically shows that under the overparametrazation condition and ERM algorithm, the minority groups will tend to have worse performance on the regression tasks, and also shows that subsampling majority groups can improve the performance of the minority groups. The theoretical results are consistent with the empirical studies observed in previous works. ",
            "strength_and_weaknesses": "Strength: This paper is well written, easy to follow, and has solid results.\n\nI think the paper has the following key weakness\nThe results seem to be very straightforward when compared to under-parametrized cases, but this paper is lacking the comparison of over-parametrized cases and over-parametrized cases, including the clarifications/justifications whether and why the conclusions made for under-parametrized cases can or cannot apply the over-parametrized cases. If the results for over-parametrized cases do not differ from the under-parametrized cases, the novelty and necessity of this paper may be problematic. \n\nAlso, while this paper is discussing the over-parametrized ML, its analysis and/or conclusions do not cover how the minority groups' performance change with the number of parameters from under-parametrized cases to over-parametrized cases. \n\nFinally, the two key conclusions drawn from the paper (minority groups perform worse and sub-sampling can help) are pretty intuitive and within the expectation of most people. This paper does not show great novelties of the methods, so the contribution of this paper to the community does not seem high.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and the quality of the paper are good. This paper does not have problem of reproducibility. However, this paper lacks enough novelty to meet the bar of ICLR.",
            "summary_of_the_review": "This paper has three key limitations stated in section \"strength and weakness\" and lacks the novelty and impacts to the community. So I tend to vote a reject to this paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3005/Reviewer_Fo6C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3005/Reviewer_Fo6C"
        ]
    }
]