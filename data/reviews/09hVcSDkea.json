[
    {
        "id": "RU94kcAg0A8",
        "original": null,
        "number": 1,
        "cdate": 1666666058237,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666058237,
        "tmdate": 1666666058237,
        "tddate": null,
        "forum": "09hVcSDkea",
        "replyto": "09hVcSDkea",
        "invitation": "ICLR.cc/2023/Conference/Paper2355/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This manuscript proposes Corrupted Image Modeling, which is a self-supervised learning framework for generic architectures, e.g., CNN and ViT. Specifically, the proposed method reconstructs or predicts the original image from a generated image whose are partially reconstructed from [MASK] tokens. The extensive experiments showed the effectiveness of the proposed method on various vision tasks, such as image classification and semantic segmentation.",
            "strength_and_weaknesses": "Strengths\n- The writing is clear and easy to understand.\n- The proposed method is a generic self-supervised learning scheme in a Masked Image Modeling manner, which can be applied to CNN and ViT.\n- Extensive experimental results demonstrate the effectiveness of the proposed method.\n\nWeaknesses\n- The proposed method has a weakness in that a pre-trained model is necessarily required in any form (e.g., Dall-E models) as a part of the generator.\n- In the case of the ViT model, I am not sure if the proposed method is a better way than the existing BeiT or MAE-style MIM approaches. Also, related explanations and comparisons are lacking. For example, the comparison with MAE (83.1% in Table 1) will be fair when the MAE generator is used instead of external Dall-E models (82.6% in Table 10). In this case, MAE shows better performance than the proposed method.\n- The explanation of the need for a generator is somewhat lacking. Is it not possible to replace this by applying strong augmentations (e.g., cutout, ColorJitter, etc.) only to random patches?\n- I think the main novelty comes from that made MIM pre-training in CNN. Is there any unique advantage of MIM pre-training in CNN?\n- Missing comparison; BeiT and MAE have also known for their superior performances on ADE20K semantic segmentation. However, the authors only report the ADE20K performances for CNNs. Could the proposed method achieve better performances on the ADE20K than BeiT or MAE?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n- The writing is clear. \n\nQuality\n- It requires more explanations and comparisons to emphasize the strengths compared to existing methodologies to improve, as I mentioned above.\n\nNovelty\n- The technical contribution is incremental, but the introduced approach is worth sharing. \n\nReproducibility\n- The code is provided, but some components are missing, e.g., model architecture for the proposed method (\"CNNForMaskedImageModeling\")",
            "summary_of_the_review": "Overall, I think the paper is worth sharing, although I have several concerns, as mentioned above. I hope the authors will address the issues and questions raised.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_wVPU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_wVPU"
        ]
    },
    {
        "id": "3Cfw4xBYcfQ",
        "original": null,
        "number": 2,
        "cdate": 1666725936990,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725936990,
        "tmdate": 1666725936990,
        "tddate": null,
        "forum": "09hVcSDkea",
        "replyto": "09hVcSDkea",
        "invitation": "ICLR.cc/2023/Conference/Paper2355/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a self-supervised visual pretraining approach called corrupted image modeling. The idea is to reconstruct the original image pixels or predict whether each token is corrupted or not. The resulting presentation shows good results on ImageNet classification and ADE20K segmentation using either ViT or CNN.",
            "strength_and_weaknesses": "Strength:\n* The task of unifying self-supervised learning approach for ViT and CNN is important\n* The approach avoids using [MASK] tokens on pretrained models \n* Results on ImageNet classification and semantic segmentation are good using either ViT or CNN\n* Ablations show the efficacy of different design choices. The discussion of tokenizer overhead is informative.\n\nWeakness:\n* At a higher level, it seems to me that CIM is very similar to MIM. Both require the model to learn to reconstruct the masked/corrupted tokens from the surrounding context.\n* The gain over BEiT is small in Table 1 and 3. Since CIM depends on a small BEiT generator, I wonder what the real advantage of the proposed approach over BEiT is, especially when the ViT backbone is used.\n* For ResNet, I wonder if there can still be a (BEiT-like) Masked Image Modeling baseline to help us understand the benefits of CIM for CNN. All entries in Table 2 are contrastive based.\n* The proposed approach requires a generator (small BEiT) to produce the corrupted image, which makes the system more complicated and reliant on another SSL model.\n* The generative and discriminative objective (RESPIX, REVDET) are both similarly effective. Is there any further boost when you combine both? It\u2019d be great to have one best choice at the end.\n* In Table 9, it seems the benefits of scaling are not very significant e.g. 1 point from ViT-Base to ViT-Large. It may be useful to investigate this further whether it\u2019s caused by suboptimal fine-tuning recipe, generator/tokenizer choice, etc.\n* [Option] flalI am curious if CIM-REVDET would do better than CIM-RESPIX and BEiT on ImageNet linear eval or KNN eval since it uses a discriminative objective.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to understand. The proposed approach is somewhat novel and reproducible by the details provided in the paper.",
            "summary_of_the_review": "The paper proposes an interesting self-supervised learning approach to model corrupted images to show competitive performance on classification and segmentation. However, in my opinion, the gains over existing approaches are not significant enough to justify the higher system complexity. In addition, it seems to me the proposed CIM is similar to MIM at a higher level.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_JSbL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_JSbL"
        ]
    },
    {
        "id": "Gq3W_qiH2M",
        "original": null,
        "number": 3,
        "cdate": 1666838857400,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666838857400,
        "tmdate": 1666838857400,
        "tddate": null,
        "forum": "09hVcSDkea",
        "replyto": "09hVcSDkea",
        "invitation": "ICLR.cc/2023/Conference/Paper2355/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper explores the problem of self-supervised visual pre-training, which aims to learn better visual representations with unlabeled data. This paper proposes corrupted image modeling (CIM), a new self-supervised learning framework that takes the corrupted and reconstructed images as input, instead of using images with mask tokens. Given the corrupted images, two types of pretext tasks are proposed, i.e., Pixel Residual learning (ResPix) and Replaced Visual token Detection (RevDet). ResPix regresses the corrupted image patches, while RevDet localizes the corrupted patches via binary classification. Experiments on downstream image classification and semantic segmentation tasks demonstrate the effectiveness of the proposed methods.",
            "strength_and_weaknesses": "## Strengths\n- The motivation of the paper makes a lot of sense. Designing more robust and universal pretraining methods that can benefit various model architectures is an important problem in visual representation learning.\n- The paper shows high quality in writing and presentation, which makes the readers pretty easy to follow and comprehend.\n- The paper explores new generative and discriminative pretext tasks with a single and simple CIM framework. It's very interesting that the discriminative method RevDet, inspired by the ELECTRA in language modeling, shows comparable performance with ResPix. This may provide insights into future research on designing better pretraining objectives.\n- Extensive experiments on downstream tasks and datasets clearly demonstrate the effectiveness of the proposed methods and how each component works.\n\n## Weaknesses\n- The visualization on ResPix is intuitive and shows how the model learns from the corrupted images and infers the raw image signals. But there is no visualization of RevDet. How well does the model solve this pretext task? Some visualization or discussion will improve the quality of the paper.\n- The pertaining objectives of ResPix and RevDet seem independent to me. What if combining them together? Is there any difficulty to do that or do you think this may raise any problems? I'm curious that whether using the generative and discriminative objectives together will lead to better representations.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think it's a high-quality paper with good novelty and originality.",
            "summary_of_the_review": "This paper proposes both new methods and perspectives for the problem of self-supervised visual pre-training. The experiments well support the claims. I think the community will be interested. I would like to give an accept, but the above concerns should be addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_1bnH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_1bnH"
        ]
    },
    {
        "id": "QJMxZClZiO9",
        "original": null,
        "number": 4,
        "cdate": 1666989774394,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666989774394,
        "tmdate": 1669133732138,
        "tddate": null,
        "forum": "09hVcSDkea",
        "replyto": "09hVcSDkea",
        "invitation": "ICLR.cc/2023/Conference/Paper2355/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a self-supervised visual pretraining technique called CIM: Corrupted Image Modeling. The main idea is to randomly select patches and replace them with plausible alternatives, instead of using a MASK token. An enhancer network tries to solve pretext tasks on these augmented images (generating all original pixels or classifying the sample into augmented/non-augmented). Once trained, the enhancer can be used as a visual encoder for downstream tasks. Extensive results and ablation studies validate the proposed technique.\nMoreover, the paper introduces a sliding window normalization procedure (Fig. 4) that improves performance.\n",
            "strength_and_weaknesses": "Strengths:\n- The paper builds on top of BEiT to define pretext tasks with plausible images. Dealing with plausible images instead of masked tokens should intuitively improve the quality of the extracted features.\n- The paper introduces a general non-Siamese framework for visual pre-trained that achieves compelling results.\n- The experiments are extensive with many ablation studies.\n\nWeaknesses & Questions:\n- In Table 1 and Table 2 the reported results are the median of 3 independent runs.  Are the competitors using the same convention? Why the median instead of the mean and std? Do all three independent runs yield comparable performance?  It is unclear if that is the same in all the other tables, or if the other tables report the performance of a single run.\n- Table 6 proves that sliding window normalization is beneficial to the performance of the proposed method. I did not understand if the same normalization procedure is applicable also to the competitors. I think the paper would benefit from either: (1) specifying why the sliding normalization is not applicable to the competitors or (2) a comparison with the best-performing competitor to exclude most of the performance gains are caused by the normalization procedure.\n\nI am willing to raise my score, especially if my doubts regarding the median performance are unfounded.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and well-explained, apart from the concerns expressed in the previous section. The idea builds heavily on top of prior works that are smartly composed together. Overall, the technique can be considered novel.\n\nMany implementation details are provided in the main paper and in the appendix. The code is provided as supplementary material. The libraries are specified in a `requirements.txt` file, with pinned version for critical packages (e.g. torch). There is a README.md with precise commands to pre-train and fine-tune the model. Precise commands/instructions to reproduce all the reported results seems to be missing, although all the code seems to be present. Overall, the paper appears to be reproducible, and the authors encourage reproducibility.\n",
            "summary_of_the_review": "The paper introduces a novel technique for visual pre-training that achieves compelling results. Overall, the experiments are convincing and the ablation extensive. The method appears to be general and useful for many downstream tasks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_hKbV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_hKbV"
        ]
    },
    {
        "id": "7xMLbIqP1Ua",
        "original": null,
        "number": 5,
        "cdate": 1667061391563,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667061391563,
        "tmdate": 1667061391563,
        "tddate": null,
        "forum": "09hVcSDkea",
        "replyto": "09hVcSDkea",
        "invitation": "ICLR.cc/2023/Conference/Paper2355/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a visual pretraining method that replaces vanilla masking in mask image modeling (MIM) with image corruption. A small BEiT is used to generate the corrupted images. An enhancer network is trained to predict the original image and used as the final pretrained network. The paper explores two training objectives: one to reconstruct the original image and the other to predict where the corrupted region is. Experiments are done with ResNet and ViT on ImageNet-1K and ADE20K. Image classification and semantic segmentation are downstream tasks to test the performance of the pretrained networks.   ",
            "strength_and_weaknesses": "**Strength:**\n\n1. The method is reasonably motivated at the high level that more sophisticated operators may produce better results than vanilla masking. \n2. The method successfully leveraged BEiT to corrupt the image, which is interesting. \n3. Experimental results show improvements in downstream tasks for small and base size models. \n\n**Weaknesses:**\n\n1. The enhancer network's reconstruction ability does not seem very good. It is not very clear why making minor changes (mostly smoothing from the figures) can lead to better pretraining. The paper tries to make some arguments, but they are a bit convolved and do not convey a good intuition. \n2. The improvements on small and base size models are incremental, especially compared to BEiT. It does not seem to be statistically significant or rewarding enough to justify the complexity of training the model. \n3. The results on larger CNN and ViT are not impressive. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper needs more rigorous formulations. Zero equations in the main paper, and the formulations provided in the appendix are also sparse. It makes it hard to grip the specific details to reproduce the work.\n\nThe proposed method is novel in its detailed formulation to use BEiT to corrupt image regions.\n\n",
            "summary_of_the_review": "The paper is moderately novel and clear. The results have encouraging aspects but are not strong overall. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_E2vC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_E2vC"
        ]
    },
    {
        "id": "bbL4Yb-gfj",
        "original": null,
        "number": 6,
        "cdate": 1667195243656,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667195243656,
        "tmdate": 1669005670991,
        "tddate": null,
        "forum": "09hVcSDkea",
        "replyto": "09hVcSDkea",
        "invitation": "ICLR.cc/2023/Conference/Paper2355/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the self-supervised learning. Based on masked image modeling, the authors introduce a new algorithm which uses corrupted as training sources instead of masked ones. They utilize a new trainable module including a pretrained transformer decoder to generate such corrupted images. The models are trained to reconstruct the original images or predict whether a patch is corrupted for self-supervision.",
            "strength_and_weaknesses": "Strength:\n1. The analysis about the difference between CIM and MIM and the advantage of CIM is interesting.\n2. The authors provide abundant experiments on several datasets to validation the effectiveness of their method.\n\nWeaknesses:\n1. How are the special mask embeddings designed? Are they trainable?\n2. The authors refer to \u201cgolden token\u201d several times before providing its definition in the REVDET paragraph in Sec.2.1. It would be better if the authors can reorganize for this problem.\n3. It seems that the generated corrupted images from the pretrained dVAE decoder are somehow the smoothed version of the original images. Therefore I wonder if it is possible to directly produce corrupted image by artificial smoothing on some random regions without training any new modules?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and in good quality. The whole pipeline is easy to reproduce based on the code provided by the authors.",
            "summary_of_the_review": "The proposed method is generally a good one, with inspiring information on using non-contrasive self-supervised learning on CNN. However I am concerned with the necessity of using an extra module for the generation of corrupted images. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_UGiU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2355/Reviewer_UGiU"
        ]
    }
]