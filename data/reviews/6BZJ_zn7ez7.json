[
    {
        "id": "aT_tYRszPLF",
        "original": null,
        "number": 1,
        "cdate": 1666153541427,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666153541427,
        "tmdate": 1666153607706,
        "tddate": null,
        "forum": "6BZJ_zn7ez7",
        "replyto": "6BZJ_zn7ez7",
        "invitation": "ICLR.cc/2023/Conference/Paper962/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors propose to tackle temporal modeling in video data. To achieve this goal, they introduce a PatchBlender layer, which uses a learnable matrix to describe the temporal dynamics among frames. They use MOVi-A, Something-Something v2 and Kinetics for model evaluation.",
            "strength_and_weaknesses": "1 Strength\n1) The paper is well written.\n2) The temporal modeling in ViT is an important problem for video representation learning.\n\n2 Weakness\n1) The novelty is relatively limited. Such PatchBlender is quite similar to temporal convolution. Hence, the design is bascially to insert a temporal-conv-like layer into the spatial ViT. Such architectures are not new in the literature such as UniFormer.\n2) The experiment is weak to support the claim.\n2.1) It would be interesting to show the experiment where temporal convloution is inserted into ViT.  This is an opportunity to show what is the difference between two operations, and which one is better.\n2.2) It lacks the state-of-the-art comparison on these video benchmarks.",
            "clarity,_quality,_novelty_and_reproducibility": "1 Clarity: This paper is written well. Most parts are clear.\n\n2 Quality: The experiments are weak.\n\n3 Novelty: The novelty is limited to some degree. Such operatoion is closely relevant to temporal convolution.\n\n4 Reproducibility: It seems to be OK for simple re-implementation.",
            "summary_of_the_review": "The paper considers an important problem in video understanding. However, the proposed operation is similar to temporal convolution, and the experiments ate weak to support the claim.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper962/Reviewer_4JAh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper962/Reviewer_4JAh"
        ]
    },
    {
        "id": "-Jnejzvm4b",
        "original": null,
        "number": 2,
        "cdate": 1666525639790,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666525639790,
        "tmdate": 1666525639790,
        "tddate": null,
        "forum": "6BZJ_zn7ez7",
        "replyto": "6BZJ_zn7ez7",
        "invitation": "ICLR.cc/2023/Conference/Paper962/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "To prompt token interaction over time, this paper proposes PatchBlender, a learnable matrix to mix tokens over time. Via inserting the PatchBlender in the middle layers of ViT, it improves the ability for temporal modeling. Experiments on MOVi-A and Something-Something v2 support the effectiveness of PatchBlender.",
            "strength_and_weaknesses": "Strength:\n1. The design of PatchBlender is simple yet effective for MOVi-A. \n2. The visualizations clearly show the motivation of PatchBlender.\n3. The experiment details are provided and easy to follow.\n\nWeakness\n1. Considering MOVi-A is an easy benchmark without complicated object interaction and camera movement, the real improvement of PatchBlender for daily activity recognition is doubted. As expected, the improvement on Something-Something v2 is marginal. And it even decreases the performance on Kinetics.\n2. The paper only conducts experiments based on ViT, thus the generality is not well-demonstrated. For example, does PatchBlender also help popular video transformer like TimeSformer[1], VideoSwin[2], and UniFormer[3]? In my opinion, those models with temporal operation may not be complementary to PatchBlender.\n3. The PatchBlender is actually a learnable linear layer conducted in the temporal dimension. Considering the optimization problem, it may not work as well as simpler temporal convolution, which has been demonstrated to be effective in previous 3D CNNs.\n\n> [1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In International Conference on Machine Learning, 2021. \n>\n> [2] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. \n>\n> [3] Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatial-temporal representation learning. In International Conference on Learning Representations, 2022. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly organized and the experiments are easy to reproduce. Considering the similarity of PatchBlender and the temporal linear layer, the novelty of the work is limited. Most of the experiments are conducted on simple MOVi-A, and the quality of experiments is doubted.",
            "summary_of_the_review": "The paper proposes a simple PatchBlender, which is plug-and-play and helpful for temporal modeling. However, it seems PatchBlender only works for vanilla ViT and the easy benchmark without complicated object interaction and camera movement. Considering the diversity of human action, the effectiveness of PatchBlender for daily activity is limited, which is also represented in the unsatisfactory results on Something-Something and Kinetics. Besides, it may not work for the popular video transformer due to the simple design of the temporal linear layer. More experiments of different backbones and more comparisons are needed to demonstrate its generality and effectiveness.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper962/Reviewer_4WoX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper962/Reviewer_4WoX"
        ]
    },
    {
        "id": "mnPsc9hpgHj",
        "original": null,
        "number": 3,
        "cdate": 1666976167341,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666976167341,
        "tmdate": 1666976167341,
        "tddate": null,
        "forum": "6BZJ_zn7ez7",
        "replyto": "6BZJ_zn7ez7",
        "invitation": "ICLR.cc/2023/Conference/Paper962/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThe authors aim at learning a generic motion prior, to ease different video-related tasks (motion prediction, video labeling). They argue that learning the linear combination of temporal embeddings of a Transformer from videos enables to encode the dynamic of an observed moving object. The approach is validated on motion prediction from a single input frame (synthetic dataset) and video labelling (on two classic benchmarks). Results show the effect of such a prior. ",
            "strength_and_weaknesses": "\nLearning a motion prior is certainly a long-standing open issue. The approach proposed by the author is extremely simple (learn a weight matrix). Several applications are illustrated and experimental results developed. \n\nThe first set of experiments (Movi-A) seems controversial to me. Given a single frame as input, the prediction of future motion of a given object is not unique (there is no deterministic motion from a single frame, there are potentially an infinity of plausible motions), at least in real videos. Hence, I do believe that the prior learnt in this context is not what is expected in real life and that quantitative results in this set of experiments are irrelevant. The learnt motion prior will however certainly 'remenber' simple dynamics, a similar task that in SSv2. \n\nIn SSv2, the quantitative improvement between ViT and ViT+PatchBlender is not significant (0.27%). It would have been interesting to plug-in the PatchBlender onto several other approaches dedicated to video labeling (most are now using Transformers in a way or in an other) and to report these additional results. \n\nIt seems to me that although the idea of linear combination of temporal embedings via the learnt weights matrix R might have some interest, it might be a very 'rigid' prior, ie limited to learn very generic dynamics (eg, go right, left, etc), but unable to cope with more complex motions of semi-rigid objects (ie human activity) or of multiple objects. It would be interesting to discuss the limitations of the approach. \n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe paper is well presented. The authors performed thorough experiments, attempting to show the pros of the proposed approach. The results are well illustrated and analysed. The approach is simple  and well described, it could be re-implemented by a graduate students. \n",
            "summary_of_the_review": "Learning motion prior is a super interesting subject. The merit of the present work is to try to validate the approach on different tasks, to illustrate the generability of the prior. The proposed weight matrix however does not seem  to do a significantly better job that the transformer itself. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethic concern",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper962/Reviewer_rJNz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper962/Reviewer_rJNz"
        ]
    }
]