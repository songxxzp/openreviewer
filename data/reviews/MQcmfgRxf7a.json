[
    {
        "id": "9YsB7K_ASBP",
        "original": null,
        "number": 2,
        "cdate": 1666632716108,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632716108,
        "tmdate": 1668583872723,
        "tddate": null,
        "forum": "MQcmfgRxf7a",
        "replyto": "MQcmfgRxf7a",
        "invitation": "ICLR.cc/2023/Conference/Paper1973/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new objective for model-based RL that lower bounds the true objective and can be used to optimize the encoder, model, and the policy at once. The proposed model shows higher sample efficiency compared to the baselines.",
            "strength_and_weaknesses": "Strengths\n- Recent works (especially on offline RL) have shown that we can find simpler alternatives to current RL objectives without compromising performance. This paper adds up nicely to those works by showing that we can also simplify model-based RL objectives.\n- The work proposes a principled way to train state encoders for RL.\n- The proposed method has higher sample efficiency compared to the baselines.\n\nWeaknesses\n- As the authors note, it seems the method still requires a few tweaks to work well empirically. For example, we need to omit the log of the true rewards and scale the KL term in the policy objective to 0.1. While the authors provide a brief intuition on why those modifications are needed, I think the authors should provide more concrete analysis (e.g., empirical results) on what problems the original formula have and how the modifications fix them. Also, it would be better if the authors provide ablation results on those modifications. For example, does the performance drop drastically if the scale of the KL term changes (to 0.05, 0.2, 0.5, ...) ?\n- The compute comparison vs. REDQ  on Figure 3 seems to be misleading. First, less runtime does not necessarily mean less computational cost. Second, if the authors had used the official implementation of REDQ [1], it should be emphasized that this implementation is very inefficient in terms of runtime. In detail, the implementation feeds forward to each Q-network one at a time while this feed-forward process is embarrassingly parallelizable. The runtime of REDQ will drop significantly if we parallelize this process.\n- The ablation experiments seem to show that the value term for the encoder is not necessary. It would be better to provide an explanation on this result.\n- Some of the equations seem to have typos. 1) On equation (4), the first product and the second product have exactly the same form. 2) On algorithm 1 Line 8, shouldn't we use s_n instead of s_t?\n\nQuestions\n- I am curious of the asymptotic performance of the proposed method. If possible, can the authors provide average return results with more env steps?\n\n[1] https://github.com/watchernyu/REDQ",
            "clarity,_quality,_novelty_and_reproducibility": "Mentioned above.",
            "summary_of_the_review": "The paper proposes a nice approach to unify (and simplify) model-based RL objective. With some additional few tweaks, the proposed methods shows high empirical performance compared to the baselines. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1973/Reviewer_km6q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1973/Reviewer_km6q"
        ]
    },
    {
        "id": "WQVtP8uW6sC",
        "original": null,
        "number": 3,
        "cdate": 1666723005359,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666723005359,
        "tmdate": 1667502172312,
        "tddate": null,
        "forum": "MQcmfgRxf7a",
        "replyto": "MQcmfgRxf7a",
        "invitation": "ICLR.cc/2023/Conference/Paper1973/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose to train a policy, a state-encoder, and a dynamic model in model-based RL with the novel evidence lower bound on expected returns loss. In this way policy, encoder, and dynamic model have the same objective to maximize which differs this work from the previous ones. \n",
            "strength_and_weaknesses": "In model-based RL algorithms like SAC-SVG (Amos et al., 2020 [2]) one needs to train an actor as well as a critic in a way that actor-model output should be the input to the critic-model to enable backpropagation to the actor through the critic. This makes the boundaries between an actor and a critic vague. For example, the representation model and dynamic model can be parts of the actor and optimized with actor loss, or be parts of the critic and optimized with critic loss. In more complex cases these models can be jointly optimized with critic and actor losses, or even with some auxiliary losses (such as state reconstructions), or with some combinations of the before-mentioned losses.  \n\nOrthogonal to these choices is the decision whether to use evidence lower bound on these objectives or not. As far as I know, this paper is the first work that successfully explores the possibility of training a representation model and a dynamic model only with actor loss and it did so using evidence lower bound.\n\nThe authors discuss that the proposed objective leads to self-consistency for the actor. However, I want to point out that the paper's objective leads to inconsistency for a critic as the authors train the critic and the reward subnetwork on top of representations that are optimized for the actor and not for the critic. Plus, I believe it is an open question whether the actor's consistency or the critic's consistency is more important, or whether one needs to jointly optimize representation with the actor's and the critic's objectives. It would be great if the authors can comment on this. Plus, I recommend adding experiments if it is possible: \n1) try to optimize the representation model and the dynamic model with critic and reward losses but optimize the policy model with objective (8) derived in the paper. \n2) try to optimize the representation model and the dynamic model with the actor (objective (8)), critic, and reward objective jointly.\n3) the same as (1) and (2) but change only the representation model objective as it seems to be easier with the current coding. \n\nIn the paper, the main baseline (SAC-SVG) uses an auxiliary state-reconstruction objective and performs worse than the proposed method. However, in my opinion, the actor objective is much more aligned with the critic objective than with auxiliary objectives such as state-reconstruction objectives or SSL-objective. So I miss a comparison with the strong baseline that uses only actor and/or critic objectives to train model parts (e.g. with [1]). \n\nStrength\n1) novel evidence low bound on RL objective is derived;\n2) novel loss is well-explained, motivated, discussed, and analyzed. (E.g. that the loss encouraged mode-seeking behavior)\n3) The method shows that axillary objectives (such as state reconstruction) may be hurtful to the performance. \n4) The paper is well-written;\n5) The authors show that the proposed algorithm beats similar SAC-like algorithms that use auxiliary objectives as state reconstruction to learn representations. Plus, the proposed method is less computationally demanding than methods that use ensembles (MBPO, REDQ), but archives similar or better performance. \n6) An ablation study is present.\n\nMinor issues and questions\n1) Authors claim that the proposed method is robust to the number of rollouts, but in Fig.5b (bottom) ALM(5) performance is quite low. \n2) Why not use the same parametrization, objective (8) with the classifier, to train the encoder and dynamics model?\n3) Maybe a small table that shows the main differences between the proposed algorithm and the baselines would be helpful;\n4) In the introduction, the authors mention that MuZero uses value prediction to train a representation model and a dynamic model. I would like to point out that MuZero uses not only value prediction loss but also use the loss for the policy model (actor objective) to train these models. So representation model and a dynamic model are jointly optimized with actor and critic losses in MuZero and I argue this makes representation consistent with both actor and critic objectives.\n\n[1] Hubert et al, Learning and Planning in Complex Action Spaces, 2021.\n\n[2] Amos et al, On the model-based stochastic value gradient for continuous reinforcement learning, 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "To the best of my knowledge, the proposed objective is novel. The paper and the code are well-written and organized. I believe the results are reproducible thanks to the paper and the code. ",
            "summary_of_the_review": "Overall the paper is well-written and motivated. The experimental results are very interesting. However, in my opinion, some key experiments and baselines (e.g. [1] Learning and Planning in Complex Action Spaces) are missing.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1973/Reviewer_PJmH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1973/Reviewer_PJmH"
        ]
    },
    {
        "id": "kf3UjjdDsc",
        "original": null,
        "number": 4,
        "cdate": 1666799717242,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799717242,
        "tmdate": 1667794532090,
        "tddate": null,
        "forum": "MQcmfgRxf7a",
        "replyto": "MQcmfgRxf7a",
        "invitation": "ICLR.cc/2023/Conference/Paper1973/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "A theoretical sound model based rl algorithm derivation that can process high dimensional inputs (i.e. images) that derive one objetive function to learn the latent space of the observations, the model and the policy, which usually is done using separate objectives. This work also provides a practical algorithm, that was shown empirically to have. greater sample efficiency that SOTA. The work also provide a lower bound on the RL loss.",
            "strength_and_weaknesses": "Very strong derivation on the objective and losses. End-to-end learning also is another strength of the paper. High sample efficacy for high dimensional states also is a strength. Outperforming model free RL on high dimensional states also seems to be novel, without using pre-training or other self-supervised technique,\n\nWeakness: Classifier training time seems not to be taken in account, also if there are situation where learning the classifier is difficult and how this could impact the overall algorithm. Automatic K selection would be a good addition into the algorithm.\n\nIt would interesting to see the performance of algorithm with different domains that capture different levels of aleatoric uncertainty.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Its a very clear and well derive paper.\n\nThe quality of the paper also its on the top on the domain with clear mathematical derivations and explanations.\n\nThe code is reproducible because there is a public website already with the code available. <<REMOVED BY PCs>> (which actually bring the problem of blind review)\n\nThe losses are novel, the idea of one objective is novel.\n\n\n\n",
            "summary_of_the_review": "This work derive a lower bound on the RL loss with a strong theoretical , well supported, derivation, constructing a practical model based RL algorithm, that have higher sample efficiency than SOTA, its end-to-end learning solution, that can handle high dimensional state spaces learning the a latent representation for this states,  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1973/Reviewer_11kg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1973/Reviewer_11kg"
        ]
    },
    {
        "id": "ojZDTcrdh0",
        "original": null,
        "number": 5,
        "cdate": 1666808851727,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666808851727,
        "tmdate": 1666808851727,
        "tddate": null,
        "forum": "MQcmfgRxf7a",
        "replyto": "MQcmfgRxf7a",
        "invitation": "ICLR.cc/2023/Conference/Paper1973/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper builds on the latent variable view of reinforcement learning (RL) to provide a model-based RL method that jointly learns representations of observations, their transition dynamics (the representations), and the optimal policy.\n\nIt introduces the Aligned Latent Model objective, which is derived from the typical latent-variable view of RL, and a particularly defined variational distribution. This is then used in a DDPG style RL algorithm.\n\nExperiments compare against several baselines, including SAC-SVG on several complex tasks. Key questions regarding the usefulness of the contribution is analysed using appropriate experiments, including ablations.",
            "strength_and_weaknesses": "### Strengths\n- The theory is built nicely, by introducing the notation first, followed by a high level outline before diving into the maths.\n- The technique appears to have some good benefits.\n- Discussions are nice, with good explanations of points made. \n- Good literature review.\n- It was good that network hyperparameters were chosen to be consistent with baselines.\n\n### Weaknesses\n- See questions below.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nThis paper is written in clear language. Some points:\n- In the last paragraph of Page 2, second sentence,  could you write how a sentence about how the methods mentioned (decision-aware loss functions, etc) tackle the objective mismatch problem? This is so that it is clear how they don't help with latent-space models. \n- The paper introduces the basics of RL in Sectino 3.1. However, in Section 3.2 and 3.3, it mentions Q-functions and Q-values without mentioning / defining it before hand. \n- Similarly, the notation Q in equatino 5 is introduced without any definition.\n- I think details about the classifier used in Section 4 should be moved to the main text. \n- I am confused about the  the second line of Equation 4. How is this infinite product carried out in practice? From Algorithm 1, it seems only the first K terms are used. If so, why is this part in Equation 5?\n- In MBRL, I thought that the dynamics model is used to sample data that can be used for training the policy. Does this method do this? I might be missing something, but from Algorithm 1, I can only see that samples are taken from the environment? Is this right?\n\n### Quality\n\nThis paper is generally of good quality. Some questions:\n- Does the Soft-actor critic nature of the main baseline affect any of the conclusions? Since Section 5.1 mainly compares with SAC-SVG, are there any other differences between the methods that could lead to the differences in performance?\n- For Section 5.1, could it be possible to compare against itself, where the model-based components are trained separately?\n\n### Novelty\nTo the best of my knowledge, this is a novel paper. However, I haven't carried out an in-dept literature review recently.\n\n### Reproducibility\nCode is provided as an open source implementation. I haven't tried this code out, but being available helps with reproducibility.",
            "summary_of_the_review": "In general, the paper is written well, and introduces its concepts cleanly. \n\nSome details are used without defining terms appropriately. \n\nIn order for me to improve my rating, I would like the questions / concerns I had raised addressed. In particular, I am worried that some conclusions are drawn without appropriate control over the experiments. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1973/Reviewer_JaMR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1973/Reviewer_JaMR"
        ]
    },
    {
        "id": "5c3lKMHuyY",
        "original": null,
        "number": 6,
        "cdate": 1667346104258,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667346104258,
        "tmdate": 1668813519797,
        "tddate": null,
        "forum": "MQcmfgRxf7a",
        "replyto": "MQcmfgRxf7a",
        "invitation": "ICLR.cc/2023/Conference/Paper1973/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a proxy model-based RL objective which is proved to be the lower bound of the usual RL objective. The proxy objective can be optimized with respect to the policy, the latent-space model, and the representation. Experiments on a range of continuous control tasks show that a practical algorithm loosely based on the proxy objective achieves better sample complexity than several competitive existing model-based and model-free algorithms.",
            "strength_and_weaknesses": "The strength of the paper is that the proposed practical algorithm achieves superior performance on some continuous control tasks without using the common ensemble technique for MBRL. In addition, an abundance of ablation experiments is performed to validate parameter K's sensitivity, the components' effectiveness, and the learned values and representations.\n\nHowever, there is a major weakness in this paper. The algorithm is inconsistent with the theory. Notably, the log of rewards and Q function in Eq (5) is omitted in the actual implementation. In addition, an extra hyperparameter is added to tune the effect of the KL term. More importantly, there is no ablation study on these two important design changes.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and of good quality in terms of empirical evaluation. Following previous work (Eysenbach et al., 2021) that proposes a single objective for the policy and the state-space model, it derives a different objective for the policy and the latent-space model. The novelty of this paper has two aspects: (1) Different from the previous objective that uses a state-space model, it uses a latent-space model. (2) The objective derived in this paper covers the scenario with k-step rollouts and is a generalization of the previous objective. However, it should be noted that while the theory is nice and has some novelty, the practical algorithm deviates from it.\n\nOn reproducibility, the source code is included in the supplementary material.",
            "summary_of_the_review": "Overall, this is a nice and well-written paper. It develops an extension of an existing objective (Eysenbach et al., 2021). It has strong empirical results for the practical algorithm it proposes. However, in my opinion, there is a major weakness - the practical algorithm deviates from the theoretical analysis without providing enough empirical/theoretical analysis.\n\nSome questions to be addressed that impact the score:\n1. What empirical evidence motivates using a latent-space model instead of a state-space model? How does it compare to MnM (Eysenbach et al., 2021)?\n2. What empirical/theoretical evidence motivates omitting the log of rewards and Q function? Could you provide further justification for it?\n3. How does the coefficient of the KL term affect performance?\n\nSome suggestions for improvement that do not impact the score:\n1. The definition of the Q function is in the appendix. It should be introduced in the main text before using it in Eq (5).\n2. In Eq (7), I believe the KL term is reversed. It should be written as $KL(m_\\phi(\\cdot | z_i, a_i) || e_{\\phi_\\text{targ}}(\\cdot | s_{i+1}))$. Otherwise, it is inconsistent with Eq (6) and the discussion in Sec 3.4.\n3. Inaccurate/misleading statement after Eq (8): \u201cwe reduce the variance of this objective by computing this objective for multiple horizons and then taking an average.\u201d However, in the experiment section, only fixed horizons are used.\n4. Typo in the paragraph before the conclusion section: \u201cwe that\u201d.\n5. Typo in Lemma A.2: $Q(s_t, a_t)$ should be $Q(s_K, a_K)$.\n6. In the proof of Theorem 3.1, I suggest changing ${d\\tau dH}$ to ${dH d\\tau}$ in equation g for better clarity.\n7. In the proof of Theorem 3.1, it should be $H-1$ instead of $\\max(0, H-1)$ above the summation. The latter will incorrectly calculate $\\log e_\\phi(z_1 | s_1) - \\log m_\\phi(z_1 | z_0, a_0)$ twice.\n8. Appendix B, the comparison to prior work on lower bounds for RL (MNM), should be moved to the main text since it is the most relevant related work.\n9. It would be nice if there were an analysis of the gap of the lower bound with respect to the model error.\n\nBenjamin Eysenbach, Alexander Khazatsky, Sergey Levine, and Ruslan Salakhutdinov. Mismatched no more: Joint model-policy optimization for model-based rl, 2021. URL https://arxiv.org/abs/2110.02758.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1973/Reviewer_RXsd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1973/Reviewer_RXsd"
        ]
    }
]