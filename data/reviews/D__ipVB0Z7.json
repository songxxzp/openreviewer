[
    {
        "id": "z5wyi5nGOl",
        "original": null,
        "number": 1,
        "cdate": 1666252626447,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666252626447,
        "tmdate": 1666252626447,
        "tddate": null,
        "forum": "D__ipVB0Z7",
        "replyto": "D__ipVB0Z7",
        "invitation": "ICLR.cc/2023/Conference/Paper5652/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents disentangled conditional VAE for anomaly detection. The model and lower bound are based on a combination of total correlation and CVAE in the $\\beta$-VAE framework. The paper then conducts experiments on several grey-scale image datasets. Specifically, the paper uses reconstruction error to detect outliers, similar to some prior work. Results show a higher AUC compared to those baselines. ",
            "strength_and_weaknesses": "The paper has a good overview on related work, including derivations of those baseline methods and building blocks of the proposed method. However, the paper has the following major weakness. First, in the proposed method, every term is in fact the same as CorEx except that they are conditioned on $c$. I do not see any novel ideas or insights from the proposed method. Second, the experiments are only ran on simple datasets based on MNIST and its variants. The paper does not conduct experiments on more complicated datasets, especially those reported in corresponding papers of the baseline methods. Furthermore, there is only one metric (reconstruction error) that has been evaluated. However, simple metrics sometimes fail to detect anomaly (Choi et al 2021). ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe first half of the paper is clear. The overview of related work is very detailed. The derivation of the proposed method and its lower bound is also clear. \n\nThe experiments are less clear, however. It seems EMNIST is considered as the anomaly and the other three are the training sets. However, Table 1 reports results for EMNIST too, which is confusing. Another point I find confusing is that the paper does not explain the use of conditioner $c$ in experiments. Are they labels? In that case, what is the $c$ for the test set?\n\n**Quality**\n\nThere are some minor technical issues. For example, typo in eq (5). Also, there is no definition for the notation $r$ or $r_{\\alpha}$. \n\n**Novelty**\n\nThis is a major weakness of the paper. I do not see much novelty in methodology except that everything is now conditioned. The experiments also lack novelty: the method is only tested on several simple datasets with slightly increased AUC. \n\n**Reproducibility**\n\nThe supplementary file includes the full code to reproduce the experiments. I went over the code but did not run the experiments. ",
            "summary_of_the_review": "Overall I think the paper does not have much contribution to methodology or experiments. Therefore, I believe it needs further improvement.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5652/Reviewer_Pbh4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5652/Reviewer_Pbh4"
        ]
    },
    {
        "id": "kgNLqq3qj7T",
        "original": null,
        "number": 2,
        "cdate": 1666714291751,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666714291751,
        "tmdate": 1666714291751,
        "tddate": null,
        "forum": "D__ipVB0Z7",
        "replyto": "D__ipVB0Z7",
        "invitation": "ICLR.cc/2023/Conference/Paper5652/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a conditional VAE that uses total correlation to form their loss function. It builds off the CorEx work. They show many experimental results, some of which are promising. ",
            "strength_and_weaknesses": "\nStrengths: \n- The overall idea seems interesting\n- The results of Figure 3 and Table 1 look good\n\nMajor Weaknesses:\n- It seems like the primary contribution of this paper is to condition previous work with a class variable c. Everything from equation 12 onwards follows the same exact things presented in Section 2, except with a conditioning on c. Furthermore, nothing is described on why this particular architecture is relevant or useful for anomaly detection. \n\n- Why is CorEx not compared to in the experiments? Particularly because this work heavily depends on and builds off the model from CorEx. \n\nOther Weaknesses:\n\n- The notation is either inconsistent or extremely hard to follow. Making it difficult to review. Up until section 2.4 it seems that x is d dimensional and z is m dimensional and factorizable when conditioned on x. Although for some reason the index switches from i to j in the final sentence before equation 8. Do i and j represent different things? In section 2.4, j is used as the index and n is used to denote that there are N samples. Additionally k is used to denote the K factors v of z_j, which implies that each z_j has K factors v_k? Also what is total number of j (the sum and products are missing top values). In section 2.5, the j index is used, but now x is p dimensional and z is d dimensional? This confusion in notation makes it impossible to use the definition of TC in equation 7 with the decompositions in equation 13 to get equation 14. It also seems equation 8 is more useful in understanding the relationship between the TC for and the mutual information form of the loss function. \n\n- Equation 16 is derived using 13 and 15, not 14 and 15? \n\n- Below equation 7 you have I(x : z) instead of I(x ; z), Equation 13 and 14 are also using : instead of ;\n\n- The objective function in equation 18 has no regularization / penalty term? What does it have to do with the various \\beta or Factor models? \n\n- The equations take up a lot of space and could be organized better so that more detail about the equations is described above / below it. Some equations definitely do not need to take multiples lines and do. \n\n- There is a random l in \\hat{x} above equation 19. Also what are x and \\hat{x} paired or is the anomaly score over all x in the training set? This is not clear. \n\n- Despite the text, in Figures 1 and 2, all methods have higher A for the top row (anomalous) versus the bottom row, which according to bullet a) of better disentanglement, is desired. Also all the \\Epsilon values seems relatively close for all methods.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper have very unclear notation that seems to be changing. The language / grammar in the paper is fine though. ",
            "summary_of_the_review": "Between the issues with clarity and the lack of comparison with the CorEx model in the experimental results, it is completely unclear whether there is significant improvement of the CorEx model. In general, this paper seems to rely heavily on previous work, with questionable amounts of novelty on its own. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5652/Reviewer_8jzN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5652/Reviewer_8jzN"
        ]
    },
    {
        "id": "Mqagc5YPcNE",
        "original": null,
        "number": 3,
        "cdate": 1667023330172,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667023330172,
        "tmdate": 1667023330172,
        "tddate": null,
        "forum": "D__ipVB0Z7",
        "replyto": "D__ipVB0Z7",
        "invitation": "ICLR.cc/2023/Conference/Paper5652/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on addressing unsupervised anomaly detection via disentangled conditional VAE.  The new architecture combines three core components: beta-VAE, CVAE and the principle of TC.  The authors claim that the new method improves the disentanglement of latent features, and the ability to detect anomalies.  Multiple experiments are used to demonstrate the performance and improvement, specifically from the perspective of capturing disentangled features. ",
            "strength_and_weaknesses": "- Strength\n\n1. Detail introduction about the related works, specifically the core techniques, i.e., beta-VAE, CVAE, and TC \n2. The idea is simple to follow and the paper is well-written. \n\n- Weaknesses \n\n1. The novelty is weak. \n2. Lack of ablation study on different components \n3. The methodology section is not well explained with enough details and theoretical proof\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: it is good to follow. The idea is straightforward.\n- Quality: related works are high but the methodology section is not solid \n- Novelty: it is weak and lack of major contribution ",
            "summary_of_the_review": "The paper seems to be close to the original CVAE method, which is not very new.  Combining three core methods is fine but we did not understand why the new ensemble works better than before, and why one is more important.  The authors should provide an ablation study to investigate the sensitivity of each component. \n\nFor experiments, the major datasets are MNIST-based data, which is a naive and easy case. It would be more strong if the authors can apply the method for large datasets typically used in AD/OOD detection tasks, e.g., CIFAR-10/100 or ImageNet. \n\nIn addition, the authors may put more effort into improving the new methodology section with more details and explanations. The related works are good enough but should be compressed if the space is not enough. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns ",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5652/Reviewer_HT5F"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5652/Reviewer_HT5F"
        ]
    },
    {
        "id": "XCUJEaV_hxQ",
        "original": null,
        "number": 4,
        "cdate": 1667099025492,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667099025492,
        "tmdate": 1667099025492,
        "tddate": null,
        "forum": "D__ipVB0Z7",
        "replyto": "D__ipVB0Z7",
        "invitation": "ICLR.cc/2023/Conference/Paper5652/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed a framework for unsupervised anomaly detection which builds on top of existing prior art on disentangled variational autoencoders (VAE's). Authors do a deep dive on existing prior art techniques such as $\\beta$ - VAE, Factor VAE and others by introducing a conditional variable to avoid loss of information and produce more accurate reconstructions. Authors present empirical results on several image datasets by demonstrating reconstruction score, accuracy and other important metrics for anomaly detection. ",
            "strength_and_weaknesses": "**Strengths**\n\n- Paper is straightforward and relatively easy to follow.\n\n\n**Weakness**\n\n- The paper has this major flaw in its structure\na) Excessive emphasis on related work and prior art sections which is not necessary\n\n- For reasons not known, the authors have conveniently ignored most prominent unsupervised deep anomaly baselines such as \n  ### Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. 2018. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International Conference on Learning Representations.\n\n ### Unsupervised Anomaly Detection with Adversarial Mirrored AutoEncoders. Somepelli et al. UAI 2021\n\n\n\n- I also believe that anomaly detection frameworks need to benchmarked with results on metrics such as AUPR and not just indicate AUC improvements.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is highly incremental in nature and the empirical analysis is insufficient in its current form.",
            "summary_of_the_review": "Unsupervised anomaly detection is an important problem, but new papers in this space need to be more novel as there is exists a significant body of work in this space (within both VAE's and GAN's). In addition, empirical evaluation should make sure the coverage is exhaustive to conclude if the results are significant. The current paper lacks most of these insights.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5652/Reviewer_DSrs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5652/Reviewer_DSrs"
        ]
    }
]