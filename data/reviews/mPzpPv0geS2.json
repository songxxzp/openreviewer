[
    {
        "id": "rYhWA_jU7F",
        "original": null,
        "number": 1,
        "cdate": 1666582857944,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582857944,
        "tmdate": 1666582857944,
        "tddate": null,
        "forum": "mPzpPv0geS2",
        "replyto": "mPzpPv0geS2",
        "invitation": "ICLR.cc/2023/Conference/Paper1065/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes another variant of Adam by combining the idea of Nesterov momentum with the adaptive algorithms. The authors prove that in the nonconvex optimization setting, the proposed algorithm converges faster than the other adaptive algorithms. Experimental results show that the proposed algorithm performs better than the other algorithms in multiple tasks and different settings.",
            "strength_and_weaknesses": "Strength:\n  1. The paper is well-written and easy-to-follow\n  2. This is by far the first paper that have shown theoretical benefits over existing algorithms, as far as I am aware of, which is highly appreciated.\n  3. The experimental results are on both small-scale and large-scale datasets, which are quite strong and convincing.\n\nWeaknesses:\n  1. The idea of combining Nesterov momentum with Adam is, as far as I am concerned, a very straightforward idea. I am actually very surprised that this hasn't been put into practice in common deep learning tools such as PyTorch or Tensorflow. Therefore, I find the idea of the paper not novel enough.\n  2. The theoretical benefit looks marginal to me. Isn't the difference between all the optimizers only a multiplicative constant-level difference? When is the convergence of the algorithm better than the others (i.e., under what assumptions of $d$, $c_2$, $c_\\infty$? Can the authors elaborate more? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, with some novelty and good reproducibility given the code references.",
            "summary_of_the_review": "Overall, I find the idea of introducing Nesterov momentum to adaptive optimization algorithm not novel enough, but I am impressed by the theoretical results and the experiments the authors have provided. The performance of the proposed algorithm does look significantly better than the existing ones. Therefore, I vote for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1065/Reviewer_SpFC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1065/Reviewer_SpFC"
        ]
    },
    {
        "id": "uDPW5MIwmBL",
        "original": null,
        "number": 2,
        "cdate": 1666692034986,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692034986,
        "tmdate": 1666692328037,
        "tddate": null,
        "forum": "mPzpPv0geS2",
        "replyto": "mPzpPv0geS2",
        "invitation": "ICLR.cc/2023/Conference/Paper1065/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper first reformulate the update expression of the Nesterov acceleration technique and then modify Adam accordingly to develop a new optimizer named Adan, which is different from NAdam. The main difference between Adan and Adam is that the gradient difference g_k-g_{k-1} is incorporated in computation of both the 1st and 2nd momentum. Both convergence analysis and experimental evaluation are conducted for Adan. ",
            "strength_and_weaknesses": "Strengths: \n1. The new method Adan is developed by incorporating the gradient difference g_k-g_{k-1} in Adam based on a reformulation of the Nesterov acceleration technique.\n2.  Both convergence analysis and experimental evaluation are conducted for Adan, showing its advantage over other optimizers. \n\nWeaknesses:\n1.  The authors claim that  Adan has lower complexity than a number of existing optimizers including AdaBelief and LAMB without an explanation. Is it because of the separation of the l2 regularizer with the objective, or because of the introduction of g_k- g_{k-1} in Adan, or because of improved mathematical deviation. This is very important. It helps the readers to understand what is the reason of the the low complexity of the new method without reading all the proofs. If it is because of the separation of the l2 regularizer with the objective or  improved mathematical deviation, it suggests that the convergence of other optimizers can also be improved.  \n2. It seems that the authors are aware of AdaBelief and Adam+ but didn't evaluate them in the experiments. AdaBelief is empirically found to have better performance than quite a number of opimizers including AdamW and Adam for training different types of DNN models including Transformer. Recently, a new method named Aida in the paper \"On exploiting layerwise gradient statistics for effective training of deep neural networks\" is found to perform better than AdaBelief.  I highly suggest the authors to also evaluate AdamBelief, Adam+ and Aida in their experiments. Furthermore, the parameter epsilon needs to be searched for each method to achieve best performance. It seems from page 7 that the authors only tune the hyper-parameters of Adan. \n3. Compared to Adam, AdamW, AdaBelief, Adan needs to tune an additional parameter beta_2.",
            "clarity,_quality,_novelty_and_reproducibility": "(1) One page 4, the parameter epsilon appears inside the sqrt operation in both RMSProp and Adam. While they are implemented differently on Pytorch platform where epsilon is outside of the sqrt operation. My experience is that the placement of epsilon makes a difference in the validation performance for training DNN models.  Do you use default implementation of RMSProp and Adam on Pytorch in your experiment or you use the update expressions in your paper? The presentation for the algorithms need to be consistent in the whole paper. \n(2) Source code is not available for reproducibility. \n(3) There is no de-bias term in their new method Adan. Is it ignored for presentation convenience like RMSProp and Adam, or the de-bias term is removed in their implementation for better performance?  \n(4) I think the overhead of Nesterov acceleration technique is negligible compared to NME. There is no need for Nesterov acceleration technique to maintain both theta_k and theta_k'. NME is just a reformulation of Nesterov acceleration technique. I suggest the authors rewrite the contribution part on page 2. If the authors have different opinions, please verify their argument via experiments by implementing the Nesterov acceleration technique properly. \n(5) The authors made quite a few assumptions to be able to analyze the convergence of Adan. I wonder if the assumptions are the same for other optimizers. If they are not the same, then Table-1 is an unfair comparison.  For example, why 3 is introduced in ||g_k||_{infty}\\leq c_{infty}/3? Do other optimizers use the same assumption? \n(6) The statement \"update theta_1 by Line 7 in Algorithm 1\" is not correct.  \n",
            "summary_of_the_review": "The authors develop a new method Adan by incorporating the gradient difference g_k-g_{k-1} in Adam based on a reformulation of the Nesterov acceleration technique. (1) Explanation of why Adan has lower theoretical complexity is needed for the readers to better understand the method. (2) Paper presentation needs further improvement (see my comments above). (3) I don't find anywhere that the hyper-parameters of the reference methods in the experiments are manually tuned while the hyper-parameters of Adan are tuned. Also it is suggested to evaluate AdaBelied, Adam+, Aida in the experiments. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1065/Reviewer_5SGt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1065/Reviewer_5SGt"
        ]
    },
    {
        "id": "eHheXwe1cJ",
        "original": null,
        "number": 3,
        "cdate": 1666806340139,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666806340139,
        "tmdate": 1670449110254,
        "tddate": null,
        "forum": "mPzpPv0geS2",
        "replyto": "mPzpPv0geS2",
        "invitation": "ICLR.cc/2023/Conference/Paper1065/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduce a new adaptive method that built upon ADAM and Nesterov momentum. First, it reformulate Nesterov momentum update and combine it with ADAM; Second, it provides $O(\\epsilon^{-4})$ complexity for Lipschitz-smooth case; Third, it provides $O(\\epsilon^{-3.5})$ complexity for Hessian Lipschitz case. ",
            "strength_and_weaknesses": "### Strength\n1. The idea is clear and straightforward\n2. The experiments are conducted for multiple tasks\n\n### Weaknesses\n1. The claim that $O(\\epsilon^{-3.5})$ is the lower bound is not correct. In Assumption 1, it assumes the function is smooth with respect to each value of $\\zeta$, which will already lead to a lower bound of $O(\\epsilon^{-3})$ even without Lipschitz Hessian. I also found that Assumption 2 is stronger than usual, as it assume $\\mathbb{E}||\\xi|| \\leq \\sigma$ rather than $\\mathbb{E}||\\xi||^2 \\leq \\sigma^2$. So I think lower bound could be much better than $O(\\epsilon^{-4})$ and $O(\\epsilon^{-3.5})$ for each case. \n2. From the change in equation (4), the algorithm introduce a new hyperparameter $\\beta_2$. In the new algorithm, there are three momentum hyperparameters to pick: $\\beta_1, \\beta_2$ and $\\beta_3$. When $\\beta_2 = 0$, the algorithm basically reduces to the vanilla Adam. In the experiment, it performs much better than Adam, so I am not sure whether it is simply a result of tuning $\\beta_2$. However, the ablation study in B.5 indicate that Adan is not sensitive to the value of $\\beta_2$, which contradicts that it is much better than Adam. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: I think the paper is easy to read. I also suggest to add reference for Table 1, Nesterov acceleration in the last paragraph of page 2 and lower bound $\\epsilon^{-4}$ in the first paragraph of page 3. \n\nNovelty: The algorithm seems new to me and the construction is interesting. ",
            "summary_of_the_review": "Overall, the algorithm looks promising, but does not match the lower bounds in theory and I am not fully convinced by the experiments. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1065/Reviewer_wTd4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1065/Reviewer_wTd4"
        ]
    }
]