[
    {
        "id": "KZICagDwll",
        "original": null,
        "number": 1,
        "cdate": 1666557828452,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666557828452,
        "tmdate": 1668753502469,
        "tddate": null,
        "forum": "8JCg5xJCTPR",
        "replyto": "8JCg5xJCTPR",
        "invitation": "ICLR.cc/2023/Conference/Paper2027/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper gives an upper bound on the number of parameters required to remember $N$ length-$n$ sequences, where each token is of dimension $d$.\nThe parameter upper bound is $\\tilde{O}(d + n + \\sqrt{nN})$. Together with bit complexity $\\tilde{O}(\\sqrt{nN})$, the upper bound is optimal up to log factors in terms of bit counts.\n\nThe proof technique is closely related to that of Vardi et al. 2022, with novelty lying in the use of self-attention, which improves parameter efficiency. As a comparison, directly applying the result in Vardi et al. (which is for fully-connected ReLU nets) will give a parameter count of $\\tilde{O}(dn + \\sqrt{nN})$, i.e. there is a factor of $dn$ as opposed to $d+n$ as in this paper.\n\nThe paper also provides a similar upper bound for sequence classification setup (i.e. the label is a scalar, rather than a sequence as in the main result), as well as empirical evidence that the parameter counts roughly follow the trend predicted by theory.",
            "strength_and_weaknesses": "In addition to the upper bound in main result, the paper provides remarks on:\n- extending the results to real-valued / vector-valued output;\n- allowing wider width (the main result has width=16) or bounded bit complexity (the main result has bit complexity growing roughly as $O(\\sqrt{nN})$);\n- connection to fully-connected ReLU net, i.e. the bound if we apply Vardi et al. 2022 directly.\n\nI have no major complaint about the technical part of the paper. The main novelty over Vardi et al. 2022 comes from the use of self-attention blocks, which allows parameter sharing across tokens in a sequence, saving a factor linear in the sequence length.\nI should note though that even though I read Vardi et al., I'm not well versed with the literature so may miss some subtleties.\n\nSome minor comments about the experiments:\n- Fig 1: how many runs are performed for each cell of the heatmap? Could you increase the granularity of Fig 1(b) to show a clearer trend?\n- Fig 2: do the points correspond to models that have reached 0 (or near 0) training error? How many runs are there for each point? If there's only 1 run, then the trends may not be trustworthy. Please consider using multiple runs and show the median and standard error.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is cleanly written and pleasant to read.\nProofs and experiment details are provided in the appendix.\n\nA minor clarification question: $L$ is defined das the depth of the Transformer, but the depth is also $\\tilde{O}(n+L)$ in Remark 3.3. Does this mean that given a depth-$L$ Transformer, we can modify it into another wider Transformer with depth $\\tilde{O}(n+L)$?",
            "summary_of_the_review": "To my knowledge, this paper is the first to provide memorization capacity of Transformers with finite precision. The paper is well written and the results are of interests to the community, so I recommend an accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2027/Reviewer_YzgV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2027/Reviewer_YzgV"
        ]
    },
    {
        "id": "Gv-gLDvMQl",
        "original": null,
        "number": 2,
        "cdate": 1666661550898,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661550898,
        "tmdate": 1666662174265,
        "tddate": null,
        "forum": "8JCg5xJCTPR",
        "replyto": "8JCg5xJCTPR",
        "invitation": "ICLR.cc/2023/Conference/Paper2027/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors construct a transformer that can memorize $N$ sequences of length $n$ and dimension $d$ that have permutation equivariance, with a transformer that has width 16 and depth $\\tilde{\\mathcal{O}}(n + \\sqrt{N})$, with bit complexity $\\tilde{\\mathcal{O}}(\\sqrt{N})$. Furthermore, their theory can show memorization for sequences without permutation equivariance using positional embeddings.",
            "strength_and_weaknesses": "The major strength of the paper is the parameter efficient transformer (which is nearly optimal in the number of bit counts) constructed for memorizing $N$ sequences. Furthermore, the authors show experiments verifying some of their claims.\n\nHowever, I feel that the use of the self-attention module in the construction hasn't been highlighted in the paper and is difficult to read from the proof in the appendix (please see more questions below). One of the major questions that I have is whether we can use the result of [1] in two steps: first, we project each token in the $n$ length sequence into a single bit, and then further project the $n$ dimensional representation into a single bit.  \n\nA section similar to 3.1 in [1] would have helped a lot.\n\nMoreover, I have the following questions:\n\n(a) Can the authors explain the role of the self-attention module inside the construction? Can one simply use a feed-forward network in place of the self-attention module? My reasoning is that after stage 1, you only need to store a single bit for each token in the sequence. Hence, is it possible to use a feedforward layer of dimension $n$ that \"approximates maximum token id among the\nfirst coordinate values and outputs in the second coordinate\"? \n\nIf there is a difference, this can be used to compare attention models with MLP-only models introduced in [2, 3].\n\n(b) Can the authors provide a discussion on how the contextual mapping used in Stage 2 improves over the selective shifting-based contextual mapping from Yun et al. (2020a)? Does the pre-processing in Stage 1 help reduce the overhead in Stage 2 for contextual mapping? Also, can the authors provide some discussion on how their contextual mapping is capable of \"incorporating sparse self-attention settings with minimal parameter overhead\"?\n\n(c) In figure 2, how do the authors observe a square root relation between training data size and model size? The plots seem very linear. In plot 2(b), if we remove the point (30000, 2.5e7) which could have been a noisy point, the plot will become more linear.\n\n(d) The experiments have been conducted with a fixed hyperparameter set (batch size, learning rate, etc.). That implies the plots in 2(a) and (b) could be highly dependent on the hyperparameters used. How do the plots look with a different set of hyperparameters?\n\n(e) Is there a relation between the entropy observed in the experiments in Figure 3 and the number of sequences $N$? (By theorem 3.1, there should be a square root dependence between bit complexity and $N$). \n\n(f) How do the plots in Figures 2 and 3 change with changes in the length of the sequences? \n\n(g) The construction doesn't require the number of attention heads to change with a change in the number of sequences. Do the authors observe the same in their experiments, i.e. the behavior of the training loss doesn't depend on the number of attention heads used?\n\n\n1: On the Optimal Memorization Power of ReLU Neural Networks. Gal Vardi, Gilad Yehudai, and Ohad Shamir\n\n2: MLP-Mixer: An all-MLP Architecture for Vision. Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy\n\n3: Pay Attention to MLPs. Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le\n \n",
            "clarity,_quality,_novelty_and_reproducibility": "The theoretical statements have been clearly presented. However, as suggested above, the novelty of the work is difficult to check because of a lack of a proper proof sketch in the main paper or the appendix.",
            "summary_of_the_review": "Overall, my scores are slightly on the negative side, mainly because (a) I would like to see how the attention module has been used in the proof sketch, and (b) I would like to see how the authors managed to improve upon Yun et al (2020a)'s construction. I would be very happy to discuss with the authors in the rebuttal period on the above questions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2027/Reviewer_Nzx7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2027/Reviewer_Nzx7"
        ]
    },
    {
        "id": "BEGImD4xyW",
        "original": null,
        "number": 3,
        "cdate": 1666732581490,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666732581490,
        "tmdate": 1666732581490,
        "tddate": null,
        "forum": "8JCg5xJCTPR",
        "replyto": "8JCg5xJCTPR",
        "invitation": "ICLR.cc/2023/Conference/Paper2027/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper conducts the theoretical analysis of the memorization capacity of the Transformer model with finite machine precision and shows $\\tilde{O}(d +n \\sqrt{nN})$ model parameters are enough to memorize $N$ seq2seq samples with/without permutation equivariance assumption, where $d$ is the embedding dimension/model size and $n$ is the sequence length, which improves the results in previous literature (e.g., Yun et al., (2020ab) and Kratsios et al., (2022)). From the technical perspective, the authors consider a novel contextual mapping, which facilitates the analysis extending to the sparse attention settings. Moreover, the authors also use numerical experiments to verify the theoretical findings.",
            "strength_and_weaknesses": "Strength:\nThis paper contains the technical in-depth results and sheds the light on explaining the efficiency of the transformer model.\n\nWeakness:\n1. The theoretical analysis is mainly an extension based on Vardi et al. (2022).\n2. the current analysis requires $\\mathcal{O}(n)$ transformer layers, which is a little bit too large for the common practice.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is well-written and I enjoy reading it.\n\nQuality:\nThe paper contains promising theoretical findings on Transformer models. I checked the detailed proof in the appendix twice and no major flaw is found. \n\nNovelty:\nAlthough the analysis is mainly based on the literature, the theoretical findings are new and interesting.\n\nReproducibility:\nThis is a theoretical paper and the proof contains enough details to help the readers to follow. \n",
            "summary_of_the_review": "This paper proves that Transformers are capable of memorizing $N$ seq2seq samples with length up to $n$ with $\\tilde{\\mathcal{O}}(d + n + \\sqrt{nN})$ parameters. The authors also use experiments to verify the theoretical findings.\n\n\nMinor Issue:\n\nFigure 2: From my point of view, figures 2(a)/(b) show the linear dependence even if in the beginning. The claim *..the square root relation between the training data size and the model size in the beginning...* is not well supported. I suggest the authors using separate figures to explicitly show square root dependence for the beginning stage.\n\nFigure 3: The description of the x-axis in Figure 3 is missing and I suggest the authors adding some detailed explanation of this experiment, as based on the current presentation, it is confusing to me how the entropy of parameters is computed.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2027/Reviewer_XAtG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2027/Reviewer_XAtG"
        ]
    },
    {
        "id": "xc6oe_TnwY",
        "original": null,
        "number": 4,
        "cdate": 1666946645961,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666946645961,
        "tmdate": 1666946645961,
        "tddate": null,
        "forum": "8JCg5xJCTPR",
        "replyto": "8JCg5xJCTPR",
        "invitation": "ICLR.cc/2023/Conference/Paper2027/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proves the memorization capacity of Transformer. The main technical theory is: given $N$ input-output pairs of sequence, this paper constructs a transformer that memorizes them with permutation equivariance, under slightly stronger assumptions. The mathematical tools used in this paper adopts the approach from Vardi et al., but with its own technical novelties. Finally, experiments with BERT validates the proposed theory. ",
            "strength_and_weaknesses": "Strength: the memorization capacity of transformers is an interesting and important, though not surprising, topic to study. This paper is the first to prove the memorization capacity of transformers. The paper is well-written overall, with minor typos. \n\nWeakness:  Two minor weakness are as follows. The technical novelty seems to be limited, as it mainly follows from Vardi et al. Also, there are some typos in the paper, e.g., in section 3.1 should $Y[:, k]$ and $Y[1, k]$ be using the same notation?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear. This paper is novel in the sense that it is the first to prove a memorization capacity for transformers, though the technical novelty may be limited. I find no errors in the proof so far, but I did not check every steps of the proof.",
            "summary_of_the_review": "The paper makes a solid contribution to first provide a memorization capacity result for transformers. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2027/Reviewer_SL1m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2027/Reviewer_SL1m"
        ]
    }
]