[
    {
        "id": "jtfedj9j21D",
        "original": null,
        "number": 1,
        "cdate": 1666421248330,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666421248330,
        "tmdate": 1666421248330,
        "tddate": null,
        "forum": "lRdhvzMpVYV",
        "replyto": "lRdhvzMpVYV",
        "invitation": "ICLR.cc/2023/Conference/Paper2436/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies the explainability on evolving graphs through the lens of \u201cdifferential geometry\u201d. While prior literature primarily focuses on static graphs, there are some works in the thread of evolving graphs against which the authors highlight certain differences in the introduction and related work. More concretely, the contributions of the work include - (i) embedding a manifold in an extrinsic euclidean space, (ii) designing extrinsic coordinates based on the contribution of paths to the predicted distributions on the computation graph of the GNN, (iii) defining a metric on the manifold and (iv) lastly, learning the curve that connects the two distributions and provide the desired explainability. The design of the coordinates is primarily based on prior work defining the set of $m$ length paths rooted at the node at hand $J$ with at least one altered edge from the initial graph to the final graph. The rationale is that these altered paths primarily lead to the altered predictions in the graph at the desired snapshot. The manifold is defined using the set $Pr(Y|G)$. The intrinsic dimension of the manifold is given by the sufficient statistics of $Pr(Y|G)$, which is parametrized by the contribution matrix (as described in eq 6, 7 and 8). Furthermore, they emphasize the idea that the evolution of $Pr(Y|G)$ is nonlinear on the manifold, which is based on the approximation of KL divergence provided in eq 9. This nonlinearity is attributed to the definitions - eq 6, 7 and 8. Lastly, they describe the curves using the specific types of parameterizations against the argument $\\Delta{C}_{J}(G_0, G_1)$ in eq 10. The final objective described in eq 12 involves the constrained optimization of KL divergence term based on the curve formulation. Experiments have been performed on multiple benchmark datasets along with comparisons against the baseline explainability techniques. \n",
            "strength_and_weaknesses": "1. The proposed method is intuitively clear and well supported by treferenes to the literature.\n2. The results provided in the figures - 2, 3, 6, 7 and 8 clearly demonstrate that the proposed method outperforms the baselines (almost always).\n3. Comparing the runtime against other baseline method/s (potentially the runner-up) will be helpful in identifying the runtime performance tradeoff.\n4. As the work considers evolving graphs, discussing more than 2 snapshots in the evolution will be highly interesting. Have the authors performed some analyses on that? It will be interesting to study and helpful in demonstrating the effectiveness of the work.\n5. Some analyses on the threshold described in section A.4.2 in order to select the target node/edge/graph will be useful. Studying how the method performs on the spectrum of varying $KL(P_J(G_1)||P_J(G_0))$ for the node/edge/graph $J$ can help explore the limit of the work. \n6. Providing more details regarding the GNN hyperparameters and the training procedure can help assess the reproducibility of the work.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writeup of the paper is very clear and the paper is easy to follow. Providing the complete hyperparameter detail will help assess the reproducibility. The empirical evidence clearly demonstrates that the approach is effective. Lastly, studying the problem through the lens of differential geometry is a new perspective.  ",
            "summary_of_the_review": "Based on the questions and comments raised in the aforementioned sections, I lean towards acceptance of the work. I am willing to reconsider my score if the authors can try to address some of the questions.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2436/Reviewer_uccj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2436/Reviewer_uccj"
        ]
    },
    {
        "id": "JQDKl6Xavq",
        "original": null,
        "number": 2,
        "cdate": 1666452052378,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666452052378,
        "tmdate": 1666452052378,
        "tddate": null,
        "forum": "lRdhvzMpVYV",
        "replyto": "lRdhvzMpVYV",
        "invitation": "ICLR.cc/2023/Conference/Paper2436/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a probabilistic metric for explaining the change of GNN predictions in an evolving graph from the differential geometric perspective. The author first reformulates the distribution of GNN predictions in the context of path contributions. Then, the author establishes the distance metric based on the approximated KL divergence between the distributions of each class for evolving graphs. And therefore, it constructs the Reiman Manifold for measuring distribution shifts on predictions regarding the change in graphs. Last, the authors formulate the problem of explaining GNN predictions into the problem of minimizing the KL divergence between distribution shifts. In experiments, a subset of paths, which contribute the most to the evolving process, is disabled, and the KL divergence is calculated and appears to be small, which validates the explanation quality of the proposed methods.",
            "strength_and_weaknesses": "Strength:\n1. The main contribution is marginal: the proposed manifold-based distance metric for measuring the smoothing evolutions of a graph. \n2. The experimental parts demonstrate that, for node classification tasks, the proposed method can find the most significant paths that explain the predictions of evolving graphs, thus, produce a good explanation. \n\nWeakness:\n1. Section 3.2 and 3.3 is kind of confusing. Need explanations about how Eq (10) and (11) are derived and calculated. How is the KL divergence second-ordered approximated by the Fisher information matrix? \n2. The proposed methods select a subset of paths that contributes most to the expansibility, yet the number of selected paths is vague. Please explain the criteria for selecting the paths from Table 3. \n3. A minor issue is the evolution of the graph is only constrained to adding or removing a subset of edges while the node features remain unchanged.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly organized with interesting new ideas. A few places need more explanations.\n",
            "summary_of_the_review": "The paper is in general well written with coherence logic. The contributions are incremental. The experimental results appear to be good and support the conclusions. More explanations are needed to clarify a few ideas.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2436/Reviewer_gpiW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2436/Reviewer_gpiW"
        ]
    },
    {
        "id": "Mi1GHI26scw",
        "original": null,
        "number": 3,
        "cdate": 1666587423298,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666587423298,
        "tmdate": 1669383889992,
        "tddate": null,
        "forum": "lRdhvzMpVYV",
        "replyto": "lRdhvzMpVYV",
        "invitation": "ICLR.cc/2023/Conference/Paper2436/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method that gives a geometric viewpoint on the evolution of graphs and enhances their explanatory and interpretability. To capture changes in the graph of a discrete, we view the graph of each state as a subgraph of a larger graph, consider the space of probability distributions with respect to the links of that larger graph, and view them as curves on that space. The authors give specific procedures for interpreting the above perspectives based on differential geometry formulations. Experiments have also been conducted to verify the effectiveness of the method for the three tasks in the graph.",
            "strength_and_weaknesses": "Strength\n\n- Interpretation of the evolution of graphs is more difficult than, for example, changes in images, so it is worthwhile to realize them.\n- It is organized according to a clearly systematized differential geometry scheme. Interpretations based on differential geometry are also used in the generative model, a method that is not new but is acceptable to those in this domain.\n\nWeakness\n\n- One concern is whether the graph can be handled continuously on the GNN manifold, since it will be discrete. That is, when perturbed on a GNN manifold, is there a graph corresponding to that point? Basically, all graphs may be considered as weighted graphs and considered continuous, which may be fine on a general level. However, I think the reference to this point is weak. The text may be structured in a misleading manner.\n- This method constructs a large graph containing all data nodes. However, it is limited to the case where data including all nodes are present at training time, and it is likely that it will not work well when nodes that are not present at training time are input. This is considered a more cautionary situation than the usual Out of Distribution. It is not a big problem for cases where all the nodes are known or for the purpose of analyzing a model that has already been trained, so please clarify in what situations it can be used appropriately.\n- I am not sure if what is shown in the experiment demonstrates the claim. I believe the authors' claim is that the proposed method makes graph evolution easier to understand. For example, it is expected to track the length of the corresponding paths and the evolution of the graph in multiple evolutions between the same starting and ending data, etc. Please clarify why the experiment demonstrates the claim of this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "It is clearly well written with respect to the theory and algorithms behind the proposed method. The goals we want to achieve are common in recent research, but systematizing them in terms of graph evolution is new.\n",
            "summary_of_the_review": "The evolution of a graph is difficult to understand as a graph as it is, and being able to understand it as a curve on a manifold is of great value. This method is clearly described in the framework of differential and information geometry. On the other hand, there are some unclear areas, such as necessary conditions that are implicit. Clarifying these would make for a better paper.\n\n**Conclusion following discussion with the authors**\n\n It is now clear that the authors' claimed problem set is not affected by the concerns I had. On the other hand, I have the impression that the results obtained are more rudimentary than those discussed in similar directions, for example, in image representation learning. I think this is a good result for the future, so I have a positive impression, but I won't change the score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2436/Reviewer_JnV5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2436/Reviewer_JnV5"
        ]
    },
    {
        "id": "uOjdfsP-yZM",
        "original": null,
        "number": 4,
        "cdate": 1666668340557,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668340557,
        "tmdate": 1670347918868,
        "tddate": null,
        "forum": "lRdhvzMpVYV",
        "replyto": "lRdhvzMpVYV",
        "invitation": "ICLR.cc/2023/Conference/Paper2436/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a new method of explaining how GNNs respond to evolutions of graph structure through adding or removing edges. It models the outputs of the GNN as a probability distribution depending on the input graph which varies over time. In contrast to previous work, time is taken as a continuous parameter. The most relevant edge additions and deletions are calculated and used in a convex optimization problem to determine a time-dependent smooth parameterization of the GNN.",
            "strength_and_weaknesses": "Strengths:\n-\n\n- The evolution of paths is formulated nicely as a convex optimization problem with a theoretical motivation by KL divergence minimization.\n\nWeaknesses:\n-\n\n- The focus is on interpolating between two graphs G0 and G1, but it's not clear why we care about intermediate stages when the data itself comes in discrete intervals, and the modifications to the graph are themselves discrete. The experiments do not seem to shed further light on this.\n\n- The interpolation doesn't seem to use global time information, only the probability distributions P(Y|G0) and P(Y|G1) at consecutive time intervals.\n\n- The convex optimization selects over a small number of paths in the computation graph which contribute the most to the change in the output vector. Determining the contribution of each path seems to be expensive to determine, especially for a GNN with many layers or a relatively dense edge structure. If this is not the case, it would be good to have some mention of the computational complexity of determining the highest-contributing paths.",
            "clarity,_quality,_novelty_and_reproducibility": "The work appears to be original in its method of constructing a smooth parameterization between probability distributions for GNNs. Several parts of the paper can be difficult to follow. For instance, the Riemannian metric on M is repeatedly emphasized, but the paper is somewhat vague about how this metric is invoked. On page 6, it is mentioned that \"the curves should move according to the geometry of the manifold M(G, J)\", but it's not made precise what this means. This is only elaborated on later in the section through the explanation \"the above optimization does not change the Riemannian metric I(vec(CJ(G1))) at Pr(Y|G1) since Pr(Y|G1) is what we aim to approach on the manifold\", but this is a trivial statement.\n\nThe experiments section of the paper could benefit from more detail on the setup. In particular, what GNN architectures were used, and what were your choices of hyperparameters?",
            "summary_of_the_review": "The paper provides an interesting geometric perspective on time-dependent graph changes. But some of the choices of methods used seem arbitrary and could use more justification, specifically to how this particular choice of curve contributes to explainability or performance of the GNN.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2436/Reviewer_yCBP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2436/Reviewer_yCBP"
        ]
    }
]