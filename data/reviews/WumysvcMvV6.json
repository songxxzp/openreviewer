[
    {
        "id": "Iysb4wXTQcJ",
        "original": null,
        "number": 1,
        "cdate": 1666548053354,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666548053354,
        "tmdate": 1666621259955,
        "tddate": null,
        "forum": "WumysvcMvV6",
        "replyto": "WumysvcMvV6",
        "invitation": "ICLR.cc/2023/Conference/Paper448/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study the offline RL setting with imperfect rewards. That is, a known portion of the trajectories contain possibly corrupted rewards. The approach proposed to tackle this problem is Reward Gap Minimization (RGM), which finds the reward function (correction) which, when used to solve for the optimal policy, agrees with the expert trajectory distribution. This requires a dual-optimization problem to be solved and a number of reformulations are provided to get a practical form. The approach is empirically tested, with some analysis and ablations, in three continuous control problems (hopper/half-cheetah/hopper).",
            "strength_and_weaknesses": "The right context and background is provided to describe the approach. The proposed RGM formulation is intriguing and seems natural mathematically, even if intuitively it seems like a challenging optimization objective. \n\nAs far as I could tell, all derivations are correct and they are explained clearly. The experimental section is generally well designed, with some good illustrative examples in Fig 3 and a number of useful experiments to establish the validity of the method.\n\nSome questions and comments:\n\n- The classification problem for h seems hard in general. As if I understand correctly, it amounts to detecting whether an action is optimal or not. Couldn\u2019t this by itself, if learned perfectly, be used to generate the optimal/expert policy.\n- It would be useful to have some notion of the computational cost of the method (e.g. as a function of the dataset size) and how that compares to the perfect reward case.\n- It would be nice to visualize how well the reward is recovered in the non-tabular scenarios.\n- The fact that RGM does better than other offline RL methods even with perfect rewards seems a bit surprising (sec 5.1). Why should this be expected?\n- It seems a bit unfair to evaluate offline IL approaches that expect expert data on these mixed datasets. Are there other methods that can deal naturally with the B (imperfect reward) condition?   \n- The RGM formulation makes no assumption on the way the rewards might be corrupted (versus for example some noise process etc.) It would be good to comment on the pros/cons of that. \n- Could you clarify how the approach relates to \"Generative Adversarial Imitation Learning\"?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The description of the problem, of the approach, and of the experiments is generally really clear. \n\nThings to improve: \n- the last paragraph about ablations + Fig 5 could be made a bit clearer, I found it hard to parse.\n- The r/m/m-r/m-e versions of the datasets in Table 1 should be explained in the text (maybe it\u2019s there but couldn\u2019t find it, had to look in the D4RL paper)\n",
            "summary_of_the_review": "Good and clear paper about a principled and practical way to deal with imperfect rewards in offline RL. I think the paper has enough empirical evidence as it is, but I do wonder whether the approach can be scaled to more complex settings (larger datasets, more difficult/higher-dim task).\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper448/Reviewer_gH6T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper448/Reviewer_gH6T"
        ]
    },
    {
        "id": "FCwoGQ-RPlR",
        "original": null,
        "number": 2,
        "cdate": 1666867033387,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666867033387,
        "tmdate": 1666867033387,
        "tddate": null,
        "forum": "WumysvcMvV6",
        "replyto": "WumysvcMvV6",
        "invitation": "ICLR.cc/2023/Conference/Paper448/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new unified offline policy optimization approach, named RGM (Reward gap minimization), that can handle imperfect rewards. RGM has two layers: (1) upper layer learns reward correction term by minimizing the reward gap towards expert behaviors, (2) lower layer solves a pessimistic RL problem under the corrected rewards. More precisely, the authors use f-divergence matching to learn the correction term, and OptiDICE to solve the pessimistic RL problem. Then, policy extraction is used to extract optimal policy from the learned stationary distribution. Finally, this paper provides an empirical study on D4RL datasets.",
            "strength_and_weaknesses": "## **Strength**\n- RGM uses bi-level optimization and this approach is very interesting.\n## **Weakness**\nI have following questions and concerns:\n### **Theory**\n- RGM does not consider the degree of imperfection of rewards. Theoretically, it seems that the degree has no effect on the performance of RGM - it is weird to me. Intuitively, if rewards are perfect, RGM should perform better than offline IL algorithms and comparable to offline RL algorithms, and if rewards are too imperfect, RGM should perform poorly. Consequently, it is questionable whether the imperfection of rewards affects RGM performance.\n- (Related to the previous question) Suppose that we can obtain optimal values for each optimization problem. In this ideal case, RGM obtains the optimal reward correction term, optimal stationary distribution, and optimal policy. Then, what is the main difference between RGM and existing offline IL algorithms in theory? I think RGM is equivalent to DemoDICE (if f-divergence is KL divergence) and SMODICE if the max / min can be computed exactly.\n- In footnote 4, page 8, the authors claim that \u201cSMODICE and DemoDICE treat $\\log\\frac{d^E}{d^D}$ as rewards, which can only provide correct signals when $d^E>0$.\u201c. However, in thes upper level problem (learning reward correction term), RGM also uses $w:=\\frac{d^E}{d^D}$, especially in Eq. (17) \u2013 therefore, in my opinion, learnable reward correction term also can provide correct signals when $d^E>0$.\n### **Experiments**\n- In the beginning of the paper, the authors claim that the proposed methods can handle all three types of rewards (perfect, partially correct, incorrect). However, in the experiments consisting of D4RL dataset, there is only one imperfect reward setting (50% of rewards are sign-flipped). Thus, I think there should be additional empirical studies on (1) perfect and incorrect rewards (2) other types of imperfect rewards (e.g. adding i.i.d. gaussian noise for each reward).\n- There should be RGM (T) in Table 1.\n- What is your intuition about imperfect rewards used in the experiments in this paper? I do not think flipping the signs is a natural setting.\n- In addition, I am curious about the ablation study on performance of RGM, offline RL algorithms, and offline IL algorithms according to (1) the degree of imperfection of rewards (including perfect rewards and completely incorrect reward) (2) the number of expert trajectories in $D^E$.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is easy to follow. In addition, combining OptiDICE with reward correction term learning is interesting.",
            "summary_of_the_review": "I think this paper proposes a novel algorithm to handle imperfect rewards. However, I have some questions and I believe they are important in evaluating this paper. Thus, I vote to weakly reject until these questions are resolved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper448/Reviewer_KWzN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper448/Reviewer_KWzN"
        ]
    },
    {
        "id": "8EhRUR4rWp",
        "original": null,
        "number": 3,
        "cdate": 1666911091597,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666911091597,
        "tmdate": 1669102802886,
        "tddate": null,
        "forum": "WumysvcMvV6",
        "replyto": "WumysvcMvV6",
        "invitation": "ICLR.cc/2023/Conference/Paper448/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper propose an approach to address the problem of imperfect reward under offline settings. \n\nThe main idea is to formulate the problem as a bi-level optimization problem, where the upper problem is to minimize the gap between the stationary distribution of the optimal policy defined by the imperfect reward, and the distribution of the expert data. The lower optimization problem is to use the optimized reward function to obtain a new policy under the offline RL settings. To track the bi-level optimization problem, the authors leverage the fenchel duality to obtain a tractable optimization solution. \n\nEmpirically the authors demonstrate that their approach can obtain better performance when the reward function is imperfect. ",
            "strength_and_weaknesses": "**Strength**\n- The authors conducted detailed empirical experiments to demonstrate the advantage of their approach under the offline settings. \n- The derivations in the paper are quite comprehensive, which are good for authors to follow in detail, and check out how the results are derived.  \n\n\n**Weakness**\n1. I believe there are some fundamental technical faults in the basic formulation of the paper, regardless of the superior empirical performance.  Here are some technical flaws I am concerned about: \n      - In Equation (5) & (6), you define the gap between the current reward function and the optimal reward function as $$D_{f}( d^{\\pi^{*}}_{\\hat{r}} | d^{E}) $$\n\n     According to your definition, $ d^{\\pi^{*}}_{\\hat{r}}$ is the  state-action visitation distribution defined in Equation (4). In Equation (4), I believe the discounted visitation distribution is defined together with $\\gamma$ and initial distribution. As I can infer from your following derivation, the stationary distribution of the expert data $d^{E}$ you used is actually the sampling distribution, which is not related to the discounted factor $\\gamma$ and initial distribution. If so, how could you actually minimize the gap? They are two different kinds of stationary distribution. Let me illustrate a more intuitive case, suppose $\\gamma$ is very small, close to zero (e.g., $\\gamma=0.01$), your expert distribution is define on the whole trajectory (unrelated to $\\gamma$), while the stationary distribution of the policy obtained by the reward function $\\hat{r}$ is actually close to the initial state distribution and the policy distribution, then your reward gap definition is totally problematic. \n\n  Similar thing happens when you try to estimate Eq (16). The density ratio you estimated is the undiscounted ($\\gamma=1$ or sampling) distribution, while in your whole derivations $d$ is the discounted distribution. I feel it is not correct to match the discounted stationary distribution to the undiscounted stationary (sampling) distribution, unless you proof that it is okay to do that, and you can convince me the previous example I illustrated is wrong. \n\n3. For estimating Eq(13) or Eq(43), I am wondering how would actually deal with double sampling? your choice of conjugate function does now allow you to obtain unbiased estimation since there is an expectation in $TV^{*}(s)$? Do you assume the transitions and policy is deterministic? If so, I do not see any related assumptions in your paper. \n\n4. The whole algorithm is too complex, I feel it may require lots of efforts and hyperparameter tunning to make it work. \n\n5. Why you want to conduct the exepriments in the offline setting? I believe the imperfect problem issue exists in all settings of RL. \n\n6. And also, how do you define **imperfect** and **perfect** reward in the mujoco environments? the mujoco reward functions you used are human designed, why would you think they are perfect. One case I can think about is that in the sparse reward settings (or multi-goal RL), the perfect reward is that you would achieve higher success rate when testing. So I don't think mujoco is the perfect environments to demonstrate your claim, since there is no definition of perfect reward here (correct me if I am wrong, this is my personal thoughts).\n  ",
            "clarity,_quality,_novelty_and_reproducibility": "**Quality**\n\nI believe there are some technical flaws in the paper, which require the authors to clarify. \n\n\n**Clarity**\n\nThe paper is well written and is easy to follow the derivation. \n\n**Originality**\n\nAs far as I know, there are no previous work utilizes density ratio estimation for imperfect reward learning and policy improvement. ",
            "summary_of_the_review": "overall I think the paper is not ready in terms of the following two main reasons:\n\n- There are fundamental issues of their reward gap and followed up derivations. \n- The experiments they conducted I think is not the right settings to demonstrate the advantage of their approach. In most of the scenarios that perfect reward functions are hard to obtain, such as sparse reward settings, they did nothing on that. \n\nAs a result, I recommend a major revision of the paper.  \n\nFor authors' rebuttal message: I know I am quite strict about your definition and derivation details, but please correct me if there is anything wrong of my points. I am very happy to modify scores if I miss anything important. \n\n----\n\nTypos:\n- Line under Eq(11), it should be $TV(s)$, not $TV(s,a)$",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper448/Reviewer_1KgB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper448/Reviewer_1KgB"
        ]
    }
]