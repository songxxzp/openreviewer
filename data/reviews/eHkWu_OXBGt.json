[
    {
        "id": "RRVnsI_moD2",
        "original": null,
        "number": 1,
        "cdate": 1666667392212,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667392212,
        "tmdate": 1666667392212,
        "tddate": null,
        "forum": "eHkWu_OXBGt",
        "replyto": "eHkWu_OXBGt",
        "invitation": "ICLR.cc/2023/Conference/Paper4730/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work aims to train Gausssian-Bermoulli restricted Boltzmann machines (GRBMs) for generative modeling. For inference, it proposes to use a hybrid sampling that combines Gibbs sampling steps with Langevin samplers for GRBMs instead of the vanilla Gibbs sampling such that the gradient information of the log density can be leveraged. For learning, a modified contrastive divergence is proposed that encourages the model to generate samples from noises. The proposed training framework is evaluated on both synthetic and real-world datasets: Gaussian Mixtures, MNIST, FashionMNIST, and CelebA.\n",
            "strength_and_weaknesses": "The main contribution of this work is the proposed Gibbs-Langevin sampling for GRBMs and the modified contrastive divergence that allows the GRBMs to generate images from noise and to be compared with other existing deep generative models. It is interesting to see GRBMS, models with relatively simple architectures, are capable of generating reasonable images. All the proposed methods are nicely explained and seem technically sound. Still, here are some concerns/confusions:\n- I find the motivation on why using GRBMs for generative modeling and when they are preferable to other models is somewhat lacking. Even though the paper claims that an important motivation is that a GRBM can convert real-valued data to stochastic binary data, it is not something unique to GRBMs since a VAE with discrete latent space can do the same thing. In terms of empirical evaluations, the FID scores of GRBMs are still a lot worse than the existing generative models.\n- In related work section, it says that the proposed learning algorithm allows to learn a much lower variances compared with other methods. I wonder why smaller learned variances are a good thing. Would they lead to a less diverse model distribution?\n- In Section 3.1, paragraph Gibbs-Langevin sampling, the authors claim that it performs better than generic Gibbs due to the leverage of gradient information of log density. Similar comparison between Gibbs-Langevin and Langevin is missing. I wonder why in Table one it is much better than Langevin. Any intuition why.\n- In experimental section, it is vague what does adjustment refer to. Do you mean gradient clipping, modified CD or something else? This greatly harms the readability of empirical results.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is overall well-written and nicely polished. It makes a reasonable attempt to provide sufficient background for the readers to understand the proposed methods. One minor issue is that the font size in fig 1 is too tiny to read.",
            "summary_of_the_review": "Overall, nice improvements on challenging GRBMs.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4730/Reviewer_sW5s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4730/Reviewer_sW5s"
        ]
    },
    {
        "id": "cTMfSB7Wae8",
        "original": null,
        "number": 2,
        "cdate": 1666680295048,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680295048,
        "tmdate": 1670900221399,
        "tddate": null,
        "forum": "eHkWu_OXBGt",
        "replyto": "eHkWu_OXBGt",
        "invitation": "ICLR.cc/2023/Conference/Paper4730/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces many new insights that ease the training of and push the capabilities of Gaussian-Bernoulli Restricted Boltzmann Machines (GRBMs). In particular, they propose a hybrid Gibbs-Langevin sampling algorithm for inference, as well as a modified CD algorithm using two Markov chains (paired with gradient clipping) that allows for training GRBMs without many of the empirical hacks necessary to stabilize training in practice. The authors demonstrate the effectiveness of their approach on synthetic and benchmark image datasets (e.g. MNIST, CelebA).",
            "strength_and_weaknesses": "Strengths:\n- The theoretical insights in the paper were sound and important.\n- This is also the first time that GRBMs can generate images unconditionally. Although the sample qualities can be improved, the fact that this could be done is impressive even with a small model architecture is impressive.\n\nWeaknesses:\n- Although I don\u2019t expect the FIDs for samples generated from other datasets (e.g. CelebA) to be competitive, would the authors comment on the gap between GRBMs and existing (maybe simple) baselines? \n- It would have also been nice to have additional experimental details (e.g. does the VAE architecture match that of the GRBM?).\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Quality: The paper was high quality and clear.\n- Novelty: The techniques and theoretical insights provided in the paper were novel, and significantly advance the potential for the practical usefulness of GRBMs.\n- Reproducibility: No code was attached with the submission.\n",
            "summary_of_the_review": "The paper provides novel and interesting theoretical insights improving the performance of GRBMs in practice, as well as several proof of concept experiments for how they can be leveraged for generative modeling. This paper will be of interest to the community, and I imagine this encouraging new follow-up works.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4730/Reviewer_SJ4K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4730/Reviewer_SJ4K"
        ]
    },
    {
        "id": "mSYtqDDNXVH",
        "original": null,
        "number": 3,
        "cdate": 1666694873507,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694873507,
        "tmdate": 1666694873507,
        "tddate": null,
        "forum": "eHkWu_OXBGt",
        "replyto": "eHkWu_OXBGt",
        "invitation": "ICLR.cc/2023/Conference/Paper4730/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work revisits the GRBM to improve its learning and sampling process. One key innovation is introducing Langevin sampling to GRBMs. Two variants are proposed: sampling the visible marginal potential, and hybrid Gibbs-Langevin sampling that trades off between Gibbs updates of hidden units and Langevin updates of visible units. The other key innovation is initializing MCMC samples from noise so that the model can generate samples from noise once training is over. Experiments show that the learning method can successfully train GRBMs that can generate reasonable images from scratch. ",
            "strength_and_weaknesses": "STRENGTHS:\n\n1. The work improves on GRBM learning and introduces the first RBM method that can generate images from scratch.\n2. Sample quality is reasonable given the restrictions of the potential energy.\n\nWEAKNESSES:\n\n1. The motivation of using GRBMs is unclear. From one perspective, the visible GRBM is simply a deep EBM with a restricted architecture. The hidden units distinguish the GRBM and RBM family, but the applications do not depend on the hidden representations. A clear motivation for the use of a GRBM would greatly strengthen the paper.\n2. The presentation of the modified CD initialization is missing reference to related EBM works such as [1] that also use noise-initialized MCMC for both training and testing. It might be useful to perform an ablation study to investigate the effect of the proportion of data samples and noise initialized samples. It is possible that the data samples might not be needed at all.\n\n[1] Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model\nhttps://arxiv.org/pdf/1904.09770.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of this work is somewhat lacking because it uses an established energy function and established methods such as Langevin sampling and noise-initialized MCMC from the EBM literature. The quality of results are good for the RBM family but very limited compared to virtually all contemporary generative model families.",
            "summary_of_the_review": "The work shows improvements within the GRBM family, but this potential is known to be very restrictive and even in ideal circumstances will likely be unable to match a wide variety of contemporary generative models. Without a clear motivation for using the GRBM, I recommend against accepting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4730/Reviewer_Uikc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4730/Reviewer_Uikc"
        ]
    }
]