[
    {
        "id": "joryHPXRPW",
        "original": null,
        "number": 1,
        "cdate": 1666139724799,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666139724799,
        "tmdate": 1669058586341,
        "tddate": null,
        "forum": "yHIIM9BgOo",
        "replyto": "yHIIM9BgOo",
        "invitation": "ICLR.cc/2023/Conference/Paper4014/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a novel formulation for repetitive combinatorial optimization problems (R-COPs) over graphs that is particularized to both independent R-COPs (i.e., problems where the graph data is independent at different time steps) and graph MDPs. It also introduces a novel actor-critic framework to solve these problems---GDPG-Twin---where the critic consists of a differentiable twin model that is trained to represents the states resulting from the application of a policy to a non-differentiable network process (typically based on a greedy heuristic). ",
            "strength_and_weaknesses": "Strengths:\n\n- The proposed learning framework is novel and broadly applicable.\n- The numerical results are convincing:\n    - On synthetic graphs, GDPG-Twin either improves upon or achieves faster computations than the conventional heuristics in four independent R-COPs, and optimizes the long-term objective in a graph MDP.\n    - GDPG-Twin with random sampling around the current policy converges faster and has better sampling efficiency than the comparable method (ZOO).\n\nWeaknesses:\n\n- While the proposed framework is pretty general and is tested on four examples of COPs, many of the examples are very similar. It would have been interesting to see how the framework fares in different types of COPs, such as graph coloring (which also has applications in scheduling).\n- The graph machine learning models used to parametrize GDPG-Twin are not described in detail. Moreover, the effect of the choice of parametrization is neither discussed nor illustrated with numerical experiments. \n- Very little intuition is given as to the real-world applications of MWIS, MWIDS, NWST, and MWCDS and the differences among them. It is understandable that there is no real-world data for these problems, but a better description of their application to real-world problems would help ground the paper and strengthen its contribution. \n- The paper is very hard to understand in its current form. See below.\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This is a good paper with poor presentation. \n\n- Certain variables are either undefined, or vaguely defined, or have inconsistent definitions. A non-exhaustive list:\n    - What are the dimensions of $\\mathbf{S}$ in the introduction? What do you mean by \"data supported on $\\mathcal{V}$\"? Is $\\mathbf{S}$ a matrix representation of the graph, node features, edge features...?\n    - $f_{net}$, a key component of the optimization problem in (3), is not defined in Section 2.1, and neither is $f_{obj}$ in Section 2.2 (these are only vaguely defined in the introduction).\n    - $\\mathbf{o}$ is referred to as both a reward and a value function (although $\\mathbf{r}$ is also used for the reward).\n    - Both $\\mathcal{G}(t)$ and $\\mathbf{S}(t)$ are used to denote the state, and both $\\mathbf{x}$ and $\\mathbf{z}$ for the action. The difference between $\\mathbf{Z}$ and $\\mathbf{z}$ is also unclear.\n\nPlease correct inconsistent notations/definitions and either list them all before the technical sections, or make the technical sections self-contained.\n- The parallels between the reformulations of the optimization problems in (3) and (5), and (4) and (10), are not clear. For example, from the text, it is not clear how \"the objectives in (3a) and (5a) are equal due to the linearity of expectation\". The variables that appear in these equations are not even the same. Please improve the description of these parallels, explaining how the different variables can be substituted.\n\nMinor:\n\n- Section 1 is too long and difficult to read. I suggest shortening the introduction, and using some of the space to describe the graph machine learning architectures used to parametrize GDPG-Twin.\n- Use \\citep for citations.\n- What is the reusing factor in Figure 4?\n- \"Approximation ratio\" (used to compare the proposed model with heuristic methods in the numerical experiments) is misleading, as it can be interpreted to mean that larger approximation ratio is always better. Consider referring to this quantity by another name.\n\n",
            "summary_of_the_review": "The contributions of the paper are novel significant, but the presentation is poor. I recommend acceptance conditional on improving notation and presentation in the rebuttal.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4014/Reviewer_fA3E"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4014/Reviewer_fA3E"
        ]
    },
    {
        "id": "xillbSOUDn",
        "original": null,
        "number": 2,
        "cdate": 1666680355852,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680355852,
        "tmdate": 1666680355852,
        "tddate": null,
        "forum": "yHIIM9BgOo",
        "replyto": "yHIIM9BgOo",
        "invitation": "ICLR.cc/2023/Conference/Paper4014/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel GNN and RL-based approach to R-COPs with hard constraints and without supervised labels. The proposed approach has several advantages compared to existing methods and show clear improvements emperically. ",
            "strength_and_weaknesses": "Strength:\nThe proposed method is novel and gives clear improvements compared to existing methods. The introduction and literature review gives a good context of the problem. The experiment results are strong.\n\nWeakness:\nCould improve the citation style.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, and novelty of the paper are good. The author shares the code for reproducing.",
            "summary_of_the_review": "The paper is well-written and the proposed method is novel and has clear improvements.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4014/Reviewer_XHWP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4014/Reviewer_XHWP"
        ]
    },
    {
        "id": "fXRaTuBoiS",
        "original": null,
        "number": 3,
        "cdate": 1666820618400,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666820618400,
        "tmdate": 1671139461387,
        "tddate": null,
        "forum": "yHIIM9BgOo",
        "replyto": "yHIIM9BgOo",
        "invitation": "ICLR.cc/2023/Conference/Paper4014/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an RL approach to address two families of CO problems: (i) independent repetitive COPs and (ii) COPs embedded in graph-based MDPs. It claims to learn reusable node and edge information to improve existing heuristics to these problem. The proposed approach is illustrated on graph CO problems.\n",
            "strength_and_weaknesses": "**Strengths**\n1. The paper cites and explains a lot of related works\n1. The approach is tested on 4 problems\n\n**Weaknesses**\n1. Important issues with the formalization of the problem or the notations, that make understanding the primal goal/setting of the paper difficult:\n    * The initial problem Eq (1) optimize f_{obj}(o) where o = f_net(G) but G is the network state that looks like the input. So I don\u2019t see what is the decision variable here\n    * Sec 2: \u201cWe define a function space F as a set of functions that will output solutions satisfying all the constraints of the COP in (2).\u201d What is the input of these functions? \n1. Some mathematical statements lack rigor or are wrong in my opinion :\n    * Sec 2.1: We relax the objective function in (2a) to its expectation over \u03a9c. How is this a relaxation (in the mathematical optimization sense)? Problem (3a-b) is another optimization problem, where the decision variable is F_net\n    * Sec 2.1 \u201coptimal solution for (3), x^\u2217 = f_net^*(V, E, c), is still an exact solver for (2) for every given c\u201d.  f_net^* is by definition the optimal solution when minimizing the expected objective over c. Therefore for a given c, there is no guarantee that it is optimal. \n    * Sec 3.1. The fct h in eq (5) is presented as a valid heuristic, but it looks like a constraint in (5c). Is it an input ? (in which case h \\in F should not appear as a constraint);  Or a variable ? (in this case how is it linked to the decision variable Z?)\n1. The paper is very dense and was not easy to read for me \u2014 although I have a strong background in CO and ML but admittedly not so much in network problems specifically.\n    * E,g. Sec 1.1 could use some more structure like paragraphs \n    * I don\u2019t understand: Sec 1.2: \u201cImitation learning \u2026. However, it requires a good guiding algorithm, which set the upper limit of the system and could be costly\u201d \n    * Even after reading the paper carefully, the first sentence of the conclusion  is still hard to get \"...address repetitive combinatorial optimization problems by parameterizing the input node weights of a non-differentiable, fast and/or distributed heuristic, fnet(\u00b7), through a continuous-valued high-dimensional action generated by an actor GNN\"\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity & Quality**\nThe paper is not clear to me, mainly because of the mathematical issues with the very initial formulations which did not help to understand what was exactly the problem being addressed. \n\n**Novelty**\nI did not understand the paper enough to be able to judge precisely.\n\n**Reproducibility**\nThe source code is provided.\n\n**Remarks/questions**\n* Sec 1.1. \u201cnetwork state of a time slot. \u201d \u2014> at a time solt?\n* The reference for the Branch and Bound algorithm should be fixed. The paper references recent improvements (2019-2021 ML papers) while B&B was invented in the 60s in the OR community.\n* Sec 2.1: \u201cthe smoothened objective function in (3a) makes it easier to approximate f_net^*, as illustrated in Section 3.\u201d Can the paper be more specific about how it is illustrated? \n* Sec 3.1.1. In Actor Critic algorithms, we always sample from the policy to generate the next action (for exploration). What is the argument for introducing a perturbation of Z instead of sampling Z from the current policy?\n",
            "summary_of_the_review": "I would vote for reject because the paper is not clear and lacks mathematical correctness. \n\n\n******* **After Rebuttal** ******\n\nI thank the authors for their replies. After reading the revised manuscript and the extensive rebuttal, I think the paper is more clear and many of my questions were answered. I still have a few minor doubts/remarks:\n\n* The first statement of the abstract: \u201cWe propose an actor-critic framework for graph-based machine learning pipelines with non-differentiable blocks, and apply it to repetitive combinatorial optimization problems (COPs) under hard constraints\u201d but the paper really focuses on repetitive COPs and the framework is only defined for repetitive COPs.\n* In Eq (4), should the sets of vertices $\\mathcal{V}(t)$ and edges $\\mathcal{E}(t)$ depend on $t$? They don\u2019t seem to be updated for different $t$s. Only $S(t+1)$ is defined in Eq (4f).\n* In (4b), since the expectation is taken with respect to the heuristic $h$ and cost $\\psi$, are these functions stochastic? \n* This expectation in (4b) should also depend on $f_r$ and $f_s$ which are defined as stochastic.\n* In (3a), $h$ does not appear in the expectation \u2014 is it deterministic there?\n* The paper says that the local function in (5d) \u201c\u2026can be chosen as, e.g., a multiplier, a single neuron, or even a small neural network,..\u201d  If it is a neural model then it is not clear how it can be learned within the framework. I see in the Appendix that in the experiments the local function is \u201ca multiplier\u201d: $f_{loc}(c_i, z_i) = c_i z_i$ \u2014 therefore no learning needed. I think the paper should explain how $f_{loc} $ can be trained or remove the statement that it can be a neuron/neural network.\n\nDetails:\n* In Eq (7) $\\bar{c}$ could be used for readability.\n* Sec 2.2: \u201c$0 \\leq \\gamma \\leq 1$\u201d: the discount factor should be strictly positive \n* Sec 3.1 \u201c$0 \\leq \\alpha_p \\leq 1$\": similarly the learning rates $\\alpha_p$ and $\\alpha_c$ should be strictly positive, and I\u2019m not sure about why the paper uses 1 as an upper bound. Of course in practice the learning rate is much smaller than 1 but I don\u2019t think it **has** to be smaller than 1.  \n\nOverall I think the paper presents an interesting approach to improve the performance of heuristics for repetitive COPs over graphs. The proposed framework is successfully demonstrated on 5 problems. Therefore I am happy to increase my score and confidence level.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4014/Reviewer_gr9w"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4014/Reviewer_gr9w"
        ]
    }
]