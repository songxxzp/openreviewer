[
    {
        "id": "wh9AyM6X0-",
        "original": null,
        "number": 1,
        "cdate": 1666390476916,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666390476916,
        "tmdate": 1669661890901,
        "tddate": null,
        "forum": "bXNl-myZkJl",
        "replyto": "bXNl-myZkJl",
        "invitation": "ICLR.cc/2023/Conference/Paper2919/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a mechanism for scaling up the receptive field of the convolutional kernels in CNNs, and for improving the performance without extra FLOPS via sparsity. Positive results are shown for several visual tasks.",
            "strength_and_weaknesses": "The paper proposes a simple way for scaling up the size of the convolutional kernels, and empirically demonstrates the effectiveness of this  on several visual tasks. The kernels can be scaled to a larger size than before, while boosting the performance. \n\nThe paper proposes a mechanism for introducing dynamic sparsity, which improves the performance at no extra cost in FLOPS.\n\nThe experimental results explore different training schedules, different tasks (classification, segmentation, detection), and (positively) compare to several SOTA architectures. \n\nIt was not clear to me what the main message of the paper is, other than that it is possible to train CNNs with large kernels. For example, is it the case that the best practice is to use such models? If so, I would have liked to see experiments on more data and tasks, including ones with large-data pretraining and possibly transfer learning (as with e.g. the VTAB suite of benchmarks). Comparing to more architectures, with results perhaps presented as a pareto frontier in the cost / performance plane, would be instructive. \n\nIf the main message is the fact that sparsity is important, this should be combined with the analysis of the actual cost of sparse inference, which (as the paper mentions) cannot be measured just using FLOPS. \n\nIf the message is that a larger receptive field could be achieved by a parallel combination of non-square filters, this seems known (since at least the Inception models). I should also note that while the paper refers to 51x51 filters, it is my understanding that it considers a restricted form of such filters, which are cross-shaped. It would be useful to understand why such a shape is a good one to use -- as opposed to e.g. convolving with the two filters sequentially rather than in parallel which would give something closer to a square filter, or atrous convolutions which provide a sparse coverage of a large receptive field.\n\nI was looking forward to understanding why previous work did not use larger kernels, and the first sentence of sec. 3 seemed to promise 3 reasons for it, which I did not find.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear in its description and provides necessary information for the experimental setup. It could do a better job at clarifying for the reader what message they should walk away with.\n\nMy biggest concern is the novelty, as the building blocks used are (if I understand correctly) known, and their combination is natural based on the existing art. This would not be a concern if the paper demonstrated that the particular combination is the thing practitioners should do going forward, but for that I would want to see a broader and more thorough empirical analysis (comparing to more architectures, on different datasets, tasks, and perhaps even domains).",
            "summary_of_the_review": "The method is simple and results are encouraging. However, I have concerns with novelty. Even if the method is a natural combination of known components, the paper could clarify / analyze why the specific set of choices is the \"right\" one to use, and provide more complete empirical validation. \n\n~In the current form, I believe the paper falls short for ICLR.~\n\n*Edit 11/28:* During the discussion phase, we have identified ways for the paper to be edited to call attention to the salient aspects, specifically one around the cross-shaped filters allowing a beneficial trade-off between the parameter efficiency and the receptive field size. Raising the score from 5 to 6.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2919/Reviewer_h6xx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2919/Reviewer_h6xx"
        ]
    },
    {
        "id": "IBa2gHkt8r",
        "original": null,
        "number": 2,
        "cdate": 1666599606634,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599606634,
        "tmdate": 1666599606634,
        "tddate": null,
        "forum": "bXNl-myZkJl",
        "replyto": "bXNl-myZkJl",
        "invitation": "ICLR.cc/2023/Conference/Paper2919/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores the use of large convolution kernels in CNNs, and demonstrates they can match of outperform transformer based architectures if appropriate allowances are made to reduce the parameter count (by factorisation and sparsity induction). Experiments on ImageNet classification, Semantic segmentation (ADE20K, COCO) and object detection (VOC2007, COCO) show the power of the proposed approach. ",
            "strength_and_weaknesses": "### Strengths:\n\n- Clear idea nicely explained.\n- Good evaluation, comparing against a number of alternative state of the art models.\n\n### Weaknesses:\n\n- Some of the results might arguable lie within the bounds of error - whilst performaing a sensitivity analysis might not be possible, could the authors say something about learnability of the models? are they easier or more difficult to train than the alternatives?\n- The last paragraph of section 6.1 is probably going a bit far - whilst it is true that human vision is foveated, the way that a human looks at an image is somewhat different to how a CNN percieves it. (I'd personally just tone down the links to human vision here)\n- The statement on negative societal impacts is very naive and should be either removed or reconsidered (whilst I agree that this work is scientific, others will undoubtedly build on it and could build software with a negative impact using this technique - you should be open about this even though it is out of your control)\n",
            "clarity,_quality,_novelty_and_reproducibility": "To the best of my knowledge the proposed approach is new. The paper is clearly written, and re-implementation of the idea and experiments should be possible from just the paper alone.\n",
            "summary_of_the_review": "This work provides a new approach to making large-kernel convolutions in CNNs realisable. The paper is well written on the whole and the evaluation is good.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2919/Reviewer_YfwT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2919/Reviewer_YfwT"
        ]
    },
    {
        "id": "c5YLE0MvKo",
        "original": null,
        "number": 3,
        "cdate": 1666668485588,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666668485588,
        "tmdate": 1666668485588,
        "tddate": null,
        "forum": "bXNl-myZkJl",
        "replyto": "bXNl-myZkJl",
        "invitation": "ICLR.cc/2023/Conference/Paper2919/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors propose a novel convolutional network structure with very large kernel size, up to 51x51 or even larger. The key ideas is to replace the convolutional kerenl in ConvNeXt by three kernels of size mx5 / 5xm / 5xm respectively. The mx5 + 5xm provides a low-rank approximation to mxm kernel, which can be too large to be learned efficiently. The 5x5 kernel captures the local dependency more efficienctly. During the training, dynamic sparsity is used to adaptive prune-and-grow the kernel weights. Large-scale experiments on ImageNet show that the proposed network can outperform ViT models under the same params and FLOPs.",
            "strength_and_weaknesses": "[Strength]\n\n* While using a large kernel size is not a novel idea, it is still the first work that scales kernel size to 51x51 without training difficulty. The most interesting design is the 5x5 small kerenl to capture locality.\n\n* The large-scale experiments show very promising results comparing to ViT-based models. Especially, it outperforms Swin-B by 1% at 384x384 resolution, only using convolutional operators.\n\n* Extensive experiments on ImageNet classification as well as multiple downstream tasks. All showing impressive improvements over baseline methods.\n\n* It is interesting that the rank-5 decomposition of the large kernel is chosen, instead of mx1+1xm. Also, the dynamic training is shown to improve the final accuracy while reducing the model size.\n\n[Weakness]\n\nAlthough this work already did lots of experiments, I would suggest a few more explorations to enlarge the impact of this work:\n\n* Can we use the proposed model in NLP problems?\n\n* What about stacking multiple 3x3 conv instead of using mx5+5mx+5x5 kernel?",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is well written and easy to follow.\n* The idea is very novel, with impressive SOTA results on multiple benchmark datasets.\n* The authors provided suffcient materials to reproduce the results.",
            "summary_of_the_review": "This is a very strong work, with strong novelty and SOTA results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2919/Reviewer_X26i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2919/Reviewer_X26i"
        ]
    },
    {
        "id": "EwPInDWhSq",
        "original": null,
        "number": 4,
        "cdate": 1666680951166,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680951166,
        "tmdate": 1666681900091,
        "tddate": null,
        "forum": "bXNl-myZkJl",
        "replyto": "bXNl-myZkJl",
        "invitation": "ICLR.cc/2023/Conference/Paper2919/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Recent works on modern ConvNets defend the essential roles of convolution in computer vision by designing advanced architectures and plugging large kernels. This paper pushes the extreme of kernel sizes, and proposed to build a pure ConvNet model that smoothly scales up the kernel size beyond 51\u00d751. Both empirical results and scientific insights appear to be strong.",
            "strength_and_weaknesses": "This is a strong empirical paper studying a highly unusual architecture. This paper did an excellent job in solidifying their approach, validating a large variety of datasets/tasks, and delivering the performance that one expects. I am hence positive overall. The main strengths of this paper are enlisted as follows:\n- The problem considered is very interesting: how much gain one could squeeze from ConvNets, by purely enlarging this kernel size. Provided with the recent trend of large-kernel CNNs, scaling kernel size up to 51x51 or even 61x61 was still rarely discussed before. \n\n- Asking this question is also scientifically meaningful: while ViTs seem to dominate vision now with superior performance, it is unclear which truly matters: the larger/global receptive field or the sophisticated adaptive attention weights per location. The observation presented in this paper, that a ConvNet (which sticks to translation-invariant kernels) with (unprecedently) large kernel size outperforms best ViTs, provides a much clearer answer to this understanding. \n\n- The authors show that naively enlarging kernel sizes won\u2019t work, and introduce two tricks: two-way factorization, and dynamic sparsity with more width \u2013 the second one is more non-trivial and leverages the recent progress in dynamic sparse training. Their recipe seems very concise and compact, avoiding any ad-hoc architecture modification, and seemingly insensitive to hyperparameters too. Tables 1 and 2 demonstrate whether adding each step of receipt would work or not alone, so that clarifies which part contributes how much to the final performance. \n\n- The performance gains are significant and consistent on ImageNet, surpassing some of the best available models (SwinT, ConvNext, RepLKNet) across four scales (T, S, B, and B with high-resolution 384 input). It performs comparably with the strongest hybrid model of CSwin Transformer.\n\n- The authors further evaluate on three dense prediction tasks: ADE20K, Pascal VOC, and MS COCO. The gains of large kernels become more obvious on (i) dense tasks; and (ii) high-resolution images. In particular, SLaK-T with larger kernels (51\u00d751) further brings 1.2% mIoU improvement over ConvNeXt-T (RepLKNet), surpassing the performance of ConvNeXt-S. On MS-COCO, SLaK fine-tuned with Cascade Mask R-CNN also outperforms the same fine-tuned ConvNeX-T by around 1% in all AP metrics. \n\nThe weakness is perhaps that the model is not as advantageous on real hardware. It\u2019s encouraging to see the authors reported some real hardware measurements in Section 6.2, but those are 4x compared with the dense large-kernel networks (which are supposed to be extremely slow), not the off-the-shelf-competitors such as SwinT or ConvNext. \n\nConsidering the highly unconventional architecture that this paper innovates with, I won\u2019t hold this point as a fatal weakness against this paper. However, I do suggest the authors to openly discuss this issue and acknowledge their real hardware speed limitation w.r.t. existing competitors, so that readers would get the full picture right for selecting models. A further interesting question is whether the authors could consider co-designing their novel NNs with some customizable hardware such as FPGA.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is very well written, clear and concise.  Novelty is overall good bold despite some limitations. Codes are available and results are reproducible to my best knowledge.",
            "summary_of_the_review": "Overall this paper looks good and the authors are encouraged to provide more discussions for the performance on real hardware.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2919/Reviewer_rrkL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2919/Reviewer_rrkL"
        ]
    },
    {
        "id": "gJNyVZHP-Ka",
        "original": null,
        "number": 5,
        "cdate": 1666688551666,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688551666,
        "tmdate": 1666767350608,
        "tddate": null,
        "forum": "bXNl-myZkJl",
        "replyto": "bXNl-myZkJl",
        "invitation": "ICLR.cc/2023/Conference/Paper2919/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a large kernel ConvNets which scaled up the kernel size beyond 51x51 up to 61x61 upon the RepVGG-like ConvNet baseline. The authors progress the idea from ConvNeXt and show the difference between the recently published RepLKNet (CVPR 2022) with the proposed model by introducing the concept of dynamic sparsity of weights consisting of sparse initialization and sparse weight training. This goes with the weight pruning and growing (i.e., adding new ones) processes during training to reach a sparse final network. This seems to be a key idea for expanding the kernel size beyond 31x31; another is decomposing a square kernel into a rectangular one (i.e., from a single NxN kernel to two successive kernels, NxM and MxN). Finally, utilizing the model with a reduced FLOP can give an opportunity of extending the network width, so it becomes a final ingredient of the proposed kernel expansion method. The authors provide ImageNet-1k evaluation compared with ConvNeXt, ViT-based models including Swin, Cswin, and DeiT. Finetuning evaluations on ADE20k, Pascal VOC2007, and MS COCO are equipped to support the effectiveness of the proposed model. Some analyses are also provided.",
            "strength_and_weaknesses": "Pros)\n+ Overall, the paper is easy to follow.\n\n\nCons & comments)\n- The novelty is limited. The following recipes to reach the goal are not new and widely-used prior arts:\n   - The kernel decomposition of a large kernel into smaller ones was proposed in Inception families, and many successors adopted the idea to realize efficiency or avoid overfit. \n   - Pruning and retraining (+growing) process similar to Iterative Magnitude Pruning (IMP) has also been widely used aiming the same purpose and is turned out to be very effective for maintaining precision. \n   - Increasing network width has also been a regular tweak to meet the computational demand and improve the model accuracy as well since WideResNet primitively introduced it.\n- Since the accuracy number in Table 3 looks promising, the accuracy gap between it and the one in Table 2 is quite large. I wonder if this is technically due to the parameterization trick in RepVGG and RepLKNet using (31x31 kernels). Specifically, Table 2 shows the performance boosts with the largest kernel size 61x61 are clearly marginal (compared with 31x31 and 51x51, +0, -0.1 for all the cases), so it seems that the proposed method does not produce a final network leveraging a larger kernel effect.\n- Latency or throughput is not reported. I understand the current hardware architecture may not support the kind of sparse matrix computation, but I believe this type of work should include a speed measure.\n- As the authors mention as well, the implementation of this work was done with weight masking, so using the model in practice is cumbersome, and the latency in practice may not beat the competitors'.\n- The authors claim that the lack of locality is the rationale behind the performance degradation after naive training with a large kernel size (e.g., 51x51 or 61x61). However, I respectfully disagree with the claim because there is little evidence, and a more valid reason is that the model was overfitted (in Table 1). Because the training was only done with 120 epochs which is a limited training epoch; and seemingly identical data augmentation (such as the fixed RandAug coefficients, weight decay parameters, and so on) was used for all the models, so the expanded weights could be easily overfitted under weak data augmentation training regime. I recommend the authors train it for more epochs with stronger data augmentations to justify the claim. Furthermore, further backups concerning the locality should also be provided.\n- The detailed procedure of the dynamic sparsity (in Figure 2) is not provided. How did the authors initialize the weights before training? How did SNIP use for giving sparsity - at a layer level? or channel level? How to tune the hyperparameters for scheduling the procedure, such as the interval of pruning/growing and the threshold of pruning?\n- Particularly, the dynamic sparsity may not be universally applicable for other large-kernel networks like this method.\n- How did the authors pad the feature map with such a large kernel (>56)? Zero-padded? If so, zero-padding may harm the performance due to an imbalance scale at the borders, so how did the authors handle it?\n- How did the authors use the pretrained models for finetuning? All the mask was fixed, and the corresponding weights also remained frozen?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly presented to show the effectiveness of the proposed method with the limitation in practice. Although the number using the largest kernel ever looks interesting, the novelty seems limited due to the aforementioned reasons. The reproducibility may not be confirmed because the details of the dynamic sparsity process are not clearly presented, so I have no guarantee whether one can reach the reported score when trained from scratch.",
            "summary_of_the_review": "This paper shows the potential of using larger kernel sizes beyond 31x31 in the previous CVPR work. I'm concerned that the overall idea seems to be well organized with sub-ideas, but they are existing arts, so the novelty of this paper is limited. More specifically, one may imagine that combining a weight decomposition, training with an iterative weight pruning and growing process, and expanding the network width will naturally lead to the performance of a given network. Furthermore, the proposed method does not seem to contribute to improving the resultant model compared with the 31x31 kernel one (Table 2); but the final accuracy was reached rather leveraging the impact of the parameterization technique (Table 3). Finally, using the resultant model in practice with the trained mask is currently not straightforward for practitioners. I believe the work should need newer items and report either raised accuracies of 51x51 or 61x61 kernel models to support the main claim, so I am currently voting to reject this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2919/Reviewer_oAn8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2919/Reviewer_oAn8"
        ]
    }
]