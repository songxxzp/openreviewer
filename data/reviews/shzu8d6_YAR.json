[
    {
        "id": "gN-AnT1X9K",
        "original": null,
        "number": 1,
        "cdate": 1666664005820,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664005820,
        "tmdate": 1666664005820,
        "tddate": null,
        "forum": "shzu8d6_YAR",
        "replyto": "shzu8d6_YAR",
        "invitation": "ICLR.cc/2023/Conference/Paper2868/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new post-processing algorithm (FaiREE) to satisfy group fairness constraints in classification with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfy various group fairness notions, e.g., Equality of Opportunity, Equalized Odds, and Demographic Parity. Synthetic and real data experiments further support these theoretical guarantees.",
            "strength_and_weaknesses": "Strength:\n\nThe proposed post-processing algorithm satisfies two theoretical guarantees (Thm 2 and 3), i.e., a) The output classifier satisfies fairness constraint with high probability, and b) if the input classifier is optimal, the output classifier will be near optimal with high probability. To my knowledge, these theoretical guarantees are novel and critical in trustworthy machine-learning applications.\n\n\nWeaknesses:\n\nThe second guarantee of the proposed post-processing algorithm in Theorem 2 requires a strong assumption that the input classifier is Bayes-optimal. Is there a way to bound the performance of the proposed algorithm with any classifier $f$ using the approximation error between $f$ and $f^*$?\n\nIn addition, if the classifier $f$ is first trained using some dataset $S_{train}$, all the theoretical results only hold when the dataset $S$ used in the post-processing is independent of $S_{train}$. Is it correct? Thus, a train/validation/test split is still needed in this setting, and the sample complexity results in table 2 should be interpreted as the validation sample complexity since it does not include the sample needed for training $f$.\n\nMinor comments:\n\nFigure 1 is really difficult for readers to digest at the beginning of the paper. Readers need to understand $\\alpha$ and DEOO to appreciate the results, so maybe deferring it to later sections will be better. Also, figures are more readable than tables, why not include more figures in the main body and move those tables to the appendix?\n\nTypo: 2 line above Paper Organization, should be \u201cgroup\u201d instead of \u201cgrup\u201d\n",
            "clarity,_quality,_novelty_and_reproducibility": "The contribution of the paper is novel and significant, and the paper is well-written.  ",
            "summary_of_the_review": "This paper tackles an interesting problem with critical societal impacts, and I would recommend accepting the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2868/Reviewer_6zkG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2868/Reviewer_6zkG"
        ]
    },
    {
        "id": "2kz1NNmZ4m1",
        "original": null,
        "number": 2,
        "cdate": 1666699930214,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699930214,
        "tmdate": 1668781170419,
        "tddate": null,
        "forum": "shzu8d6_YAR",
        "replyto": "shzu8d6_YAR",
        "invitation": "ICLR.cc/2023/Conference/Paper2868/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes FaiREE, a post-processing algorithm for fair classification with theoretical guarantees in a finite-sample and distribution-free manner. FaiREE can be applied to a wide range of group fairness notions, and is shown to achieve small mis-classification error while the fairness constraints. Numerical studies on both synthetic and real data shows the practical value of FaiREE in achieving a superior fairness-accuracy trade-off than the state-of-the-art methods.",
            "strength_and_weaknesses": "Strength:\n- The considered problem of fair classification with finite samples is relevant.\n- The approach is interesting that includes candidate set construction and selection.\n- The empirical results show that previous fair classification has limitations with finite samples.\n\nWeaknesses:\n- The empirical performance of FairREE does not have an advantage compared to baselines. For example, in Figure 1, FairBayes can achieve higher accuracy than FairREE for small alpha.\n- The motivation of considering post-processing approaches seems not enough to me. Why do you select post-processing instead of pre/in-processing methods for the finite sample setting?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The writing is good.\nQuality: The theoretical guarantee is good, but the empirical results are not that powerful.\nNovelty: The proposed approach is interesting and novel.",
            "summary_of_the_review": "The paper proposes FaiREE, a post-processing algorithm for fair classification with theoretical guarantees in a finite-sample and distribution-free manner. The approach is interesting that includes candidate set construction and selection. However, the empirical performance of FairREE does not have an advantage compared to baselines. Overall, I tend to reject the paper for the current version.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2868/Reviewer_sJJW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2868/Reviewer_sJJW"
        ]
    },
    {
        "id": "HRdI_Mu4W_u",
        "original": null,
        "number": 3,
        "cdate": 1667515158307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667515158307,
        "tmdate": 1667515158307,
        "tddate": null,
        "forum": "shzu8d6_YAR",
        "replyto": "shzu8d6_YAR",
        "invitation": "ICLR.cc/2023/Conference/Paper2868/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThis paper considers the problem of fair classification by post-processing the decisions of an existing (possibly unfair) classifier. The authors focus on two metrics \u2014 equality of opportunity and equalized odds. The main idea is to set the decision threshold appropriately based on the protected group and true outcome. \n\nThe proposed algorithm consists of two main steps \u2014 (a) candidate set construction, and (b) candidate selection. For candidate set construction, the authors first provide a bound that measures the probability that the fairness constraint is violated for a given threshold. Based on this lemma and a recent result from fair Bayes classification, the algorithm selects a set of candidate threshold pairs. In the next step, the threshold pair with the best empirical performance is selected as the final pair of thresholds.\n\nThe proposed algorithm is evaluated on two simulated datasets, and several standard datasets on fair classification and compared against existing post-processing based fair classifiers. The results show that the proposed algorithm (FaiR-EE) has improved fairness guarantee, but it doesn\u2019t seem to provide any Pareto improvement in terms of the fairness-accuracy trade-off.",
            "strength_and_weaknesses": "Strengths:\n- The proposed method is quite general as a post-processing method and probably can be adapted for other fairness measures as well. I also like proposition 1 which provides a bound on the fairness violation of a score-based classifier given a threshold.\n- The experimental results show improved fairness guarantees compared to other post-processing based fair classifiers.\n\nWeaknesses:\n  - I think the proposed bound only holds when the dataset is not imbalanced. This is evident in the statement of theorem 2 where the bound scales with the minimum number of samples across different group, outcome pairs. Since most of the datasets encountered in fair classification are under-represented, this is a major concern.\n- The main weakness of the paper is experimental evaluation. Table 3 shows improvement in fairness but for smaller values of $\\alpha$ (e.g. 0.08 or 0.12) accuracy is significantly lower for FairEE-EO. The same observation holds for the Compass dataset (in the appendix). This makes me think that the proposed algorithm doesn\u2019t provide right trade-off between accuracy and fairness.\n- Most of the guarantees are in-sample fairness guarantees. The authors didn\u2019t provide generalization guarantees even though the lack of generalization of  post-processing based methods was emphasized in the motivation.\n- Finally, in order to implement the candidate set construction efficiently, one needs to implement functions $g_1$ (from proposition 1) efficiently. It was not mentioned how these statistics can be evaluated efficiently.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The main algorithm and its analysis were explained carefully in the paper. However, I thought that the author should have explained the experimental details in more detail. In particular, why does the accuracy become quite low on certain datasets for smaller values of $\\alpha$? Moreover, it would also have been nice to see a pareto-curve showing the trade-off between fairness and accuracy. \n\nNovelty: I think the idea of using threshold based classification for post-processing decisions of a classifier is quite well-established, and this is not a novel contribution. But the authors did provide an interesting application of the recent results on fair Bayes classification. I also did not understand the claim about distribution free approach. Since lemma 1 uses knowledge of posterior distribution, isn't this dependent on the underlying distribution?",
            "summary_of_the_review": "Overall, I think the paper considers an interesting approach for designing post-processing based fair classifier. However, the main proposed algorithm has some requirements (e.g. balanced data, knowledge of $g_1$, etc). Moreover, the experimental section seems weak and suggests that the proposed approach doesn't provide right trade-off between fairness and accuracy.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2868/Reviewer_8rQh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2868/Reviewer_8rQh"
        ]
    },
    {
        "id": "00detx9ATu",
        "original": null,
        "number": 4,
        "cdate": 1667543087493,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667543087493,
        "tmdate": 1670124015071,
        "tddate": null,
        "forum": "shzu8d6_YAR",
        "replyto": "shzu8d6_YAR",
        "invitation": "ICLR.cc/2023/Conference/Paper2868/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to address one challenge in post-processing fairness mitigation algorithms: they require either distributional assumptions or access to infinite data examples. This paper presents a framework that takes in any score-based black-box classifier and outputs group-wise thresholds to correct for the fairness violations. The advantage over existing literature is it doesn\u2019t require additional assumption on data distribution except for a minimum sample size.\n\nThere are three steps in the FaiREE framework. First, score each data example using the given black-box classifier. Second, conform a candidate set of thresholds that promise the fairness constraints. Finally, select the classifier that has the smallest mis-classification error from the candidate set. The key improvement is that the authors provide an upper bound of the probability of fairness violations and lend the upper bound to shrink the candidate set.\n\nThe FaiREE framework can be extended to multi-group fairness notions. Experiments are conducted on both a synthetic dataset and the Adult dataset to validate the effectiveness of the post-processing algorithm.",
            "strength_and_weaknesses": "Strengths:\n- The scope and motivation of this paper is very clear. The proposed algorithm seems to be a novel work in post-processing fairness mitigations.\n- The authors demonstrate how the proposed algorithm can be extended to multi-group fairness notions, which will be useful in practice.\n- The proposed algorithm has strong theoretical guarantee to bound the error of post-processed classifier.\n\nWeaknesses:\n- The writing of this paper is not in a good shape. There are many typos. The wordings are sometimes confusing. The heavy notations make it not easy to understand the paper. In this sense, I don't think this paper is ready for publication. Please see the comments below.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality of this paper can be significantly improved. Here I just listed some:\n\n- Page 1, paragraph 1: \u201cTo address this fairness problem, many fairness notions have been proposed.\u201d Are these fairness notions proposed to just formulate the fairness measures or mitigate the unfairness?\n- Page 1, paragraph 1: \u201cBased on these fairness notions or constraints, corresponding algorithms\u2026\u2026 Among these fairness algorithms\u2026\u201d Where are the examples?\n- Page 1, paragraph 2: \u201cmost fairness constraints are non-convex, some papers propose convex relaxation-based methods\u201d The convex relaxations are designed for in-processing mitigation algorithms. How did those relaxations motivate your research problem?\n- Page 2, paragraph 1: achieve group fairness notions -> achieve group fairness constraints; fairness classification -> fair classification\n- Page 1, Figure 1: consider combine the subfigures into one trade-off graphic for accuracy and fairness violation\n- Page 2, paragraph 3: \u201cgrup fairness\u201d typo\n- Proposition 2, $\\phi \\neq y$ is ill-defined, as $\\phi(x, a)$ has appeared in Proposition 1.",
            "summary_of_the_review": "I did not have the time to go over any of the proofs, but I do have some concerns that this paper is not ready for publication. I would be open to adjust my score after receiving author's response.\n\nQuestions:\n1. Does Theorem 2 require the input classifier f to be Bayes-optimal? If true, how practical is this additional assumption?\n2. The big-$O$ notation after the w.p. phrases in Theorem 2 and Theorem 4 does not make sense to me. How do you interpret the big-O inside a probability?\n3. In the experiments, the baseline methods are compared on fairness notions of which the algorithms are not designed to mitigate the violations. For example, Eq is compared on predictive equality and equalized accuracy constraints. Is it a fair comparison?\n4. In Table 2 and Table 3, the accuracy of classifiers are around 0.5. Is that informative on the synthetic dataset?\n\n---------\nThe authors have properly addressed most of my concerns and made significant improvements over the draft. In consequence, I updated the score from 5 to 6 accordingly.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2868/Reviewer_P2N4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2868/Reviewer_P2N4"
        ]
    }
]