[
    {
        "id": "YZUmPft_Rkz",
        "original": null,
        "number": 1,
        "cdate": 1666401049258,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666401049258,
        "tmdate": 1666401049258,
        "tddate": null,
        "forum": "fUX3bszZSOw",
        "replyto": "fUX3bszZSOw",
        "invitation": "ICLR.cc/2023/Conference/Paper4338/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a continual learning algorithm, called Regularized Adaptive Weight Modification (RAWM) to overcome catastrophic forgetting for fake audio detection. RAWM is based on the previously published orthogonal weight modification (OWM). Because OWM does not consider the similarity of some audio, including fake audio obtained by the same algorithm and real audio, on different datasets. To solve this limitation, adaptive modification direction and a regularization constraint are proposed. Experimental results show a good performance improvement compared to baselines.",
            "strength_and_weaknesses": "Strength:\nThe proposed continual learning algorithm is designed for the application of fake audio detection. The authors observe some problems when using conventional continual learning algorithms and propose two methods (adaptive modification direction and a regularization constraint) to overcome them. \n\nWeaknesses:\nTo avoid using training data from the previous tasks, the proposed regularization constraint is based on a teacher model, which is not a very novel idea. On the other hand, the way to apply an adaptive modification direction is not very clear to me. In page 4, the authors claim that \u201cIn the training process on the new dataset, they should be trained without any modification.\u201d However, the authors propose a new projector Q which is orthogonal to the projector P from OWM. My question is: why not just use the original gradient from SGD, \u2206W^BP in this case? \nOther comments:\n1. Is P a square matrix? Otherwise, how to compute Q using e.q. (4)?\n\n2. In the original paper of \u2018OWM\u2019 (Continual Learning of Context-dependent Processing in Neural Networks), the authors claim that \u201d Recursive Least Square (RLS) algorithm which can be used to train feedforward and recurrent neural networks to achieve fast convergence\u2019 However in this paper, the proposed fake audio detection model is based on Wav2vec (which includes convolutional encoder) and a CNN classifier. I\u2019m not sure how OWM performs on convolutional layers. My concern is that I think OWM assumes the calculation between input and model weights is through matrix multiplication instead of convolutional operation.\n\n3. In Fig. 1. of the original paper of \u2018OWM\u2019, they claim that OWM can reach the position inside the overlapping subspace between two tasks. However, in Fig. 1. of your paper, you mention that OWM can not reach the region of dataset 2. Please explain why there is such a difference.\n\n4. In Table 6, why even Fine-tune cannot obtain the best performance on the new task compared to other methods?\n\n5. It would be great to also report the \u2018experience-replay-based\u2019 method in the experiment.\n\nTypo:\n1. Page 1, fae audio detection -> fake audio detection \n\n2. Page 3, yo and yo are the old and new ground truth -> yo and yn are the old and new ground truth\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper presents the problem quite clearly, however, the reason for using projector Q instead of normal gradient need to be explained. ",
            "summary_of_the_review": "This paper is based on the previously published OWM paper, and two modifications (adaptive modification direction and a regularization constraint) are proposed. As pointed out earlier, the reason to apply Q in adaptive modification direction is unclear, and the novelty of using a teacher model for regularization constraint is somewhat limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4338/Reviewer_VFqH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4338/Reviewer_VFqH"
        ]
    },
    {
        "id": "EbzvcSEfuA",
        "original": null,
        "number": 2,
        "cdate": 1666647679806,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647679806,
        "tmdate": 1666647679806,
        "tddate": null,
        "forum": "fUX3bszZSOw",
        "replyto": "fUX3bszZSOw",
        "invitation": "ICLR.cc/2023/Conference/Paper4338/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new method for continual learning which leverages structure found in the particular task of fake audio detection. Specifically, the proposed method augments continual learning algorithms by leveraging structural differences found in fake audio detection between two individual distribution shifts latent in the source and target datasets: source fake -> target fake, and source real -> target real. Compared to other continual learning algorithms, the proposed algorithm leads to large improvements in performance on both source and target datasets.",
            "strength_and_weaknesses": "The primary strength of this paper is that of the results: the proposed algorithm clearly outperforms other comparable continual learning algorithms for standard fake audio detection tasks. The primary weakness is a lack of motivation for the problem formulation explored here. Specifically, this paper explores continual learning for fake audio detection, for which there are two key issues:\n\n(1) I would argue that the biggest practical challenge in fake audio detection is not _adapting_ to new domains w/ many examples (as is explored in continual learning) but rather improving _zero/few-shot generalization_ to new domains. The threat model in fake audio detection should emphasize the persistent development of new methods for generating fake audio. Instead, the exploration of continual learning suggests a perpetual cat-and-mouse game where detection models must be manually adapted to new domains.\n\n(2) The motivation for the _continual_ learning setup (where source data is inaccessible) is unclear. The introduction says in one sentence that \u201cin some practical situations, it is almost impossible to obtain the old data\u201d, but does not suggest any particular situations where that would be the case. Moreover, all of the experiments _contrive_ such situations (since the source dataset is always accessible), and there are no comparisons to \u201coracle\u201d performance w/ access to source data.\n\nAdditionally, this paper struggles w/ clarity (see next section).",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nOverall, I found this paper to be quite unclear and required several passes to understand. A primary issue is definition and consistency of terminology. For example, this paper primarily concerns \u201cfake audio detection\u201d and \u201ccontinual learning\u201d but neither are defined anywhere. Also, this paper fluctuates between using \u201csource / target\u201d (in Figure 1 / Experiments) and \u201cold / new\u201d (throughout) naming conventions for adaptation. Important notations like T_1, T_2, and T_3 are introduced in the header of Table 1 instead of Section 5.1 where the reader might expect to see them.\n\n\nA key question I was left with after reading this paper: in Table 6, why do the majority of the continual learning algorithms outperform simple fine-tuning on the target dataset? This may be an understood phenomenon in the continual learning research community but I find it incredibly counterintuitive as an outsider. Can the authors clarify the \u201cfree lunch\u201d aspect of continual learning, especially in this context of fake audio detection?\n\nAnother comment is that the introduction could do more to clarify the specific assumptions about fake audio detection that motivate the need for tailored continual learning algorithms. The third paragraph of the intro attempts to do this in one sentence (starting with \u201cBecause the\u2026\u201d), but this sentence is exceptionally difficult to parse and entangled w/ the proposed method (instead of stated in a method-agnostic fashion). A clearer upfront explanation of this would have greatly improved my ability to understand the rest of the paper on the first pass.\n\nThere are also numerous places throughout the paper containing misleading or ambiguous statements. For example, the first paragraph of section 5.1 suggests that the training subset of the source dataset will be used, but the training subsets of the target datasets will not be (which is wrong in the context of continual learning).\n\n** Quality **: Accepting the sole focus on continual learning, the experiments are reasonably well-designed. However, I would still have very much liked to see a comparison to \u201coracle\u201d methods w/ access to the source dataset - how much performance do we lose by making the assumption that source datasets are inaccessible?\n\n** Novelty **: The proposed method is adequately interesting and novel - a stronger version of this paper might explore the proposed method for other tasks (besides fake audio detection) with similar structure.\n\n** Reproducibility **: It would be an extraordinary challenge to reproduce this algorithm from the notation / information in this paper alone - would the authors be able to release code?\n",
            "summary_of_the_review": "Overall, this paper presents some interesting ideas and compelling results, but the current version struggles w/ (1) motivating the setting of interest (continual learning for fake audio detection) including lack of comparison to methods which can access the source data, and (2) clarity of explanation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4338/Reviewer_jUDi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4338/Reviewer_jUDi"
        ]
    },
    {
        "id": "iYWlaz9Cd5",
        "original": null,
        "number": 3,
        "cdate": 1666833453185,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666833453185,
        "tmdate": 1666833453185,
        "tddate": null,
        "forum": "fUX3bszZSOw",
        "replyto": "fUX3bszZSOw",
        "invitation": "ICLR.cc/2023/Conference/Paper4338/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors address the problem of \"catastrophic forgetting\" in the context of fake audio detection. When a network trained on one dataset (D1) and is fine tuned on another dataset (D2), the fine-tuned network loses its original performance on the D1. This is referred to as catastrophic forgetting for the considered task of fake audio detection. There are several approaches addressing this problem. One of the recent approaches is using the orthogonal weight modification (OWM) technique. This is based on the viewpoint that modifying the weights in a direction orthogonal to the Weights subspace corresponding to D1 would ensure that the modified network's performance on D1 would not deteriorate. Thus, the idea while fine-tuning on D2 is to modify the weight update equation by modifying the weights in a direction orthogonal to that corresponding to weights subspace representing D1. Such a weight modification has a regularization effect on the network to retain its performance on D1. This however presents a significant drawback: The Performance of this regularized network on D2 suffers compared to without regularization. \n\nIn this paper the authors address the above two issues in OWM by proposing Regularized Adaptive Weight Modification (RAWM) where the above two problems are addressed respectively by a nice observation that blindly updating the weights in an orthogonal direction to the subspace corresponding to D1 might be a bad idea as there could be several genuine audios in D1 which are similar to D2 and for those samples - the regular update might not harm network performance on D2 while improving its performance on D2. This is accounted in the weight update by modifying the direction of update to be based on the number of genuine vs fake examples in each batch and using this to adapt the orthogonal weight update. In the degenrate case of all fake examples this proposed approach would boil down to the OWM method.   \n\nOverall the paper is well written with clear problem definition, experimental results and useful comparisons. ",
            "strength_and_weaknesses": "Strengths:\n1. Paper is well written and the solution proposed addresses the problem quite well. \n2. The experimental validation is sufficiently elaborate and shows good improvement over the baselines considered. \n\nWeakness:\n1. There are a few parts of the paper which are not particularly well written and makes it a difficult read. For instance:\n a) In Section 4.2 the notations and equations 9, 10 are not in sync.\n b) Symbols and notations are not clearly explained on several occasions. \n c) Out of nowhere in Section 5.3, authors start talking about a \"Model-4\n2. How does the proposed method compare to training on all datasets ?\n3. It is not clear why the assumption that genuine audio across different datasets is more likely to be similar than fake audios from different datasets is necessary ? Can the authors not think about exploiting fake audio similarity as well in their technique ?",
            "clarity,_quality,_novelty_and_reproducibility": "1. Clear Enough\n2. Good Quality\n3. I am not entirely sure this work is reproducible. ",
            "summary_of_the_review": "Overall the proposed solution of adapting the weight modification by accounting for the genuine and fake samples in each batch seems interesting and shows promising results. The methodology is clearly explained with detailed insightful evaluation. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4338/Reviewer_d2it"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4338/Reviewer_d2it"
        ]
    },
    {
        "id": "O0V3eRIJez",
        "original": null,
        "number": 4,
        "cdate": 1667287681110,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667287681110,
        "tmdate": 1667287681110,
        "tddate": null,
        "forum": "fUX3bszZSOw",
        "replyto": "fUX3bszZSOw",
        "invitation": "ICLR.cc/2023/Conference/Paper4338/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper considers the problem of continual learning in the context of fake audio detection. One of the main challenges of continual learning is catastrophic forgetting. The paper proposes a new algorithm called Regularized Adaptive Weight Modification (RAWM). The motivation is genuine audios are more similar than fake audios in different datasets. The proposed approach can adaptively modify the direction of weights according to the ratio of genuine audio and fake audio of each batch in the process of fine-tuning. The paper evaluated the proposed approach across multiple fake audio detection datasets and showed promising results in the continual learning setting. \n",
            "strength_and_weaknesses": "Strength \n\nThe paper proposes a novel method called Regularized Adaptive Weight Modification (RAWM) to address the problem of catastrophic forgetting in the context of fake audio detection. The proposed method builds on prior work orthogonal weight modification (OWM).\n\nThe approach is well-motivated (genuine audios are more similar than fake audios in different datasets. The proposed method obtains a better tradeoff between learning from a new dataset while not forgetting the past knowledge.)\n\nThe paper provides a satisfactory literature review.\n\nThe paper evaluates the proposed approach across multiple fake audio detection datasets and shows promising results in the continual learning setting. \n\nWeaknesses\n\nThe proposed method seems to be general for continual learning. However, experiments only demonstrate its value for the task of fake audio detection. Can authors comment on whether the proposed method can benefit any other tasks other than fake audio detection? The paper can be made stronger if the proposed method can be extended to more applications or to general recognition tasks (e.g., image/video/audio recognition).\n\nDoes the proposed method also work for multiclass classification tasks? If so, what are the necessary changes?\n\nPresentation of section 3 can be improved due to lack of context. It will be useful to include a paragraph to describe the problem setting (e.g., notation of model layers, notation for each dataset, notation for gradient etc.). A lot of the context is only introduced in section 4 (\u201cWe consider a feed-forward network consisting of L + 1 layers..\u201d).\n\n\nMinor comments:\n\nIn the second equation of EQ1,  the numerator is P_l(i-1,j) \\bar{X}(l-1). Should \\bar{X}(l-1) be indexed with i,j ?\n\nSection 3.2, \u201cy_o and y_o are the old and new ground truth\u201d (typos?).\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Overall the paper is clear. However, some of the presentation can be made more clear (see Weaknesses) and there are also a few typos in the paper (see Minor comments).\n\nQuality: The proposed method is well-motivated and experiments demonstrate the effectiveness of the proposed method. However, the proposed method is limited to the task of fake audio detection. \n\nNovelty: The proposed method is incremental to OWM, but provides additional novel technical insights for the problem of fake audio detection.\n\nReproducibility: The paper seems reproducible. \n",
            "summary_of_the_review": "The paper proposes a novel method (RAWM) for continual learning in the context of fake audio detection. Experiments demonstrate that the proposed method obtains strong performance compared to the prior methods. The paper can be made stronger if the proposed method can be extended to more applications or to general recognition tasks.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4338/Reviewer_a37q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4338/Reviewer_a37q"
        ]
    }
]