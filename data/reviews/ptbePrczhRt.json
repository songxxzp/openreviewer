[
    {
        "id": "6Gc1Vou0r0",
        "original": null,
        "number": 1,
        "cdate": 1666326781234,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666326781234,
        "tmdate": 1666329502068,
        "tddate": null,
        "forum": "ptbePrczhRt",
        "replyto": "ptbePrczhRt",
        "invitation": "ICLR.cc/2023/Conference/Paper105/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new training strategy to improve the performance of DETR-based object detectors. The authors indicate the main reason for the low convergence rate of DETR-based object detectors is in the one-to-one matching and propose a simple strategy of group-wise one-to-many assignment to address. It is a simple plug-and-play strategy that can be applied to any DETR-based object detectors in the training phase. Extensive experiments have been conducted on 2D object detection (MS COCO), instance segmentation (MS COCO), and multi-view 3D object detection (Nuscenes) to show the advantage of the proposed approach. ",
            "strength_and_weaknesses": "*Strengths*\n+ The paper is clearly written and easy to follow. \n+ The proposed method is simple.\n+ Extensive experiments have been conducted to verify the performance of the proposed method.\n\n*Weaknesses*\n+ This is not mandatory but it would be better to cite the Hybrid DETR (H-DETR) paper: https://arxiv.org/abs/2207.13080 and have a discussion to show to clear difference between the two. \n+ The proposed method is orthogonal to the previous improvements of DETR so that when adding group-wise one-to-many assignments to others the performance is improved with a cost in training memory and time. There is no clear significant improvement over prior improvement. For example in Tab. 1, comparing the DN-DETR on top of the DAB-DETR with the denoising part and the Group-DETR on top of the DAB-DETR, the DN-DETR and Group-DETR are comparable (+3.4 vs +3.9 for C5 backbone and +4.4 vs +4.4 for DC5 backbone). Also, when adding Group-DETR on top of DINO, the improvement becomes less significant. In other words, adding Group-DETR on top of similar improvements results in less effectiveness. \n+ The experiments are quite not fair. As Group-DETR increases the number of queries K times (K is the number of groups), it takes a longer time to train an epoch. Therefore, it would be better to compare the results of the proposed method with the previous methods on the same training hours, not the same number of epochs.\n+ It is not clear why adding new groups improve the performance since there are more negative queries forcing in-object queries to be negative as well. The authors should have a better explanation.\n+ Failure cases and limitations of the proposed method should be added since the proposed approach is not perfect.\n",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned in the strengths section, this paper is well-written. However, no provide codes can be found, I only can see the pdf file, so I am not sure about the reproducibility. The paper proposes a novel but has similar effectiveness to other prior improvements of DETR, not a significant one. ",
            "summary_of_the_review": "+ The proposed method is simple and achieved impressive results on several DETR-based detectors and multiple datasets. However, there is a trade-off between performance versus runtime and memory consumption.\n+ The idea of group-wise one-to-many assignments is quite similar to H-DETR. I am happy to increase this paper\u2019s score if the authors provide experiments with the suggested setting above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper105/Reviewer_ySfN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper105/Reviewer_ySfN"
        ]
    },
    {
        "id": "SCR4Y49e6K",
        "original": null,
        "number": 2,
        "cdate": 1666503655090,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666503655090,
        "tmdate": 1666503699780,
        "tddate": null,
        "forum": "ptbePrczhRt",
        "replyto": "ptbePrczhRt",
        "invitation": "ICLR.cc/2023/Conference/Paper105/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a simple and effective method to accelerate the convergence speed and improve the performance of detection transformers, which only need to randomly initialize different groups of queries and use them to train the decoder separately. The method can be applied to all existing detection transformers and exhibits consistent improvements.",
            "strength_and_weaknesses": "## Strength\n1. The paper is well-written and easy to follow\n2. The idea is simple yet effective, which only use multiple group of queries to train the decoder.\n3. The experimental results show that the idea can be applied to and improve many kinds of detection transformers.\n\n## Weakness\n1. The explanation about why this method could work is not sufficient, i.e., the reason why the authors do this modification and why it works need further explanation. For now, we only know that it could work, but providing more about why could bring more insights and values to the community.\n2. The explanation of the method increases the number and diversity of positive samples seem can only to explain it can help the model coverage faster. But the paper shows that it still works when training 50 epochs. Will the method still work for a longer training schedule (, e.g., by 150 or 300 epochs)? If yes, does the explanation claimed in the paper still make sense?\n3. How to apply the method to some dense version DETRs (such as DINO) is not described in detail in the paper. The paper only provides experiment results for the combination of DINO and Group DETR. Furthermore, given the minor improvements over DINO,  can the explanation that increasing the number and diversity of positive samples still holds for these dense version DETRs?\n4. There might be some contradiction in the explanation in the last several sentences in Sec 4.4. For example, although all similar queries in different groups are positive samples in Group DETR, the positive query in one group may also be negative samples in another group (or in other groups, there might be some queries that are very similar to the positive query in the current group), which is not shown in Figure 6.\n5. Typos: group-truth matching -> ground-truth matching in the last sentence of the 2nd paragraph in Sec.1.",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\nThe paper is clear to follow.\n\n## Quality\nThe experimental results is complete and sufficient, and the quality is thus not bad.\n\n## Novelty\nThe proposed method can be seen as another version of Denoising DETR, which somehow limits its novelty.\nFurthermore, the paper does not provide a convincing explanation about why it works.\n\n## Reproducibility\nSome details about GroupDETR + DINO is missing, which makes some results cannot be easily reproduced.",
            "summary_of_the_review": "Overall, this paper adopts a simple yet effective strategy to train detection transformers and brings consistent improvements in both performance and convergence speed on multiple kinds of detection transformers.\nHowever, the paper does not give a sufficient explanation or insights about how and why it works, which limits its contribution and makes the paper more like a technical report rather than a paper for this venue.\nFurthermore, some implementation details are missing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper105/Reviewer_5yMc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper105/Reviewer_5yMc"
        ]
    },
    {
        "id": "fqGv3y_yAW5",
        "original": null,
        "number": 3,
        "cdate": 1666599669112,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599669112,
        "tmdate": 1666599669112,
        "tddate": null,
        "forum": "ptbePrczhRt",
        "replyto": "ptbePrczhRt",
        "invitation": "ICLR.cc/2023/Conference/Paper105/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a training approach to improve the performance of DETR-like object detectors. Namely, it add more query groups as the decoder inputs, and each group of queries is completely independent of each other during training. It means that the original one-to-one label assignment is adopted for each group. At inference, only the first group of queries is retained and the rest are discarded. The paper conducts experiments with a few DETR-like detectors and show various improvements over its original counterpart.",
            "strength_and_weaknesses": "Strengths\n++ The proposed training method is conceptually simple and can be adopted by different DETR-like methods.\n++ There are adequate experiments, including 2D and 3D object detection, instance segmentation. The proposed method can bring benefits to different extend.\n++ The paper is easy to follow.\n\nWeakness\n-- The technical novelty is limited. Similar to DN-DETR, the motivation of group DETR is simply adding more training to the decoder. Moreover, the choice of retaining the first group seems very hand-designed and lack a theoretical explanation.\n-- There are some key experiments missing. First, for a baseline (a standard DN-DETR, for example), what is the performance using K*N queries, comparing to group DETR using K groups of N query/group?  Second, although the total training epochs are the same between group DETR and its counterparts, the training time increases for group DETR (~15% increase). So, what is the performance of training a baseline with 15% more training epochs? In this case, the total training time is comparable, rather than the training epochs.\n-- The increase in GPU memory (i.e. almost double) is significant.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality, clarity and originality is good. ",
            "summary_of_the_review": "As stated in the strengths and weakness, this paper shows some improvements over its counterparts, however, the proposed group queries seems more like an engineering trick rather than a novelty. Furthermore, it lacks some important experiments.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper105/Reviewer_MZFA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper105/Reviewer_MZFA"
        ]
    }
]