[
    {
        "id": "6IWOvz7DQXT",
        "original": null,
        "number": 1,
        "cdate": 1666352747472,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666352747472,
        "tmdate": 1666352747472,
        "tddate": null,
        "forum": "S0v71vsLBYhM",
        "replyto": "S0v71vsLBYhM",
        "invitation": "ICLR.cc/2023/Conference/Paper1890/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a deep kernel learning method for Gaussian processes. Deep kernel learning parametrises the covariance function using a neural network, and the proposed method further optimises a set of inducing points in the transformed space as well as employs stochastic gradient descent, allowing for faster inference. The authors demonstrate the expressiveness of their method through experiments on tabular data for regression, image classification, and classification of text and graphs.\n",
            "strength_and_weaknesses": "### Strengths\nDeep kernel learning is an interesting field for the GP community, so the paper should be interesting to that part of the ICLR community. The paper also builds on recent advances in mini-batch optimisation for GPs, which is an exciting approach, and the empirical results are quite impressive.\n\n### Weaknesses\nThe main weakness, in my view, is the novelty, which I consider limited. Deep kernel learning with learnable inducing points in the learnt feature space has been explored several times (for instance by Bradshaw et al. (2017), which the authors cite, and Ober et al. (2021)), and the fact that the authors highlight this as a novelty is somewhat misleading, I think. The main thing, in my view, that distinguishes this paper from others is that the authors use mini-batch optimisation of the inducing point log-marginal likelihood from Snelson & Ghahramani (2005), whereas other papers (at least the ones I could find) seem to use a variational sparse approximation in the learnt feature space. This is interesting since the convergence of SGD for GP inference was only recently established (Chen et al., 2020), but it is still an incremental change. This is also reflected in section 2, which is simply a summary of sparse GP regression and classification.\n\nWith that in mind, I think the experimental section is a little uninteresting. DKL has been shown to be a powerful approach many times, so the good performance of the proposed method is not that surprising. Instead, given the incremental nature, I would have been more interested in seeing a comparison between this method and other ways of using inducing inputs in feature space. For instance, Bradshaw et al. (2017) use a variational sparse approximation and optimise the resulting ELBO. Is this approximation better or faster than the one proposed here? Ober et al. (2021) show that DKL is prone to overfitting; is that also the case here? Similarly, the reason for computing the inducing outputs is not clear to me (see the questions in the next section), and it would have been interesting to see a comparison of ways to handle the outputs (e.g., representing them with a function as proposed vs optimising their values directly vs integrating them out).\n\n\n### References\n- Bradshaw et al. (2017): https://arxiv.org/abs/1707.02476\n- Chen et al. (2020): https://arxiv.org/abs/2111.10461\n- Ober et al. (2021): https://arxiv.org/abs/2102.12108",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, reproducible (the authors provide code for the experiments, though I did not try to run this), and the quality is generally good. As mentioned above, I think the novelty is limited.\n\n\n### Questions for the authors\n1. You explicitly represent the inducing outputs $\\mathbf{r}$ as the output of a function on the inducing inputs. Generally, however, in the sparse GP framework, the inducing outputs are integrated out. What is the reason for keeping them here? Further to this, since the inducing outputs are scalars, why not simply optimise these scalars directly?\n2. You mention that the DKL method by Wilson et al. (2016) uses inducing inputs in the input space. However, my understanding of the method is that the inducing points are placed in the learnt feature space since they replace the base covariance matrix with the KISS covariance matrix (Eq. (8) in Wilson et al. (2016)). Where do you see that the inducing inputs are placed in the input space?\n3. What is a \"straw man method\"?\n4. You mention that inducing points add explainability to the model. How so? I understand that you can look at the inducing inputs (or the nearest real inputs), but what will this tell you?",
            "summary_of_the_review": "The proposed method is a variation of deep kernel learning for Gaussian processes. The novelty seems to be that the authors use mini-batch stochastic gradient descent to optimise the inducing point log-marginal likelihood from Snelson & Ghahramani (2005) instead of a variational objective used in previous work. This is an incremental change, however, and the experimental section is a little off in my opinion; instead of only showing that the method performs well on a range of tasks, there should have been a focus on comparing it to the other strongly related methods, trying to understand the strengths and weaknesses of each approach.\n\nDeep kernel learning as a topic is interesting to the GP community broadly, and the proposed method does achieve good empirical results, which should make it useful for practitioners as well as a strong baseline for future papers. However, given its current focus (and, in my opinion, incorrectly claimed novelty), I think it's not strong enough for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1890/Reviewer_MpyF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1890/Reviewer_MpyF"
        ]
    },
    {
        "id": "zKE7hCLwNu",
        "original": null,
        "number": 2,
        "cdate": 1666605814723,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605814723,
        "tmdate": 1666605814723,
        "tddate": null,
        "forum": "S0v71vsLBYhM",
        "replyto": "S0v71vsLBYhM",
        "invitation": "ICLR.cc/2023/Conference/Paper1890/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Inducing points have traditionally been heavily relied on to alleviate the cubic computational complexity associated with Gaussian process training and inference. There have been various different ways in which inducing points have been used in the literature, but they have nearly always been limited to either the input space of the data, or else were constrained by either the positions (e.g., to form a grid structure) or choice of embedding (such as trigonometric representations of the input space). In this work, the authors propose an approach for learning the inducing points along with the deep kernel function itself, which no longer has to be pre-trained. This allows for greater flexibility in the adaptation of inducing points since they are not constrained by the complexity or structure of the input space. The authors demonstrate how the proposed Gaussian process network models (IGN) achieve comparable performance to existing techniques on standard tabular and image data, while also being easily extendable to datasets having more specialized structures, such as text and graphs.",
            "strength_and_weaknesses": "- The paper identifies an interesting limitation of existing inducing point approaches, and sets up a clear problem statement that is well framed in the context of related work. Showing how the proposed model can be extended to classification tasks via the Laplace approximation also makes the paper come across as more complete.\n- I appreciated how the paper considered an extensive selection of varied datasets in the experimental evaluation, which effectively demonstrates how the proposed IGNs can be used by practitioners across different use cases. The ablation studies assessing how the variance changes as more inducing points are added is also interesting, if not entirely surprising.\n\n------\n\n- I believe that some of the experiments would benefit from a more thorough deep dive into why certain results were obtained. For example, IGNs appear to perform quite inconsistently on the tabular regression datasets where the method varies from over-performing on some datasets while comparing less favorably to competing techniques on others.\n- I expect there to be a substantial risk of overfitting using this scheme, especially compared to methods using variational inference to optimize the location of inducing points. I believe this to be a particularly important consideration for practitioners, however I didn\u2019t find much discussion in the paper.\n- I would have liked to see the method be framed in a similar context to the discussion featured in _\u201cUnderstanding probabilistic sparse gaussian process approximations\u201d_ by Bauer et al (2016) which presents a more principled discussion of how different inducing point GP approximations compare against each other. Such insight is currently missing from this work.\n- Although the paper stresses improved computational complexity as being one the primary benefits of using IGN models, this comparison is currently relegated to the supplementary material, whereas I believe this would also have been interesting to feature in the main paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Although I found the problem statement to be both interesting and relevant, the contributions currently rely too heavily on obtaining empirical gains, whereas the overall theoretical motivation is less complete.\n- The paper is well-written overall and fairly easy to follow. There are some aspects of the derivation which I think could be clarified further however, such as the role and selection of the pseudo-label function.\n- There are a few typos and minor careless mistakes in the paper\u2019s writing, but these should be fairly easy to iron out in a future revision of the paper. The references need to be properly cleaned however - certain words (e.g. Gaussian) need to be consistently capitalized across paper titles, and the detail included for citations also needs to be properly homogenized.\n- There appears to be sufficient detail included in the main paper and supplement for reproducing the featured experiments. Appropriate tables and plots with error bars were also included throughout.",
            "summary_of_the_review": "I think there are some solid ideas in this paper that are backed by a fairly extensive experimental evaluation. Nevertheless, I feel quite ambivalent about the paper\u2019s novelty and contributions overall, and would like to see more depth and insight included in a future revision of the paper. I am presently borderline on this work, tending towards rejection in the belief that there is still considerable room for improvement on the current submission.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1890/Reviewer_r62e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1890/Reviewer_r62e"
        ]
    },
    {
        "id": "uiCDa3FCzV",
        "original": null,
        "number": 3,
        "cdate": 1666674563661,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674563661,
        "tmdate": 1666674563661,
        "tddate": null,
        "forum": "S0v71vsLBYhM",
        "replyto": "S0v71vsLBYhM",
        "invitation": "ICLR.cc/2023/Conference/Paper1890/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new Gaussian process model that combines inducing point technique and deep kernel learning. The proposed method can be used for regression and classification. Experimental results verify the effectiveness of the proposed method on various types of real data.",
            "strength_and_weaknesses": "S1. The authors proposes a scalable learning method for Gaussian processes with deep kernel. The proposed method is simple but useful.\n\nS2. Experiments demonstrate the effectiveness of the proposed method on various datasets.\n\nW1. Although the proposed method is practically useful, it seems a straightforward combination of inducing points and deep kernel learning.\n\nW2. There is no discussion of predictive variance. It would be good to find out how accurately it can be estimated by the proposed method.\n\nW3. There is still some concern about the number of inducing points, although an ablation study on it has been done in Section 5. This hyperparameter may be quite difficult to determine, since the appropriate value may vary depending on the difficulty of the problem and the dimension of z.",
            "clarity,_quality,_novelty_and_reproducibility": "- This manuscript is well-written.\n- Technical novelity is somewhat low.\n- Although the authors have experimented with a variety of datasets, some concerns remain.",
            "summary_of_the_review": "I agree that the proposed method is simple but useful. But the technical novelity seems moderate.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1890/Reviewer_dTCT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1890/Reviewer_dTCT"
        ]
    }
]