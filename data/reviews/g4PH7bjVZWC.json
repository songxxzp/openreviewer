[
    {
        "id": "Pt1SorQ4Ncg",
        "original": null,
        "number": 1,
        "cdate": 1666267450209,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666267450209,
        "tmdate": 1666267450209,
        "tddate": null,
        "forum": "g4PH7bjVZWC",
        "replyto": "g4PH7bjVZWC",
        "invitation": "ICLR.cc/2023/Conference/Paper460/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of minimising an objective function over all $s$-sparse vectors. In order to solve this non-convex problem, the authors study stochastic mini-batch of version of the well-known iterative hard thresholding algorithm.\n\nThis paper makes the following contributions:\n1. It establishes a stochastic gradient descent property (Theorem 2).\n2. This property is used to show that the objective values at each iteration form a super martingale, which is claimed to imply convergence. \n",
            "strength_and_weaknesses": "Unfortunately, the manuscript at hand seems very incomplete. Thus, at its current state it is impossible to assess its contributions. \n\n1. The current paper does not contain a single. Thus, I am not able to see whether some of those statements are correct..\n2. Some of the assumptions for the theoretical statements are too vague and not clearly stated. (I will give some examples below.)\n3. The current paper does not include any simulations. It would be great if the authors could include some numerical experiments to corroborate their theoretical results. This is maybe less important in comparison to the first two points but I also think that a complete submission should also contain numerical experiments.\n\nHere are some more specific points:\n1. Lemma 2: You say that B is a random set. What is the probability distribution? Where is it specified? In a similar vein, what is the probability that the Bernoulli random variable $z_i (B)$ is one? Are the  $z_i (B)$  i.i.d. random variables?\n2. Lemma 3: What is meant by \"either of the following holds\". I only can see one statement.\n3. Theorem 2: What is the distribution of the random set $\\mathcal{J}$?\n4. Theorem 4: How is this random variable $f^{\\star}$ characterized?\n\n\nTypos:\n1. page 1, abstract: \"...we do not assume the function to be (a) restricted strongly convex.\"\n2. Lemma 4: \"Assume $g(x,\\omega)$ be (is) an...\"\n3. Theorem 2: \"where $ B \\subset  {1; ... ; N } $ be (is)\"\n4. Claim 2, definition of $f(x, \\Xi)$: Is there an $f^{(i)}$ too much? Also should $f^{(i)} be $f$ instead?!  \n5. Claim 2: Should \"$VI_J^T I_J=...$\" read \"$ VI_J^T I_J V^T=...$\" instead?\n6. Theorem 4: differential -> differentiable?!",
            "clarity,_quality,_novelty_and_reproducibility": "As described in the above section, due to the lack of details (proofs + unclear statement of assumptions in theoretical results) it is very hard to reproduce the results and the paper is very unclear. \n\nI even find it hard to make a thorough assessment of the novelty and quality of the results.",
            "summary_of_the_review": "Due to the shortcomings described above, the paper seems to be very incomplete and I cannot recommend acceptance.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper460/Reviewer_HcXm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper460/Reviewer_HcXm"
        ]
    },
    {
        "id": "8TTp2z6AKAV",
        "original": null,
        "number": 2,
        "cdate": 1666679987464,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679987464,
        "tmdate": 1666679987464,
        "tddate": null,
        "forum": "g4PH7bjVZWC",
        "replyto": "g4PH7bjVZWC",
        "invitation": "ICLR.cc/2023/Conference/Paper460/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors study the mini-batch Stochastic IHT (SIHT) algorithm for solving sparse optimization problems.  A new lower bound is found to help design the mini-batch size. The loss sequence generated by SIHT is proved to be a supermartingale sequence and converges with probability one. ",
            "strength_and_weaknesses": "1. A new lower bound is provided.\n2. The designed method is proven to be supermartingale.",
            "clarity,_quality,_novelty_and_reproducibility": "It is hard to evaluate as there is a lack of proof in Theorems and lemmas.",
            "summary_of_the_review": "The reviewer assumes the submission is incomplete work. There is a lack of proof in Theorems and lemmas. No experiments to demonstrate the designed method is better/worse than current methods.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper460/Reviewer_jszD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper460/Reviewer_jszD"
        ]
    },
    {
        "id": "_VlOI05VSE",
        "original": null,
        "number": 3,
        "cdate": 1666825906504,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666825906504,
        "tmdate": 1666825906504,
        "tddate": null,
        "forum": "g4PH7bjVZWC",
        "replyto": "g4PH7bjVZWC",
        "invitation": "ICLR.cc/2023/Conference/Paper460/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors study mini-batch stochastic iterative hard thresholding (IHT). IHT is a well known algorithm for sparse optimization with extensive literature. IHT is essentially $\\ell_2$-projected gradient descent on the set of s-sparse vectors. Given the ML-wide focus on stochastic optimization algorithms, it is natural to study the guarantees of stochastic IHT.\n\nThe main result of the paper is a martingale convergence result that claims stochastic IHT converges to the optimal function value, although no convergence rate is given.",
            "strength_and_weaknesses": "Strengths\n- Advancing the theoretical understanding of stochastic IHT is an important task.\n- The writing is generally easy to follow.\n\nWeaknesses\n- The literature review is quite limited.\n- The proofs are missing, so it is not possible to fully verify the claimed results.\n- In Claim 2, the minimum singular value should be 0, because the matrix is low-rank. If the authors define minimum singular value differently, it is hard to know because the proof is not included.\n- In Claim 2, an upper bound is derived for c (10). Based on this, $c$ seems to be $\\geq N^2$. When I plug that back into Theorem 2, I get a batch size of roughly $N - \\Theta(1)$. This does not seem like a great choice for a batch size, which typically should be much less than $N$. It would be great if the authors could comment on whether I am missing something here.\n- There are no experiments, which could shed light for example on what is the value of c in practice.",
            "clarity,_quality,_novelty_and_reproducibility": "On novelty, I find that at least one very related paper is missing from the discussion [1] (probably others too). This paper presents a convergence analysis of stochastic IHT for functions that have the restricted smooth and strongly convexity assumptions. By the way, the strong convexity assumption can be guaranteed by regularizing the loss function. \n\nOn reproducibility, some of the results like Lemmas 3-4 seem true, while e.g. Claim 2 is hard to verify. Again, since the proofs aren't included, this has a negative effect on reproducibility.\n\n[1] \nAc/dc: Alternating compressed/decompressed training of deep neural networks\nPeste, Alexandra and Iofinova, Eugenia and Vladu, Adrian and Alistarh, Dan",
            "summary_of_the_review": "In summary, while I like the problem of minibatch stochastic IHT and the approach, I find that there are significant issues with novelty that should be resolved in the literature review section, as well as my other concerns outlined above. I encourage the authors to continue working on the manuscript since I think this is an important subject. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper460/Reviewer_mF9G"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper460/Reviewer_mF9G"
        ]
    }
]