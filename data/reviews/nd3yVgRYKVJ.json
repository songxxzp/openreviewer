[
    {
        "id": "7QJSWiw_ZWg",
        "original": null,
        "number": 1,
        "cdate": 1666340120015,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666340120015,
        "tmdate": 1666340153808,
        "tddate": null,
        "forum": "nd3yVgRYKVJ",
        "replyto": "nd3yVgRYKVJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1908/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies maximum entropy reinforcement learning in an actor-critic setting. Two improvements are proposed. First, a learnable state-dependent weighting between two critics is intended to balance over- and underestimation bias of the value estimates provided by the critic. Second, the temperature parameter of the entropy regularizing the actor is also made a learnable state-dependent variable in order to smooth the entropy policy exploration. The suggestions are incorporated into SAC and results are presented.",
            "strength_and_weaknesses": "The paper is written clearly. However, the organization of the paper does not convince me. A lot of space is used to describe the base algorithm SAC and very little space to describe the suggested method. For example, equation 7 and especially equation 15 describing the soft update seem unnecessary to state in the paper. Instead, I suggest the authors include at least a short description of the function modeling the state-dependent factors in the main paper and not just in the appendix. Further, Figure 1 showing the mujoco environments is not needed in the main paper and Figures 2 and 3 could be made such that they use a lot less space by plotting the legend only once and rearranging the plots in one or two rows. This would allow the experiment about the value estimates to be in the main paper.\n\nConceptually, I do not understand and also can not see a given argument why updating the parameters of the state-dependent networks $\\lambda$ and $\\mu$ according to eq. 11(i.e. the actor loss)  makes sense.\n\nEvaluation:\nThe evaluation is poor. There is no reason why the proposed method APAA is compared with several baselines in Figure 2 and an extra Figure is used to compare it with SAC-t (SAC with automatic entropy tuning). This could and should all be in one Figure. Then suddenly HalfCheetah is not showing in the second Figure and judging from the performance of APAA on HalfCheetah in Figure 2 this is because it is far worse than SAC with entropy tuning. Further, the baselines for HalfCheetah in Figure 2) have clearly worse performance than what is otherwise reported in the literature.\nI do not see justifications for the claim that APAA has an 'overwhelmed advantage [...] over other baselines'.\n\nRelated work: There are some papers coming to my mind that are relevant but not mentioned. In [1] a parameter Is learned to penalize overestimation of the critics. [2] adapts a parameter online during training that balances over-and understimation of the critics. Other works use a (stochastic) weighting of critics to generate better value estimates [3, 4].\n\n\n[1] Cetin, Edoardo, and Oya Celiktutan. \"Learning pessimism for robust and efficient off-policy reinforcement learning.\" arXiv preprint arXiv:2110.03375 (2021).\n\n[2] Dorka, Nicolai, Joschka Boedecker, and Wolfram Burgard. \"Adaptively Calibrated Critic Estimates for Deep Reinforcement Learning.\" Deep RL Workshop NeurIPS 2021. 2021.\n\n[3] P. Lv, X. Wang, Y. Cheng, and Z. Duan. Stochastic double deep q-network. IEEE Access, 7:\n79446\u201379454, 2019. doi: 10.1109/ACCESS.2019.2922706.\n\n[4] Zongzhang Zhang, Zhiyuan Pan, and Mykel J. Kochenderfer. Weighted double q-learning. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI\u201917, pages 3455\u20133461. AAAI Press, 2017",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly, although more explanations about the proposed method would be useful.\nThe work is rather incremental on top of SAC. At the same time, the results are not thorough and strong enough to make the work very impactful.\nThe algorithm is described in detail (partly in the appendix) and should be easy to reproduce.",
            "summary_of_the_review": "The paper does not convince me why the proposed method makes conceptual sense. The evaluation is not thorough and hence also does not convince me that the proposed method is very useful.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1908/Reviewer_6A7k"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1908/Reviewer_6A7k"
        ]
    },
    {
        "id": "L_hsjo3rNzy",
        "original": null,
        "number": 2,
        "cdate": 1666538858199,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666538858199,
        "tmdate": 1666538858199,
        "tddate": null,
        "forum": "nd3yVgRYKVJ",
        "replyto": "nd3yVgRYKVJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1908/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to use adaptive pairwise critics and adaptive asymptotic maximum entropy to address the estimation errors.",
            "strength_and_weaknesses": "Pros: (1) The paper writing is clear.\n\n(2) The experiments show the benefit of the proposed algorithm in several tasks.\n\nCons: (1) The main concern I have is why a per-state adaptive variable/weight benefits training. For example, for the state-dependent adaptive random weight that assigns weights to the two critics, is it actually mitigating the overestimation or something else? \n\n(2) The introduction mentioned bootstrapping error, extrapolation error, overestimation error, etc. I'm confused about how the proposed method addressed them. Analysis that supports the error-mitigation claim is lacking.\n\n(3) I'm also interested in which of the above errors are the contributing factors in policy degradation and which source of error the method addresses. I don't think a single design can address all of them at once since they are clearly not the same type of error.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written.",
            "summary_of_the_review": "A deep and thorough analysis to support the authors' claims is lacking.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1908/Reviewer_gnMQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1908/Reviewer_gnMQ"
        ]
    },
    {
        "id": "mUXgJPe_jV",
        "original": null,
        "number": 3,
        "cdate": 1666589079121,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589079121,
        "tmdate": 1666589874163,
        "tddate": null,
        "forum": "nd3yVgRYKVJ",
        "replyto": "nd3yVgRYKVJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1908/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper deals with two issues in deep reinforcement learning. One is to reduce the estimation bias by learning a state-dependent weighting function for two Q functions. The other is to tune a state-dependent coefficient of the entropy regularization term of the policy. The state-dependent weighting and coefficient functions are updated as part of the policy parameter. The proposed method is evaluated on the standard MuJoCo benchmarks, and the experimental results show that the proposed method outperforms the baselines. ",
            "strength_and_weaknesses": "Strength\n1. The authors propose a state-dependent coefficient of the entropy regularization term that can be updated during learning. The update rule is different from SAC-v2.\n2. To reduce the estimation bias, multiple Q-value estimates are often used. The proposed method adaptively mixes two Q functions, where mixing weights are state-dependent and learnable. \n\nWeakness\n1. The proposed method does not reduce the estimation bias explicitly. Specifically, the estimation bias was not evaluated in the experiments, although it is one of the main paper's motivations. \n2. The state-dependent coefficient of the entropy term is not explained well. In particular, it is unclear why it is not given as a simple function \\alpha(s). \n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The joint Q-value function is given by the weighted sum of two Q-functions, and the weight is the parameterized function of the state. It is similar to modular reinforcement learning, where $\\Gamma (s)$ is the responsibility signal. In that sense, the joint Q-value function may use a deeper neural network than those used in the previous methods. Therefore, the ablation study on how the joint value function reduces the estimation bias is needed. \n\n2. The authors do not explain why the coefficient is given by $\\alpha \\Lambda_\\lambda (s) + k$, which is more complicated than $\\alpha (s)$. Then, the range of $\\Lambda_\\lambda (s)$ is not shown in the manuscript. I think $\\Lambda_\\lambda (s) \\in (0, 1)$ because Figure 4 in the appendix shows that the activation function of the output layer is the sigmoid function. Is it correct? In addition, the proposed method needs more hyperparameters ($\\alpha$ and $\\beta$) than SAC-v2. Finally, the ablation study on how the adaptive entropy function improves the performance by comparing APAA and SAC-v2 using the joint Q-value function.\n\n3. I am unsure whether Lemmas 1 and 2 are helpful for the proposed method because they do not consider the adaptive pairwise critic functions and adaptive asymptotic actor. For example, the proposed method updates $\\Gamma(s)$ and $\\Lambda(s)$, but the authors do not discuss the convergence. \n\n4. The equations in Appendix are referred to in the main text. For example, Lemma 3 and 4 should be Lemma 1 and 2, respectively. Equation number (17) should be (5) for consistency.  \n\n5. Duan et al. (2021a) and Duan et al. (2021b) are the same references. \n",
            "summary_of_the_review": "Although this paper proposes a new idea, the experimental results do not support the main claim. Therefore, the ablation studies discussed above are needed to understand how the proposed method improves performance. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1908/Reviewer_9zpF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1908/Reviewer_9zpF"
        ]
    },
    {
        "id": "Gga3oAFi-zS",
        "original": null,
        "number": 4,
        "cdate": 1666648064900,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648064900,
        "tmdate": 1666685335943,
        "tddate": null,
        "forum": "nd3yVgRYKVJ",
        "replyto": "nd3yVgRYKVJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1908/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new algorithm over a baseline like SAC with 2 main modifications: a) adaptively learn a weight that balances between 2 critics and controls how the Q target value is computed b) adaptively learn a temperature for max entropy exploration. \n\nThe authors claim the proposed method can better balance the tradeoff between efficiency and stability. The proposed method is evaluated on 5 gym tasks and compared to a number of other baselines to show that the proposed method has better performance. Some theoretical results and some empirical analysis are provided. ",
            "strength_and_weaknesses": "Strength:\n- technical details: 10 seeds for each experiment, and hyperparameter table in the appendix, it seems in most cases the same hyperparameter setting is used, which is good and makes results more reliable. \n- Novelty: adaptive bias control can be an interesting and somewhat novel method\n- Results: compared to baselines are somewhat good assume they are reliable\n- Simplicity: method is not too complicated\n\nWeaknesses: \n\n**Technical details**\n- With the additional parameters and the extra complexity, how does your method compare to others in terms of computation and parameter efficiency? \n- Although your adaptive parts are learned, there are some new hyperparameters, how hard it is to tune them? \n- You are combining two main modifications, might want to ablate them\n\n**Reliability of results and fairness of comparison**\n- Why does SAC get 2000 in HalfCheetah? SAC is a very popular baseline and should get at least about 12K score at 1e6. \n- For Figure 3, it seems a target entropy of 0.5 is chosen for SAC adaptive? Wouldn\u2019t it reduce the performance of SAC adaptive if you arbitrarily set a temperature? What makes that a fair comparison? \n\n**related work**\n- \u201cHow to design a mixture of Q-values is still largely left untouched. \u201d not exactly sure if I agree with that. Figuring out how how get more accurate Q values is certainly important, but there are a number of other recent works that deal with this, in the paper there is no discussion on how your work relate to or has an advantage over these newer methods. (though I do acknowledge these works do not study adaptive bias control methods)\n- Randomized Ensembled Double Q-Learning: Learning Fast Without a Model (ICLR 2021)\n- Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics (ICML 2020)\n- Dropout Q-Functions for Doubly Efficient Reinforcement Learning (ICLR 2022)\n\n**soundness of arguments**\n- It is unclear to me how the stability issue is evaluated. And why is there a tradeoff between efficiency and stability? Shouldn\u2019t a more stable algorithm be also more efficient? The discussion on this point seems to be a bit vague, and there is no quantitative analysis on the results, for example, if it is about stable performance across seeds, you might want to measure that and show your method actually works better (has a better measured value) than other methods. If it is about more stable performance over test runs for a single training seed, you want to measure sth else. \n\n**writing and clarity**\n- Some minor typos or citation formatting issues, for example:\n- \u201cthe twin delayed deep deterministic policy gradient (TD3) Fujimoto et al. (2018) for example\u201d\n\u201cSeveral of recent works deal with errors like bootstrapping error caused by out-of-distribution (OOD) actions Kumar et al. (2019; 2020)\u201d \n- Some parts of writing is not clear for example \u201cIn Hopper environment, since the converged value is far lower than other benchmarks, the tolerance for the fluctuation around convergence is much lower, which causes\u2026\u201d would be good to go through the paper carefully and rewrite some of the arguments for better clarity. \n- and there is a number of other issues please go through your paper and fix these writing issues. \n\n**novelty**\n- not exactly sure how novel the adaptive entropy part is... SAC adaptive already has some adaptiveness there\n\nOther questions:\n- is beta your only new hyperparameter? If so you might want to emphasize it\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The method is not too hard to understand, but some of the writing can be improved for better clarity\n- Quality: The overall quality can be improved\n- Novelty: Not a big change from existing methods, but the adaptive part can be seen as somewhat novel\n- Reproducibility: technical details, a hyperparameter table are provided to help reproducibility. ",
            "summary_of_the_review": "The proposed method can be seen as somewhat novel and interesting, however, I have a lot of concerns on writing clarity, and on reliability of the comparison results. Additionally, the paper can benefit from more ablations, hyperparameter sensitivity study, and the claim on tradeoff should be backup up with more quantitative comparisons. \n\nA bit difficult to recommend the paper at its current state. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1908/Reviewer_djw4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1908/Reviewer_djw4"
        ]
    }
]