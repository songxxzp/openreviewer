[
    {
        "id": "oi-sBBKZiV",
        "original": null,
        "number": 1,
        "cdate": 1666603208642,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603208642,
        "tmdate": 1666603208642,
        "tddate": null,
        "forum": "lb8wXVGWn0E",
        "replyto": "lb8wXVGWn0E",
        "invitation": "ICLR.cc/2023/Conference/Paper1433/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose to learn visual words to explain deep neural networks specifically for the task of image recognition. Certain visual words are learned based on the target category and these words are encouraged to be shared between different categories. Moreover, they also include the attention guided semantic alignment that encourages the leaned visual words to focus on the same conceptual regions for prediction. They show results on several benchmark datasets on both accuracy and interpretation fidelity. They also show that their method is generalizable to ground visual words for unseen categories,.",
            "strength_and_weaknesses": "Strengths - The paper tackles the problem of understanding and interpreting  deep neural networks by learning visual words and grounding the words aligned with the attention of the base model. They perform experiments on several benchmarks and present  several analysis. \n\nWeaknesses - \n\n1. The idea about aligning the maps of visual words to the base model attention map will prevent stronger deviations from learning wrong attentions but at the same time also will prevent the model to learn correct disambiguations. How do you ensure that the models learn part speciifc attentions for the learned visual words ? \n\n2. Also what would happen when the base model's attention is completely wrong ? \n\n3. The contributions are very thin compared to previous literature  and the improvements in the IoU metric is obvious since the model's objective is to improve the overlap between attentions. But does this metric correctly state that the model is more interpretable ? \n\n4. It would be worthwhile to share some failure cases of the model to get a more in depth idea of what are the limiting factors.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with analysis and experiments and can be reproduced from the details in the paper.",
            "summary_of_the_review": "I have some concerns regarding the contributions and the training process adopted to encourage interpretability. It is not very clear if the scores are high because of correct attention map guidance or something else. An in-depth evaluation of learned visual words, failure cases and how the attention maps differ from baseline when these are wrong would be interesting to see. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1433/Reviewer_HEcT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1433/Reviewer_HEcT"
        ]
    },
    {
        "id": "IxGh-tQm7H",
        "original": null,
        "number": 2,
        "cdate": 1666684761910,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684761910,
        "tmdate": 1666684761910,
        "tddate": null,
        "forum": "lb8wXVGWn0E",
        "replyto": "lb8wXVGWn0E",
        "invitation": "ICLR.cc/2023/Conference/Paper1433/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a learnable visual word model that consists of two modules to predict model prediction behaviors. Concretely, a semantic visual words learning model is proposed to relax the category-specific constraint and thus enable the generic visual words shared across multiple categories. A dual fidelity preservation module is proposed to encourage the learned visual words to focus on the same conceptual regions for prediction.\n",
            "strength_and_weaknesses": "Strength \n1. I think model interpretability is quite an interesting task. \n2. The paper is well-written and easy to understand.\n\nWeakness\n1. It seems the novelty is limited as it is merely a simplification of the current work ProtoPNet. It simply removes the separation loss. However, the motivation for removing this loss is not well clarified. Besides, as it is a key contribution of this work, but its effectiveness is also not verified in the experiments.\n2. It seems that semantic alignment is also used in many model interpretability works. Its originality should also be better clarified.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the originality is limited.\n",
            "summary_of_the_review": "I think the originality is limited, and the experiments seem not sufficient.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1433/Reviewer_Tm8e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1433/Reviewer_Tm8e"
        ]
    },
    {
        "id": "zbek_7diLb",
        "original": null,
        "number": 3,
        "cdate": 1666829468162,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666829468162,
        "tmdate": 1666829468162,
        "tddate": null,
        "forum": "lb8wXVGWn0E",
        "replyto": "lb8wXVGWn0E",
        "invitation": "ICLR.cc/2023/Conference/Paper1433/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a learnable visual word method to interpret the model prediction behaviors. The authors proposed 2 new modules: semantic visual word learning and dual fidelity preservation. The semantic visual word removes the separation loss from the protopnet, and the dual fidelity preservation adds attention alignment with the grad-cam outputs. Experiments show the proposed method achieves good performance on the metric. ",
            "strength_and_weaknesses": "[Strength]\n\n- The paper is well-written and clear. \n\n- Experiments results show the proposed method is better than previous methods. \n\n[Weakness]\n\n- My major concern with the proposed method is its novelty. The proposed LVW basically follows ProtoPNet's objective and removes the separation loss. The ProtoPNet's objective is also learnable. \n\n- The metric of IoU is also problematic, it's unfair to compare to other methods using this metric since the proposed method trained with l2 normalization loss. \n\n- What is the performance with separation loss? It seems there is no such number in the paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clearly written. and seems reproducible. ",
            "summary_of_the_review": "This paper provides two improvements over the prior ProtoPNet's architecture: 1: remove the separation loss; 2: add attention-guided alignments. The proposed IoU metric is a little bit concerning since the model directly optimizes for this target compared to other methods. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1433/Reviewer_zev4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1433/Reviewer_zev4"
        ]
    }
]