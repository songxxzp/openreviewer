[
    {
        "id": "Bojnpyuuzr",
        "original": null,
        "number": 1,
        "cdate": 1666064169392,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666064169392,
        "tmdate": 1666064169392,
        "tddate": null,
        "forum": "BDjGGZk9yz",
        "replyto": "BDjGGZk9yz",
        "invitation": "ICLR.cc/2023/Conference/Paper6460/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors proposed a supervised random features (SRF) regression method that combines the ideas of RF kernel learning and of (simple) neural networks (NNs) model.\nThe authors claimed that the proposed SRF approach improves a few previous efforts in that it yields better performance with a relatively small amount of data (when compared with, e.g., kernel neural networks), and can be computed very efficiently (when compared with, e.g., implicit kernel learning or kernel alignment methods mentioned at the end of the second page). However, none of these claims are well-supported by solid and rigorous theory.\nSome limited experiments were provided on a few datasets and a few simple models to illustrate the advantages of the proposed SRF approach.\n",
            "strength_and_weaknesses": "**Strength**: I do not see very strong points in the paper, from either a theoretical or empirical viewpoint.\n** Weaknesses**: There is (almost) no theory in the paper, and the baselines compared in the experiments are weak. The presentation of the paper is, in general, poor.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The presentation in this paper can be clearly improved.\n**Quality and Novelty**: The theoretical and empirical contributions in the paper are of limited significance and novelty. Please see my detailed comments below.\n**Reproducibility**: good.",
            "summary_of_the_review": "As I mentioned above, the theoretical and empirical contributions in the paper are of limited significance and novelty, and the presentation in the paper can be clearly improved. Please see my detailed comments as follows: \n* P1, abstract: my personal, and possibly naive, understanding of the neural network (NN) model is that it is parameterized by a sequence of weight matrices and bias terms, and therefore a parametric model in some sense.\n* P1, introduction: \"reduce the extensive hand tuning form the user for training\": I get confused by this sentence, does this mean classical kernel learning needs a lot of hand tuning and that can be avoided by applying RF techniques?\n* Please use $\\max$ instead of max, $\\sin$, and $\\cos$ for sin and cos activation functions, respectively.\n* P2: The author claimed that this contribution improves kernel neural networks (KDL), which need a huge number of data, by proposing a novel supervised RF approach that works well even with a limited amount of training data. This advantage, however, is only evaluated empirically, no solid theoretical arguments are provided in the paper.\n* P2: Most computational or statistical advantages of the proposed SRF approach are stated without any empirical or theoretical evaluations.\n* P2 and P3: the introduction contains extremely long and wordy paragraphs that try to discuss the advantages of the proposed SRF approach, which is hardly readable and not easy to understand. It would be helpful to at least divide them into subsections or paragraphs.\n* The figures are hardly visible, which clearly harms the readability of the paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6460/Reviewer_ZaoF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6460/Reviewer_ZaoF"
        ]
    },
    {
        "id": "s2r_-feQjM",
        "original": null,
        "number": 2,
        "cdate": 1666547429278,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666547429278,
        "tmdate": 1666547550812,
        "tddate": null,
        "forum": "BDjGGZk9yz",
        "replyto": "BDjGGZk9yz",
        "invitation": "ICLR.cc/2023/Conference/Paper6460/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an adaptation of the random feature method approach for kernel methods akin to a two-layer neural network. ",
            "strength_and_weaknesses": "The literature on RF models uses a single layer for analytical tractability. This manuscript does not provide any analytical results on the two-layer extension.\n \nThe claimed interpretability aspects are not clear to me - the inherent use of random features would obscure the interpretation of the learning process. From the description of the numerical experiment it appears that the approach to interpretability may involve multitudes of RF models, but this does not seem like a comparable approach to that of other methods that rely on a single learned model.\n\nThe other claimed contributions are also present in the original RF method.\n\nTable 1 and Figure 1 are not surprising - two-layer models perform better than one when the learning problem is sufficiently difficult.\nFigures 1 and 3 should be larger (unreadable text).\n\nTable 2 also does not show a clear distinction between existing methods and the proposed on performance.\n\nUnfortunately it is not clear to me how these weaknesses can be addressed in a revision.",
            "clarity,_quality,_novelty_and_reproducibility": "There are several parts of the paper that in my opinion are not sufficiently detailed to be clear.\n\nThe description in page 6 is confusing - the test design here sues prior knowledge of the covariates involved; this is in contrast to methods that evaluate each covariate under some score function and show distinct values for covariates involved vs. others. Similarly, the description in Page 8 does not clearly state how the covariates were found to be significant (e.g., how the \"ranking\" is performed).\n\nAlgorithm 1 step 3 refers to eq. 4, but the equation does not provide a method to obtain supervised random features. It is also not clear how eqs. 5 and 6 is to be minimized in Steps 4 and 5, respectively. These should more clearly describe the minimization procedure (which I assume would describe how optimal coefficients alpha and beta are found).\nSimilarly a clearer description of the initialization of the estimators for the f_j should be provided (which seems to me would have to restrict to a class of candidates).\n\nGiven that the proposed approach is a combination of two well-known methods, the numerical results do not provide clear trends, and there is no analytical contribution, my opinion is that the novelty is scant.",
            "summary_of_the_review": "One would expect that a dual-layer extension of RFs would perform better than a single layer version. Furthermore there are no analytical contributions for the extension proposed. Finally, several portions of the paper are not sufficiently detailed to make a clear argument for the claimed new benefit of interpretability. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6460/Reviewer_hYRF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6460/Reviewer_hYRF"
        ]
    },
    {
        "id": "g4OMsBfEu2n",
        "original": null,
        "number": 3,
        "cdate": 1666871687113,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666871687113,
        "tmdate": 1666871687113,
        "tddate": null,
        "forum": "BDjGGZk9yz",
        "replyto": "BDjGGZk9yz",
        "invitation": "ICLR.cc/2023/Conference/Paper6460/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an approach for boosting the effectiveness of kernelized Ridge regression by first learning a set of random features through a deep-learning inspired preprocessing step, then applying kernelized Ridge regression to the learned features.",
            "strength_and_weaknesses": "\n- **Strength:** While much work has been done on constructing arbitrarily expressive kernels [1, 2] and scaling up kernel-based regression methods, more work is needed to make learning more efficient in kernel methods. Borrowing from deep learning to learn expressive families of kernels, as attempted by this paper, is laudable.\n\n\n- **Weaknesses:**\n\n*Clarity:* The paper is hard to read and contains too many typos.\n\n*Comparison to Deep Learning:* It is unclear why this approach would perform better than vanilla deep learning. A strong intuition and substantially more experiments are needed to make this case.\n\n*Comparison to expressive kernel:* This paper is missing much of the literature on expressive kernel methods, especially Generalized Spectral Kernels [1]. In particular, [2] introduced kernel families (namely GSKs) that are general-purpose in that they contain kernel that can perform as well as any other kernel not in the family, stationary or non-stationary. Additionally, a flurry of methods have been developed to scale up kernel regression. It would have been interesting to discuss what benefits this approach has over GSKs.\n\n- **Additional Comments:** Page 1: No condition is required for Eq (1) to be Kernel Ridge regression. Basis function regression with Ridge penalty is always kernelized Ridge regression. The kernel implied by Eq (1) is random and the behavior as $N \\to \\infty$ pertains to the convergence of the random kernel to a deterministic kernel.\n\n\n[1] Samo, Y.L.K. and Roberts, S., 2015. Generalized spectral kernels. arXiv preprint arXiv:1506.02236.\n[2] Samo, Y.L.K., 2017. Advances in kernel methods: towards general-purpose and scalable models (Doctoral dissertation, University of Oxford).",
            "clarity,_quality,_novelty_and_reproducibility": "- **Clarity:** The paper could be made easier to read and could benefit from some proofreading to iron out typos (e.g. Page 2: \"This is because that the RF\", \"Comparing to other kernel methods that mapping x to a high dimensional space [...]\" etc.)\n\n- **Originality:** I did not find any idea in this paper particularly original.\n",
            "summary_of_the_review": "The paper should be proofread, and more intuition and more experiments should be added to argue the benefits relative to vanilla deep learning or expressive kernel methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6460/Reviewer_iHFk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6460/Reviewer_iHFk"
        ]
    }
]