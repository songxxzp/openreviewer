[
    {
        "id": "yBhi5G8drE",
        "original": null,
        "number": 1,
        "cdate": 1665685767635,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665685767635,
        "tmdate": 1665685767635,
        "tddate": null,
        "forum": "uBKBoix9NXa",
        "replyto": "uBKBoix9NXa",
        "invitation": "ICLR.cc/2023/Conference/Paper4368/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "For binary neural networks, some hyperparameters and optimization tricks such as weight decay lose their meaning. In this paper, the authors reinterpret these methods through a perspective of gradient filtering. Specifically, if the binary weights are taken to be the sign of latent weights the authors show that SGD with momentum and weight decay is equal to smoothing the gradient twice with an EMA filter. \n\n\n",
            "strength_and_weaknesses": "Strengths:\n\n* The authors provide a clear and intuitive explanation for how hyperparameters interact when training binary networks with SGD.\n* The authors clearly verify their mathematical arguments experimentally.\n\nWeaknesses:\n\n* The mathematical arguments are rather simple and not surprising. \n* It is not clear how the analysis extends to adaptive optimizers such as Adam. Note that Adam is used for the imagenet experiments.\n* The authors refer to \u201cour optimizer\u201d, but it seems to me like this is just a way to parametrize well-known methods. \n* Only results matching SOTA are reached.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Regarding the mathematical arguments in the paper, the clarity could be improved by using more formal language with theorems, assumptions, and so on explicitly stated. The current mathematical exposition is rather hand-wavy. One weakness is that the analysis might not extend to adaptive optimizers, could the authors comment on this?\n\nThe paper also repeatedly refers to \u201cour optimizer\u201d. Could you clarify what this optimizer is?\n\nRegarding the experiments, the figures and experimental setup are rather clear. However, I find the hyperparameters to be rather strange. E.g. a batch size of 510 is used. Could the authors motivate their choice of hyperparameters? Furthermore, could the authors discuss how the choice of hyperparameters for their method and their competitors be discussed?\n\nThe empirical results are also broken down by one or two-stage training. Is there any reason why two-stage training might not be preferred? It seems to give better results, but maybe it consumes more resources? Furthermore, could your method be applied to two-stage training and then improve the final results?\n\n\n\n",
            "summary_of_the_review": "The paper visits an interesting topic and brings some relevant and thoughtful, albeit maybe simple, mathematical arguments which illuminate how hyperparameters interact when using SGD for binary networks. It is not clear how the analysis extends to adaptive optimizers, and what exactly the proposed method is is also not clear. While the authors clearly verify the theoretical results, the method does not seem to bring any performance benefits over two-stage methods. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4368/Reviewer_H9io"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4368/Reviewer_H9io"
        ]
    },
    {
        "id": "xWAlROZFFK",
        "original": null,
        "number": 2,
        "cdate": 1665784445427,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665784445427,
        "tmdate": 1668978922412,
        "tddate": null,
        "forum": "uBKBoix9NXa",
        "replyto": "uBKBoix9NXa",
        "invitation": "ICLR.cc/2023/Conference/Paper4368/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides an analysis of standard optimization hyperparameters like weight initialization, learning rate, weight decay,  and momentum in the context of the latent-weight free interpretation of binary neural networks.\n\nIn a binary neural network, all weights and activations are +1 or -1. Although the weights are binary, they are typically optimized with real-valued gradients. In the latent-weight free interpretation, optimization can be viewed as a weight-flipping algorithm. When the gradients have accumulated past a certain threshold, the optimizer flips the weights.\n\nBut what should our understanding of traditional optimizer hyperparameters like weight initialization and learning rate be if we are to view optimization as weight-flipping? This paper argues that we should interpret them as impulse response filters on the gradients. This understanding reduces the number of hyperparameters that need to be tuned.",
            "strength_and_weaknesses": "Strengths:\n- This paper builds upon the latent-weight free interpretation of binary neural networks, and provides a new understanding of traditional optimization hyperparameters in such a setting.\n\nWeaknesses:\n- This paper fails to demonstrate a significant empirical benefit from their new interpretation of binary neural network optimization beyond a reduction in the number of hyperparameters in some cases.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and original. While the authors did not include code with their submission, they have stated that they intend to release their code to make their experiments reproducible.",
            "summary_of_the_review": "This paper provides an interesting new understanding of optimization in the binary neural network setting. However, it also fails to show significant empirical benefits from such an understanding.\n\nI am on the fence about recommending this paper for acceptance, because it's not clear to me how the community can use this newfound understanding to significantly advance our theoretical understanding of binary neural networks or their empirical application. However, I also have low confidence in my assessment. I think the authors can improve the paper by adding new empirical or theoretical results, or at least sketching out how their contribution is significant to the community.\n\nEdit: Thanks for the authors' response. The quotes from relevant papers in the literature has increased my confidence in the significance of the work. Even though the empirical contributions are still weak, I see the great value this piece of work brings in filling this gap in the literature. I have updated my score to reflect that. I highly encourage the authors to re-write the Introduction section to emphasize the significance of this work by highlight the gaps in the literature and how this work fills them.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4368/Reviewer_YST8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4368/Reviewer_YST8"
        ]
    },
    {
        "id": "6GxB-lqqf3",
        "original": null,
        "number": 3,
        "cdate": 1666649779622,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649779622,
        "tmdate": 1666649779622,
        "tddate": null,
        "forum": "uBKBoix9NXa",
        "replyto": "uBKBoix9NXa",
        "invitation": "ICLR.cc/2023/Conference/Paper4368/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Binary Neural Networks use values of -1 and +1 for the network's weights instead of floating point number to save on memory. Successful training of these networks, and their ultimate performance, depends on several hyperparameters. This paper redefines or removes many of these hyperparameters. This means that there are fewer hyperparameters to tune and removes the need for latent weights. Experiments support that there is little loss in performance compared to methods with latent weights.",
            "strength_and_weaknesses": "**Strengths**\n- The main idea of the paper is simple.\n- Removing hyperparameters and reducing the need for tuning is important---especially in architectures that are large or expensive to train.\n- The paper is easy to follow.\n\n**Weaknesses**\n- Experiments are limited to two models and datasets, both of which are image classification.",
            "clarity,_quality,_novelty_and_reproducibility": "- Figure 1 seems pretty unnecessary. I imagine most readers will be familiar with with binary vs real valued numbers.\n- The analysis is quite simple. This is a good thing, but I don't know this area well enough to comment on how novel it is.",
            "summary_of_the_review": "The paper is clear and presents a nice idea. The experiments support the main claim of the paper but are limited in scope. I do not know the literature on Binary Neural Networks well enough to comment on the paper's novelty.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4368/Reviewer_AvuJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4368/Reviewer_AvuJ"
        ]
    },
    {
        "id": "QGgvb-qlf1",
        "original": null,
        "number": 4,
        "cdate": 1666834342162,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666834342162,
        "tmdate": 1669910748459,
        "tddate": null,
        "forum": "uBKBoix9NXa",
        "replyto": "uBKBoix9NXa",
        "invitation": "ICLR.cc/2023/Conference/Paper4368/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the analysis of hyperparameters in binary neural networks (BNN). The hyperparameters could explain the BNNs with latent real-valued weights during training. However, the magnitude of binary weights is not interpretable with these hyperparameters yet. This paper provides an interpretation of these magnitude-based hyperparameters based on higher-order gradient filtering during network optimization.",
            "strength_and_weaknesses": "Pros:\n- This paper has good motivation for interpreting the magnitude-based hyperparameters on BNN. \n- It provides some interesting insights such as: \n  - A gradient filtering perspective on latent weight hyperparameters.\n  - A justification of which magnitude-based hyperparameters to use and offer a setting with fewer hyperparameters to tune.\n\nCons:\n- It would be better to add a summary paragraph in section 3 to state their concise conclusion of magnitude-based hyperparameters.\n- As mentioned, the performance improvement is marginal in Table 2 based on ReActNet-A. \n- The paper claimed that it provided a \u201csimplified\u201d setting with fewer hyperparameters to tune. However, I did not see the big advantages of the proposed optimizer compared to latent-weight optimization in terms of accuracy, and training/optimization speed.\n- The experiment section is relatively weak as the experiment is given on one architecture based on a few baseline methods, \u201cReActNet-A\u201d on Imagenet and \u201cBi-Real\u201d on CIFAR. It is expected to show how the proposed optimizer performs on several recent BNN optimization methods with various types of architecture and different types of datasets (modality such as visual and text, distribution such as natural, digital, fine-grained) and task (classification, detection, and segmentation.)\n- The metrics in Table 2 haven't been explained well. More details should be given.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper well clarifies the interpretability issue of binary BNN weights and mag-based hyperparameters. The writing is good and easy to follow; yet the experiment is expected to be strengthened. \n\nThe paper does not provide code and pseudo code/algorithm flow. \n",
            "summary_of_the_review": "Overall, this paper provides some interesting insights for interpreting binary weights of BNN to magnitude-based hyperparameters. However, the major concern arises in the lack of experiments to support their statements. Also, the proposed optimizer does not offer many advantages compared with the latent-weight-based optimization.\n\n**Post after rebuttal**\n\nI appreciate the additional experimental results and detailed responses provided by the authors. After checking the latest revision in the manuscript, I decided to raise my score to encourage further exploration of hyperparameters for binary neural networks. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4368/Reviewer_gyrn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4368/Reviewer_gyrn"
        ]
    }
]