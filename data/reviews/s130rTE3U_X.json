[
    {
        "id": "jU45HCxnVL",
        "original": null,
        "number": 1,
        "cdate": 1666667113352,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667113352,
        "tmdate": 1666667113352,
        "tddate": null,
        "forum": "s130rTE3U_X",
        "replyto": "s130rTE3U_X",
        "invitation": "ICLR.cc/2023/Conference/Paper3621/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the role of nonlinearity in training dynamics of contrastive learning. In general, understanding the role of nonlinearity is a very important problem in deep learning. Basically, the authors show that nonlinear models can recover multiple patterns while the linear model can only recover the single pattern that corresponds to the maximal eigenvalue. Moreover, the authors provide simulations to validate their theory.",
            "strength_and_weaknesses": "## Strength \n- This paper studies a very important problem, i.e., what is the role of nonlinearity in contrastive learning? They study 1/2 layer neural networks with a general $\\mathbb{C}_{\\alpha}$ loss function. In particular, they show that linear neural networks can only learn one feature corresponding to the maximal eigenvalue, while the nonlinear neural network can recover multiple features.\n\n- This writing is quite good.\n## Weaknesses\n- Though the authors provide simulations to verify the proposed theory, it is still not clear how realistic the theory is and what is the gap between the considered scenario and the realistic settings. \n- At the end of page 2, the authors mention that in this paper they consider the case that $\\alpha$ is fixed. My concern is how realistic is this assumption. Taking InfoNCE as an example, $\\alpha$ depends on the neural network parameter $\\theta$. I suspect the reason that one reason the authors think $\\alpha$ can be fixed is the existence of the stop gradient operator. However, in reality, people will modify it dynamically even if the existence of stop gradient operator. Hence, the setting considered in this paper might not be realistic.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-structured and reads well. The perspective of counting the number of patterns seems very interesting to me.",
            "summary_of_the_review": "Please see the strength and weaknesses section.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3621/Reviewer_U4MU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3621/Reviewer_U4MU"
        ]
    },
    {
        "id": "F1elpmXWGSi",
        "original": null,
        "number": 2,
        "cdate": 1666674778894,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674778894,
        "tmdate": 1666692225610,
        "tddate": null,
        "forum": "s130rTE3U_X",
        "replyto": "s130rTE3U_X",
        "invitation": "ICLR.cc/2023/Conference/Paper3621/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies the role of nonlinearity in the training dynamics of contrastive learning on one and two-layer nonlinear networks with homogeneous activation. The authors obtain two major results. First, for one-layer neural networks, the nonlinearity can lead to many local optima and each corresponding to certain patterns from the data distribution, while sfor the linear counterpart only one major pattern can be learned. Second, in the two-layer case, the nonlinearity is capable of learning specialized weights into diverse patterns. Besides, the authors discover global modulation namely those local patterns discriminative from the perspective of global-level patterns are prioritized to learn. Finally, the authors conduct experiments to verify theoretical findings.",
            "strength_and_weaknesses": "**Strength**\n\n* This work studies the role of nonlinearity in contrastive learning which is a critical problem and of great interest to the deep learning theory community.\n\n* The paper presents an insightful and novel idea by introducing $\\alpha$-CL that proposes a general CL framework that covers a broad\nfamily of existing CL losses.\n\n* I like the solid theoretical analyis of this work most. From one-layer to two-layer neural network, the authors fully demonstrates the role of nonlinear activation functions in contrastive learning, by contrasting with linear activations. \n\n* The empirical analysis connecting the presented theory is very interesting.\n\n\n**Weakness**\n\n* The related works part is somewhat inadequate. A missing related work [1].\n\n* The mathematical notation of the article is somewhat comfusing, especially when demontrating the energy function $\\mathcal{E}_\\alpha(\\boldsymbol{\\theta})$. Perhaps, the authos could unify the representatin of vector, matrix and scalar.\n\n* Assumption 2 is strong, especially for no augmentation. In my understanding, the augmenttation is indispenable part of contrastive learning.\n\n[1] On the Impact of the Activation function on Deep Neural Networks Training. ICML 2019. Soufiane Hayou, Arnaud Doucet, Judith Rousseau",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is very clear. Very organized and enjoyable to read.\n\n* The paper is of high quality.\n\n* The paper is quite novel to me.\n\n* Question to authors:\n   + Have you studied the case without disjoint receptive fields, which means the exact neural network case?\n   + Do you think global modulation can be used to inspire new algorithms or applications?",
            "summary_of_the_review": "The paper presented a relatively novel idea (to me) that studies nonlinearity in contrastive learning. It presented a set of thorough theoretical analyses and experiments to validate the method. I tend to accept this paper. However, the authors are encouraged to address the weakness I list above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3621/Reviewer_h1XP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3621/Reviewer_h1XP"
        ]
    },
    {
        "id": "oZhbHM0GOs",
        "original": null,
        "number": 3,
        "cdate": 1666745905368,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666745905368,
        "tmdate": 1666745905368,
        "tddate": null,
        "forum": "s130rTE3U_X",
        "replyto": "s130rTE3U_X",
        "invitation": "ICLR.cc/2023/Conference/Paper3621/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers training dynamics of simple neural networks on contrastive learning tasks, and studies the effect of non-linearities such as the ReLU on the resulting optima. In the setting considered, the authors find that linear activations lead to simple solutions involving leading eigenvectors of some fixed covariance-like matrix, while the use of non-linear activations leads to a significantly richer picture, with different choices of stationary points that may lead to more diverse features. In the two-layer case, the authors show a \"global modulation\" effect which may better capture latent variables. These theoretical results are illustrated with numerical experiments on a synthetic dataset involving token sequences with some latent structure.",
            "strength_and_weaknesses": "The paper provides interesting and novel insight on the role of non-linear activations in contrastive learning. In particular, the finding that non-linearities induce more diverse representations, while linear activations only recover leading components, is quite interesting and significant.\n\nThe theoretical analysis also provides useful formulations of the training dynamics for a few different network models, and studies useful conditions for convergence to certain relevant stationary points, namely what the authors call \"locally maximal eigenvectors\" (Definition 3).\n\nThe work also points to interesting directions for future work, for instance on obtaining the probability of finding specific stationary points and on the role of over-parameterization for this setup.\n\nIn this sense, this is a good paper and I support acceptance.\n\nSome aspects of the work could nevertheless be improved to strengthen the paper:\n\n* it is currently unclear if the benefits of the non-linearity are due to (i) the fact that linear activations only recover the top principal component instead of all components, or (ii) non-linear activations are allowing a truly non-linear behavior that is not captured by PCA. It would help to discuss whether the provided examples (in section 3.1, and in the latent-variable model used in the experiments) are an instance of (i) or (ii), and whether a simple procedure like PCA could already recover these latent features. (on this point: are there variants of these CL algorithms/losses that can recover multiple PCA components instead of just the leading one?)\n\n* various results in the paper are still valid even when there are no augmentations, a setting which is often considered difficult in practice due to the risk of collapse. How is the present work positioned in this regard? Is such a collapse avoided here by having a restricted model? If so, could this have practical implications for CL without augmentations?\n\n* the presentation could sometimes provide more insight and intuition. For instance, in the two-layer case (section 4) it would be useful to see what the involved quantities are in a specific example, such as the one from section 5. In Theorem 5, what is the intuition behind the $Delta_k Delta_k^\\top$ term, and how it arises?\n\nminor additional comments:\n- related work: it would be helpful to include other works on training dynamics for CL here and provide a clear comparison to them (e.g. Tian 2022 or Jing et al. 2021), even if they are cited elsewhere in the manuscript.\n- \"$alpha$ is fixed\": some comments on what is lost by considering $alpha$ fixed instead of having it evolve would be a helpful addition\n- after Thm 1 \"lowering roughness could lead to more local optima\": this should be clarified\n- sec. 3.3 \"Let L be the Lipschitz...\": the existence of L should be an assumption\n- Def. 4: add a brief explanation of $\\kappa$?\n- sec. 4 \"how does the network prioritizes\" -> prioritize\n- \"receptive field\": \"patch\" or \"token\" seems more appropriate? but feel free to keep unchanged\n- it would be helpful to write the precise form of the 2-layer network before Lemma 4.\n- after Thm 4 \"the gradient\" -> gradients? \"all points\" -> all point?\n- sec 4.2 \"close form\" -> closed form\n- Figure 5: d = 8 seems to be inconsistent with the d=20 in the text? does it mean you have fewer tokens in this experiment?",
            "clarity,_quality,_novelty_and_reproducibility": "see above",
            "summary_of_the_review": "good paper. Some questions should be clarified re: non-linear learning vs simply finding multiple principal components",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3621/Reviewer_TWc2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3621/Reviewer_TWc2"
        ]
    },
    {
        "id": "s7XJQS-BwQ",
        "original": null,
        "number": 4,
        "cdate": 1666970587754,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666970587754,
        "tmdate": 1666970587754,
        "tddate": null,
        "forum": "s130rTE3U_X",
        "replyto": "s130rTE3U_X",
        "invitation": "ICLR.cc/2023/Conference/Paper3621/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present an interesting training dynamics perspective on the role of nonlinearity in contrastive learning, specifically geared towards self-supervised learning. To this end, they leverage the $\\alpha$-CL loss formulation and study the training dynamics for a single layer and 2-layer MLP. Firstly, the authors show that a linear single-layer MLP is only capable of learning the principal directions in the data but a single-layer MLP with reversible nonlinearity (e.g. ReLU) has several attractors in the state space and can thereby learn more complex data distributions. Furthermore, they extend their analysis to two-layer MLPs and demonstrate the ability to learn specialized weights and the phenomenon of global modulation that alters the structure of the attractor basin. Taken together, this work advances our theoretical understanding of contrastive learning dynamics and enables better network design for specific applications.",
            "strength_and_weaknesses": "Strength:\n1. This work presents a strong theoretical analysis of the dynamics of contrastive learning. \n2. The analysis spans both single and 2-layer MLPs. Although it is unclear to me immediately if these results could be extended (using some form of induction) to deeper networks, it is still useful to see the effect of depth and how it interacts with non-linearity.\n3. The theoretical inferences are validated using simple empiricial simulations, which supports the theoretical result and also indicates the robustness of the theoretical results even when the assumptions are not satisfied in practice.\n\nWeaknesses:\n1. In its current form, I feel the paper might be a bit hard to parse for deep learning practitioners who are also interested in theoretical understanding of contrastive learning. I believe this limits the impact of this paper. Furthermore, it would be nice to have a better intuition into what each assumption entails and the context behind each theorem or theoretical result. Currently, the context seems to follow the theoretical result, explaining what it implies to the reader, but adding some context of what theoretical insight is desired might help the reader follow the work better. \n2. The core results of this work rely on the $\\alpha$-CL formulation and is validated using toy data simulations. It would be nice to extend this to slightly complicated datasets, like maybe MNIST, to illustrate how valid the results are for realistic setups. I believe this would greatly enhance the utility of the work and can be impactful to the SSL community.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing lacks sufficient clarity. I believe rewriting to address weakenss point 1 could improve the clarity of the presentation.\nThe work is definitely of high quality and offers several theoretical insights about contrastive learning setup. To the best of my knowledge, the theoretical analysis and results are novel.",
            "summary_of_the_review": "I think this work has interesting insights about dynamics of contrastive learning and would be highly valuable to the SSL community. In its present form, it might be a bit hard to distill the information to the community, but with minor rewriting, it can be a very impactful paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3621/Reviewer_Nm96"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3621/Reviewer_Nm96"
        ]
    }
]