[
    {
        "id": "71R21MeA8Re",
        "original": null,
        "number": 1,
        "cdate": 1666552030130,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666552030130,
        "tmdate": 1666552030130,
        "tddate": null,
        "forum": "n_SwMH9o-oT",
        "replyto": "n_SwMH9o-oT",
        "invitation": "ICLR.cc/2023/Conference/Paper5521/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper investigates how a specific fairness metric varies with the weights initialization and the batches data ordering used in the training of neural networks on three datasets. The authors find that the order in which data observations are fed to the networks during training significantly affects the variance in the fairness metric and conduct experiments to corroborate this claim.",
            "strength_and_weaknesses": "The authors make an interesting observation. This, however, seems to be only a starting point and more discussion and experiments are needed. I'll detail below some of the reasons behind this consideration. \n\nFirst, section 3 is titled \"Sources of variance\" but the only two sources of variance that are considered are weights initialization and random reshuffling. Batch size (which is fixed, I believe), architectures (two, fixed), and possibly other parameters are not discussed. These parameters most likely influence variance. In particular, as batch size increases, random order of the data will play less of a role. Why hasn't batch size been considered?\n\nSecond, I would expect data reshuffling to possibly play a role in the initial epochs but less in the later ones when the weights have converged to one of the minima. I'm not sure why this doesn't occur in the experiments. For this, it would be useful to see how overall accuracy increases with the number of epochs. Relatedly, I am very surprised that there is the similar amount of variance for a fixed epoch after 100 and 200 epochs. We could look into that by visualizing the curves for each individual model in figure 3 right, rather than the distribution for a given epoch. \n\nThird, the authors consider three datasets and only one measure fairness measure. Does this behavior generalize to other fairness measures? Why was this fairness measure chosen?\n\nFourth, section 5 is based on the idea of reweighing the data distribution to change the fairness properties of the model evaluated on the original data distribution. This idea has been widely studied in the literature (both in the fairness and machine learning in general) and I'm not sure that I fully understand where the novelty is, especially how this affects the . The observed behavior may also not even persist as one tunes the batch size and increases the number of epochs (see https://arxiv.org/abs/1812.03372).\n\nA paper that was not cited but is certainly relevant is https://arxiv.org/abs/2107.10171\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and the experiments are, in principle, reproducible. I don't think that the work is significantly novel. ",
            "summary_of_the_review": "The paper builds upon an interesting observations but more work is needed in order to support the conclusions drawn in the paper (e.g., those at the end of section 4.2). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5521/Reviewer_ofwu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5521/Reviewer_ofwu"
        ]
    },
    {
        "id": "doKcBKlT7o-",
        "original": null,
        "number": 2,
        "cdate": 1666636197507,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636197507,
        "tmdate": 1670356824621,
        "tddate": null,
        "forum": "n_SwMH9o-oT",
        "replyto": "n_SwMH9o-oT",
        "invitation": "ICLR.cc/2023/Conference/Paper5521/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper observes that the performance of a deep learning model might exhibit a high variance between different training instances and hence this might lead to a high variance in the performance gap across different groups. The paper proposes a solution based on the reordering of data, to lower the variance.",
            "strength_and_weaknesses": "---------------------------------\nStrengths:\n---------------------------------\n-- The paper is fairly well-written and it is easy to understand the paper.\n\n---------------------------------\nWeaknesses:\n---------------------------------\n-- The technical contribution is limited. The paper does not provide any algorithms or analysis.\n\n-- While having theory is not always required, I find the empirical analysis of the paper to be insufficient. The paper mainly focuses on one dataset and there should be more empirical evidence to justify the provided solution. More intuition about the proposed solution would also be appreciated. \n\n-- I may have not fully misunderstood the paper, but what is the rationale behind lowering the variance besides the training time? Can the results be repeated and averaged to lower the variance?\n\n---------------------------------\nDisclaimer:\n---------------------------------\n-- I do not an expert on the topic and it is possible that I have missed the major points of the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow, and it is generally well-written. I am not an expert in the area so I cannot assess the novelty of this work. It seems like similar observations have been made in a setting with no fairness consideration. In that sense, observing variance in performance when restricting to a subset of data is not surprising. The authors claim that the code is attached but I do not see a link. ",
            "summary_of_the_review": "In summary, this paper is outside my area of expertise but based on what I understand, the contributions and novelty are limited.\n\n----------------------------------------\nPost Rebuttal:\n----------------------------------------\nI want to thank the authors for their detailed responses. I have read the other reviews and the rebuttal and I stand by my original assessment.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5521/Reviewer_XbQB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5521/Reviewer_XbQB"
        ]
    },
    {
        "id": "NGmi-0fGP8",
        "original": null,
        "number": 3,
        "cdate": 1667688371665,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667688371665,
        "tmdate": 1667688371665,
        "tddate": null,
        "forum": "n_SwMH9o-oT",
        "replyto": "n_SwMH9o-oT",
        "invitation": "ICLR.cc/2023/Conference/Paper5521/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tries to find empirically validated reasoning for the cause of high variance in fair deep learning, where the fairness metric of interest is average odds (AO), i.e., the average disparity between true and false positive rates. This is generally attributed to randomness (what they call non-determinism) in training, e.g., randomness in data order and weight initialization. A series of empirical evaluations are conducted that investigate the source of this variance. The following assertions are made in light of the empirical evidence:\n-  Variance (wrt to AO) can be attributed more to data shuffling than to weight initialization. \n- A model's fairness is predictable based on the most recent training points.\nThe work also asserts that a proxy for understanding model fairness across runs can be variance in fairness across epochs. Moreover, the work shows that sub-group performance on the fairness metric can be manipulated by rearranging the training data instances. \n",
            "strength_and_weaknesses": "Strengths\n- The topic of this work is important, i.e., what are some of the causes of high variance in fair deep learning?\n- The concept of rearranging data instances to change sub-group performance is technically interesting. \n\nWeaknesses \n- There are other sources of randomness besides weight initialization and the training data, e.g., various regularization techniques and train/fit/validation sets (as the authors mentioned in the text). Can the authors provide reasoning for why only weight init and data ordering are considered in this work? \n- Unfortunately, I do not understand section 3.2 (see `clarity' section for questions) ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n- sec 2 first para. Could please explain these datasets in the appendix so that readers do not have to hunt in numerous different works? The datasets are important to the empirical evaluations, and the additional description will make the work more modular. \n- description of the experimental setup is clear \n- sec 3.2 Can you be exact in what it means for predictions to change? Does this mean that before including the data instance, the model would have predicted the instance incorrectly? In light of this confusion I am unable to understand Figure 4b. \n- sec 4 is well-written and has a nice experimental setup\n\nNovelty: I am unfamiliar with work related to this paper but did a quick search on arXiv, and did not find similar work. \n\nReproducibility: I believe I could reproduce this work if need be---enough experimental detail has been given throughout the work and in the appendices. \n\nSmall suggestions:\n- section 6 second-to-last line: \"practise of\" to \"practice of\" \n- section 5 first para last line: \"and place at the\" to \"and placed at the\" \n- missing period eqn (1) \n",
            "summary_of_the_review": "The paper empirically shows that the order of data instances in model training can impact the quality of the averaged odds fairness metric, introduces variance in fairness performance across epochs as a proxy for understanding fairness across runs, and shows that sub-group performance on the fairness metric can be manipulated by rearranging the training data instances. Some aspects of the experimental evaluation related to model predictability and recent data instances are unclear. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5521/Reviewer_Qe5p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5521/Reviewer_Qe5p"
        ]
    }
]