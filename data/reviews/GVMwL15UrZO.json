[
    {
        "id": "41HQMZzi4Tq",
        "original": null,
        "number": 1,
        "cdate": 1666580056038,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580056038,
        "tmdate": 1666580056038,
        "tddate": null,
        "forum": "GVMwL15UrZO",
        "replyto": "GVMwL15UrZO",
        "invitation": "ICLR.cc/2023/Conference/Paper5828/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes two main contributions to the direction of HTML modeling/understanding:\n\n1. A new task and dataset, **Description Generation**, derived from CommonCrawl with 85K (HTML, element, description) tuples. \n\n2. Experiments across a suite of three HTML tasks (Description Generation, semantic classification from Gur et al., MiniWob web navigation), investigating the importance of pre-training corpora (natural language and/or HTML), model architecture (encoder-only, decoder-only, encoder-decoder), the scaling effect, and so on. The main finding is the effectiveness of large language models (LLM) pre-trained on natural language and fine-tuned on HTML task data, with T5-like encoder-decoder architecture being the best.",
            "strength_and_weaknesses": "Pros: The direction of HTML understanding is increasingly important, as recent work has started to show the promise of utilizing the semantics of HTML and web navigation. The paper adds solid contributions to this direction by creating a novel and interesting task, and conducting many empirical studies with insights about architecture, (sub-linear) scaling effect, importance of context window/action history, etc. \n\nCons: \n\n- On the proposed Description Generation task, authors only spend two paragraphs presenting results, where WebD-T55-3B can obtain 90.8% accuracy (exact string match??). Does it mean the task is too easy? How to justify it from the existing semantic classification task, as in Figure 1(a) they look similar? Why in Table 3, dev numbers are much lower than test set? I feel the new task/dataset needs more explanation and justification.\n\n- The modeling part lacks novelty, just basic pre-training + fine-tuning on LLMs, and seems there's no model design based on the features of HTML?\n\n- On MiniWoB, it's not very surprising that using a larger model with pre-training, you can beat smaller models from scratch. Instead, the fact that LLMs are worse than RL performances should be discussed more -- does it mean the task is bad, or RL is important (but it's hard to do RL for LLM)?\n\n- (Minor) In intro authors claim \"no processing of HTML\", but there is still some pre-processing?\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is generally clear and easy to understand, some tables and figures in experiments are confusing or hard to read.\n\n- Novelty: The new task is somewhat interesting and novel. The modeling part is not that novel.\n\n- Reproducibility: Authors claim to open-source the Description Generation dataset.",
            "summary_of_the_review": "I believe the paper makes solid empirical contributions by introducing a new task, and studying many LLM variants on three HTML tasks, with some insights about what's important for modeling HTML. My main concern is that the new task is not clearly justified, and the modeling part might be too standard -- there is no HTML-specific model design involved, just studying basic LLMs. I think the paper can benefit from more evidence about the value of the new benchmark (e.g. what new insights/modeling capabilities it enables), or more interesting modeling.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5828/Reviewer_d1QN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5828/Reviewer_d1QN"
        ]
    },
    {
        "id": "Fgd1GEiQZfD",
        "original": null,
        "number": 2,
        "cdate": 1666627993907,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627993907,
        "tmdate": 1666628671163,
        "tddate": null,
        "forum": "GVMwL15UrZO",
        "replyto": "GVMwL15UrZO",
        "invitation": "ICLR.cc/2023/Conference/Paper5828/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper investigates whether LLMs can be applied to HTML understanding to produce better-performing, more sample-efficient HTML understanding models without the need for custom NN architecture design. To achieve this goal, the authors present a suite of three benchmarking tasks for HTML understanding: (1) Semantic Classification of HTML elements, (2) Description Generation for HTML inputs, and (3) Autonomous Web Navigation of HTML pages. The authors find that a) pretraining is critical for the performance and can reduce labeled data requirements / improve sample efficiency up to 200x; b) model architecture is the second most important factor, and T5 models with bi-directional attention and encoder-decoder architecture performs the best across the board; c) given a choice, model size should be evaluated in the context of the models training and inference performance, as the model size sub-linearly predicts its performance. ",
            "strength_and_weaknesses": "Strengths:\n1. The paper is well-written and easy to follow.\n2. The authors conduct comprehensive evaluations and analyses over a range of architectures, dataset sizes, and baselines to make the experimental conclusion convincing.\n3. The authors create and open-source a new dataset/benchmark for HTML understanding.\n\nWeaknesses:\n1. The novelty is primarily empirical, and the technical approach is essentially an application of known techniques (the standard pretrain-finetune paradigm of LLMs).\n2. I examined the supplementary material provided by the authors and only found the data files. It would be better if there were codes to prove its reproducibility.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Most things are explained clearly either in the main paper or the appendix, except for the few points that are raised in the Weaknesses above.\nQuality: well-written work.\nNovelty: The novelty is primarily empirical, and the technical approach is essentially an application of known techniques (the standard pretrain-finetune paradigm of LLMs).\nReproducibility: The Supplementary Material only provides the data file. It is better to make the codes available for reproducing these results. ",
            "summary_of_the_review": "In my opinion, the audience could gain insights from the thorough empirical analysis of HTML understanding from this paper. However, the lack of technical novelty & contribution makes this work slightly below the acceptance bar of the conference. Overall, I recommend a weak reject. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5828/Reviewer_wKTj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5828/Reviewer_wKTj"
        ]
    },
    {
        "id": "4KEhfqKJDLi",
        "original": null,
        "number": 3,
        "cdate": 1666666091754,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666666091754,
        "tmdate": 1669989231959,
        "tddate": null,
        "forum": "GVMwL15UrZO",
        "replyto": "GVMwL15UrZO",
        "invitation": "ICLR.cc/2023/Conference/Paper5828/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies how pre-trained large language models (LLMs) perform on three HTML understanding tasks: (1) semantic classification of HTML elements, (2) description generation for HTML inputs and (3) autonomous web navigation of HTML pages. It found that pre-trained LLMs can work on these tasks effectively after fine-tuning. They require less task-specific data while perform better than the previous best supervised model.",
            "strength_and_weaknesses": "## Strength\n- The paper is easy to follow.\n- The study of how LLMs perform on raw HTML texts is interesting and novel.\n\n## Weaknesses\n- Models that are trained on task specific data to compare against is questionable. I expect to see more task-specific models such as StructuralLM [1], MarkupLM [2] that respect the tree structure of HTML (or some other methods mentioned in the paper) to be evaluated against. However, the results seem to focus on LLMs only. I don't see the point of compare different variants of LLMs with or without pre-training. Basically, I need to see the following question answered: \"For specific web tasks, do I want to use a HTML specific model trained on the small dataset or just to fine-tune LLMs\"?\n\n[1] Li, C., Bi, B., Yan, M., Wang, W., Huang, S., Huang, F. and Si, L., 2021. StructuralLM: Structural Pre-training for Form Understanding. arXiv preprint arXiv:2105.11210.\n[1] Li, J., Xu, Y., Cui, L. and Wei, F., 2021. MarkupLM: Pre-training of text and markup language for visually-rich document understanding. arXiv preprint arXiv:2110.08518.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is easy to follow as the content of the paper is simple.\n- Quality: The experiments seem to be well-done.\n- Novelty: The novelty is limited in terms of methodology. It is mostly an empirical study for large LLMs on HTML texts.\n- Reproducibility: Some details are missing. Either code or more detailed experimental setup should be provided in the supplementary material",
            "summary_of_the_review": "I like the idea of studying LLMs on HTML texts. However, it seems to me that the experiment setup (re. models to compare against) cannot support the main claim of why one would like to do so. Besides, the technical novelty of the paper is limited for a ML conference like ICLR; a NLP conference might be a better fit for this submission.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5828/Reviewer_hMUi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5828/Reviewer_hMUi"
        ]
    },
    {
        "id": "GH-g3558JF",
        "original": null,
        "number": 4,
        "cdate": 1667152392163,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667152392163,
        "tmdate": 1669523397713,
        "tddate": null,
        "forum": "GVMwL15UrZO",
        "replyto": "GVMwL15UrZO",
        "invitation": "ICLR.cc/2023/Conference/Paper5828/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work addresses the problem of using large language models for understanding HTML. Unlike prior work which attempt to solve this problem using dedicated architectures and training procedures and/or large HTML corpora, this work employs large language models pretrained on natural language text and evaluates their performance on three HTML understanding tasks - Semantic Classification of HTML elements, Description Generation for HTML inputs, and Autonomous Web Navigation of HTML pages, thus potentially eliminating the need for dedicated architectures and training procedures. Further, using only a small HTML corpus for finetuning a pretrained LM, the work reports encouraging results compared to LMs trained exclusively on the task dataset.\n\n\nThe key question asked by this work is can off-the-shelf LLM trained on a large text corpus be used in tasks that require some level of understanding of HTML. As canonical tasks in HTML understanding, the work looks at three tasks. \n\nIn Semantic Classification, the ask from the model is to classify a salient HTML element into one of a set of role categories that are commonly used in automated form-filling applications. E.g. address, email, password. \n\nIn Description Generation, the ask from the model is to, given a HTML snippet as the input, extract a small text sequence from the snippet as the natural language description of the snippet. \n\nIn Autonomous Web Navigation, the ask from the model is to, given a HTML page and a natural language command as the input, identify the appropriate HTML elements and the actions on those elements that would satisfy the command. \n\n\nThe work tests the idea of using pre-trained LLM for the three canonical tasks with several pretrained LLMs with different architecture  encoder-only, encoder-decoder, or decoder-only, different model size, and training data. Best results are obtained with encoder-decoder architectures with bi-directional attention.\n\nThe input to the models is the raw HTML text sequence. However, when the sequence is too big to fit into the context window of LLM, a snippet of appropriate size is extracted using a heuristic algorithm.\n\nThe work uses MiniWoB benchmark (demonstrations like email forwarding and social media interactions) for Autonomous Web Navigation task, a new dataset consisting of URLs from the real shopping websites for Semantic Classification, and a dataset derived from CommonCrawl for Description Generation.\n\n",
            "strength_and_weaknesses": "Strengths:\n\t1. That pre-trained natural language LLM can be effective for tasks involving HTML pages  is interesting and can potentially find use in several interesting practical applications.\n\t2. As no retraining of LLM with large HTML datasets is necessary, models for tasks  involving HTML pages can be developed quickly and less expensively. \n\t3. That raw HTML text can be used as input without needing parsing is an advantage.\n        4. Experimental results are very encouraging and validate the claim that pretrained LLMs can be effective for the three tasks.\n\nWeaknesses:\n        1. It is claimed that these three tasks require understanding of both structure and content of the web-page. While it is easy to see that textual content plays a key role in each of the three tasks, the role played by the structure of the web-page is not clear. It can be argued that no significant HTML structure analysis or understanding is needed for these tasks. For example, in Semantic Classification, what is most important for classifying HTML element 'input' into, say, 'username' is the value of its two attributes,  'type' and 'id'. As these attributes are in the close neighbourhood of 'input', parsing of HTML is not strictly necessary. Therefore, it might a good idea to do some experiments that demonstrate unequivocally the need for HTML structure analysis or understanding in these tasks. One such experiment could be to map all HTML tags in the web-page except the salient tags to the same token (say, ***) so that the input is now a sequence of salient tags, and ***. \n      2. There is not much novelty in the methodological aspects of the work. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The submission is well written and easy to understand. The three canonical tasks are described well and the adaptation of the various LLM for building models for these tasks are well explained. The proposed solution is simple and appears to be effective for the tasks considered and the datasets chosen. There is not much novelty in methodological aspects and the work is primarily empirical in nature. Experiments are designed well and should be easy to reproduce. Datasets used in the experiments have been promised to be released. The work should be interesting for practitioners.\n\n\n\n",
            "summary_of_the_review": "This work asks the question can off-the-shelf LLM trained on natural language text be used effectively for tasks that involve HTML pages. It proposes three tasks as canonical tasks in understanding HTML. It employs a variety of LLM to build models for the three tasks using a small amount of HTML data for fine tuning. It shows that LLM does help these tasks significantly. One key question not answered in this context is how much of HTML structure analysis and understanding is truly required for these questions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5828/Reviewer_5MM1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5828/Reviewer_5MM1"
        ]
    }
]