[
    {
        "id": "ODBs6-OuLnl",
        "original": null,
        "number": 1,
        "cdate": 1666575628133,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666575628133,
        "tmdate": 1666575628133,
        "tddate": null,
        "forum": "Oh0cnNTn5Di",
        "replyto": "Oh0cnNTn5Di",
        "invitation": "ICLR.cc/2023/Conference/Paper3562/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In the context of Variational Monte Carlo for finding the ground state of quantum many-body systems, convolutional neural networks have proven to be a powerful ansatz for solving square lattices. This paper proposes a method for extending ConvNet quantum states to non-square lattices by projecting them onto square lattices with non-uniform paddings to fill the holes in the projection. Prior works on neural networks for non-square lattices have focused on graph neural networks instead, which relied on hand-crafted features to encode the geometry of the lattice. By building on the success of ConvNets, this work aims to offer a simpler, more principled approach. The experimental results on the challenging J1-J2 Heisenberg model demonstrate that the proposed method can perform as well or better in most cases without relying on any hand-crafted features.",
            "strength_and_weaknesses": "Strengths:\n\n* Simpler more principled novel method that builds on the power of ConvNets, instead of trying to adapt GNN to less conventional settings, and using less external knowledge on the given problem. This could be crucial when tackling more intricate systems without the need to carefully tune the method to them.\n\n* Clearly written.\n\n* Extensive experiments demonstrating that in *most* (but not all) cases lattice ConvNets (LCN) can achieve better accuracy than the competing GNN approach.\n\nWeaknesses:\n\n* It is clear that GNN perform significantly better than LCN in some cases, where LCN results achieve a relative error of ~10^-2 (compared to GNN). This raises questions about whether LCN might lack the needed inductive bias that GNNs utilize in their construction. Given the stated goal of a method that relies less on prior knowledge, these negative results might show that LCNs might not yet fulfill this role.\n\n* There are a few issues with the presentation of the results. For one, the \"reference energy\" column is a bit problematic and requires more careful discussion. Specifically, in many cases there is not yet a consensus that a correct estimate of the energy is presently available (e.g., for J1/J2 = 0.5 for 10x10 lattice), and so the last column in some cases is not really a reference that can be used to evaluate how well LCN solve the task, but is in fact a competing neural network that solves it more effectively. Another issue is that it should be noted that the specific paper on GNN they compare against uses a completely different optimization method (IT-SWO) and not the conventional VMC -- when comparing architectures it is better to ensure all other factors remain the same. Finally, in cases where the competing method is better, it should be in bold-face to emphasize that instead of keeping the entire row in regular font.\n\n* The claim regarding LCN not needing any prior knowledge unlike previous works is a bit misleading. First, because they do incorporate prior knowledge into the architecture, and second, because some of the examples of prior knowledge they try to avoid (Marshal sign rule) are not at all addressed by their proposed method. Namely, the locality in the convolutional layers assumes mostly short-range interactions -- a true departure from the assumption of locality would use only the non-local layers (which is really self-attention by another name). The authors note the use of symmetry in previous works as an example of prior knowledge, but periodic symmetry is part of the LCN architecture -- so it is not clear what is different here. As for the use of the Marshal sign rule, while indeed other works have pointed out the difficulty of learning the sign structure without relying on some prior knowledge or heuristic, this work does not propose any arguments for why LCN helps with this challenging problem.\n\n* Some of the hyperparameters in the experiments are missing: How many channels / kernels were used in each layer? Is the window size of the convolutional layers always 3x3? Are the number of MCMC chains equal to the batch size, or are there fewer chains and multiple samples are produced per step?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The method and general problem are clearly described. The \"reference energy\" column is not clearly explained in the paper, and could cause confusion as to what it really means. I suggest clearly articulating when a reference energy is assumed to be representative of the true ground state energy and when it is simply the current best method (and in those cases, why is it fine not to directly compare to that).\n\nQuality: Some of the claims in the paper are not well supported, as previously discussed regarding the claim that LCN does not rely on any prior knowledge. Moreover, the results and the comparison to prior works are not always framed correctly.\n\nNovelty: The proposed method is definitely novel. It is addressing the non-trivial problem of extending convolutional networks to non-square lattices, instead of relying on more complicated solutions that require more careful calibration to each instance.\n\nReproducibility: With the exception of a few missing hyper-parameters of the architectural model and other very technical details, the work should be very reproducible, and the mentioned exception could be easily addressed in a revision.",
            "summary_of_the_review": "While the proposed method is novel and offers a principled solution to a non-trivial problem, the presentation of the results and prior works, as well as the lack of support for the core arguments leads me to recommend rejecting it. It could be that with a major revision addressing the presentation issues, and perhaps improving some of the results that showed GNN to be superior, the paper could be accepted.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3562/Reviewer_aQ26"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3562/Reviewer_aQ26"
        ]
    },
    {
        "id": "mHhRwwG5Qx",
        "original": null,
        "number": 2,
        "cdate": 1666641688455,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641688455,
        "tmdate": 1666641688455,
        "tddate": null,
        "forum": "Oh0cnNTn5Di",
        "replyto": "Oh0cnNTn5Di",
        "invitation": "ICLR.cc/2023/Conference/Paper3562/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper investigates application of neural networks based on lattice convolutions as variational ansatz for a quantum many body problem on a set of lattices (including non-square variations such as triangular, honeycomb and kagome). Authors test their approach on commonly studied Heisenberg models with up-to second nearest neighbor interactions. They compare their results to other methods compatible with non-square geometry such as graph neural networks finding their method competitive.",
            "strength_and_weaknesses": "Strengths:\n* Authors provide results on a range of model problems and obtain competitive variational energies\n* The proposed method compares favorably to relevant baselines\n* Proposed architecture should be able to utilize hardware accelerators well\n\nWeaknesses:\n* Additional modeling choices: (squeeze-and-excitation block, non-local block, SE-non-local layer) are studied in the appendix and mostly on small system size. As an exciting addition, it would be great to see whether this holds true for larger problem (108 spins) (interesting regardless of the outcome). Does this ablation depend on the underlying physics of the system? (e.g. ordered vs disordered)\n* The primary difference with respect to a GNN baseline is misrepresented/obscured (see main review for details)\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and provides enough details to understand the setup and the model being tested. Authors cite relevant works and describe concepts that might be new to a general audience interested in the topic. It is conceivable that the results could be reproduced solely from the paper, but a commitment from the authors to release the code upon acceptance would be a big plus.",
            "summary_of_the_review": "Overall a solid paper that achieves good results on a set of challenging benchmark problems (that are still an open research topic). The proposed method would be of value to the research community interested in applying machine learning to problems in many body quantum physics, as well as could be of interest to a wider ML for science audience.\n\nThe paper features good results and authors make logical & clear efforts to support the claims. There are a few additional ablation studies that could improve the paper from a physics insight standpoint: (on small system size on Kagome lattice including nonlocal information seems to noticeably help - does this still hold for larger system sizes?)\n\nOne aspect in which I disagree with the authors is their structural comparison with graph neural networks. Both the GNN and the proposed LCN options attempt to take advantage of the lattice symmetries to make the ansatz more suitable for the problem at hand. For a general lattice symmetry this comes at a price of limited representability (assuming standard CNN-based or graph network based implementations) which in general hurts the variational energies. Both GNN and the proposed LCN address this tension by selecting a sublattice, which in GNN is encoded as additional input features and in the LCN case is determined by the choice of the unit cell for the underlying convolutions (both uses the same heuristic that does not involve any \u201cdomain knowledge\u201d). The real advantage of the proposed method, from my perspective, is that the paper uses regular convolutions that are likely to be much more efficient on modern hardware than message passing steps, enabling faster iteration and better variational energies given the same compute budget.\n\nI\u2019m also a believer that statements of \u201cprimacy\u201d and \u201cnovelty\u201d at ICLR submissions are generally not load bearing and should be discouraged.\n\nMinor comments, confusions, suggestions:\n* Related work: \u201cto augment system configurations in order to respect underlying quantum symmetry\u201d - I don\u2019t think graph neural networks have any \u201cquantum symmetries\u201d, am I misunderstanding?\n* Introduction: \u201csuffers entanglement problems\u201d\n* Consider switching network ablation study (ideally extended :) ) from the appendix and reproduction of the GNN ablation study, since the former is new and provides an interesting piece of evidence.\n\nSuper minor nits:\n* Variational principle in quantum mechanics (Lanczos method has been shown up to ~48)\n* Background and related work: \u201call combinations of spins form a basis\u201d (remove \u201ca set\u201d)?\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3562/Reviewer_maSg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3562/Reviewer_maSg"
        ]
    },
    {
        "id": "3QWMgp-Wt0D",
        "original": null,
        "number": 3,
        "cdate": 1666764991664,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666764991664,
        "tmdate": 1666764991664,
        "tddate": null,
        "forum": "Oh0cnNTn5Di",
        "replyto": "Oh0cnNTn5Di",
        "invitation": "ICLR.cc/2023/Conference/Paper3562/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose lattice convolutions to convert non-square lattices into grid-like augmented lattices where regular convolution can be applied. Based on this and self-gating and attention, they design lattice convolutional networks and apply the networks to some experiments.",
            "strength_and_weaknesses": "Learning ground-states of quantum many-body systems by deep neural networks is an interesting topic. Adapting the usual CNNs to non-square lattices for replacing graph neural networks is an interesting idea. But a linear transformation should easily transfer a non-square lattice to the square one. Unless I miss something, the problem seems to be easy. The authors should convince the readers why this simple approach does not work. ",
            "clarity,_quality,_novelty_and_reproducibility": "Unless I miss something, a linear transformation should easily transfer a non-square lattice to the square one. The authors should clarify why this simple approach does not work. ",
            "summary_of_the_review": "Learning ground-states of quantum many-body systems by deep neural networks is an interesting topic. But it seems that the approach in the paper of adapting the usual CNNs to non-square lattices can be easily replaced by using a linear transformation. The authors should convince the readers why this simple approach does not work. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3562/Reviewer_xp3P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3562/Reviewer_xp3P"
        ]
    }
]