[
    {
        "id": "0lls2iOtV9q",
        "original": null,
        "number": 1,
        "cdate": 1666403253922,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666403253922,
        "tmdate": 1666403253922,
        "tddate": null,
        "forum": "qY1hlv7gwg",
        "replyto": "qY1hlv7gwg",
        "invitation": "ICLR.cc/2023/Conference/Paper1159/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes an active learning approach to select annotated examples for in-context learning in large PLMs. The proposed method is made up of three components: (1) nearest neighbors: diverse examples are selected from a graph constructed with nearest neighbors; (2) pre-trained language model: more examples are selected according to the scores of the PLM; and (3) prompt retrieval. Empirical studies on a number of public datasets demonstrate the effectiveness of the proposed method in comparison with random selection. ",
            "strength_and_weaknesses": "Strength:\nThe proposed method is simple yet effective.\nThe paper is well-written and thus easy to follow.\nThe empirical studies provide interesting insights about how the method works. \n\nWeaknesses:\nSome important alternatives and baselines are not involved in the comparison.\nThere is inconsistency in presentation. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. \nIt is easy for others to reproduce the method.\nI do not think the proposed method is novel, though it is effective. It is an ensemble of existing techniques. ",
            "summary_of_the_review": "The paper proposes an active learning approach for in-context learning for large PLMs.  \n\nThe method is simple yet effective, and the insights from the experiments are interesting. However, I have a few concerns:\nRegarding to comparison: \n(1) can you compare vote-k with the following alternative? (a) replace vote-k with random selection; (b) run PLM and obtain confident scores; and (c) select annotations according to the confident scores. \n(2) can you compare the proposed method with some soft prompt tuning method? \n\nRegarding to presentation.\nFrom the description in Page 3, it seems that the remaining 9M/10 examples are selected according to the confident scores, but  Line 11-16 in Algorithm 1 has nothing to do with the confident scores. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1159/Reviewer_dEqn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1159/Reviewer_dEqn"
        ]
    },
    {
        "id": "zgwPPma1DH",
        "original": null,
        "number": 2,
        "cdate": 1666663416502,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663416502,
        "tmdate": 1666663416502,
        "tddate": null,
        "forum": "qY1hlv7gwg",
        "replyto": "qY1hlv7gwg",
        "invitation": "ICLR.cc/2023/Conference/Paper1159/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method to select a pool of diverse examples to be annotated before evaluation with in-context learning. During evaluation, a cosine-similarity based method is used to retrieve supporting examples from this pool. The combination of these methods outperforms randomly selecting examples. \n\nKey contribution of the paper is the selective annotation method called vote-k to make a pool of diverse examples.\n",
            "strength_and_weaknesses": "### Strength\n\nThe efficient approach to select a pool of examples to annotate from unlabeled data which are shown to be diverse. The authors evaluate their method on several tasks including classification, reasoning, dialogue, semantic parsing and generation.\n\nThe authors also provide a detailed analysis of their method compared to other baselines.\n\n### Weaknesses\nI have a few comments:\n\n* Do the authors take the cost of encoding every unlabeled training instance using Sentence-BERT into account when comparing methods? I believe this was not mentioned in the paper.\n* In section 2.1, It is not very clear how the first M/10 examples are labeled. It is mentioned that \u201cthe current labeled $\\mathcal{L}$ has M/10 samples\u201d which is used to label other instances. Are they manually labeled?\n* It is stated that the vote-k method encourages diversity. Since all the labels are removed first, does this method encourage or keep inclusivity of all the classes? Have the authors looked at whether their method favors examples from certain classes?\n* In section 2.1, \u201cwe conduct experiments three times and report the average score\u201d. What/where is the randomness? Examples selected or model prediction? \n* Figure 2 shows a direct comparison between fine-tuned RoBERTa large and models such as DaVinci-002 while it is stated that the authors do not aim to conduct a head-to-head comparison. What is the comparison the authors are making here and perhaps could it be shown better with another figure? Results from this direct and not fair comparison are also mentioned in the abstract which could be misleading.\n* In section 4.4, I think some more detail is needed as to why the combination of vote-k and similarity retrieval works, but vote-k alone with random selection of supporting examples has almost the same score as random selection.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Apart from some of my comments above, the rest of the paper is well-written and easy to understand. ",
            "summary_of_the_review": "The proposed method to collect a pool of examples and annotate them performs well in combination with similarity based retrieval for support examples for in-context learning. The authors motivated this approach well, and a detailed analysis is carried out. Some parts about the proposed method are not clear, and there are a few missing and potentially misleading pieces in the paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1159/Reviewer_bT5a"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1159/Reviewer_bT5a"
        ]
    },
    {
        "id": "dGEZTIZWEg",
        "original": null,
        "number": 3,
        "cdate": 1666853305209,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666853305209,
        "tmdate": 1666853770799,
        "tddate": null,
        "forum": "qY1hlv7gwg",
        "replyto": "qY1hlv7gwg",
        "invitation": "ICLR.cc/2023/Conference/Paper1159/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, the authors propose a two-step technique for improved few-shot learning with large language models on a variety of NLP tasks:\n1. *Selective annotation*, an unsupervised similarity-based procedure referred to as vote-*k*, that selects diverse and representative examples that are subsequently labeled, and\n2. *Prompt retrieval* at test time that retrieves in-context examples from the labeled set that are most similar to each test instance.\n\nvote-*k* is found to improve performance on 10 diverse tasks by a large margin compared to random selections. Under similar budget constraints, in-context learning is shown to be a much better few-shot learner compared to standard model fine-tuning. vote-*k* is also shown to outperform existing methods for selective annotation.",
            "strength_and_weaknesses": "Strengths:\n1. The authors present a simple technique for in-context learning with large language models that achieves consistently good results across a variety of NLP tasks.\n2. The proposed ideas in isolation do not yield significant improvements. The authors find that the main ingredients for the success of in-context learning are a combination of selective annotation with similarity-based prompt retrieval.\n3. The experimental section is thorough. Along with ablations and trying LMs of varying sizes, their technique is compared against many other existing selective annotation approaches and shown to consistently outperform the latter.\n\nWeaknesses: Some of the design choices need to be elaborated on further and additional analysis-based experiments would also be useful. I elaborate on these further below for the authors to address.\n\n- For the classification tasks, what do the label distributions look like over the labeled subset $\\mathcal L$? Since the selective annotation is based entirely on similarities derived from sentence embeddings, there is nothing explicit ensuring that the label distribution over the selected subset is not skewed. Or, is the label distribution on the annotated subsets derived via vote-*k* indeed skewed for some tasks and the performance improvements are mainly coming from improvements on the labels that are well-represented? Some discussion of this would be useful.\n- It seems like the optimal value of $k$ in vote-*k* would depend on the number of instances in the unlabeled set that changes with the tasks. A single value of $k=150$ was chosen for experiments across all tasks. Could the authors justify this choice further?\n- From the diversity and representativeness measures shown in Table 10 in Appendix F, the difference between Random and vote-*k* does not appear to be very large. In the least, it's unclear how to assess the differences shown in this table. A qualitative analysis might be more revealing here. It might be interesting to see some examples, especially when the annotation budget is 18, of the kinds of instances that get selected depending on the task. t-SNE plots of these selected examples in the larger context of unlabeled instances might also be a good visualization to show. Please comment.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. The proposed technique has been empirically validated well and code for this work has been made available to encourage reproducibility.",
            "summary_of_the_review": "I have rated this submission as a marginal accept for now. Look forward to the authors' responses to my questions.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1159/Reviewer_3Ufu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1159/Reviewer_3Ufu"
        ]
    },
    {
        "id": "gRltDu_A77",
        "original": null,
        "number": 4,
        "cdate": 1667374068220,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667374068220,
        "tmdate": 1667374068220,
        "tddate": null,
        "forum": "qY1hlv7gwg",
        "replyto": "qY1hlv7gwg",
        "invitation": "ICLR.cc/2023/Conference/Paper1159/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies NLP tasks in the context of large-language models that are capable of in-context learning. Specifically, the work advocates for constructing prompts dynamically, based on textual similarity with input query, relying on a small pre-annotated corpus. The paper focuses on describing a new technique for selecting good annotation candidates from an unannotated pool of examples to serve as a database for in-context learning. Specifically, they propose a graph-based algorithm, vote-k, which maps unlabeled examples into a similarity graph. The work then describes how retrieval of the examples can be done at inference time to construct prompts for in-context learning. The paper describes several experiments on different large language models (Codex-da-vinci, GPT-J, OPT-175B, etc.) using 10 different datasets. Finally, the paper also compares the vote-k selection strategy with existing selective annotation strategies previously used in active-learning or few-shot learning regimes.\n",
            "strength_and_weaknesses": "## Strengths\n\n1. In-context learning is a hot, rapidly evolving area of research and of high interest to the ICLR and NLP communities.  This paper presents a thorough set of evaluations of the impact of the choice of examples to include in the prompt, demonstrating that this can have a huge impact in final model behavior.\n\n1. The paper is clear and easy to read. The evaluation section contains several interesting results that should be valuable to share with the broader community.\n\n1. The gains of vote-k over other existing active learning methods presented is quite strong (Section 4.5).\n\n1. The system proposed seems to be practical and relatively easy to implement in practice. As such, there is a chance that it could be applied in practice in real world applications of in-context learning.\n\n1. The idea of automatically constructing graphs of unlabeled examples using similarity measures is relatively old in the label propagation literature. The proposed use-case in this work is interesting and novel.\n\n## Weaknesses\n\n1. The main weakness of the paper is the lack of a thorough description and analysis (including ablations) of the vote-k algorithm and its relationship to existing clustering algorithms. I would have hoped for a more complete discussion in Section 2.1, including the contents of Appendix G which seem important for readers to fully understand the proposed vote-k algorithm.\n\n1. One particularly confusing part of vote-k: it was not immediately clear to me how this differs or is better than simple k-means clustering to discover centroids? Once a weighted graph is constructed (as described in Section 2.1, using cosine similarities of sentence-level representations), it should be possible to run any graph-based algorithm for finding centroids. I think a naive baseline (that does not use model prediction confidence scores) would be to use k-means.\n\n1. Related to the point above, I would have liked to see more ablations on the vote-k selection process. How important is it to use the model confidence \u201cstratification\u201d strategy in the algorithm? This is particularly useful to know since, given a very large set of unlabeled examples `|U|`, it can be costly to run all `u in |U|` model predictions to collect confidence scores.\n\n1. One source of bias that was not addressed in the paper relates to the choice of unlabeled examples (|U|). In this study, the unlabeled corpus for all 10 datasets has been extracted from an existing labeled/supervised corpus (that has been previously annotated). This may not seem like it, but I believe there is huge amount of supervision and work simply *choosing* which examples to label in the first place.  In practice, however, for a new task/dataset, this will not be the case if we want to do only \u201cselective annotation\u201d. I was quite unsure (maybe a little skeptical) that the proposed procedure would work for a \u201crandom\u201d or uncurated corpus of unlabeled examples.\n\n1. There is also no discussion on label distribution in the vote-k process. Again, I think the current experimental evaluation is taking advantage of curated training datasets where labels tend to be nicely distributed/represented. So focusing on performing input similarity and model confidence may be enough. For example, for some tasks, a fairly random unlabeled corpus may contain a huge number of \u201cnegative\u201d examples (say you want to automatically find pairs of sentences for entailment - most random pairs of sentences will be NEUTRAL). Measuring similarities of inputs alone does not seem to be sufficient to match the expected posterior distribution of labels? Can you please elaborate on this point?\n\n1. With respect to the experimental setup, it is a bit jarring the number of combinations of (dataset, model) pairs (Table 1). I understand that it is expensive/infeasible to run all datasets on all models, and I don\u2019t have a good suggestion to improve on this. But I fear that the end result is that subsequent/future work that builds on vote-k may need to re-evaluate your implementation on a partial set of dataset/tasks.\n\n## Other questions\n\n1. Question about potential clarification in initialization of Algorithm 1. Maybe I did not understand this part of the algorithm, but when running initially |U| contains all examples, and |L| is empty. My understanding is that the score in line 4 of Algorithm 1 (also equation in Section 2.1) generates score of 0 for every node since no u in |U| are yet connected to a node in |L|. Is the argmax using some stable ordering when all scores are 0 to guarantee determinism? Does this choice not influence the final set of examples in |L| ?\n\n1. Recently, the Chain-of-Thought prompting (https://arxiv.org/pdf/2201.11903.pdf) has been used by many papers using in-context learning, with reported improvements in model performance (the paper seems to have been cited already 100 times since its release earlier this year). One question this work does not address is whether model sensitivity/variance to retrieved prompts is also present when used in conjunction with chain-of-thought. And whether the vote-k scheme would be useful to limit the number of chain-of-thought annotations needed to improve in-context learning performance.",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity:\nI think the paper is well written overall, with a clear presentation. Aside from a small question about algorithm initialization, I felt like I understood the work.\n\n### Quality:\nBoth the proposed algorithm and experimental evaluations presented seem high quality and interesting.\n\n### Novelty:\nThe novelty of this work is mixed: the idea of using retrieval for in-context learning prompts is not new (this work cites Liu et al., 2022 and Rubin et al., 2022).. The idea of constructing graphs of unlabeled examples is also not new (common in label propagation literature). But novelty here is (1) in combining these ideas to make a practical system, and (2) the experimental work demonstrating some insights comparing this with fine-tuning and other selective annotation schemes.\n\n### Reproducibility:\nGiven that the code for vote-k be released, there should be no impediments to reproducing the results presented in the paper.\n",
            "summary_of_the_review": "Overall, I think this is a well produced work with clear writing and interesting and insightful experiments. The proposed vote-k algorithm is pretty simple to implement and has potential to be of practical use to users of in-context learning. I do think there are some unanswered questions that I list above, including (1) whether this scheme would be robust to a truly unlabeled or uncurated corpus, and (2) the relationship of vote-k with simpler k-means and other clustering algorithms. Given that these issues do not seem overly problematic, I\u2019m recommending the acceptance of this work.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1159/Reviewer_ZPh1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1159/Reviewer_ZPh1"
        ]
    }
]