[
    {
        "id": "WsCbjHOMKHr",
        "original": null,
        "number": 1,
        "cdate": 1666482769491,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666482769491,
        "tmdate": 1669668670694,
        "tddate": null,
        "forum": "gSHyqBijPFO",
        "replyto": "gSHyqBijPFO",
        "invitation": "ICLR.cc/2023/Conference/Paper4932/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work explores editing prompts in large language models on a per-instance level via reinforcement learning. In contrast to existing works, the editing is conditioned on the input query, rather than a task, and the action space for editing is hand-designed. The approach is tested on different natural language few shot classification tasks, showing better performance than few-shot fine-tuning and discrete prompt optimization methods, being more data-efficient for tuning, interpretable, and robust to different hyperparameters in prompting.",
            "strength_and_weaknesses": "**Strengths:**\n- Relevance:\n\t- Given the wide use of prompting for tasks in LLMs, and the effect it can have in final performance of downstream tasks, exploring methods to design better prompts is a very important problem.\n- Writing:\n\t- I like how the related work is structured, motivating first the need for prompt learning, and later providing an overview of prompt learning methods and methods based on retrieval examples from the training set.\n\t- Formulation in section 4 makes the method clear to understand and connect to RL frameworks.\n\t- Table 1 is useful to understand action space\n- Method:\n\t- The policy used to modify prompts is constrained, which may allow for more interpretability and sampling efficiency (Fig 1), while at the same rich in some parts (swap example, verbalizer change), allowing different kinds of operations ito the prompts.\n\n- Experiments:\n\t- Thorough and valuable ablation studies, comparison with solid baselines and in different tasks.\n\t- SOTA or comparable results in a wide set of tasks.\n\t- Very good properties in few-shot settings.\n\t- Very cool example of exemplar editing in Table 5, I would love to see this together with the other exemplars. In general, I think it would be really interesting to see if there are common properties in the exemplars edited by the policy. Does the model tend to balance the number of exemplars per class, or does it try to select more exemplars close to the query class?\n\n\n**Weaknesses:**\n\n\n- Writing Clarity \n\t- From my understanding of the paper, the prompts are edited at test time using a pre-trained policy, but the policy is not updated at test-time but training time. While this is reasonable because at test time we do not have access to the GT label and therefore cannot compute the reward, the test-time optimization generally makes me think about having a surrogate reward at test time used to update the policy. I would strongly encourage to clarify this more explicitly in the paper, maybe by saying, that the policy is trained first by constructing a training set xyz, and at test time it is deployed to edit the prompts. Referencing directly Algorithm 1 would also help a lot.\n\t- In line with the previous comment, it is hard for me to understand whether K in the training set corresponds to the number of examples for in-context demonstrations, or an external training set, that contains description examples and query. When I think about a few-shot dataset for NLP I am thinking about in-context learning, but here it seems otherwise?\n\t- Figure 2 would benefit a lot from a walkthrough example. It is hard to understand what exactly is a verbalizer, E1, ... En etc.\n\t- Section 3.2 could be condensed, a lot of it is already in related work.\n\t- Clarify, in sec. 4, \"... contains both information\", what is both information?\n\t- How is the initial instruction provided? Is it task specific?\n\t- A figure of the main methods from prompt-search vs the existing method would be helpful to quickly grasp the main differences, or maybe a table of features.\n\t- Minor: It would be useful in 3.2 to repeat the difference between He et al. 2022, Deng et al. 2022 for section 3.2 since authors talk about query-dependent prompts and then point at limitations of query-agnostic prompts. This seems addressed in the next paragraph, but it is difficult to follow form there that previous methods are these 2 methods. In fact, from my understanding He at al. 2022 is prompt specific, but Deng et al. 2022 is not. Could authors confirm and correct citation in 3.2 in that case?\n\t- How is Tempera No-TTE different than promptRL. One main thing is the parametrization of the policy, is there something else?\n- Method:\n\t- I would appreciate more details of the policy, there is attention but how is it done? Is it flat over all the techniques and candidates, or is it hierarchical in that first a technique is selected and then a candidate.\n\t- Table 4 would be good to include No verb, but Inst\n- Results\n\t- Regarding test-time efficiency in Sec. 5.1, while the claim could be true, it is not validated that the run-time will be smaller. A table with clock time or num ops would be good to validate, and if it cannot be provided, make clear that it is a hypothesis.\n\t- Few-shot is a very valid setting, but it would be really interesting to see if the current method also improves zero-shot performance.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nWhile the description of related work and where the method is positioned, background, and the discussion of the results is clear, I am missing several details and clarification questions in the method section, which are the main reason for my rating. I would strongly suggest editing the text and figures to reflect those.\n\nNovelty:\nWhile using RL to improve prompts has been used in other works, conditioning the prompt search on the query, and the action space for editing are novel and effective.\n\nReproducibility:\nClarifying aspects of the method would help in reproducibility, as well as providing the code. On the other hand, the appendix contains details about training hyper-parameters and datasets, which help in reproducibility. Correcting the clarity questions will improve reproducibility.",
            "summary_of_the_review": "This paper discussed a relevant topic, has strong ideas, a new method, thorough experiments, and SOTA results. I generally think it would be a valuable contribution to the community, but in the current state there are important details that are difficult to understand for me, and I assume for other readers in the community. If the aspects described above are clarified, I am very open to changing my rating to accept.\n\n[Edit] The authors have addressed my concerns in the revised version. I therefore have updated my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4932/Reviewer_Hk2t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4932/Reviewer_Hk2t"
        ]
    },
    {
        "id": "uOBKeTj938q",
        "original": null,
        "number": 2,
        "cdate": 1666675189470,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675189470,
        "tmdate": 1669209966620,
        "tddate": null,
        "forum": "gSHyqBijPFO",
        "replyto": "gSHyqBijPFO",
        "invitation": "ICLR.cc/2023/Conference/Paper4932/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes RL-based test-time editing for ",
            "strength_and_weaknesses": "## Strengths\n1. The paper is overall well-written and easy to follow.\n2. The paper includes a wide range of baselines, both continuous and discrete.\n\n## Weaknesses\n1. The scalability of the proposed method is not good. As shown in Figure 3, TEMPERA holds an advantage against fine-tuning but cannot scale. Also, I would like to see other few-shot prompting methods illustrated in the same figure.\n1. Figure 1 is misleading. It is the best-performing dataset in Figure 3. It's inappropriate to single it out and put it in the introduction.\n1. The editing patterns are not diverse enough. The case study is not very convincing. The improvement doesn't seem explainable. \n1. The proposed method is in essence similar to prompt/verbalizer searching. \n1. How do you justify the use of RL here? Is there a strong reason not to use search instead?\n1. I think GLUE or SuperGLUE can be more convincing. Yelp, MR, CR and AG News are considered easy.\n\n-----\nAfter rebuttal: The authors have addressed many concerns mentioned in my review during the response period. I've updated my score to 8.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and has some technical novelty. I recommend the authors release the code.",
            "summary_of_the_review": "This paper proposes test-time prompt editing with RL but may have some issues.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4932/Reviewer_Hybx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4932/Reviewer_Hybx"
        ]
    },
    {
        "id": "NjnEsCYKxwJ",
        "original": null,
        "number": 3,
        "cdate": 1666768283007,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666768283007,
        "tmdate": 1666768283007,
        "tddate": null,
        "forum": "gSHyqBijPFO",
        "replyto": "gSHyqBijPFO",
        "invitation": "ICLR.cc/2023/Conference/Paper4932/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new model that is able to construct the instance-dependent prompt through reinforcement learning. It allows the agent to perform different editing techniques to update instructions, few-shot exemplars, and verbalizers at test time to construct query-dependent prompts efficiently. Results on different datasets show the effectiveness of the proposed model.",
            "strength_and_weaknesses": "**Strengths**\n\n**1. The proposed method is novel and effective.** \n\nThe paper proposes a novel method to edit the instance-relevant prompt in the test time via RL. The editing/action space covers a comprehensive set of commonly-used components like instructions, few-shot exemplars, and verbalizers. Results show the effectiveness of the proposed method in most datasets.\n\n**2. The paper is well-written and the experiments are comprehensive.** \n\n**Weaknesses/Feedback**\n\n**The experimental section could be improved.** \n\nSome experimental details are missing. For example, what are the dataset scales for SST-2, Yelp P., MR, CR, and AG News? And what is the value of |Y| in \u201cTask Settings\u201d?\n\nWhat is the exact number of training examples for the results in Figure 1? What are the details of the standard fine-tuning method?\n\nIn Table 4, only three different editing techniques are included. It could be critical to include more editing techniques.\n\nOn datasets like SST-2, Yelp P., and MR, the improvements of TEMPERA are not obvious compared to existing baselines.\n\nI have no idea of the details of the compared method in Figure 1. So I am concerned it might not be fair to claim that TEMPERA has greater data efficiency than the standard fine-tuning model. Look forward to clarifications in the rebuttal phase.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper looks good to me since the paper is well written, the proposed model is novel, and the comprehensive results show its effectiveness.",
            "summary_of_the_review": "The paper is competitive and I am looking forward to seeing my questions answered in the rebuttal stage and in the revised paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4932/Reviewer_cYCQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4932/Reviewer_cYCQ"
        ]
    },
    {
        "id": "E2-D2EDw39",
        "original": null,
        "number": 4,
        "cdate": 1667544477990,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667544477990,
        "tmdate": 1667544477990,
        "tddate": null,
        "forum": "gSHyqBijPFO",
        "replyto": "gSHyqBijPFO",
        "invitation": "ICLR.cc/2023/Conference/Paper4932/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "* This paper proposes a test-time prompt editing technique with reinforcement learning. Compare to prior methods, TEMPERA can efficiently leverage prior knowledge, adaptive to different queries, and provides an interpretable prompt for every query. This method achieves 5.33x on average improvement in sample efficiency when compared to traditional fine-tuning methods.\n",
            "strength_and_weaknesses": "Strength:\n* This paper proposes a novel method: It formulates discrete prompt optimization as using RL to edit an initial prompt. It also proposes carefully designed action space, and a set of techniques to improve the final performance.\nWeakness:\n* It seems that this method mostly improves sample efficiency but not absolute performance (e.g. not in few-shot setting). \n",
            "clarity,_quality,_novelty_and_reproducibility": "* This paper is clear, novel and seems to provide a lot of implementation details (in appendices). \n",
            "summary_of_the_review": "This paper proposes to treat prompt tuning as a test-time editing problem with RL, which can give more flexibility and demonstrate promising empirical results. However, it's novel because it's the first to apply RL on prompt editing, but the RL technique itself is not much novel and seems like it mostly improves sample inefficiency but not absolute accuracy (e.g. not in few-shot setting). So, I'd recommend weak accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4932/Reviewer_F772"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4932/Reviewer_F772"
        ]
    }
]