[
    {
        "id": "4rt6UtG5EUw",
        "original": null,
        "number": 1,
        "cdate": 1665716892304,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665716892304,
        "tmdate": 1666522203099,
        "tddate": null,
        "forum": "lrzX-rNuRvw",
        "replyto": "lrzX-rNuRvw",
        "invitation": "ICLR.cc/2023/Conference/Paper1253/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies how different numbers of spurious training examples (from one to thousands) affect neural nets. They measure how the predictions of test examples change when the spurious features are added. They show even a few spurious training examples can make the model outputs be affected by the spurious feature.\n\nThe authors consider two different types of spurious features. The first one is some artificial patterns, e.g., a square of yellow pixels. The second one is some natural spurious features in real data. They use the NICO++ dataset that has different backgrounds (grass, rock, water..) for each class. The authors then study the consequences of learning rare spurious features on privacy and accuracy. They also analyze the findings with a toy theoretical model and study several simple mitigations.\n",
            "strength_and_weaknesses": "**Strength**\n\n1.Studying the relation between the number of spurious examples and the strength of spurious correlations is an important problem. The experiments show that neural nets could learn natural rare spurious correlations are new and interesting. \n\n2.This paper is well-written. Extensive results are well organized. \n\n3.The analysis on the privacy risk of spurious examples is new.\n\n**Weaknesses**\n\n1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model. \n\n2.How neural nets learn natural rare spurious correlations is unknown to the community (to the best of my knowledge). However, most of analysis and ablation studies use the artificial patterns instead of natural spurious correlations. Duplicating the same artificial pattern for multiple times is different from natural spurious features, which are complex and different in every example.\n\n3.What\u2019s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.).\n\n[1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf",
            "clarity,_quality,_novelty_and_reproducibility": "Please see **Strength And Weaknesses**.",
            "summary_of_the_review": "Given that 1) the findings about artificial spurious features seem incremental because of the backdoor attack literature; 2) how neural nets learn natural rare spurious features is not well studied. I'm leaning to reject this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1253/Reviewer_yphb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1253/Reviewer_yphb"
        ]
    },
    {
        "id": "2iOyCrLKXq",
        "original": null,
        "number": 2,
        "cdate": 1666465756870,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666465756870,
        "tmdate": 1669671400431,
        "tddate": null,
        "forum": "lrzX-rNuRvw",
        "replyto": "lrzX-rNuRvw",
        "invitation": "ICLR.cc/2023/Conference/Paper1253/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Most existing papers working on spurious correlation focus on the correlation that existed in the majority of the examples. This paper discovered that even a few spurious examples can still lead to the model learning the spurious correlation. Moreover, these \"rare\" spurious correlations bring negative effects regardless of the strength of spurious patterns, network architectures, and optimization methods.  Finally, the paper provides a theoretic analysis of a simple binary classification model. Based on the analysis, a few techniques are proposed to improve learning with rare spurious correlations. ",
            "strength_and_weaknesses": "Strength:\n- Extensive experiments are performed across datasets, the strength of spurious patterns, network architectures, and optimization methods, showing that even a small amount of spurious correlation will also influence the model performance. \n- Theoretical analysis of a simple but meaningful model is conducted, moreover, inspired by the analysis, several techniques are proposed and validated with experiments. \n\nWeaknesses:\n- Does the technique proposed also work when the majority of the data present spurious correlation, e.g. Adding l2 regularization? Because as mentioned in the paper: l2 regularization only \"suppresses the use of features that only appears on a small number of training examples\". ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear and of good quality. ",
            "summary_of_the_review": "The paper discovers that even a few spurious examples can still lead to the model learning the spurious correlation. Many interesting results are shown with experiments: It shows that spurious patterns with larger empirical norms can cause spurious correlation more easily, and network architectures with higher sensitivity to its input are more susceptible to learning spurious correlations. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1253/Reviewer_ajJR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1253/Reviewer_ajJR"
        ]
    },
    {
        "id": "jc0Yt4iXT5",
        "original": null,
        "number": 3,
        "cdate": 1666576130795,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576130795,
        "tmdate": 1666576130795,
        "tddate": null,
        "forum": "lrzX-rNuRvw",
        "replyto": "lrzX-rNuRvw",
        "invitation": "ICLR.cc/2023/Conference/Paper1253/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates how sensitive neural networks are with respect to rare spurious correlations from three perspectives. First, studying how many training points with the spurious pattern would cause noticeable spurious correlations in the synthetic and real datasets. Second, studying how rare spurious correlations affect neural networks\u2019 privacy and test accuracy. Third, studying how to mitigate the effects of rare spurious correlation.\n",
            "strength_and_weaknesses": "Strength:\n1. The paper studies an interesting topic of learning with rare spurious correlations via both experimental and theoretical approaches.\n2. The paper studies whether rare spurious correlations are learned on both synthetic and real data.\n3. The paper studies the consequences of rare spurious correlations from a privacy and test accuracy perspective.\n4. The paper introduces simple and effective methods suggested by theoretical evidence to reduce the undesirable consequence of spurious correlation.\n5. The paper is well written.\n\nWeaknesses and questions:\n\n1. In Figure 1, it seems like the seven different spurious patterns (adding different kinds of noise) are not strong enough to generate a spurious correlation. How about using more challenging datasets, such as Colored MNIST?\n\n2. The main observations that neural networks can learn rare spurious correlations with few spurious examples are a little weak for me. It is not a big surprise to find out that adding perturbated patterns into training samples can impact the confidence of prediction on target classes during inference. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper contains a lot of content and is easy to follow which is good, but the main observation and conclusion are a bit weak for me.",
            "summary_of_the_review": "With the findings above, I currently give the paper a borderline score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1253/Reviewer_LUVv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1253/Reviewer_LUVv"
        ]
    },
    {
        "id": "hzJwyyXMXj",
        "original": null,
        "number": 4,
        "cdate": 1666686761794,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666686761794,
        "tmdate": 1666723964778,
        "tddate": null,
        "forum": "lrzX-rNuRvw",
        "replyto": "lrzX-rNuRvw",
        "invitation": "ICLR.cc/2023/Conference/Paper1253/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is dedicated to the problem of spurious correlations. The work systematically studies fundamental questions such as how many spuriously correlated training points are necessary for a neural net to get biased towards learning it. Specifically, it investigates the domain in which spurious correlations are rare. Interestingly the study indicates that even a single spuriously correlated sample can bias the learning of a neural network. Finally, the authors highlight three regularization methods for mitigating spurious correlations.",
            "strength_and_weaknesses": "Strength:\n\n1. The paper is well-motivated, organized, easy to follow and well-written.\n2. The study aims to answer fundamental research questions, which can provide new insights to the community.\n3. The authors provide theoretical justification for their findings\n\nWeakness:\n1. The technical contribution is somewhat limited in the sense that prior works have already shown that neural networks are more biased towards \"easy-to-learn\" spurious attributes. \n2. A major chunk of the experimental analysis is based on cases where the spurious correlation is injected synthetically. It would be more interesting if the authors also showed results on commonly studied spuriously correlated datasets such as Waterbirds, and CelebA.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good. The content is well-organized and easy to follow. This work provides extensive experimental evaluations and sufficient details. Novelty is fair. ",
            "summary_of_the_review": "The paper studies some important research questions in the domain of spurious correlations. The findings also provide non-trivial contributions to the community. However, because of the synthetic setup, the overall impression of this work is borderline.  I suggest the authors comment on the weaknesses mentioned above and I am open to changing my rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1253/Reviewer_bwQt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1253/Reviewer_bwQt"
        ]
    }
]