[
    {
        "id": "JJJ6aIq1Az",
        "original": null,
        "number": 1,
        "cdate": 1666619623941,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619623941,
        "tmdate": 1667497613684,
        "tddate": null,
        "forum": "zA7hVj3rR19",
        "replyto": "zA7hVj3rR19",
        "invitation": "ICLR.cc/2023/Conference/Paper3712/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduce a behavioral approach for identifying challenging images in large image datasets. They use their method to find thousands of challenging images in ImageNet and ObjectNet, and find that CLIP has to a great extent matched humans on these images. Through further analyses they elaborate on the failures seen by different classes of models. They release their datasets and code for the community to build off.",
            "strength_and_weaknesses": "\nStrengths:\n1. Timely investigation of challenging images for humans, with the goal of using those to drive the development of better models.\n2. The authors introduce a toolkit for mining existing datasets for these challenging images.\n3. A combination of web-based and in-lab experimental validation. Very nice agreement between these experiments.\n\n\n\nWeaknesses:\n\n1. Can these images be used to develop better models of AI or human cognition? There's a forward inference/positive result missing here in my opinion. It's not news that CLIP is a good model, so restating that isn't all that interesting to me. It doesn't look like there's any open challenge for AI revealed in these experiments.\n\n2. Does the introduced method identify challenging images consistently, or are some of these images merely poorly labeled exemplars that humans have trouble categorizing?\n\n3. I think the behavioral method is very complicated. Specifically, giving individuals the ability to make a 50-way classification judgement. We don't know the psychological distances between those categories so it's difficult to say whether or not a preponderance of certain categorical choices could makes it easier/harder to find a given category (i.e., it might be harder to distinguish between species of dogs )\n\n4. Make sure you ref [1] as that is most similar to this work for OOD classification.\n\n5. Measuring difficulty in humans is tough -- I agree that there should be images that are unrecognizable in fast presentations, but these same images should be recognizable with enough time. Otherwise, I am worried that the objects are just mislabeled. What do you think?\n\n6. Figure 7 is really cool but also complicated. Would it be possible to add some additional plots to highlight the insights you describe in the text about SimCLR etc?\n\n7. \"We are of course not the first to carry out such viewing time experiments\" I think you should cite the much longer history of rapid visual classification experiments: [2], [3], [4] to give a sampling of works over the years that investigate performance on natural image databases (ImageNet in the case of [4]) as a function of stimulus exposure time. The goal of these works is to limit viewing time to better interogate visual system mechanisms associated with the \"feedforward\" sweep.\n\n8. I suspect the findings of c-score and Figure 8 could be idiosyncratic for different architectures. For instance, \"This analysis reveals that images that require more viewing time... are predicted by later layers in the network.\" This is consistent with the plethora of studies analogizing ResNets and RNNs, which say that the skip connections enable stronger non-linearities and as a result the ability to process more complicated stimuli. What happens when you try other architectures like the ViT in CLIP?\n\n[1] Geirhos et al. Partial success in closing the gap between human and machine vision. 2021.\n\n[2] Fabre-Thorpe. The Characteristics and Limits of Rapid Visual Categorization. 2011.\n[3] Serre et al. A feedforward architecture accounts for rapid categorization. 2007.\n[4] Eberhardt et al. How Deep is the Feature Analysis underlying Rapid Visual Categorization? 2016.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear and the writing is high quality. The figures are fine, nothing fancy. The work is not very novel, and the results mostly align with Geirhos et al., 2021 who did a similar analysis, but the ability to extract challenge images from any dataset could be important. The results appear to be highly reproducible and I commend the authors for replicating their web-based experiments in the lab.",
            "summary_of_the_review": "I am totally borderline on this. I think the method for curating datasets is important but I have some quibbles about its details (50-way classification potentially introduces issues in the data \u2014 totally non-ecological). I think some of the analyses are interesting but there's no key finding here I can take to my group to say this is changing the way we think about AI or human vision. I look forward to seeing what the other reviewers say. The work is high quality but the impact of the presented results is what keeps me from recommending acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3712/Reviewer_aKwB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3712/Reviewer_aKwB"
        ]
    },
    {
        "id": "13H6v_fJtie",
        "original": null,
        "number": 3,
        "cdate": 1666894010014,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666894010014,
        "tmdate": 1666894010014,
        "tddate": null,
        "forum": "zA7hVj3rR19",
        "replyto": "zA7hVj3rR19",
        "invitation": "ICLR.cc/2023/Conference/Paper3712/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies a new way of measuring difficulty of computer vision datasets. The authors propose to use humans\u2019 viewing time as a difficulty measure and show that longer viewing time correlates with difficulty. They also show that the measured difficulty can be computationally modeled with a combination of prediction depth, c-score, and adversarial robustness. Analysis of SOTA models reveals that there exists larger gap on harder images.",
            "strength_and_weaknesses": "Strength\n\n* Paper is very well written and easy to follow. Experiments are well designed and systematically executed. Analyses and questions asked in the paper are thought provoking and high quality.\n* Demonstrated the feasibility of this metric as a difficulty measure and identified key next steps we need to work towards as a community.\n* The proposed approach is much more scalable than gaze tracking and can be potentially adopted by others with low cost.\n\n\nWeakness\n\n* Further discussion on what would be a good way to incorporate this metric intro a new data collection protocol  would be useful; how can researchers plan ahead and target at collecting more harder samples in the beginning?\n* It remains unclear whether viewing time would generalize well and serve as a good difficulty metric for other vision tasks such as optical flow\n\n\nI have a minor question on the study design; a set of words are displayed at the end of the viewing session - what if the viewer is not familiar with certain word and fails to answer it correctly because they do not know how the object looks like, but not because they do not recognize the image? \n",
            "clarity,_quality,_novelty_and_reproducibility": "This is a very well written and high quality work. The idea itself may not be original, but the way the problem was studied and analyzed is novel. Dataset and analysis code is released publicly.",
            "summary_of_the_review": "This paper studies an important problem for the computer vision community. The experiments and analysis are well organized and provides new insights. Given the overall strengths of the paper, I recommend acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3712/Reviewer_ZZZd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3712/Reviewer_ZZZd"
        ]
    },
    {
        "id": "sFg26mcrx2U",
        "original": null,
        "number": 4,
        "cdate": 1667488886341,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667488886341,
        "tmdate": 1667488886341,
        "tddate": null,
        "forum": "zA7hVj3rR19",
        "replyto": "zA7hVj3rR19",
        "invitation": "ICLR.cc/2023/Conference/Paper3712/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduce a novel dataset difficulty metric based on how long humans have to view an image in order to classify it correctly. The authors release the difficulty metrics for ImageNet and ObjectNet datasets as well as distribution of image difficulties in those datasets. The paper also introduce a new metric predicting object difficulty. ",
            "strength_and_weaknesses": "Strength:\n- Novel metric for evaluating image difficulty.\n- Valuable and deep analysis of the difficulty, easy and hard samples of ImageNet and ObjectNet datasets. \n- The analysis of various model performance on the samples of different difficulty\n- Promising results that can help constructing the new datasets for various Computer Vision Tasks\n\nWeaknesses:\n- The process of determining the difficulty of the sample is manual and tedious. The authors did not suggest the way to automate the process for the other datasets. It would be interesting to experiment whether a neural network can learn the sample difficulty function from the labeled data and whether this can be used to automatically select samples for labeling\n- The correlation of the model performance and the sample difficulty is expected but it is not proved whether additional \"hard\" samples would improve the model performance. As shown in the paper many of the \"hard\" samples do contain various degradations (occlusions, blur, bad illumination, etc.) The authors might perform a test training a model on 2 versions of the dataset: with less and more \"hard\" samples and evaluating the performance on a fair large scale test dataset.\n- The work does not introduce the updated version of the dataset and does not elaborate what the optimal distribution of the sample difficulty should be.\n- Some of the plots (Figure 7) are hard to follow, authors can change the format or the quantity of the models on the plot to improve the readability.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is relatively well written and easy to follow. The main idea of the paper is clear and well presented. The practical benefit of the difficulty function and the main claim though is somewhat questionable.",
            "summary_of_the_review": "The paper address the important topic of evaluating the dataset quality and difficulty and mining the hard samples. The authors introduce the way to manually compute the difficulty of the sample based on how ling it takes for a human to classify it. Since the process can not be automated for other tasks and it is not fully proven that more hard samples by introduced metric lead the the better model performance the benefit of the suggested metric is debatable. The additional experiments can prove the main concept and increase the usability of the suggested solution",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3712/Reviewer_y1Qy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3712/Reviewer_y1Qy"
        ]
    },
    {
        "id": "4QTwpAhF-Hv",
        "original": null,
        "number": 5,
        "cdate": 1667496728609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667496728609,
        "tmdate": 1667496728609,
        "tddate": null,
        "forum": "zA7hVj3rR19",
        "replyto": "zA7hVj3rR19",
        "invitation": "ICLR.cc/2023/Conference/Paper3712/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose to conduct psychophysical experiments using a combination of in-lab and\ncrowdsourced subjects to derive an 'objective' (that is, one decoupled from any particular model)\nmeasure of the distribution of sample-difficulty in two widely-used object recognition datasets,\nImageNet and ObjectNet. This differs from recent work that conducts similar experiments to assay\nthe gap between existing models and humans on OOD benchmarks and derive a benchmark using the\ncollected traits. The authors propose to use 'minimum viewing time needed to recognise an object\nwithin an image' as a proxy for sample difficulty, with an object considered 'recognised' if more\nthan half of participants could classify it. Four presentation times are considered for\ncategorisation, with objects recognisable within 17ms constituting the easiest samples, objects\nrecognisable within 10s constituting the hardest samples. The authors find that ImageNet and\nObjectNet are greatly skewed towards easier samples according to the proposed metric and suggest\nincreasing a representation of harder samples to be an important avenue for future work on creating\nmore robust models. The crowdsourced data is compared the data obtained under controlled lab\nconditions and found to be reasonably-well aligned, with discrepancies mainly occurring at the\nshorter presentation times. The authors show that the proposed measure of difficulty correlates\nwith performance-degradation in a variety of models and furthermore that this measure can be\npredicted to some degree using model-based difficulty measures such as c-score and prediction\ndepth.\n",
            "strength_and_weaknesses": "### Strengths\n- The motivation and its distinction from prior work is clearly established -- while similar\n  undertakings have been conducted before, the paper does explore a novel avenue in attempting to\n  quantify sample difficulty in a way decoupled from any given model via psychophysical trials.\n- The scale of the psychophysical experiments is commendable as is the rigor invested in trying to\ncontrol them and validate those conducted under less-controlled conditions (Mechanical Turk).\n  While the results for easier samples may not be entirely admissible, for the hard samples that\n  are of primary interest the two groups do seem to align reasonably well.\n- Figures are mostly clear and germane, with descriptive captions, though some figures are not\nobvious without reading of said captions (namely, Figure 5 and 7).\n- Discussing limitations in the experimental procedure is not shied away from -- the authors, for\n  instance, identify and concede issues with the shorter presentation times, stemming from \n  the use of crowdsourcing. \n- The authors are able to convincingly show correlation between their psychophysics-derived\n  metric and the performance of neural networks across a wide range of architectures. They make a\n  sound case, consistent with other recent literature, that current benchmark datasets for object\n  recognition are lacking in their ability to capture real-world diversity.\n- The authors show with large-scale experimentation that viewing time can serve as a valid\nmetric for sample difficulty across a variety of architectures.\n- The appendices are in their detailing of experimental procedure and materials and in presentation\n  of additional results.\n\n### Weaknesses\n- There is some important related work missing. Most notably, while the authors cite Geirhos et al.\n2018, they neglect to follow up the follow-up work from 2021 [1] which extends the suite of psychophysical\ntrials and shows a closing gap in the OOD performance of humans and machines. Although I understand\nthe goals of said work and the present work to be complementary (the former aiming to evaluate the\ndifferences in robustness, the latter aiming to devise an 'objective' measurement for sample\ndifficulty), given that the works employ similar methodologies to establish a benchmark with which\nto contrast human vs. machine performance (albeit under different regimes), I think some comparison\nis warranted.\n- As alluded to in the above section, while I understand the rationale behind the design choices\nfor Figure 7, it's not an easy figure to navigate visually. Similarly, Figure 5 is confusing for\nits use of presentation time both as a categoriser and as an axis (though I realise an\nexplanation of this is given in the accompanying caption).\n- The proposed method for evaluating sample difficulty is only obviously applicable to datasets of the same\ntype as ImageNet, that is those that are focused on single objects -- there's no obvious way to extend\nthe procedure to instance segmentation datasets, for instance, in which many objects may be\npresent in a scene and the task goes beyond simple image-level recognition.\n- Much of the introduction reads rather informally rather than as academic prose.\n\n(very minor) gripe: The reference associated with Geirhos et al. 2018 is an older (arxiv) version of the\ntext, not the version published at NeurIPS in 2018\n\n[1] Geirhos R, Narayanappa K, Mitzkus B, Thieringer T, Bethge M, Wichmann FA, Brendel W. Partial\nsuccess in closing the gap between human and machine vision. Advances in Neural Information\nProcessing Systems. 2021 Dec 6;34:23885-99.\n",
            "clarity,_quality,_novelty_and_reproducibility": "While the writing could be shored up in some places (e.g. instances of premature sentence\nfull-stops and abrupt transitions) the paper is generally well-written, with the premise,\nmethodology, and analysis all clearly and succinctly presented. The figures are generally germane and\nwell-constructed, though the grouping by presentation time along two axes in Figure 5 is slightly\nconfusing. Details of the experimental procedures, both psychophysical and computational, are given\nin the main text and expanded upon in the appendices. While on the surface similar to works such as\nGeirhos et al. 2021 [1], the paper adopts similar procedures to pursue the complementary objective\nof evaluating the distribution of sample difficulty within two prominent object-recognition datasets.\n\n[1] Geirhos R, Narayanappa K, Mitzkus B, Thieringer T, Bethge M, Wichmann FA, Brendel W. Partial\nsuccess in closing the gap between human and machine vision. Advances in Neural Information\nProcessing Systems. 2021 Dec 6;34:23885-99.\n",
            "summary_of_the_review": "The paper conducts an impressive array of both psychophysical -- involving human subjects -- and\ncomputational experiments in order to derive an validate an objective measure of sample-difficulty\nfor ImageNet and ObjectNet. While I'm not convinced about the extensibility of the method\nto problems beyond ImageNet-style datasets, the paper addresses and attempts to quantify an\nincreasingly-apparent problem; proposes and empirically justifies, through comprehensive analysis, an\nintuitive proxy metric for the datasets in question (which are among the most-used benchmarks in\nML); provides interesting insights into the datasets in question using the derived metric; is generally\nwell-written (consistently clear and concise); and does well in outlining the procedures for the psychophysical trials\nand addressing the ethics and limitations involved.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3712/Reviewer_KToy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3712/Reviewer_KToy"
        ]
    }
]