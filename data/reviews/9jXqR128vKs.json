[
    {
        "id": "o12Z7vw_ut",
        "original": null,
        "number": 1,
        "cdate": 1666540624875,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666540624875,
        "tmdate": 1666540624875,
        "tddate": null,
        "forum": "9jXqR128vKs",
        "replyto": "9jXqR128vKs",
        "invitation": "ICLR.cc/2023/Conference/Paper2033/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces In-Context Policy Iteration that learns to perform RL with foundation models without expert demonstrations or gradients",
            "strength_and_weaknesses": "Pros: (1) The paper is clearly written and easy to follow.\n\n(2) The proposed method is novel in my opinion.\n\nQuestions: (1) In Alg. 1, can the authors specify what are prompt \"parameters\"? Are these the only trainable parameters?\n\n(2) What will be the disadvantage of fine-tuning the foundation model parameters?\n\n(3) Can the method work on more general control tasks such as mujoco?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper writing is clear and the novelty is clear.",
            "summary_of_the_review": "The paper provides a simple idea of training RL with foundation models. But I'm not sure how previous related works conduct their experiments since the tasks seem very limited.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2033/Reviewer_8XAi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2033/Reviewer_8XAi"
        ]
    },
    {
        "id": "egEszyoTVzh",
        "original": null,
        "number": 2,
        "cdate": 1666573635183,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666573635183,
        "tmdate": 1666573635183,
        "tddate": null,
        "forum": "9jXqR128vKs",
        "replyto": "9jXqR128vKs",
        "invitation": "ICLR.cc/2023/Conference/Paper2033/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces in-context policy iteration (ICPI) an algorithm that can perform model-based policy iteration by leveraging a pre-trained large language model (LLM). ICPI uses prompts to create both a world model and a rollout policy out of the same LM. Specifically, the world model prompt uses examples from a buffer of transitions from the real environment and the rollout policy. ICPI then uses Monte Carlo rollouts of the rollout policy in the world model to estimate a Q function that is used to choose actions in the real environment. The paper presents a variety of experiments on toy problems with small, finite action spaces and small state spaces (finite or easily discretized) demonstrating improvement over some baselines.  ",
            "strength_and_weaknesses": "### Strengths\n\n1. The paper presents a clever way to leverage LLMs to perform RL entirely from in-context learning without any gradients. This is (to my knowledge) novel and seems like it could open up a new direction of research in using LLMs for RL.\n\n2. The paper is clearly written and experiments seem relatively thorough on the toy problems that were selected.\n\n### Weaknesses\n\n1. As the authors admit in the paper, the tasks presented are very small-scale. It is unclear to me what the key bottlenecks are in scaling this approach to larger tasks. It is clear that it is limited to finite action spaces and to the ability to encode the state space into simple Python code, but it does not seem too challenging to embed some more complicated environments in this format. The paper would benefit substantially from either (a) scaling up to harder tasks or (b) including a more detailed discussion of the bottlenecks that prevented scaling up in this project.\n\n2. The gains over the baselines were not always very clear and it was unclear how the baselines were chosen. On the first point, it would also be nice to see a more quantitative comparison to the baselines along with the regret curves. On the second, in particular, since all the domains are so small-scale it seems that it would be reasonable to include a more rigorous tabular baseline than Q-learning, for example a tabular model-based algorithm with more rigorous count-based exploration could potentially be a more fair comparison (especially since ICPI is using the LLM as a world model).\n\n3. There are no results that attempt to analyze what the algorithm is doing beyond plotting regret curves. It would be nice to either see (a) some more qualitative/exploratory results that explain what sorts of errors each of the considered methods makes in each task or (b) some theory about what is required from the LLM for ICPI to actually work. \n\n4. It is unclear whether how the \"hints\" fit into the standard RL framework and how they can be fairly compared against baselines that don't have access to them. This is not a major issue as the paper does provide the ablation without the hints, but it does seem like a potentially important caveat to the results that they required such hinting. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was clear and well-written.\n\nThe quality of the experiments was good up to the issues raised above.\n\nThe algorithm is novel and interesting.\n\nNo code was provided so it is difficult to say how reproducible the results are. ",
            "summary_of_the_review": "Overall, I think the core idea presented in the paper is interesting and could be valuable to the community. However, I do have some concerns about why some things were done the way they were, so I am rating the paper as a weak reject for now. If the authors are able to resolve my questions, I would consider raising my score. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2033/Reviewer_jeCL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2033/Reviewer_jeCL"
        ]
    },
    {
        "id": "YQShCSBwhHw",
        "original": null,
        "number": 3,
        "cdate": 1666614200525,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614200525,
        "tmdate": 1669283534161,
        "tddate": null,
        "forum": "9jXqR128vKs",
        "replyto": "9jXqR128vKs",
        "invitation": "ICLR.cc/2023/Conference/Paper2033/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper implements an implementation of the policy iteration algorithm using trained LLM.\n",
            "strength_and_weaknesses": "The strength of this paper lies in using an LLM without any training. This is also a weakness, at its present form, the paper feels not that significant. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, and of high quality, the presented method is correct.",
            "summary_of_the_review": "I enjoyed the paper a lot. The presented utilized an LLM and, without any training, is able to solve a few RL tasks. However, I have serious doubts about its significance. What important research questions is being answered? Further,  very simple environments are treated and even then, some tricks are required (i.e. ejecting information about reward). It does not feel like the method would scale to anything bigger?\n\nI admit not being an expert in the field of LLMs and I'd be happy to reconsider my evaluation. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2033/Reviewer_Kfri"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2033/Reviewer_Kfri"
        ]
    },
    {
        "id": "Jq183erlvvP",
        "original": null,
        "number": 4,
        "cdate": 1666641601723,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641601723,
        "tmdate": 1666641601723,
        "tddate": null,
        "forum": "9jXqR128vKs",
        "replyto": "9jXqR128vKs",
        "invitation": "ICLR.cc/2023/Conference/Paper2033/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose In-Context Policy Iteration, a Q-learning like algorithm that iteratively prompts large language models (LLM) with trajectories that have yielded better rewards and taps into the in-context learning abilities of LLMS to simulate policy rollouts. The proposed algorithm can self-improve without weight updates or expert demonstrations. \n\nAt its core, ICPI queries an LLM using past trajectory data (i.e. replay buffer) to obtain simulated rollouts that are used to estimate Q values, which are then used to take actions and augment the replay buffer. \n\nContributions:\n* The authors propose the ICPI algorithm, which enjoy the properties described above. \n* They evaluate it's performance on 6 toy tasks and report results on:\n  * a few sensible baselines, \n  * algorithm ablations, which involve some tips/tricks to get ICPI working, and \n  * the performance on ICPI using different core LLMs, including OpenAI's Codex. ",
            "strength_and_weaknesses": "**STRENGTHS**:\n* **No need for expert demonstrations:** The proposed algorithm doesn't require expert demonstrations, or any form of imitation learning. \n* **No weight updates:** There's no need to compute gradients through the LLMs during learning: only (discrete) prompts are refined over training to facilitate learning. \n* **Task difficulty appear well-chosen:** While relatively easy from an RL perspective, the six tasks presented in the paper appear to capture the right difficulty to test ICPI. Only Codex seems to get all tasks right, which gives useful signal as to what about the LLMs could be responsible for the reported performance. The chosen tasks are also in increasing complexity, and each display qualitatively different abilities. \n* **Nice variability of LLMs:** The authors seem to cover a sufficient range of publicly available LLMS. \n\n**WEAKNESSES:**\n* **Necessity of hints and tweaks:** It appears that for any sufficiently nontrivial environment, human-engineered hints are needed to get ICPI to work. The fact that some statistical rebalancing (of $D_r$ and $D_r$) also signal brittleness (which might potentially disappear as the LLMs get better).  \n* **Restricted to text domain:** Currently, ICPI is limited to domains where LLMs can exploit their pretrained knowledge (whether it be natural language or code). \n* **Issues regarding scaling:** The proposed method might have difficulty scaling to tasks with larger difficulty than those explored in the paper, as prompting LLMs with a large number of \"state-action-termination\" tuples when the dimensionality of these are high might become computationally challenging. \n\n**QUESTIONS TO THE AUTHORS:**\n* **Do LLMs play a unique role in this algoritm?** I wanted to ask further questions about the precise role LLMs play in the proposed algorithm. There doesn't appear to be an explicit pressure on the LLM to seek actions that yield higher returns (which could have been there if weight updates were allowed, or if it were prompted with expert trajectories, or perhaps the LLM were prompted with an explicit instruction to seek higher rewards). Instead, the algorithm seems to improve over time as the LLM is prompted with data that's increasingly coming from better (i.e. higher reward) rollouts. If this is the case, why would an LLM be necessary? Even if one randomly sampled termination, reward, next state and actions from $D_b$, $D_r$, $D_s$ and $D_a$ during rollouts, wouldn't learning still occur? What exactly is the benefit of using an LLM here? Perhaps the pretraining data helps pick better actions in a nontrivial way? \n* **Is Tabular Q tuned well?** All of the tasks seem simple enough that the Tabular Q algorithm should eventually learn the optimal policy (is this right?) However, that doesn't appear to be the case for certain environments. What seems to have gone wrong? Perhaps tabular-Q takes a bit longer to converge? Or perhaps the hyperparameters (most importantly the learning rate) aren't tuned separately for each task?\n* **Request for additional baselines and ablations:** At each rollout step, the LLM gets queried 4 times. Time permitting, could you please run an ablation where you systematically replace each one with a random sample from $D_x$ where $x \\in \\{b, r, s, a\\}$ while keeping the other three samples intact (i.e. they still use LLM)? I'm asking this to see if all of these queries are similarly important for the apparent superiority of ICPI over other baselines such as the \"matching model baseline\".\n* **Model outputs during the first few rollouts:** Since $D$ is empty at the beginning of training, it's not clear how the LLM should be prompted to get it to output sensible things. Is this every a problem? How do you address it? \n* **How exactly are $D_i sampled?** The text mentions that \"$D_b$ contains [...] tuples sampled randomly from the $D$\", Do you mind explaining how this exactly works? Is random sampling done uniformly? How many samples are picked? \n* **Further details on the Point-mass task:** Do you mind explaining what \"accel(pos, vel)\"does exactly?\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** For the most part, the paper is written clearly and the motivation is laid out very well. Perhaps adding further details to the descriptions outlining how $D_b$, $D_r$, $D_s$ and $D_a$ are picked during rollouts would help. \n\n**Novelty:** While the paper doesn't represent a conceptual leap (i.e. the ingredients of ICLI were present at the type of publication), the execution and precise formulation appear to be novel. \n\n**Reproducibility:** While the proposed algorithm is clear overall, producing a faithful replication of the work seems difficult (i.e. how the $D_i$ prompts are constructed is not extremely clear, etc.). There's also no released codebase. \n\n\n**Typos:**\n* Citep should be used when citing Codex in the abstract. \n* While described, $D_s$ is not mentioned in the second paragraph in the \"Computing Q-values\" section. \n* In the \"prompt-format\" section, the sentence \"Note that while hints are provided hints in the initial context, [...]\"\n* In \"comparison of ICPI with baseline algorithms:\" \"This baseline assumes access **to** a [...]\". \n\n\n",
            "summary_of_the_review": "The paper proposes ICPI, an algorithm that enables LLMS to learn to solve simple RL tasks without any expert demonstrations and weight updates. \n\nWhile there are concerns related to the broad applicability and scalability of this algorithm, I believe that the novelty and scientific value of the results justify acceptance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2033/Reviewer_wJ8R"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2033/Reviewer_wJ8R"
        ]
    }
]