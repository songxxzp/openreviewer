[
    {
        "id": "n5oAn2q5tA",
        "original": null,
        "number": 1,
        "cdate": 1666164461593,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666164461593,
        "tmdate": 1666164461593,
        "tddate": null,
        "forum": "sMsShmoszg",
        "replyto": "sMsShmoszg",
        "invitation": "ICLR.cc/2023/Conference/Paper5145/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "the paper benchmarks how various existing approaches for the problem of *DP synthetic tabular data generation* perform as the number of features and instances of the dataset change. in particular, the paper answers some previous under-studied research questions such as 1) how do methods scale with the number of columns. ",
            "strength_and_weaknesses": "strength: the paper is empirical in natural and aims to answer concrete practical questions. in this regard, the paper scores nicely as the experiments on evaluating performance is somewhat comprehensive in my opinion. \n\nweakness: i see two major drawbacks of this work, despite of its wide-covering experiments. \n\n1) many discoveries in this paper are rather intuitive and follows naturally from inspecting the algorithms. e.g., \"PrivBayes' performance on downstream tasks degrades when a stricter privacy budget is imposed...\" is the expected and much empirical verification is given in the original PrivBayes paper [1]. in this regard, there are only few truly novel empirical observations uncovered in the current work. \n\n2) the work uncovers two (somewhat) unexpected results in my opinion, but falls short in helping us understand how these empirical phenomena arise. e.g., on page 9, the paper suggests that \"we observed that more data does not always translate to improved quality for all models and evaluation.\" this is quite unexpected for DP machine learning, as workflows as such tend to be bottlenecked by fitting the training data (as opposed to combating overfitting). it'd therefore be useful to design and run experiments to understand why such observations are the case. for instance, are the results due to experimental noise? are the non-monotone results w/ GANs due to bad optimization? understanding why certain counter-intuitive behavior emerges would help future research in developing better methods.\n\n\n[1] Zhang, Jun, et al. \"Privbayes: Private data release via bayesian networks.\" ACM Transactions on Database Systems (TODS) 42.4 (2017): 1-41.",
            "clarity,_quality,_novelty_and_reproducibility": "clarity: the paper has clear writing. \nquality: the paper designed careful experiments to benchmark performance, but lacks more detailed ablation studies to uncover reasons why certain methods behave in certain ways. \nnovelty: some of the empirical observations are novel. but overall, the paper benchmarking existing methods and most observations are expected. \nrepro: results seem reproducible, to the best of my knowledge. ",
            "summary_of_the_review": "the paper designed careful experiments to benchmark performance, but lacks more detailed ablation studies to uncover reasons why certain methods behave in certain ways. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5145/Reviewer_uTaB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5145/Reviewer_uTaB"
        ]
    },
    {
        "id": "HL3B9UxpNaO",
        "original": null,
        "number": 2,
        "cdate": 1666652204440,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652204440,
        "tmdate": 1666652204440,
        "tddate": null,
        "forum": "sMsShmoszg",
        "replyto": "sMsShmoszg",
        "invitation": "ICLR.cc/2023/Conference/Paper5145/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors rigorously study the effects of varying N (number of samples) and d (dimensionality of the dataset) for 4 separate private generative modeling approaches (belonging to two different classes of algorithms). This is done at different privacy levels (epsilon) and the evaluation is based on both generative modeling metrics (correlation/mutual information between variables) as well as classification performance. As a result of this exercise, the authors are able to report some unique behaviors of different private generative modeling approaches. ",
            "strength_and_weaknesses": "Strengths: \n1) The experimental setup used by the authors is very rigorous \n2) The question of how different generative models differ as a function of N, d and epsilon are somewhat interesting questions.\n\nWeakness: \n1) Beyond highlighting some of the differences between the 4 generative modeling approaches, I am struggling to find the novel contribution of the paper. \n2) The title of the paper and the claim that it examines how the privacy budget is being utilized is quite misleading in my opinion. \n3) The authors only consider models for tabular data generation here but the paper could be significantly improved by considering models that generate more complex data modalities e.g. images. In the case of images for instance, one might be interested in wondering how the difference in learned convolutional filters as a function of N, d and epsilon. ",
            "clarity,_quality,_novelty_and_reproducibility": "While the paper is generally well written and easy to follow, the figures are uniformly too small with too much information compressed into them. ",
            "summary_of_the_review": "While I like the rigor of the evaluations done by the authors, I believe there aren't very many novel generalizable insights that come from the current version of the paper. As such, it reads more like a workshop paper at the moment. However, if the authors were to expand the paper to models for more complex data modalities, I believe it might make for an interesting conference paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5145/Reviewer_Rx8R"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5145/Reviewer_Rx8R"
        ]
    },
    {
        "id": "Y4SCyGxhPx",
        "original": null,
        "number": 3,
        "cdate": 1667258603095,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667258603095,
        "tmdate": 1667258603095,
        "tddate": null,
        "forum": "sMsShmoszg",
        "replyto": "sMsShmoszg",
        "invitation": "ICLR.cc/2023/Conference/Paper5145/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Synthetic data generation is a key application of differentially-private generative models---it allows you to build a synthetic dataset that can be used repeatedly without privacy risks for downstream ML or data science use cases. This paper presents an empirical evaluation of different DP generative models on mainly tabular datasets, e.g., studying how well they recreate the underlying data distribution and the various training costs.",
            "strength_and_weaknesses": "Strengths:\n* This is very much a \"in-the-weeds\" paper that compares various methods on different metrics and datasets. I think some of the resulting findings will be useful for practicioners, e.g., runtime of various methods as you vary the number of features and number of rows. I also think for researchers developing new DP generative models, it'd be useful for them to evaluate their models on metrics like the ones proposed here.\n* I find it interesting that there are cases when using more training data can sometimes hurt. It adds to the growing literature that tuning hyperparameters when training with DP can be quite difficult, non-intuitive, and not in line with what works best for non-private ML.\n* I like seeing downstream comparison of methods from different model classes, e.g., deep generative models trained with DP-SGD versus graphical modeling approaches.\n\nWeaknesses:\n* One concern I have is that some of the \"main findings\" of the paper are actually just statements about the properties of the methods themselves (see introduction and RQ1/RQ2 in Section 4.5). For example, the claim that \"deep generative models spend their budget per training iteration and can handle much wider datasets but become slower with more data\" can be made without an empirical evaluation---the method has those strengths/weaknesses because of the very nature of how DP-SGD and neural nets work.\n* While I think tabular datasets are important to study for a wide variety of real-world settings, 4/6 of the tasks in this paper are variants of sampling from a gaussian distribution.",
            "clarity,_quality,_novelty_and_reproducibility": "* I think the evaluation is decent but could be expanded to a wider set of tasks, including a wider set of tabular datasets, e.g., the entirety of the UCI ML repository. \n* The work is fairly original to my knowledge, it presents a wide empirical evaluation. \n* The writing is overall somewhat clear, but it is a bit repetitive between Sections 1, 4.5, and 5. It also spends a lot of the space introducing all of the methods compared.",
            "summary_of_the_review": "I think some of the findings can be useful to a broader community, but overall I find the evaluation and takeaways to be too limited to warrant a publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5145/Reviewer_vrg9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5145/Reviewer_vrg9"
        ]
    }
]