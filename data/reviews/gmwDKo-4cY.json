[
    {
        "id": "DrwZnpAXnK",
        "original": null,
        "number": 1,
        "cdate": 1666622913018,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622913018,
        "tmdate": 1670759040393,
        "tddate": null,
        "forum": "gmwDKo-4cY",
        "replyto": "gmwDKo-4cY",
        "invitation": "ICLR.cc/2023/Conference/Paper954/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an ensembling framework for use the capabilities of multiple foundational models. In cases where there isn't a unified interface to interact with a number of foundational models, it provides a unified framework to allow multiple foundational models to interact with each other. The paper proposes to divide the foundational models as \"Generators\" and \"Evaluators\". The \"Generator\" set of models generates proposals and the \"Evaluator\" sets of models evaluates the proposals and are used to iteratively refine the proposal using its gradients. ",
            "strength_and_weaknesses": "Strengths:\n\n* Elegant technique for combining the strengths of complementary models which are trained on large scale dataset \n* No retraining required which is expensive in most cases with large scale models\n* Results show effectiveness of the technique \n\nWeaknesses:\n\n* The change in the scores with the proposed technique seems minimal. \n* In industry use cases the GPU memory would overflow to fit all the models simultaneously.\n* The greatest benefit of this technique seems to be for Robotic Manipulation, reducing the scope of impact. ",
            "clarity,_quality,_novelty_and_reproducibility": "The text is quite clear. The experiments have been defined explicitly in the Appendix. The presentation of the ideas in the text can be more coherent but still sets a high bar of excellence. \n\nThe technique is novel, it provides a simple yet effective strategy of combining the strengths of related but relatively orthogonal models. The proposition of using a class of models as diverse as GLIDE, VilD, CLIP in the same framework is novel.\n\nThe technique proposed in the paper is quite clearly described and should be straight forward to reproduce it.",
            "summary_of_the_review": "This paper proposes a simple technique for combining the strengths of several large scale foundational models. It is especially interesting when large scale data across multiple domains is not readily available such as Robotics and Language Models. It would have been interesting to see some tasks which are only enabled with this system which would not be possible without the presence of this system.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper954/Reviewer_BdG8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper954/Reviewer_BdG8"
        ]
    },
    {
        "id": "7bbBSNGbK7",
        "original": null,
        "number": 2,
        "cdate": 1666644784189,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644784189,
        "tmdate": 1666644784189,
        "tddate": null,
        "forum": "gmwDKo-4cY",
        "replyto": "gmwDKo-4cY",
        "invitation": "ICLR.cc/2023/Conference/Paper954/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new framework for using pre-trained models for various recognition and reasoning tasks. The idea is to use a \"generator\" and an ensemble of \"scorers\" to iteratively improve the output: the generator proposes an output that the scorer scores. This general framework is demonstrated on multiple tasks, including visual question answering, solving math problems, image generation and robot manipulation. ",
            "strength_and_weaknesses": "Strengths:\n- The framework is very elegant. It uses the complimentary strengths of, for example, language models (as generators) and retrieval models like CLIP (as scorers).\n- The notion of iterative feedback to get consistency between the generator's output and the scorer is innovative.\n- The results, especially the zero-shot results, are quite impressive.\n\nWeaknesses\n- This proposed framework is very similar to the kind of optimization that appears in older literature on MAP estimation in structured prediction. A discussion on this would be useful.\n- Also worth discussing would be the effect of the limitations of the pretrained models on the proposed framework.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Approach looks very novel and high quality.",
            "summary_of_the_review": "I find the general framework quite interesting and novel. I think the connections to traditional iterative methods for structured prediction problems are very intriguing and should be discussed. That said, I find the approach innovative enough to be published.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper954/Reviewer_To1A"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper954/Reviewer_To1A"
        ]
    },
    {
        "id": "vc3JRNL2MJ",
        "original": null,
        "number": 3,
        "cdate": 1666830097307,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666830097307,
        "tmdate": 1669910223979,
        "tddate": null,
        "forum": "gmwDKo-4cY",
        "replyto": "gmwDKo-4cY",
        "invitation": "ICLR.cc/2023/Conference/Paper954/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the problem of leveraging individual expertise of various large pretrained models to compose an ensemble for solving various multi-modal problems in a zero-shot manner. The authors propose a unified framework, called PIC, in which the pretrained models are composed using closed-loop iterative consensus optimization. Specifically, for a given task, the framework employs a single model as a generator and an ensemble of scorers. The generator iteratively generates proposals and each scorer provides a feedback score indicating their agreement, then the generator refines its output until all the scorers achieve a final consensus. Experiments are shown on four zero-shot tasks - image generation, video question answering, grade school math, and robot manipulation. Results show that a composition of the pretrained models with an ensemble of scorers outperform baselines with a single scorer by a considerable margin.",
            "strength_and_weaknesses": "**Strengths**:\n\n- The paper writing, presentation, and figures are very good and well-structured.\n- The motivation to solve the problem of composing pretrained models is interesting and differences with prior works has been well highlighted.\n\n**Weaknesses**:\n\n-  As the main motivation of the paper is to leverage the individual expertise of the pretrained models, it would be great to have some qualitative analysis on the same. Specifically, as can be seen from Table 1 that the best performance is obtained when all the scorers E1, E2, E3 are used as an ensemble, more insights on identifying individual expertise of the scorers can make the submission stronger. E.g. \u201cwhich groups of classes does E1 expertise which E2 and E3 fails to?\u201d, \u201cWhat information does CLS-FREE provide which CLIP and CLS do not?\u201d, \u201cHow do the results change with different generators?\u201d. Getting answers to similar questions would be very interesting.\n\n- How about selecting one best scorer for providing feedback to the generative model? I understand that in the context of 3 scores it does not make much sense and the results already show that the combination works the best. However, in many practical scenarios given that HuggingFace models are easily available, there will be access to 100s of diverse models that can be considered as scorers. In those scenarios, is it still advisable to consider the combination for providing feedback to the generator? Authors should perform experiments with a diverse set of models trained using different datasets and of different sizes to verify how the capacity and knowledge of the individual models as scorer effects the final performance. The experiments in the current form are limited to demonstrate the potential of the method.\n\n- What about confidence of predictions for the different scorers? Ideally there should be a learnable weight signifying the importance of a scorer for a particular task and sample. I would encourage the authors to perform experiments for verifying the effect of such weights in the final performance.\n\n- Most of the experiments mainly compare the performance of PIC with different variants of the same method. Given that the proposed framework focuses on composing different models, how is comparable to state-of-the-art methods, e.g, Image generation or video captioning? Due to the lack of comparison with SOTA methods, it is not clear why one should use this framework in their application instead of using any other existing methods.\n\n- What about the memory requirement and FLOPs during inference and training of this method comparable to existing baselines?\n\n- How were the accuracies for the compared baselines JustAsk and JustAsk (FT) computed in Table 2? Was Amazon Mechanical Turk used for them as well? \n\n- It would be great to have a baseline experiment in which PIC is asked to select an answer from the same pre-defined set used for other baselines instead of using a free-form response approach. How is that baseline compared to state-of-the-art methods on the same dataset? A thorough comparison and analysis should be performed to justify the advantage and utility of the proposed framework.\n\n- As mentioned for Image Generation, analyzing the contribution of each of the scorers would be useful. E.g. \u201cWhat attributes does CLIP-multilingual specialize in?\u201d Analysis on the usage of GPT-2 is missing. How much does GPT-2 contribute towards the overall performance in video question answering?\n\n- The paper also lacks some implementation details of the optimization procedure. Specifically, details like number of iterative steps used for optimizing for different tasks, learning curves with iterations (can be included in supplementary), optimization time required for each task, details of the context cache - size, initialization, etc.\n\n- Experiments showing applicability of the framework to different permutations of possible generators and scorers are missing. E.g. getting results by using different generators for a given task.\n\n- Experimental comparison with the discussed prior work Socratic Models (Zeng et al., 2022) is also not provided. If it is not possible to compare them in the experimented tasks due to language interface, how about comparing the proposed framework on the tasks used for zero-shot evaluation on Socratic Models. ",
            "clarity,_quality,_novelty_and_reproducibility": "The quality and clarity of the work is good with the problem statement properly conveyed to the reader. The introduction is quite comprehensive covering the current state of research in the topic. Related works are well discussed. The method used for solving the four zero-shot tasks have been properly explained and are novel. But as mentioned in the weaknesses section above, extensive experiments and analysis is missing, which is very important for understanding the advantage of the proposed framework.\n\nThe use of Amazon Mechanical Turk to get the accuracy for the video question answering task hinders easy reproducibility of the corresponding experiments. An alternate metric not involving the requirement of human feedback would be encouraged to make the experiments more reproducible. Additionally, including details of some hyperparameters (check weaknesses) can increase reproducibility.\n",
            "summary_of_the_review": "I think the identified problem is important but the proposed method is a bit naive and experiments are not convincing. The proposed method should be evaluated and analyzed using extensive experiments resonating with the motivation to solve the problem. In the rebuttal I expect new experiments and analysis listed in the weaknesses to convince me that the proposed method has practical use and outperforms the missing baselines and exiting methods.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper954/Reviewer_dTDv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper954/Reviewer_dTDv"
        ]
    },
    {
        "id": "bT5JaSYvb2n",
        "original": null,
        "number": 4,
        "cdate": 1667261755067,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667261755067,
        "tmdate": 1667261755067,
        "tddate": null,
        "forum": "gmwDKo-4cY",
        "replyto": "gmwDKo-4cY",
        "invitation": "ICLR.cc/2023/Conference/Paper954/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed to iteratively optimize a system of one generator and multiple classifiers(scorers) for zero-shot generation tasks. It unified four multimodal generation tasks under the same framework. And experiments show that it can have a satisfactory generation ability on those tasks.",
            "strength_and_weaknesses": "Strengths:\n1. This framework doesn't need further task-specific pre-training and finetuning. It only needs to iteratively update the pre-trained models to refine the generation result. \n2. The idea of using scorers to guide generators to reach a consensus is interesting and seems to be generalizable to four different tasks as in this paper.\n\nWeaknesses:\n1. For the methodology, this paper mainly instantiate the proposed method into two procedures: (1) continuous updating on image generation. (2) Discrete optimization over samples.  As for the first one, the whole method is quite similar to VQGAN-CLIP[1] in that they both optimize the image generator's input by a CLIP model to generate a better image. The difference is: at VQGAN-CLIP, the image generator is VQGAN and the optimized input is a latent feature, while this work used a diffusion model and the optimized input is a noisy image. Despiting the minor difference, those two share similar intuition and working mechanisms. For the second procedure, the caption generation optimization is almost the same as ZeroCap[2]. As mentioned in Figure A4's caption, this paper exactly used Equation 5 in ZeroCap to do the optimization. So is the Grade School Math text generation task. Overall, Although I appreciate the author(s)' efforts in unifying the different tasks in the same framework of iterative optimization, I still think the distinction between previous works and methodology contributions to the community is limited.\n2. For the experiments, all those four tasks lack enough comparison with previous works. In Zero-shot videoQA, only JustASK is compared. In GSM8K, only GPT is compared. Moreover, the comparison seems not fair to me because the proposed PIC assumes to have a question-solution classifier, while GPT-FT(Pretrain) does not. In image generation and robot manipulation, no other baseline methods are compared.\n3. Possibly Wrong/Confusing Illustrations. a). In Figure 2c, how does the input video frame feed into GPT-2? b) The explanation of $L_{CLIP}$ in Video question answering (VQA) of Sec3.1 is so confusing. I cannot really understand until I go back to read ZeroCap.\n\nRef:\n[1] VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance, ECCV2022.\n[2] ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic, CVPR2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of some methods part is questionable, it is not friendly to readers. This paper has certain novelty in unification but the core idea is too similar to previous works. The reproducibility and quality seem okay to me.",
            "summary_of_the_review": "Overall, I prefer to weak rejection because of limited novelty and lack of comparison.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper954/Reviewer_kbow"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper954/Reviewer_kbow"
        ]
    }
]