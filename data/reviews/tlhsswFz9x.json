[
    {
        "id": "g5uT-qrz6ca",
        "original": null,
        "number": 1,
        "cdate": 1666644002725,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644002725,
        "tmdate": 1666644002725,
        "tddate": null,
        "forum": "tlhsswFz9x",
        "replyto": "tlhsswFz9x",
        "invitation": "ICLR.cc/2023/Conference/Paper2562/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an end-to-end differentiable graph generator that builds the graph topology simultaneously as learning for a downstream task. The proposed idea extends the applicability of GNNs to cases where a graph topology is not readily available. The experimental results show that the proposed model improves the performance of node classification ad trajectory prediction tasks.",
            "strength_and_weaknesses": "\n**Strengths**\n\nThe proposed method helps create a similarity graph with a learnable node degree for each node in the graph.\n\n**Weaknesses**\n\nThe proposed method does not discuss or compare with a large body of related work for learning a graph topology on the fly (e.g., [1, 2, and 3]). Without comparing with the existing works, the contribution of the proposed method is not measurable.\n\nThe time complexity of the proposed approach is not discussed. For the edge ranking step, the similarity of each pair of nodes should be computed and later used in the Gumbel-Softmax. This means O(n**2) time complexity with n as the number of nodes in the graph. \n\nThe datasets used in this paper are relatively small compared to the current benchmarks (e.g., OGB graphs [4])\n\n[1] Learning discrete structures for graph neural networks, ICML 2019.\n\n[2] Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings, NeurIPS 2020.\n\n[3] SLAPS: Self-supervision improves structure learning for graph neural networks, NeurIPS 2021.\n\n[4] Open graph benchmark: Datasets for machine learning on graphs, NeurIPS 2020.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. \n\nThe code and details on training the model is not provided.\n",
            "summary_of_the_review": "**Questions**\n\nTable 2: It is not clear to me what is used as input to DGG for this experiment. Is the original graph structure for Cora, Citeseer, ... used in this experiment? Also, as mentioned earlier, many baselines are missing here.\n\n\nThe intermediate loss: Why adding the intermediate loss can help? Why doesn't the downstream task loss do the same thing as encouraging nodes in the same class to be connected and nodes in a different class to be disconnected?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2562/Reviewer_CYQs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2562/Reviewer_CYQs"
        ]
    },
    {
        "id": "qPgkvPOwyO",
        "original": null,
        "number": 2,
        "cdate": 1666776819488,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666776819488,
        "tmdate": 1666776819488,
        "tddate": null,
        "forum": "tlhsswFz9x",
        "replyto": "tlhsswFz9x",
        "invitation": "ICLR.cc/2023/Conference/Paper2562/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a method for learning a graph topology that can be integrated in any GNN framework. The proposed probabilistic method is built on a differentiable graph operator that is able to decide the size of the neighbourhood of each node (i.e., degree of each node), as well as the corresponding edges. Experimental results on i) different GNN architectures and ii) a wide set of datasets are provided. ",
            "strength_and_weaknesses": "Strength: The topic is of interest to the (graph) machine learning community. Updating/learning/rewiring graphs for a final end-task is an open problem with some approaches already proposed. This work builds on those approaches and improves some of those aspects. The paper is very clearly written and easy to follow. \n\nWeaknesses:  I don't see a strong weaknesses of the paper expect from the fact that is it incremental in comparison to the existing literature. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, and of relatively high quality. The approach is relatively novel, and adequate details are provided to reproduce the results. I believe that once the authors release the code the work can be easily accessible. ",
            "summary_of_the_review": "Overall, this is a well-written paper addressing an interesting topic. Although there is quite a lot of work in learning the connectivity matrix in a GNN framework, this paper brings one additional solution to the problem. Thus, the problem itself is not novel, but the proposed solution is valid. \n\nSome additional comments that could hopefully help the authors are the following:\n\n1) In Eq. (8), the dimensionality of $z_i$ and $h_i$ does not seem to be the same. The former is $d$ and the later is 1. Could you please explain and clarify?\n2) Can you discuss on the type of edges that you learn? If I understand correctly the learned graph is directed. Is it possible to extend it to an undirected graph, which might be more suitable for some applications?\n3) In the training details, the authors mention that \"to speed up training, we add an intermediate loss...\". Can you please elaborate on that? It seems that the loss promotes a specific class of graphs that consists of community structures. Is that correct? Please clarify. \n4) Related to the previous comment, it would be useful to understand more the properties of the learned graph. Can the method be extended to unsupervised settings?\n\nSome examples typos that should be fixed: \n- p.7: 'fixe k': remove e \n- p.12: 'signle': single ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No specific concerns. ",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2562/Reviewer_Ds7V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2562/Reviewer_Ds7V"
        ]
    },
    {
        "id": "BcfUsaZz90T",
        "original": null,
        "number": 3,
        "cdate": 1666784393247,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666784393247,
        "tmdate": 1666857051883,
        "tddate": null,
        "forum": "tlhsswFz9x",
        "replyto": "tlhsswFz9x",
        "invitation": "ICLR.cc/2023/Conference/Paper2562/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, a joint topology and graph representation learning framework is proposed to tackle the adjacency matrix missing scenarios. The method could also be applied to the given graph topology to improve the emirical performance. The distinguishing point is the incorporation of adapative number of neighbors k which could be differentiably learnt via a Gumbel-softmax trick. However, the paper lacks more detailed discussion and experiment with recent graph structure learning models.",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper aims to optimize a graph model that jointly learns the topology and graph representation in an end-to-end differentiable manner. The cases where adjacency matrix is not provided are supported, as well as refine the given adjacency to improve the topology for better performance\n\n2. A framework is proposed that can plugs in a wide range of existing GNN methods\n\n3. An insight of this work is that it considers the variable number of neighbors in the graph topology learning. The differentiablity is implemented by a combination of degree estimator and top-k neighbor selector.\n\n\nWeaknesses:\n\n1. An important line of related works are missing. The paper claims that it is the first to consider differentiable end-to-end learning of graph topology, and however, it is wrong. Differentiable graph structure learning has been explored by quite a few existing studies, e.g. [1-5]. In particular, [1] uses bi-level optimization for learning structures/representations as the outer/inner loop, [2, 3] harnesses variational inference for joint learning topology and node representations, [4] considers iterative learning of the two things and [5] proposes a scalable model that learns optimal topology with Gumbel trick in each representation layer. A detailed comparison with these works are needed to illuminate the differences and novelty.\n\n2. The experiment only compares with the methods which drops edges from the orignal graph, instead of the more related and strong competitors that learn the graph structure, especially the works that involve graph topology and representation joint learning, e.g. the above-mentioned models.\n\n3. The proposed method seems to have O(N^2) complexity w.r.t. node numbers and not scalable enough for large datasets. The experiments are only conducted on small datasets, instead of the larger ones, e.g., from OGB. \n\n4. The paper claims that the framework fits with any \u201cgraph convolutional networks\u201d, are there any reasons that other spatial GNNs based on message passing do not fit here?\n\n5. The paper uses Gumbel-softmax for edge probabilty estimation. However, the pairwise edge probability is estimated independently of each other. Under this assumption, the dependencies among edges are not taken into account. Are there some jusfitication of this assumption?\n\n[1] Learning Discrete Structures for Graph Neural Networks, ICML19\n\n[2] Variational Inference for Graph Convolutional Networks in the Absence of Graph Data and Adversarial Settings, NeurIPS20\n\n[3] Variational Inference for Training Graph Neural Networks in Low-Data Regime through Joint Structure-Label Estimation, KDD22\n\n[4] Iterative Deep Graph Learning for Graph Neural Networks - Better and Robust Node Embeddings, NeurIPS21\n\n[5] NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification, NeurIPS22",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper in general is well organized and easy to follow.\n\nQuality:\nThe paper is of fair quality in general.\n\nNovelty:\nThe focused technical aspect of this paper is not novel. There are a few existing works that have already studied joint and differentiable learning of graph topology and node representations (see weakness part for details). Also the proposed method is a combination of techniques that have already been explored by a few existing works (see weakness part for details). In contrast, the differentialable learning of adaptive number of neighbors is somewhat novel.",
            "summary_of_the_review": "This paper studies an important problem. However, the proposed method lacks enough novelty, especially quite a few proposed components have already been explored by prior art in the same direction. Also, an important line of related works are missing without any comparison, which makes the technical contributions of this work not convincing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2562/Reviewer_9Mc3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2562/Reviewer_9Mc3"
        ]
    },
    {
        "id": "6xUQO42TeI4",
        "original": null,
        "number": 4,
        "cdate": 1666856426016,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666856426016,
        "tmdate": 1666856426016,
        "tddate": null,
        "forum": "tlhsswFz9x",
        "replyto": "tlhsswFz9x",
        "invitation": "ICLR.cc/2023/Conference/Paper2562/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a differentiable graph-generator that builds the graph topology on the fly.\n",
            "strength_and_weaknesses": "Strength:\n- The paper is clearly written.\n- The approach is technically sound.\n\nWeakness:\n- The idea of learning the graph structure for GNN is not novel. I have read several papers with similar ideas, for example, http://proceedings.mlr.press/v97/franceschi19a/franceschi19a.pdf\n- The proposed method is only evaluated with basic GNN models - the reported number is far below the state-of-the-art performance, on the classic benchmark datasets being used.\n- The approach is clearly not going to scale to large graphs. And I did not find discussions regarding the scalability of the method.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. The font of the submission is not standard, though. ",
            "summary_of_the_review": "Overall, this paper proposes yet another method for applying Graph Neural Networks to datasets without explicitly relational structure through inferring latent graphs. While the proposed method is technically sound, it lacks technical novelty. Also, the empirical performance is weak, and the method is not scalable.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2562/Reviewer_ob93"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2562/Reviewer_ob93"
        ]
    }
]