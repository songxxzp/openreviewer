[
    {
        "id": "xHweo5_mMNI",
        "original": null,
        "number": 1,
        "cdate": 1666205326128,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666205326128,
        "tmdate": 1670253532465,
        "tddate": null,
        "forum": "3YFDsSRSxB-",
        "replyto": "3YFDsSRSxB-",
        "invitation": "ICLR.cc/2023/Conference/Paper1934/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method to learn representations for image retrieval with two main ideas: 1) during pre-training, automatically clustering a large image-text dataset and randomly selecting inter-class prototypes to avoid noisy assignments, and 2) to reduce embedding dimensionality, randomly select feature dimensions to be deactivated. Experiments are performed on standard datasets in the area of image retrieval, besides on standard image classification datasets, comparing against unsupervised and supervised methods.",
            "strength_and_weaknesses": "Strengths:\n\nS1) A well-motivated idea to improve unsupervised learning via cluster discrimination. Cluster discrimination may suffer from inter-class conflicts of the prototypes, and the authors propose a very simple and efficient method to deal with it: randomly sample the negative prototypes. Ablation experiments show large improvements when using this idea.\n\nS2) The proposed pre-training method outperforms other CLIP training methods on image classification linear probe experiments and on unsupervised image retrieval experiments. The comparison is strict against the OPEN-CLIP work, where the exact same dataset is used.\n\nS3) The results on the supervised image retrieval tasks outperform other recent competitor methods.\n\nS4) Overall, the experiments are comprehensive, including ablations and even extra comparisons on GLDv2 and the recent Google Universal Embedding Challenge.\n\nWeaknesses:\n\nW1) Random selection of class prototypes seems very simple and effective, but I am wondering whether something smarter can be even better. For example, discarding \u201chard negative prototypes\u201d directly instead of only sampling randomly. The paper does not provide such comparisons, which would make the proposal stronger.\n\nW2) I wonder if it is fair to call the proposed method \u201cunsupervised\u201d. The technique works on a large dataset of images and associated text, which could be considered already some form of weak supervision (it has more details than the Instagram hashtag dataset, for example). The CLIP text embeddings are even directly used to perform clustering. Generally, \u201cunsupervised\u201d methods work directly on images only and nothing else.\n\nW3) I don\u2019t see a direct connection between equations (1) and (2) in section 3.1. How is equation (2) actually maximizing the log-likelihood function? Do you mean that p(x_i;\\theta) would be the probability of drawing the augmented image? Please explain. Similar comment for equations (3) and (4).\n\nW4) Figure (3)-a is confusing. By \u201cclass number\u201d, do the authors mean the ID of the class? By \u201cphoto number\u201d, do you mean the number of photos in that class?\n\nW5) There is a very straightforward connection between the random feature subsampling and the concept of Dropout. This connection is not made, but it should be.\n\nW6) The random feature subsampling method works comparably to PCA or the addition of an FC layer to reduce dimensionality in the network. I don\u2019t see a concrete benefit of using it, compared to a simple FC layer to reduce dimensionality, which is more principled.\n\nW7) The story of the paper is inconsistent. There are many experiments on image classification datasets (section 4.2), but the paper is directed at the image retrieval problem.\n\nMinor details:\n\nM1) In the caption of Fig 1: the word \u201cdataset\u201d is repeated on the second line.\n\nM2) Page 3: \u201ccluttering\u201d -> \u201cclustering\u201d\n\nM3) Section 3.1: the set X  is repeated twice in the same first sentence, please remove one of them to make it more concise.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is generally clear and the reader can understand what the authors are proposing. However, the story is not very clear/consistent, as discussed above.\n\nQuality: The idea of improving cluster discrimination via random negative prototype selection is shown to work clearly, and is simple. Experiments are well done. However, I see several issues as listed in the weaknesses above.\n\nNovelty: Novelty on improving cluster discrimination as mentioned above. The random feature subsampling is also novel, but doesn\u2019t sound well motivated to me.\n\nReproducibility: The approach is generally simple, I don\u2019t foresee reproducibility issues. The authors also promise to release code and models to ensure reproducibility.",
            "summary_of_the_review": "This is a paper with interesting ideas, some of which are simple and shown to work well, for an important problem in computer vision. Experiments are comprehensive. However, I also see several issues at this point, as described in the weaknesses above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1934/Reviewer_HyPg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1934/Reviewer_HyPg"
        ]
    },
    {
        "id": "U_CrR6OrINI",
        "original": null,
        "number": 2,
        "cdate": 1666567772325,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666567772325,
        "tmdate": 1666567772325,
        "tddate": null,
        "forum": "3YFDsSRSxB-",
        "replyto": "3YFDsSRSxB-",
        "invitation": "ICLR.cc/2023/Conference/Paper1934/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a self-supervised image representation approach that leverages using pre-existing CLIP image-text embedding. The approach first clusters images using CLIP embedding, and then learns image feature with softmax loss using cluster ids as class ids. In addition, random dimension sampling is employed during learning to achieve low dimensionality for retrieval tasks. The authors conducted extensive results on benchmark datasets to demonstrate the effectiveness of proposed approach in various settings.",
            "strength_and_weaknesses": "Strength \n+ Intuitive idea and easy to reproduce\n+ Extensive experimental results + ablation studies\n\nWeakness\n- Novelty is a bit low. Sampled softmax is a commonly used technique. \n- Dimension sampling is a bit strange. A more natural approach to me would be to directly learn a projection to low dimensional feature space. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Overall paper is well written. Only non-clear part is that, during softmax learning, whether the entire image encoder is learned from scratch, or fine-tuned from the pre-existing CLIP image embedding. Would be good to compare the two in ablation studies.\n\nNovelty: low (see above)\n\nReproducibility: approach is very intuitive and easy to reproduce",
            "summary_of_the_review": "While approach is appealing and practically useful, the novelty seems slightly below the bar of ICLR.  In additional, the dimension sampling part seems unnecessary to me. I feel directly learning a feature projection would be more intuitive and work better.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1934/Reviewer_FUwi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1934/Reviewer_FUwi"
        ]
    },
    {
        "id": "DI6goi1rc-W",
        "original": null,
        "number": 3,
        "cdate": 1666654502056,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654502056,
        "tmdate": 1666654502056,
        "tddate": null,
        "forum": "3YFDsSRSxB-",
        "replyto": "3YFDsSRSxB-",
        "invitation": "ICLR.cc/2023/Conference/Paper1934/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to use random sampling for samples in clustering classes to avoid inter-class conflict and a random dropout mechanism for features to generate compact features.",
            "strength_and_weaknesses": "The method is easy to understand and implement. The experiments about using VL models for zero-shot/transfer learning tasks are exhaustive.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of the paper is not significant. The sampling dimension of the feature is not too much different from Dropout method[1], which has been widely used in CV/ML pipelines.\n\n\n\n[1] Srivastava, Nitish, et al. \"Dropout: a simple way to prevent neural networks from overfitting.\" The journal of machine learning research 15.1 (2014): 1929-1958.\n",
            "summary_of_the_review": "As I mentioned above, although the method is easy to implement, the novelty of this paper is not significant. Especially for feature random selection part. For class selection, although it is easy to implement, it lacks further detailed analysis and improvement. For instance, we can use another clustering method for each cluster to exclude the potential inter-class conflict. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1934/Reviewer_1Pq2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1934/Reviewer_1Pq2"
        ]
    },
    {
        "id": "GcpPINHdnGj",
        "original": null,
        "number": 4,
        "cdate": 1667050416142,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667050416142,
        "tmdate": 1667050416142,
        "tddate": null,
        "forum": "3YFDsSRSxB-",
        "replyto": "3YFDsSRSxB-",
        "invitation": "ICLR.cc/2023/Conference/Paper1934/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the problem of representation learning for image retrieval and follow-ups on recent self-supervised methods that use cluster discrimination to handle the limitations of instance discrimination. These methods however either require to perform iterative clustering, or online clustering to avoid multiple iterations over the entire dataset but that still suffer from the collapsing problem. The authors propose a method that performs random selections along class and feature to alleviate the inter-class conflict and to improve feature compactness that only requires a single pre-step of off-line clustering.",
            "strength_and_weaknesses": "Pros\n- The idea and motivation behind the method is presented very well by pointing out the limitations of currents methods regarding their clustering approach and proposing a possible solution. \n- This idea is also very simple, which makes it widely applicable, not only to this problem/setting.\n- Experiments are also extensive and thorough.\n\nCons:\n- The fact that you need access to a large pre-trained model it makes its applicability more limiting and not directly comparable to OPEN-CLIP. I see the approach as a way of further improving the results of a given base model, CLIP in this case.\n- Related to my previous point, and given that we have assume that we have access to CLIP and that we are going to train the same backbone architecture (is this true?), would it make sense to initialize the backbone using CLIP's weights? Would this speed up training without impacting performance? This might address the problem of needing to have access to a large cluster of GPUs.\n- It would have been interesting to see this applied to other large pre-trained models used as base, eg DINO or MAE.\n- In table 8, I did not understand what are the different columns below Unicom. Could the authors elaborate please?",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well written and easy to understand.\n- This code is easily reproducible because the idea is simple, but the experiments only if you have access to 128 V100 GPUs\n- The idea of subsampling classes and features might not be novel in general, but it is when applied to this context.",
            "summary_of_the_review": "The paper is well written and the idea and motivation behind the method is presented very well. This idea, although simple (this also makes it widely applicable) it is novel in this context. I also think that the methods has some limitations, like the fact that one assumes access to a large pre-trained model.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1934/Reviewer_htKd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1934/Reviewer_htKd"
        ]
    }
]