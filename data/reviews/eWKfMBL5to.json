[
    {
        "id": "egVeoyFtZNO",
        "original": null,
        "number": 1,
        "cdate": 1666670039110,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670039110,
        "tmdate": 1666670039110,
        "tddate": null,
        "forum": "eWKfMBL5to",
        "replyto": "eWKfMBL5to",
        "invitation": "ICLR.cc/2023/Conference/Paper4994/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes contrastive corpus similarity as a scalar explanation output for unsupervised models. The method relies on a corpus (positive samples) and foil (negative samples or distribution). For a given input, it computes the average cosine similarity between its encoded representation and those of the corpus and subtracts the average similarity with the foil representations. Combined with existing feature attribution methods, the proposed approach can identify relevant input features.",
            "strength_and_weaknesses": "Strengths\n\nThe approach is quite simple, yet effective as demonstrated by the strong results.\n\nThe figures clearly help demonstrate the potential impact and applications of the approach.\n\nWeaknesses\n\nTo identify the corpus and foil, it is sometimes necessary to have some labeled data.\n\nIt is unclear why the results do not include the performance of RELAX.\n\nIf the feature representations of the corpus are multimodal (i.e. you get high similarity with some corpus examples, but low similarity with others), the proposed method might arguably assign too low of a score.",
            "clarity,_quality,_novelty_and_reproducibility": "The authors appear to have been rigorous, and I didn't check all the proofs in the appendix. The paper is mostly clear, although some of the formulae are quite complex and (arguably) hard to parse quickly. To my knowledge, the work is novel, although I may be unfamiliar with some of the existing literature. The supplementary material includes code, but the documentation (e.g. the README file) is lacking.",
            "summary_of_the_review": "The paper proposes a simple, effective and potentially impactful method to generate contrastive feature attributions for unsupervised models.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4994/Reviewer_Y75o"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4994/Reviewer_Y75o"
        ]
    },
    {
        "id": "otJM4DlD9F",
        "original": null,
        "number": 2,
        "cdate": 1666865293205,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666865293205,
        "tmdate": 1666884602705,
        "tddate": null,
        "forum": "eWKfMBL5to",
        "replyto": "eWKfMBL5to",
        "invitation": "ICLR.cc/2023/Conference/Paper4994/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This study indicates contrastive document similarity as a scalar\u00a0explanation output for\u00a0unsupervised methods. A corpora of samples collected and foil are used in the process (negative samples or distribution). It calculates the average cosine similarity between an input's encoded representation and those in the corpus, subtracting the average similarity with foil representations, for the given input. The suggested method can find pertinent input features when used in conjunction with current feature attribution methods.",
            "strength_and_weaknesses": "Strength\n1. The approach is simple, and\u00a0appears to work well, as demonstrated by the excellent results.\n2. The figures hopefully demonstrate how the method might be employed and what sort of impacts it could have.\n\nLimitation\n1. More often than not, the author to\u00a0labeled the\u00a0data to determine what the\u00a0corpus and foil are.\n2. If the feature representations of the dataset are heterogeneous in nature (high commonality with some corpus examples and low similarity with others), the proposed model might start giving a scoring rate that is excessively limited.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has been mostly straightforward to understand, although some of the equations are really quite complex and difficult and could be challenging to figure out speedily. According to what I understand, the research is innovative, and yet I may not be intimately acquainted with all of the related literature works. There should README file in the supplementary material that could easy for readers to follows.",
            "summary_of_the_review": "The article proposes a simple, efficacious, and potentially useful method for generating attribute inferences for unsupervised models that really are different from each other.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No comments",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4994/Reviewer_9VMz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4994/Reviewer_9VMz"
        ]
    },
    {
        "id": "EVEO9ib2TVj",
        "original": null,
        "number": 3,
        "cdate": 1666940569153,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666940569153,
        "tmdate": 1672558378180,
        "tddate": null,
        "forum": "eWKfMBL5to",
        "replyto": "eWKfMBL5to",
        "invitation": "ICLR.cc/2023/Conference/Paper4994/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for post-hoc explanations of unsupervised methods. At the heart of this proposed idea is the *contrastive corpus similarity* which employs a corpus set of samples (e.g. abnormal cases in an illustrative task of classifying bone abnormality) and a foil set of samples (e.g. normal cases for the same task). The proposed method uses the notion of contrastive corpus similarity to generate Contrastive Corpus Attributions (COCOA). COCOA works by identifying the features that, when removed, make the representation of an explicand (a sample to be explained) similar to the foil set and dissimilar to the corpus set (hence the term `contrastive' in the name). \n\nThe authors evaluate COCOA to establish whether the identified features are indeed related to corpus and foil samples using different models (SimCLR, SimSiam etc.), datasets, and feature attribution methods. They also demonstrate the utility towards understanding image data augmentation and mixed modality object localization. ",
            "strength_and_weaknesses": "### Strengths\n\n1. Writing and notational discipline: The idea is clearly described, and the notations introduced are well-explained. I cannot find any errors in the notations. \n\n2. Simplicity: The idea of using contrastive similarity towards a foil set and away from a corpus set is simple and elegant. \n\n3. Extensive experiments: The experiments are comprehensive, or as extensive as can be expected within one paper. The sanity checking in the Appendix is also appreciated. \n\n### Weaknesses\n\n1. Conceptual Concerns and situating within the broader concerns surrounding XAI (Explainable AI)\n\nThe paper early-on cites the paper on \"The Mythos of Model Interpretability\" by Lipton to effectively state that deep learning methods can be black boxes. However, I believe it fails to contextualize the proposed method within the larger discourse on XAI methods established in that paper. Specifically, the use of attribution maps (or saliency maps or heat maps as alternatively known) has been shown to suffer from several issues. As Barredo Arrieta et al. point out, \u201cthere is absolutely no consistency behind what is known as saliency maps, salient masks, heatmaps, neuron activations, attribution, and other approaches alike\u201d [R1, Sec. 5.3]. Also see [R2, R3] for a discussion on the issues with saliency/attribution maps. \n\nThe experiments still uses these and other SHAP based methods known to have issues to support their argument. (See [R4] for links to other papers discussing how SHAP can produce inconsistent and unstable explanations). \n\nI'd like to hear the authors' thoughts on why, broadly, their empirical argument which is built upon these methods will not suffer from similar issues. I don't see in the experiments how the proposed method would implicitly be robust to these issues surrounding instability since the method is inherently dependent on having a well-separated concept of foil and corpus samples (or at least that these samples need to already have a human understandable semantic meaning which is subject to biases in the human perception). \n\nOne of the core issues of the current state of XAI is how the methods largely depend on biases and interpretation of the observer, and it would be good to establish how this method is not adding to the same issue. \n\n\n[R1] Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. - Barredo Arrieta et al. \n[R2]  Sanity checks for saliency maps. - Adebayo et al. \n[R3] Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning - Atrey et al. \n[R4] The Disagreement Problem in Explainable Machine Learning: A Practitioner\u2019s Perspective - Krishna et al. ",
            "clarity,_quality,_novelty_and_reproducibility": "1. Clarity: The paper is clear, the notations clean and well-explained.\n2. Quality: The quality of the paper is good. The proofs are provided, the experiments are comprehensive.\n3. Novelty: The notion of adding a contrastive notion to the representation space (as opposed to methods that only see if the representation changes, but not away or towards other clusters) is interesting. It builds upon and extends these previous ideas, but I don't consider this to be a bad thing. \n4. Reproducibility: The code in the Supplementary is useful in making this work reproducible, although I have not had time to run the code. ",
            "summary_of_the_review": "The proposed idea of generating explanations for unsupervised methods is simple and elegant. Incorporating the notion of contrastive changes to the representation (towards a foil cluster and away from a corpus cluster) in response to changes in the feature space is interesting. The writing is clear and demonstrates notational discipline. \n\nHowever, my issues with this paper is that it fails in contextualizing how the proposed idea and empirical evaluation do not suffer from well-established issues with the underlying attribution methods used in buildding the empirical evidence in support of the proposed method. (Especially because the paper seems aware of the discourse in the \"The Mythos of Model Interpretability\" paper). I have added other references to help the authors address this issue. Nevertheless, this prevents me from recommending a higher pre-rebuttal score. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4994/Reviewer_chxC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4994/Reviewer_chxC"
        ]
    }
]