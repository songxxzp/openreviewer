[
    {
        "id": "wW9WjJYGf53",
        "original": null,
        "number": 1,
        "cdate": 1666535169432,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666535169432,
        "tmdate": 1666535255972,
        "tddate": null,
        "forum": "P1MaSJlwdT4",
        "replyto": "P1MaSJlwdT4",
        "invitation": "ICLR.cc/2023/Conference/Paper989/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a potential-based goal-directed intrinsic reward to improve exploration and name it GDIR. By augmenting Go-Explore with GDIR, they improve its performance on a few discrete and continuous exploration methods. They also use a hippocampal replay to directly imitate successful trajectories.",
            "strength_and_weaknesses": "# Strengths:\n- aims to solve an important problem, namely exploration in RL\n- the method seems simple and intuitive\n\n# Weaknesses:\n### Claims are not well-supported\n- In the introduction, you claim that your approach \"learns faster and adapts better to novel environments in real-world system\". However, most of these aren't well supported by your experiments. When you say it adapts better to novel environments, I imagine that you train it on some environments and finetune it on new ones, which you don't seem to do. Finally, you definitely don't show any results on real-world systems. In fact, it seems like your method only works in deterministic domains where you can reset the environment in any state, which isn't the case in the real-world. I suggest modifying the claim to be better supported by experiments. \n\n- In the introduction, you claim that GDIR improves over Go-Explore. However, your experiments seem to suggest that it is merely better at first finding a successful trajectory rather than consistently maximizing the extrinsic reward and finding the optimal policy. Again, this claim needs to be more precise to be accurate. \n\n- In the abstract, you discuss that Go-Explore is helpful in sparse-reward domains. However, all the environments you consider seem to have dense rewards. I recommend evaluating GDIR on environments such as Montezuma's Revenge, Pitfall, MiniHack, NetHack etc or at least demonstrating the sparsity of the reward in your settings. \n\n### Approach is not well-motivated\n- I don't understand the motivation for finding a satisficing solution rather than an optimal one. If your agent gets lucky once and finds a good path that achieves high extrinsic reward, there is no guarantee that it will robustly be able to solve the task (particularly in stochastic environments, see below) or that it is anywhere close to the optimal policy which may require much more exploration and exploitation to collect additional reward. \n\n- The idea that you want an intrinsic reward that always encourages exploration in a setting where the goal is to maximize extrinsic reward is quite odd. If your intrinsic reward doesn't diminish throughout training, you will end up with a suboptimal policy wrt the extrinsic reward. Hence, the motivation for your approach doesn't make much sense in this setting. It might make more sense to apply GDIR in reward-free settings or open-ended learning with non-stationary extrinsic reward that requires constant exploration. \n\n- The hippocampal replay trick is also quite limited given that it probably doesn't work well in stochastic environments. Due to this, it seems like the approach is limited to a fairly narrow class of problems which are not very challenging. In addition, it seems to require resetting the environment in any state which is a very strong assumption that doesn't apply in the real-world. I would like to see the approach evaluated in a wider range of domains, including stochastic ones. \n\n- The introduction of this paper reads as if this is the first work to propose a type of intrinsic reward, cites LeCun 2022 for introducing ideas that have been well-known way before 2022 and have been introduced by others, cites neuroscience studies that are only remotely related to the current paper, instead of much more closely related works on intrinsic motivation and exploration in RL. Hence, it ignores a huge body of related work. \n\n- GDIR isn't discussed in comparison with closely related works (other than Go-Explore) so it is unclear how it compares with HER, AMIGo, or other goal-conditioned exploration methods. What problem with existing methods is GDIR solving? This should be made more clear in the introduction. \n\n- In general, there should be more discussion about the limitations of your approach, together with experiments on settings where you expect it to work best / worst.\n\n### Empirical evaluation is limited \n- GDIR is only compared with vanilla RL and a few ablations based on Go-Explore. The evaluations are not thorough and extensive enough to draw strong conclusions. For example, I would like to see a comparison with HER and other popular goal-based or potential-based methods. What about using reward shaping of the extrinsic reward?\n\n- The authors include a model-based baseline but I'm not sure what this is meant to achieve since it's completely different from the model-free ones. \n\n- It seems like GDIR makes use of inductive biases in its cost function e.g. Manhattan distance etc. This results in unfair comparisons. What happens if you also augment the other baselines with reward shaped based on such distances or include these representations in their states. If the agent knows where it is with respect to the goal, maybe it can more quickly find it even if it doesn't explore via GDIR explicitly. \n\n- Important ablations seem to be missing. What happens if just apply goal-conditioned exploration without reward shaping or just reward shaping to the external reward without goal-conditioning?\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity \nThe paper isn't very clear. See above regarding the motivation of the paper and its positioning in the related work. Most importantly, it's not clear what exact problem their approach is solving and how it compares with similar exploration methods. \n\n## Quality \nThe paper lacks clear motivation and positioning in the broader literature, thorough experimental evaluation, and makes claims that are not well-supported by the experiments. The method also seems to have significant limitations that are not addressed, evaluated, or discussed in depth. \n\n## Novelty\nThe approach isn't particularly novel, but merely a naive extension of Go-Explore and combination of potential-based reward shaping and goal-conditioned RL. However, I wouldn't use this as a reason to reject the paper if the other issues are addressed. \n\n## Reproducibility\nI don't see any mention of open-sourcing the code. I think it would be difficult to reproduce the paper without the code so I strongly recommend the authors to open-source it and in any case tell use their plans regarding this. ",
            "summary_of_the_review": "This paper requires more work to warrant acceptance at the ICLR conference. In particular, the approach needs to be better positioned in the broader literature. The claims need to be toned down or better supported by the experiments. The approach could also use more work to alleviate clear limitations in more challenging environments or realistic settings, or at least these limitations should be openly discussed and analyzed. Finally, I am not sure the current experiments contain fair comparisons and could greatly benefit from more baselines, ablations, and environments. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper989/Reviewer_eHGy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper989/Reviewer_eHGy"
        ]
    },
    {
        "id": "bE72zYeCP_e",
        "original": null,
        "number": 2,
        "cdate": 1666654102354,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654102354,
        "tmdate": 1669418122006,
        "tddate": null,
        "forum": "P1MaSJlwdT4",
        "replyto": "P1MaSJlwdT4",
        "invitation": "ICLR.cc/2023/Conference/Paper989/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a method to speed up learning through the use of a reward function guiding the agent to complete subtasks. Additionally, they propose a replay method, which they call \"hippocampal replay.\" The authors include an empirical demonstration of their method in both continuous and discrete environments, as well as an ablation study to explore the contributions of both the choice of reward function and the use of hippocampal replay.",
            "strength_and_weaknesses": "The method described in the paper may have some interesting properties that should be further explored, but, unfortunately, I don't think the method was presented effectively. Most importantly, the choice of experiments is inappropriate. Providing the agent with a domain-specific distance function reward is providing it with domain knowledge, and so the comparisons against methods that are not provided this expert knowledge are not very meaningful. \n\nHowever, I understand the position of having an interesting idea that still has some gaps in it, and see the merit in sharing such an idea with the community, even without being able to show off a complete system. In such a case, rather than trying to compare performance, I'd recommend experiments that help build intuition, perhaps using toy domains, and being extremely clear whenever the system designers have had to fill in a gap with a bit of a hack, like providing appropriate subtasks for the agent if it was beyond the scope of the paper to find a method that could do so autonomously. I thought the ablation study was a good idea (strength) and would fit really well into a framework of experiments aimed at demonstrating the potential for the proposed method.\n\nAnother weakness of the paper is in how the context of existing work is presented. Prior work is often listed or briefly summarized without any explanation of how it is related to the paper and the proposed methods (e.g. Chentanez et al. (2004), Schmidhuber (2010), Baldassarre & Mirolli (2013), Aubret et al. (2019; 2022), Yang et al. (2022), and Gallou\u00e9dec & Dellandr\u00e9a (2022); pp. 2-3). There are also several misleading statements about prior work in the authors' descriptions of prior work, like the description of first-return-then-explore as an extension of Go-Explore (\"First return, then explore\" is a paper presenting the Go-Explore family of algorithms, not an algorithm) and the statement that Oudeyer & Kaplan (2009) give a list of computational approaches to model intrinsic motivation in psychology (many of the listed reward functions, like FM, CM or StabM, while effectively demonstrating some of the possibilities of a computational analogue of intrinsic reward, are not meant to model anything that would be called intrinsic motivation in psychology).\n\nAnother strength of the paper is the inclusion of substantial detail about the implementations of the experiments. The authors have been fairly careful to include parameter settings and have provided more than one description of each experiment to help the reader understand.\n\nSome questions/comments I would especially recommend addressing in revising this paper:\n\n1. To the best of my understanding, this method is designed to work with some kind of subgoal-state generator, like hierarchical reinforcement learning (in reference to p. 2); do the experiments include this?\n2. I found the use of the words \"goal,\" \"sub-task\" and \"task\" quite confusing\u2014I assume in most cases that these were references to the outputs of a subgoal-state generator like I mentioned in Question 1, but sometimes I wondered if the environment was also supposed to be formulated as having a single goal state? I'd recommend defining the term you use explicitly and using the same term consistently to reduce confusion.\n3. \"Recently, LeCun (2022) describes \u2026\" (p. 1) \u2192 I don't think I understand the point of your first paragraph. Removing it would likely reduce my confusion and save you some space.\n4. There are multiple references to \"curiosity-based\" or \"curiosity-driven\" intrinsic motivation but no definition of what the term refers to. The term \"curiosity\" gets used for multiple things, so it is valuable to be clear about which ones are meant. \n5. \"but the extrinsic reward may not be obtained.\" (p. 2)\u2192 I'm not sure what this means.\n6. \"For a new memory state,\" (p. 4) \u2190 Not sure what a \"memory state\" is\u2014are you just referring to when you store a state to memory for the first time?\n7. \"in a deterministic (rather than probabilistic) manner to help the algorithm learn faster.\" (p. 3) \u2190 Can you provide an explanation for how deterministic versus probabilistic choice speeds up learning (even if it is only in the appendix)? It seems like there must be a reason the original authors used the probabilistic choice.\n\nTypos and grammatical suggestions (no intended influence on score)\n\n1. \"Motivated by these observations, we seek to find such intrinsic cost/reward functions whereby it is innate to the agent, but is context dependent and can be triggered according to the task at hand.\" (p. 2) \u2192 There's some noun-verb disagreement here, so you could use something like, \"Motivated by these observations, we seek intrinsic cost/reward functions that are innate to the agent, but context dependent and which can be triggered according to the task at hand.\"\n2. \"tells us what are the sub-tasks\" (p. 2) \u2192 \"tells us the sub-tasks\n3. \"Oudeyer & Kaplan (2009) describes\" (p. 3) \u2192 \"Oudeyer & Kaplan (2009) describe\"\n4. \"Go-Explore.\" (p. 2) \u2190 I recommend providing the reader with a summary/review the the \"Go\" and \"Explore\" phases of Go-Explore in preparation for Section 3.\n5. \"Go-Explore (Ecoffet et al., 2019) describes \u2026\" (p. 2) \u2192 Go-Explore is an RL algorithm (or family of algorithms) so it typically doesn't describe, though Ecoffet et al. do! Also, as a note, detachment and derailment are also described in the published version of the paper (see p. 580 and pp. 19-20 in the supplementary material) so it might be preferable to cite the Nature publication here.\n6. \"due to the in-built stochasticity of the algorithm\" (p. 2) \u2192 I'd recommend foreshadowing how you'll address these problems here. This would also be a great place to provide some intuition about why most algorithms have in-built stochasticity.\n7. \"We seek to guide the search via a goal-directed approach, much like using a compass to find the way in the forest.\" (p. 3) \u2190 While I appreciate your aim to build intuition, I don't understand the analogy. What makes using a compass goal-directed? The goal is probably not simply to use the compass, maybe it is to go in a particular direction?\n8. \"where it can be viewed\" (p. 3) \u2190 Where what can be viewed?\n9. \"which is ideal in sparse reward settings. \u2026 GDIR is innate to the agent and hence applicable across a wide range of environments without needing to rely on a properly shaped environment reward.\" (p. 3) \u2190 There seems to be a contradiction here, because sparse reward settings obviously don't have potential-based shaped environmental reward; it would be helpful to have more intuition on how potential-based reward shaping is usually used.\n10. \"baseline for all other agents.\" (p. 3) \u2190 I'd leave this at \"serves as the worst-case baseline\" because I'm not sure how other agents come into it.\n11. \"its superior performance in them.\" (p. 3) \u2190 Not sure what \"them\" is in this sentence.\n12. \"Guillaume Chaslot, Sander Bakkes, Istvan Szita, and Pieter Spronck. Monte-carlo tree search: A new framework for game ai.\" (p. 10) \u2192 \"Guillaume Chaslot, Sander Bakkes, Istvan Szita, and Pieter Spronck. Monte-Carlo tree search: A new framework for game AI.\"\n13. \"Marek Grzes. Reward shaping in episodic reinforcement learning. 2017.\" \u2190 I think this is an AAMAS paper, but this could use more citation detail.\n14.\"P Read Montague. Reinforcement learning: an introduction, by sutton, rs and barto, ag. Trends in cognitive sciences, 3(9):360, 1999.\" (p. 11)",
            "clarity,_quality,_novelty_and_reproducibility": "address and the scope of their intentions for how their proposed method should be used and in what contexts.\n\nI consider this paper to have low quality, using inappropriate experimental comparisons and including multiple inaccuracies.\n\nI haven't come across any method like the proposed hippocampal replay (novel to the best of my knowledge) but my expertise is not around replay methods so I would not expect to be aware of similar methods. The use of a reward function that is -(distance to the target) is a fairly standard reward design, so I'm not sure what the novelty of GDIR is.\n\nI haven't checked carefully for reproducibility because I think there are higher-level concerns. However, not being familiar with Nim, one question I had was, How do I determine what K is?",
            "summary_of_the_review": "As a proviso, one issue I had with this paper is clarity. While I tried hard to understand the paper and am reasonably well-versed with most intrinsic reward methods, I really struggled to understand what the authors were trying to communicate. \n\nThe primary reason I am recommending rejection for this paper is that, while the central contribution is supposed to be an intrinsic reward method, the authors don't actually use intrinsic rewards. They hand-design different reward functions as the \"GDIR\" for each test domain (Section 4.3), which is a key indicator that the rewards are not intrinsic. This means that their experiments comparing their methods with baselines don't provide any significant information; the authors compare their method, which is provided with an appropriate distance function as domain knowledge against methods that do not use domain knowledge. The paper states that the method isn't provided with domain knowledge (p. 4), but to the best of my understanding, such a statement isn't true.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Figure 1 appears to be taken directly from the source, Joo & Frank (2018), which seems to be under Springer Nature copyright, as no mention of permissions is made. EDIT: The authors have explained that the figure is provided under a different license via PubMed, which I had not realized at review time!",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper989/Reviewer_5gPZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper989/Reviewer_5gPZ"
        ]
    },
    {
        "id": "AWvjqp_S9N",
        "original": null,
        "number": 3,
        "cdate": 1666658060762,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658060762,
        "tmdate": 1666658060762,
        "tddate": null,
        "forum": "P1MaSJlwdT4",
        "replyto": "P1MaSJlwdT4",
        "invitation": "ICLR.cc/2023/Conference/Paper989/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes the use of goal-directed intrinsic rewards (GDIR), and evaluates them in a Go-Explore-based agent on a variety of discrete environments, as well as some continuous control ones. The goal-directed intrinsic rewards are hand-crafted for each task, and the authors choose functions that are similar to the goals many human players would perceive - e.g. Manhattan distance to a goal in a discrete maze. The GDIRs are only used in the Go-Explore-based agents, not in the baselines. Generally, the GDIR-enhanced agents perform better than the baselines, although the difference with respect to GDIR-less Go-Explore is not always equally pronounced.",
            "strength_and_weaknesses": "- Strength: the method is tested, and shown to generally outperform the chosen baselines, across multiple environment types and tasks.\n- Strength: the core of the paper - the method - is explained clearly.\n- Weakness: the GDIRs are hand-crafted for each task, which harms the generalizability of the method.\n- Weakness: the Go-Explore aspect of the method relies heavily on being able to use simulators. The authors recognize this dependence, but it still reduces the applicability of the method in practice.\n- Weakness: the pre-processing of different environment types is somewhat specific to the proposed method, in particular the Go-Explore aspect.\n- Weakness: some baselines are missing, e.g. novelty signals based on learned representations, and non-Go-Explore agents with the GDIRs. Some citations are also missing, e.g. Random Network Distillation by Burda et al., Curiosity-driven Exploration by Self-supervised Prediction by Pathak et al., and Never Give Up / Agent57 by Badia et al.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: Some claims remain a bit vague: e.g. in the introduction, the authors claim they aim for a satisfactory solution, rather than a perfect one - but what is being satisfied? It looks like a change of reward to me, without stating how the reward is changed. The proposed method is explained clearly enough, so I don\u2019t have major concerns on this front. The paper would benefit from a round of language editing, see a few examples below. (I am not basing my recommendation on this, but wanted to point it out to the authors.)\n  - Silver et al. (2021) \u2026 he admits -> they admit; there are other authors on that paper.\n  - Satisficing: satisfying / satisfactory?\n  - \u201cwe seek to find such intrinsic cost/reward functions whereby it is innate to the agent, but is context dependent and can be triggered according to the task at hand\u201d - please rephrase; what is innate to the agent?\n- Quality: The reliance on simulators and hand-crafted intrinsic rewards and preprocessing functions makes the proposed method unlikely to generalize well and have many practical benefits. The experimental evaluation does demonstrate that the idea can work across various domains, although showing results on more specific hard exploration tasks would make the paper stronger. The missing baselines mentioned above make it harder to assess the benefits of the method.\n- Novelty: The individual elements in the paper may not be particularly novel, but the combination of intrinsic rewards and Go-Explore has not been used before as far as I'm aware. That is a good idea, but could be demonstrated more convincingly.\n- Reproducibility: no concerns.",
            "summary_of_the_review": "The basic idea behind the paper, using intrinsic rewards in Go-Explore, is a good idea. However, the implementation presented here looks very brittle, due to heavy reliance on handcrafting and simulators. As it stands, the paper is not making a convincing case that the proposed method will generalize well. I recommend rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper989/Reviewer_j1xa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper989/Reviewer_j1xa"
        ]
    },
    {
        "id": "E-lVboPCkVH",
        "original": null,
        "number": 4,
        "cdate": 1666745795339,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666745795339,
        "tmdate": 1666745795339,
        "tddate": null,
        "forum": "P1MaSJlwdT4",
        "replyto": "P1MaSJlwdT4",
        "invitation": "ICLR.cc/2023/Conference/Paper989/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper talks about the exploration of sparse reward environments. Based on the Go-explore paper, the paper proposed a goal-conditioned potential-based intrinsic reward for goal-conditioned tasks. The paper conduct experiments on the 2D grid maze environments, Towers of Hanoi, Game of Nim, Mountain Car and Cart Pole. ",
            "strength_and_weaknesses": "Strength: \n1. The paper tackles an interesting problem of exploration in the sparse-reward RL environments. \n2. The paper builds upon a strong baseline: Go-explore.\n3. The paper compares thoroughly against multiple baselines. \n\nWeakness: \n1. My main concern with the paper is that the experiment mostly builds on toy tasks, e.g., 2D maze, Mountain Car, and Cart Pole. In addition, the paper didn't talk about the random seed for their experiments, which is a pretty standard evaluation protocol for modern RL methods. Lastly, the evaluation metric used in the paper is somewhat strange. Why not report the success rate? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. ",
            "summary_of_the_review": "Please see the strength and weaknesses above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper989/Reviewer_JKcR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper989/Reviewer_JKcR"
        ]
    }
]