[
    {
        "id": "tQgcII2xuAT",
        "original": null,
        "number": 1,
        "cdate": 1665662450291,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665662450291,
        "tmdate": 1669360446563,
        "tddate": null,
        "forum": "LLy2vm_p35C",
        "replyto": "LLy2vm_p35C",
        "invitation": "ICLR.cc/2023/Conference/Paper868/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper develops a new algorithm for adversarially robust few-shot MAML.  The interesting part is that the adversarial robustness transfers across domains.  Experiments are conducted on multiple datasets and against multiple baseline methods.",
            "strength_and_weaknesses": "In my opinion, the strength here is the transferability of robustness across domains.  The empirical results otherwise do not seem very strong.\n\nIt would be good to test on higher performing meta-learning algorithms such as R2D2 and MetaOptNet, as in the AQ paper, at least as baselines for AQ since they may not be applicable to your method instead of only testing on MAML.  The numbers in the AQ paper look higher even for directly comparable experiments such as seen domain 5-shot mini-ImageNet.  It seems like a major flaw that your method can only be used with MAML despite MAML not being a very good few-shot learning algorithm.  This is my central critique of this paper.\n\nAlgorithm 1 is impossible to understand, especially the arguments of the loss function in the gradient computation.\n\nSome of the tables are a bit weird and contain numbers which are very hard to compare.  For example, Table 7 contains the proposed method as well as other methods trained against other stronger attacks.  In many cases, the competitors outperform the proposed method.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is generally clear, although like I indicated above, the notation in Algorithm 1 is hard to understand.",
            "summary_of_the_review": "This area of work is valuable, but the proposed method seems only applicable to MAML even though the baseline methods work with better quality few-shot learning algorithms.  This might be a major performance limitation of this paper.  In order to convince me that it is not a limitation, I would suggest either adapting the method to a better few-shot learner than MAML or demonstrate that it can outcompete competitors even when they use very strong meta-learning algorithms.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper868/Reviewer_WWPz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper868/Reviewer_WWPz"
        ]
    },
    {
        "id": "htbsyC7QvH",
        "original": null,
        "number": 2,
        "cdate": 1666724298357,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666724298357,
        "tmdate": 1666724298357,
        "tddate": null,
        "forum": "LLy2vm_p35C",
        "replyto": "LLy2vm_p35C",
        "invitation": "ICLR.cc/2023/Conference/Paper868/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "It proposes a adversarial meta-learning framework with bilevel attacks, which allows the model to learn generalizable robust representations across tasks and domains. The framework obtains higher robustness in few-shot tasks both in the seen domain and the unseen domains.",
            "strength_and_weaknesses": "\\+ The visualization of the representations of the instance-wise adversarial samples from the unseen domains in Figure 2 shows that the model is able to obtain well-separated feature space for attacked samples on this domain (CIFAR-10) even before adapting to it, while\nthe previous adversarial meta-learning framework learns a feature space with large overlaps across the adversarial instances belonging\nto different classes.\n\n\\+ TROBA shows smoother loss surface to adversarial examples in Figure 4 which is directly associated with better robustness and generalization.\n\n\\+ The ablation study checks the performance of each component, such as it compares the performance with or without augmentation, and with instance-wise or class-wise attack. It also checks the performance of the loss function compared with CE + selfsup loss or TRADES loss only. \n\n\\- The technical contribution may be limited. It adopts many techniques in this paper. Most of techniques are proposed in prior works such as \nTRADE loss,  data augmentation with random crop/flip/color distortion, and instance-wise classification loss in adversarial self-supervised learning. It combines these techniques but the technical contribution may not be significant. ",
            "clarity,_quality,_novelty_and_reproducibility": "see the strength and weaknesses. It provides detailed instructions about how to reproduce the results in figures and tables in the appendix. It  It provides visualization results to demonstrate why it performs better, which is easy to follow. The technical contribution may be limited as it adopts many existing techniques. ",
            "summary_of_the_review": "It combines a few existing techniques to achieve good performance in adversarial meta-learning. It provides visualization results to demonstrate why it performs better, which is easy to follow. My main concern is that the technical contribution may be limited as most of the techniques are adopted from prior works. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper868/Reviewer_Xygn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper868/Reviewer_Xygn"
        ]
    },
    {
        "id": "JH0MW7MpU4J",
        "original": null,
        "number": 3,
        "cdate": 1666727214332,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727214332,
        "tmdate": 1669624400407,
        "tddate": null,
        "forum": "LLy2vm_p35C",
        "replyto": "LLy2vm_p35C",
        "invitation": "ICLR.cc/2023/Conference/Paper868/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method to solve the problem of adversarial meta-learning, where the difficulty lies in the few-shot learning of each inner loop task. The proposed method consists of 1) generating adversarial query examples by attacking a contrastive loss between clean and perturbed images; 2) using two independent augmentations in both inner loop optimization and attack; 3) adversarial meta learning with TRADES loss and a contrastive loss. Experimental results show that the proposed method achieves improved robustness performance compared with several baselines.",
            "strength_and_weaknesses": "Strength:\n\nThe studied problem is interesting and practical as learning with few examples per task is important in some applications.\n\nExperimental results show the effectiveness of the proposed loss function in terms of robustness. Ablation studies also verify that each individual design is effective.\n\n\nWeaknesses:\n\nThe major weakness of this work is that the proposed loss function seems to be a combination of the adversarial meta-learning method AQ and the objective function of contrastive adversarial learning. The proposed loss function thus inherited the advantages from both domains. The technique is not novel.\n\nThe title of the paper suggests learning transferable representations for adversarial few-shot learning. My feeling is that the word transferable seems to be redundant since meta-learning for few-shot learning itself already has the implication of improving the transferability of representations. This work mainly uses self-supervised learning methods, specifically contrastive learning loss function, to learn better representations that can transfer to different tasks or domains without explicit transfer methods (e.g. things like task augmentation or domain alignment) beside meta-learning. Therefore, in my opinion it is perhaps better to claim it as few-shot self-supervised robust representation learning.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Most explanations of the proposed method are clear and are presented with high quality. The technique is not novel and some implementation details are not given.",
            "summary_of_the_review": "The proposed method targets the problem of adversarial meta-learning using techniques from contrastive learning. The incorporation of existing loss functions is not novel, and performance gain in experiments are expected due to the effectiveness of contrastive learning loss functions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper868/Reviewer_KTHv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper868/Reviewer_KTHv"
        ]
    },
    {
        "id": "ebhoqNRzTGJ",
        "original": null,
        "number": 4,
        "cdate": 1666756024112,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666756024112,
        "tmdate": 1666756024112,
        "tddate": null,
        "forum": "LLy2vm_p35C",
        "replyto": "LLy2vm_p35C",
        "invitation": "ICLR.cc/2023/Conference/Paper868/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces self-supervised adversarial learning into adversarial meta-learning, and proposes a transferable robust meta-learning via bilevel attack (TROBA) method by using a bilevel attack scheme. Specifically, TROBA is built on BOIL and TRADES by adding a self-supervised loss. Extensive experiments effectively show the effectiveness of the proposed method.  ",
            "strength_and_weaknesses": "- **Strength**: \n1. This paper is well organized and easy to follow.\n2. The core parts of the proposed TROBA are clear.\n3. The experimental results seem to be good. \n\n- **Weakness**: \nOverall, the key contributions especially the technique contribution are not clear. This is because self-supervised adversarial learning and TRADES have been widely used in general adversarial training, and BOIL is one existing meta-learning method extended from MAML. If the bilevel parameter augmentation and attack are the main technique contribution, the difference and effectiveness of this strategy compared to the normal data augmentation used in general self-supervised adversarial learning should be clearly claimed and demonstrated. If the entire framework is the main contribution, it would be interesting to see the effectiveness of this framework of using different adversarial training methods and different meta-learning methods.     \n\n- Several main concerns are as follows:\n1. What\u2019s the main difference between the proposed bilevel parameter augmentation and the normal data augmentation? Is it different only because of the usage of Siamese network with different parameters? Could you verify the effectiveness of this operation by experiments?\n2. What\u2019s the main difference between the bilevel attack and the normal self-supervised adversarial learning? Is it different only because of the swapping operation of two views? Could you verify the effectiveness of this operation by experiments?\n3. It is not clear if the adversarial example l_n^adv used in Eq. (7) is generated by class-wise attack or instance-wise attack. \n4. It is not clear how to update the backbone and classifier in Eq. (2) for readers. Note that the classifier is fixed in BOIL. \n5. I am not sure if all the comparison methods employ the same backbone and the same epsilon of attack. Does this paper re-implement all these methods in the same framework?\n6. It is doubtful for the experiment in Table 7. Are the results still for 5-shot classification? Note that the splits used in CIFAR-FS and CIFAR-100 are different. Why not directly perform other models on CIFAR-FS? How many samples used for training for TROBA and how many samples used for other models? This part should be clearly explained, otherwise the results are doubtful. \n7. Do all the comparison methods employ the same input image size of 32 * 32?\n",
            "clarity,_quality,_novelty_and_reproducibility": "- **Clarity**: Good.\n- **Quality**: Good.\n- **Novelty**: Incremental.\n- **Reproducibility**: Great.",
            "summary_of_the_review": "The overall of this work is great. However, there are still multiple parts that are not clear, which need to be further clarified.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper868/Reviewer_NDZi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper868/Reviewer_NDZi"
        ]
    }
]