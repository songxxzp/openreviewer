[
    {
        "id": "SCz1oBjRPE",
        "original": null,
        "number": 1,
        "cdate": 1665750189509,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665750189509,
        "tmdate": 1670639643836,
        "tddate": null,
        "forum": "tmIiMPl4IPa",
        "replyto": "tmIiMPl4IPa",
        "invitation": "ICLR.cc/2023/Conference/Paper6610/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents the factorized Fourier neural operator (F-FNO), which brings a set of techniques on FNO. The first technique is the separable Fourier representation to improve model stability and performance. Second, the improved residual connections. Third, the training strategies are carefully designed. Impressive results are found on a variety of benchmarks.",
            "strength_and_weaknesses": "# Strengths\n1. The results seem to be impressive. \n2. This paper clearly presented their method.\n3. Some of the visualizations might inspire other researchers in this domain.\n\n# Weaknesses\n\n## 1. The proposed contributions are incremental and are not even well-defined.\nSupporting details:\n\n(1) The residual connections are not novel, and a naive application is not enough for ICLR.\n\n(2) In the abstract, the authors say, \"carefully designed training strategies.\" This claim is too vague.\n\n(3) In the introduction, the authors claim, \"In our exploration of various regularization techniques, we find that it is possible to reduce the parameter count by up to an order of magnitude while still outperforming state of the art by a significant margin.\" This is too incremental. Applying a kind of regularization cannot be a major contribution. Also, how and why the regularization work remains unclear.\n\n(4) The core of the introduction and the method are too few. The major body of the methods is about the formulation of the problem and the baseline methods.\n\n(5) In the introduction, the authors claim three key contributions. The first is about the factorized Fourier neural operator. However, why is it called factorized? I think the only change is the residual connection. This should not be called the factorized Fourier neural operator, and this contribution is still weak.\n\n(6) In the introduction, the authors list two of the experimental findings as their contributions. However, the experimental findings are not that important because the technical contributions are weak, and the experiments are not convincing.\n\n## 2. The experimental results are not convincing. In general, I do not believe the baselines and the improved models are really working well.\n\n(1)  All the error numbers are too large. F-FNO achieves 3.16 errors in elasticity. However, the original FNO paper reports numbers at ~0.01. I understand the settings might be different. If this is the case, why do not the authors put all the methods under the same setting?\n\n(2) Can the authors provide a visualization comparing the GT with the prediction? Why does the modification work under this setting?\n\n(3) Is the baseline strong? Why don't the authors conduct experiments on the official settings of FNO?\n\n(4) In figure captions, the briefing settings should be presented instead of saying, \"our models work well, ... \". It is always better for the readers to discover the improvements from the figures rather than reading general over-stated claims.\n\n(5) In Figure 2, how to understand the figure? What are the inputs to each model? What are the outputs? Why are there big holes in the figure? What does the color mean? How to understand the differences between different modalities? Can the baselines achieve this? If the baselines cannot support varying geometries, why? What are the corresponding gt? What do geometries mean here? Are the geometries important in this domain?\n\n(6) The error bars should be shown in each figure.\n\n(7) It seems that the changes are not significant. The improvements should not be so significant.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is OK but the writing is poor. This means although the authors presented what they have done, they did not bridge the findings to their central claims well. Moreover, the contributions are not well presented.\n\nThe quality is incremental and I think the most valuable part of this work has not been presented well through the current draft.\n\nNot too much originality can be found in this work.",
            "summary_of_the_review": "Incremental contributions; Clearly below the acceptance bar; results are not convincing.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nAfter discussing with other reviewers via the Zoom meeting, some of my questions are addressed, and it would be better if the authors clarify the mentioned points in the next revision. Also, some senior reviewers with great expertise in this domain insist that this paper has a clear empirical advantage over the baselines. Therefore, I agree to accept this paper right now. I don't think a simple method should not be accepted, whereas the AC also thinks so. I think the authors should clarify their central contribution (eq.8) in this case and not hide their contribution behind a set of tricks. I think a revision would be beneficial to the community and also to the authors.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6610/Reviewer_jqw2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6610/Reviewer_jqw2"
        ]
    },
    {
        "id": "nvR_jarSqTD",
        "original": null,
        "number": 2,
        "cdate": 1666494318288,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666494318288,
        "tmdate": 1666577803653,
        "tddate": null,
        "forum": "tmIiMPl4IPa",
        "replyto": "tmIiMPl4IPa",
        "invitation": "ICLR.cc/2023/Conference/Paper6610/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the author proposee Factorized Fourier Neural Operator (F-FNO), a learning-based approach for simulating partial differential equations (PDEs). This work is seen as an improvement of the work by [Li2021] in which the Fourier Neural Operator was introduced. In this work, the author aims at improving the stability of the algorithm under complex geometries and noisy data by introducing more regularizations in the neural operator. The proposed F-FNO method is able to reduce the size of parameter by an order of magnitude while improving the performance over FNO significantly.\n\n\n[Li2021] Zongyi Li, Nikola B. Kovachki, K. Azizzadenesheli, Burigede Liu, K. Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations, 2021a.\n",
            "strength_and_weaknesses": "This paper has several aspects of strength:\n\n1. Compared to [Li2021], the F-FNO framework has several changes: 1) a residual connection after the feedforward connection; 2) a spatial factorization; and 3) during the training, instead of unrolling the model, the F-FNO use teacher forcing and make online update based on Markov assumption;\n\n2. The performance improvement of F-FNO over FNO is significant. Under several synthesis dataset  and using deep architectures, the reduction on MSE and time until correlation are both significant. \n\n3. The introduction of forcing function in the F-FNO allows the model to take additional contextual information that helps to solve the equation. \n\nThis paper has several aspects for improvement:\n\n1. While claiming to improve over the state-of-the-art algorithm, the experiments in the main paper only compare with FNO while the comparison with other methods are left in appendix. This is a missed opportunities since it is important to provide a comprehensive review on the related methods in both the parameter size, the MSE and time until correlation, as in [Li2021].  It is suggested to put some of the comprehensive study in the early main page.\n\n2. The spatial factorization is a significant simplification for the model while it may over introduce additional bias since it implicitly assumes that the solution also factorizes spatially. Does it make sense physically in the scenario when the Navier-Stokes equations are applied ? What is the tradeoff for this factorization ? \n\n3. As a PDE solver in the spectral domain, it is expected to discuss some tradeoff on the choice of spatial and spectral resolution (i.e. the design of the wavelength number vs. the dimension of input). It is known from Sampling Theorem that this choice of sampling rate would affect the performance of the system when the complex geometries require more detailed information to be kept after the discretization. This practical issue seems not discussed but it is interesting to have some thoughts on that.   \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and the code provided has been structured well with good comment and documentation. \n",
            "summary_of_the_review": "This paper has proposed an important update over existing Fourier Neural Operator and has brought it closer to practical use. With additional factorization, the F-FNO framework reduces the parameter size while attaining a great improvement in stability and performance when the layer of the neural network goes deep. This justifies my decision to accept this paper.   ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6610/Reviewer_Uuaf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6610/Reviewer_Uuaf"
        ]
    },
    {
        "id": "ABTcEA0iLDJ",
        "original": null,
        "number": 3,
        "cdate": 1666663749445,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663749445,
        "tmdate": 1669260398059,
        "tddate": null,
        "forum": "tmIiMPl4IPa",
        "replyto": "tmIiMPl4IPa",
        "invitation": "ICLR.cc/2023/Conference/Paper6610/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces Factorized-FNO, where they consider separable Fourier representation (by taking the fourier transform of each dimension separately and independently) the authors achieve more \u201cstable\u201d models, whose performance increases as the networks are made deeper (something that does not happen with baseline FNOs), for 2D navier stokes equations.\n\nThe authors also improve the performance of original FNO baseline by introducing techniques like teacher forcing, a markov property. \n\nThey also test F-FNO on irregular geometries.",
            "strength_and_weaknesses": "[Strenghts]\n\n- The main idea introduced by the authors, of factorizing the input dimension and treating the fourier transform of each dimension separately seems to work well for the various 2D Navier Stokes equations considered.\n- The method seems to get better with size of depth, and requires less parameters to reach similar performance as normal FNO.\n\n[Weakness]\n\n- The main idea (of treating each dimension independently) is only testing for ONE family of PDEs and that too in just two dimensions. It is hard to know if factorizing will *always* perform better. For example, one scenario where it might not is for PDEs that have cross terms involving derivatives w.r.t different dimensions.\n- Most of the contribution made by the authors seem to be utilizing training techniques (like teacher forcing) that have are shown to give better performance in other domains and test them for FNOs. This does not seem to be a very novel.\n- The authors test F-FNO on irregular geometries, is it established that normal FNOs definitely do not do well of these irregular geometries considered by the authors. That comparison seems to be lacking in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow and everything is well explained. ",
            "summary_of_the_review": "The authors have tested F-FNO on a single family of PDEs (and only in 2D) and it is hard to judge if it will always improve performance. The paper also lacks comparison with baselines (like FNO) for their abilities to hand irregular geometries. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6610/Reviewer_n5hB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6610/Reviewer_n5hB"
        ]
    },
    {
        "id": "DLwG8U4Q1s7",
        "original": null,
        "number": 4,
        "cdate": 1666688819485,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688819485,
        "tmdate": 1666688819485,
        "tddate": null,
        "forum": "tmIiMPl4IPa",
        "replyto": "tmIiMPl4IPa",
        "invitation": "ICLR.cc/2023/Conference/Paper6610/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper uses the Fourier transform to learn neural operators that can handle long-range spatial dependencies. By factorizing the transform, using better residual connections, and improving the training setup, the proposed F-FNO outperforms the state of the art on PDEs on a variety of geometries and domains. ",
            "strength_and_weaknesses": "Pros:  The proposed method outperforms the state of art on PDEs with geometies and domains. \nCons: The literature reviews on the classical and recent PDE solvers are not very complete. I would like to see some comparison with the classical numerical method, such as the vortex model in approximating 2D Navier-Stokes/Euler with FFO. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "It is a well-written article with high quality. It seems a continuation of the authors' previous work but it is still very important. \nI do not check the reproducibility given the constraint of time. ",
            "summary_of_the_review": "I would suggest to accept the current paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6610/Reviewer_WWi7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6610/Reviewer_WWi7"
        ]
    },
    {
        "id": "ro8Leumpc7O",
        "original": null,
        "number": 5,
        "cdate": 1666734272503,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666734272503,
        "tmdate": 1666734272503,
        "tddate": null,
        "forum": "tmIiMPl4IPa",
        "replyto": "tmIiMPl4IPa",
        "invitation": "ICLR.cc/2023/Conference/Paper6610/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this work, the authors proposed a novel neural operator architecture that factorizes the convolution on Fourier space into separate dimensions. Consequentially, the F-FNO model can scale up to a higher number of layers and achieve smaller errors. The paper has a comprehensive numerical study on multiple types of partial differential equations, considering chaotic systems and complex geometries. It also has a careful comparison with numerical solvers on the trade off between speed and accuracy.",
            "strength_and_weaknesses": "Strength:\n- The work shows significant improvement on previous methods.\n- The paper has a comprehensive study on many pde problems.\n- It has a careful cost-accuracy study with the numerical solver. I especially love figure 4.\n\nweak:\n- The author improve FNO++ with many tricks. I assume F-FNO is also equipped with these tricks. It could be better to clearly list what these tricks are, and to what aspect they contribute to the overall improvement.\n- It would be better to provide some intuition why the factorized structure help F-FNO scale with more layers.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. It has good quality. The technical novelty is not very strong.",
            "summary_of_the_review": "Overall I find this paper interesting. It has a concrete contribution to the community. I recommend acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6610/Reviewer_KRSZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6610/Reviewer_KRSZ"
        ]
    }
]