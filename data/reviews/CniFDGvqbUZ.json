[
    {
        "id": "nZrQzN6ruC",
        "original": null,
        "number": 1,
        "cdate": 1666542316101,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666542316101,
        "tmdate": 1666542316101,
        "tddate": null,
        "forum": "CniFDGvqbUZ",
        "replyto": "CniFDGvqbUZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1275/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In the paper, the authors propose a continuous and reversible memory transformation method to prevent overfitting on the memory in continual learning. The main idea is to increase the diversity of the data in the memory buffer while maintaining hardness. The authors propose Deterministic Continuous Memory Transformer to modify the data with infinite transformation functions to encourage diversity. The proposed method is compared with several existing memory editing methods. The results show the proposed method is better than existing ones. ",
            "strength_and_weaknesses": "Strength:\n\n1. The problem of preventing overfitting on memory is interesting and important for continual learning.\n2. The idea of introducing continuous transformation to the memory is also interesting. \n\n\nWeaknesses:\n\n1. What's the computational complexity of the proposed approach? The transformations seem expensive which is infeasible in online continual learning. Also, the training of the memory transformer also takes time. \n\n2. Section 3.4 is confusing. Are all the transformed data at different time stamps are combined? Will the transformed data be added to the memory buffer?\n\n3. In ER, are any random augmentations applied to the memory data?\n\n4. For ER, if the buffer size increases such that it consumes the same memory as the proposed method, how the performance will change? The proposed method introduces a memory transformer which takes more space than ER. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: overall the paper is clear.\nQuality: the paper has some new ideas for continual learning but there are several confusing points. \nNovelty: the novelty is fair.\nReproducibility: the paper doesn't seem easy to reproduce due to the introduction of many additional hyperparameters. ",
            "summary_of_the_review": "The paper proposes to edit memory buffer with continuous transformation. However, there are several confusing pieces which make it hard to understand the paper. Also, the additional complexity is also an issue. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1275/Reviewer_A5Re"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1275/Reviewer_A5Re"
        ]
    },
    {
        "id": "TeBXeDcvTE",
        "original": null,
        "number": 2,
        "cdate": 1666698061241,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666698061241,
        "tmdate": 1666698061241,
        "tddate": null,
        "forum": "CniFDGvqbUZ",
        "replyto": "CniFDGvqbUZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1275/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposed a novel method modifying the memory data in exemplar memory when performing experience replay in continual learning (CL). To address the problem of memory overfitting, the authors used deterministic and stochastic differential equations to perturb the memory data by increasing the cross-entropy loss to make the memory data hard to be classified. Furthermore, since differential equation based generative models are used, they can produce diverse memory data. In the experiments, the proposed methods achieves much higher accuracy than baselines. ",
            "strength_and_weaknesses": "**Pros:**\n\nP1. The proposed method achieves state-of-the-art performance compared to the baselines, and using differential equation based perturbation in continual learning is a novel approach. \n\n**Cons:**\n\nC1. Though authors said the proposed methods can generate diverse examples, there is no quantitative results to verify this statement. It would be better to report the numbers using a metric such as precision-recall used in evaluating a generative models.\n\nC2. The addition computation cost for performing single stochastic gradient step is not negligible. The proposed algorithm should execute the ODE or SDE solver and optimize the parameters for the generator during the learning process. \n\nC3. There is no analysis on this method for verifying the proposed approach. For example, how much DCMT or SCMT affect the forgetting measure to the previous task? Since the memory data is perturbed, this rather produces negative impact on previous tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to follow. Furthermore, the methods can be a novel approach for perturbing the memory data in exemplar memory.",
            "summary_of_the_review": "I vote to reject this paper. Though the proposed methods can be novel, authors does not verified the effectiveness of DCMD or SCMT by analyzing the important components of proposed methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1275/Reviewer_JHx6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1275/Reviewer_JHx6"
        ]
    },
    {
        "id": "CmdoTsZB_4R",
        "original": null,
        "number": 3,
        "cdate": 1667533878609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667533878609,
        "tmdate": 1667533878609,
        "tddate": null,
        "forum": "CniFDGvqbUZ",
        "replyto": "CniFDGvqbUZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1275/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on memory-replay-based approaches for Continual Learning (CL). The authors justifiably argue that a continual learner could overfit to the memory buffer reducing the generalizability of the model on previously learned tasks. Hence, one could benefit from adjusting the samples in the memory to avoid such overfitting. The authors propose to update the samples in the memory buffer via first an Ordinary Differential Equation (ODE) approach, denoted as Deterministic Continuous Memory Transformer (DCMT), and second through a Stochastic Differential Equation (SDE) based approach, denoted as Stochastic Continuous Memory Transformer (SCMT). DCMT is identified via a parametric velocity/drift term $g(\\cdot ; t, \\phi)$, with parameters $\\phi$, while SCMT uses both a drift and a diffusion term that are parameterized by $\\phi$. Then, the authors propose a bilevel optimization where the lower level optimization finds the optimal parameters $\\phi$ that provide memory updates that are adversarial/challenging (i.e., maximize the loss). At the same time, they maintain the class information (i.e., the CL provides consistent labels for the current and updated memory samples). Lastly, the authors show that DCMT and SCMT boost the performance of memory-replay-based CL approaches in task-aware (for task and class incremental learning) and task-free learning.\n",
            "strength_and_weaknesses": "## Strengths \n\n* The paper addresses an interesting challenge and important problem with memory-replay-based approaches in CL.\n\n* The proposed idea of using ODEs/SDEs for updating memory samples is novel and interesting.\n\n* The empirical results of the paper indicate that DCMT/SCMT consistently improves the performance of memory-replay-based methods. \n\n## Weaknesses\n\n* The paper's flow could be significantly improved. For instance, Equation (11) seems to be the actual objective function used for the lower optimization problem proposed in Equation (9). It would be better to add the consistency loss directly to Equation (9) and then expand on why it is needed. \n\n* Much of the presented details are unnecessary, while many critical details are completely missing from the paper. Most importantly, the velocity term $g(\\cdot;t,\\phi)$, which is at the heart of the proposed method, is never discussed in the paper (same goes for $\\mu_\\phi$ and $\\sigma_\\phi$). What classes of functions are used to update memory samples? From the results, it seems like $g(\\cdot;t,\\phi)$ acts on the color histograms, while one can use a large variety of transformations (e.g., the auto-augment approaches that also provide geometric transformations) as the parametric velocity term. \n\n* The details of how the bilevel optimization problem is solved is entirely missing from the paper. \n\n* The consistency loss in Equation (11) makes a lot of sense, and I believe it is critical for the success of the proposed methods. While the JS divergence regularizes for obtaining consistent labels, it does not guarantee it. Hence, if the regularization parameter $\\lambda$ is not carefully selected, then one can end up with memory updates that cross the decision boundaries leading to harming the performance of previous tasks. The ablation study on $\\lambda$ in Appendix B5 does not reflect this point, possibly due to the small range of $\\lambda$ used in the ablation study. \n\n* On Page 5, the authors state that even if  $g$ is not invertible they can recover $x(0)$ from $x(T)$. If I understand correctly, having $g$ we can perform forward propagation through time, i.e., naively $x(t+1)=x(t)+g(x(t);\\phi)$ (or using Runge-Kutta as described in Algorithm 2 in the Appendix). However, one cannot retrieve $x(t)$ from $x(t+1)$, unless $g$ is invertible. Could you please elaborate on your statement? ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** While the writing is good, the paper's flow and clarity can be improved significantly.\n\n**Novelty:** The idea is novel and original. \n\n**Reproducibility:** Too many details are missing from the paper to make it reproducible. \n\n**Quality:** The paper's quality can be improved (please refer to the weaknesses). I think the paper has a lot of potential, but in its current state, it feels a bit rushed. ",
            "summary_of_the_review": "In summary, the paper addresses an important and interesting problem, and the proposed approach is original. However, many details are missing from the paper, and some critical points are completely ignored (please refer to weaknesses). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1275/Reviewer_XsdS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1275/Reviewer_XsdS"
        ]
    },
    {
        "id": "xnqORPzaGmN",
        "original": null,
        "number": 4,
        "cdate": 1667600240055,
        "mdate": 1667600240055,
        "ddate": null,
        "tcdate": 1667600240055,
        "tmdate": 1667600240055,
        "tddate": null,
        "forum": "CniFDGvqbUZ",
        "replyto": "CniFDGvqbUZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1275/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method to augment the memory buffer, aiming to increase the data diversity. Specifically, this paper chooses to use differential equations to model the memory transformation. The experimental results on standard continual learning benchmarks show some improvements of the proposed method.",
            "strength_and_weaknesses": "Strength:\n\n- The idea of augmenting the memory buffer with a transformation is straightforward and interesting.\n\n- The paper is overall easy to follow.\n\nWeakness:\n\n- Why using differential equations to model the memory transformation is poorly motivatied and also lack of justificaitons.\n\n- Despite the gain of the method, there is more must-to-compare baselines. For example, how about using a standard neural network to serve as the transformation, or use standard noise to augment the memory, similar to [Prototype augmentation and selfsupervision for incremental learning. In CVPR, 2021].",
            "clarity,_quality,_novelty_and_reproducibility": "This paper proposes an interesting idea of using diffential equations to parameterize the continuous transformation for augmenting the memory buffer. The overall idea is interesting, but more justifications and evaluations are needed in order to show the effectiveness of the proposed method.\n",
            "summary_of_the_review": "Overall, I find this paper below the bar of acceptance, especially because of the lack of baselines (other perturbations for the memory buffer). It requires more ablation study and more justifications for the usage of differntial equation modeling.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1275/Reviewer_vLJC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1275/Reviewer_vLJC"
        ]
    }
]