[
    {
        "id": "_BPlyoqpyS",
        "original": null,
        "number": 1,
        "cdate": 1665688977769,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665688977769,
        "tmdate": 1665688977769,
        "tddate": null,
        "forum": "NZZoABNZECq",
        "replyto": "NZZoABNZECq",
        "invitation": "ICLR.cc/2023/Conference/Paper5987/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the optimization landscape of models which have learned different mechanisms. Beyond the standard definition of linear mode connectivity, they introduce \"mechanistic mode connectivity\" which tests mode-connectivity under changes in the data distribution. Based on these findings they propose a fine-tuning mechanism to exit the basin associated with a spurious pre-training solution.",
            "strength_and_weaknesses": "Strengths: Thorough scientific investigation of mode connectivity/mechanistic mode connectivity. Definitions, propositions, and proposed methods are well motivated, and the proposed CBFT method demonstrates good empirical performance.\n\nWeaknesses: I very much enjoyed the paper, and the following concern may reflect a bias perspective, however I thought it was still worth raising:  My main concern is with the experimental set-up. While interesting from a scientific perspective, I'm not sure that the issues addressed in this paper are interesting practically. As such, I'd recommend making this more clear in the paper. As one concrete instantiation of this concern, the paper claims: \"We have demonstrated a significant limitation of naive fine-tuning: it can fail to eliminate spurious patterns learned during pre-training.\" However, to me the point of pre-training is to pre-train on all the data you can. We see this in LM pre-training on trillions of tokens, or recently in vision models with billion scale datasets such as LAION. Accordingly, the setting studied in this paper seems like the experimental set-up in this paper may be a bit contrived. For instance, my guess is that CBFT would actually make the model less robust if applied to fine-tuning CLIP (e.g., this may be of interest: https://arxiv.org/abs/2109.01903). In short, to me the fact that pre-training biases the fine-tuning solution seems to be a feature, not a bug, of pre-training.",
            "clarity,_quality,_novelty_and_reproducibility": "In terms of clarity I thoroughly enjoyed Table 1 and Figure 6 which clearly communicate the overall findings in an easy to understand manner. The work is mostly original, and I recommend releasing code so that it is reproducible. ",
            "summary_of_the_review": "This paper is a thorough and interesting scientific investigation of the optimization landscape of mechanistically similar/dissimilar models. My concern is that the experimental set-up is detached from real world use cases of pre-training/fine-tuning.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5987/Reviewer_Y4FF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5987/Reviewer_Y4FF"
        ]
    },
    {
        "id": "ad2c5LH8eCR",
        "original": null,
        "number": 2,
        "cdate": 1666671796788,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671796788,
        "tmdate": 1666671796788,
        "tddate": null,
        "forum": "NZZoABNZECq",
        "replyto": "NZZoABNZECq",
        "invitation": "ICLR.cc/2023/Conference/Paper5987/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors aim to understand a model's sensitivity to spurious features through the lens of mode connectivity. First, they come up with a definition of the mechanistic similarity of different modes, on the basis of invariance to different interventions.  Then, they show a stronger form of connectivity between different modes, on the basis of mechanistic similarity. Finally, they conjecture that all modes with mechanistic similarity must be connected with a linear path up to permutations. They corroborate their conjecture with multiple experiments on CIFAR-10. Finally, they propose a method of connectivity-based fine-tuning to eliminate a model's sensitivity to spurious features.",
            "strength_and_weaknesses": "The paper introduces a novel connection between mode connectivity and a model's sensitivity to spurious features. The conjecture is in itself very interesting and will be useful for the scientific community. Moreover, the authors present experiments in synthetic datasets to support their conjecture and show a simple Taylor expansion-based proof for local correctness. To round it off, they propose novel connectivity based fine-tuning, based on the conjecture, to remove a model's dependence on spurious cues.\n\n\nI have the following questions:\n\n(a) In Figures 4 and 5, shouldn't train/evaluation loss be plotted, as opposed to the train/test accuracy because definitions 4, 5, proposition 1, and conjecture 1 include loss function? Do we still see barriers in linear paths between mechanistically dissimilar models in figures 4 and 5, with loss function?\n\n(b) The CBFT algorithm (in 3) assumes that we have access to both a clean dataset and a dataset with statistical cues. Can the authors comment on whether one can identify examples from both groups in an unsupervised manner? If not, why should one use CBFT and not directly train the model (from scratch) on the clean examples in the dataset?\n\n(c) It's not clear how the authors run CBFT introduced in eq. (3) on real-world models.\n\n(i)  How do the authors optimize step (i)? Do they explicitly search over the neighborhood around $\\theta_C$ for one $\\theta$ that maximizes the loss function on the line connecting $\\theta$ and $\\theta_C$? Or do they conduct an optimization of the form $\\min_{\\theta, 0 \\le t \\le 1} | \\lambda_1 - \\mathcal{L}_{CE} ( \\hat{y}  ( \\mathcal{D}_C ;  (1-t) \\theta + t \\theta_C ) | $?\n\n(ii) Furthermore, do the authors follow an alternative minimization for steps (i) and (ii)?\n\n(iii) Do the authors include permutation in step (i)?\n\n(d)  All statistical cues introduced in the paper are very simple (linearly separable) cues.\n\nWill we observe the same invariances to stronger forms of statistical cues e.g. label specific translations and rotations of the images? An example is a classification setting comparing images of cats and dogs, with the images of cats rotated by 45 degrees and the images of dogs rotated by 135 degrees.  \n\n(e) The authors point out a very important characteristic of \"quadratic paths\" in the appendix: the evaluation performance of the model on the path will depend on the examples used to find the path, and hence may not show generalization across datasets/distributions. \n\nHowever, isn't this also true for the permutation learned for linear paths, since it involves finding the best permutation $\\pi$ with a greedy algorithm on a given set of examples? \n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, with the theoretical statements and the experimental findings being easy to understand. The authors propose an interesting novel conjecture, along with experiments on well-known models in synthetic datasets, which will be of immense benefit to the scientific community.",
            "summary_of_the_review": "Overall, my scores are slightly on the positive side. The paper aims to advance our understanding of mode connectivity and its connection to the decisions of a model. However, the proposed algorithm CBFT assumes that we must have access to both clean examples and examples with statistical cues, which begs the question of the relevance of CBFT compared to the direct training on clean examples. Moreover, experiments with stronger statistical cues will help strengthen the conjecture of the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5987/Reviewer_iiL3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5987/Reviewer_iiL3"
        ]
    },
    {
        "id": "S1bfonL8Tb",
        "original": null,
        "number": 3,
        "cdate": 1666859217450,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666859217450,
        "tmdate": 1666859217450,
        "tddate": null,
        "forum": "NZZoABNZECq",
        "replyto": "NZZoABNZECq",
        "invitation": "ICLR.cc/2023/Conference/Paper5987/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies a new concept mechanistic mode-connectivity, which focusing on the mode-connectivity of models trained on different data distribution. Meanwhile, the paper also proposes connectivity based fine-tuning (CBFT), aiming at overriding existing mechanisms of a pretrained model by fine tuning it on a small out-of-distribution dataset.",
            "strength_and_weaknesses": "Pros:\n1. The paper is well-written and easy to follow, although minor grammar error also exist (e.g. 2nd paragraph line 3 \"to to\"). \n2. The introduced problem is quite interesting and well-motivated.\n3. The paper has conducted extensive experiment results to verify the effectiveness of the proposed hypothesis.\n\nCons:\n1. For figure 2, I'm confused by the last two rows of right figure. What is the difference between \"Rand. cue\" and \"Rand. Image w/ cue\"?\n2. For the augmented data, are they used along with original data for training? Or Models are only trained on the augmented data?",
            "clarity,_quality,_novelty_and_reproducibility": "1. The author doesn't provide code.\n2. The paper is somehow novel to me.",
            "summary_of_the_review": "The overall quality of this paper is good, where the analysis is comprehensive. Hence I recommend to accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5987/Reviewer_bN6r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5987/Reviewer_bN6r"
        ]
    },
    {
        "id": "jiyiJuNMUC",
        "original": null,
        "number": 4,
        "cdate": 1667064110674,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667064110674,
        "tmdate": 1667064110674,
        "tddate": null,
        "forum": "NZZoABNZECq",
        "replyto": "NZZoABNZECq",
        "invitation": "ICLR.cc/2023/Conference/Paper5987/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work associates the notion of linear mode connectivity of two models with the functional similarity (mechanistic similarity) of the two models. They argue that the linear mode connectivity of the two models indicates they both have inherited potentially spurious/undesirable representations. Furthermore, they argue that naive fine-tuning cannot remove these spurious representations. Based on this connection, the authors propose a new algorithm referred to as connectivity-based fine tuning (CBFT), which encourages two properties for optimizing the parameters:\n\n- Removal of the linear path between to spurious model\n- Encourage functional similarity of the current model across spurious and non-spurious datasets.",
            "strength_and_weaknesses": "Strengths:\n- The hypothesis this paper proposes about the optimization landscape and, specifically, the functional relationship with the energy barrier is interesting and an important area of better understanding the utility of mode connectivity. \n- Thorough appendix with corresponding loss curves. (This reviewer's opinion: loss curves are easier to interpret for mode connectivity than accuracy curves)\n\nWeaknesses:\n- Unclear on which of the two steps in CBFT contributes to the improvements. An ablation analysis of each step would help disentangle the contribution.\n- The definition of mode connectivity shown in Figure 6 seems to make mode connectivity and mechanistic similarity expected and potentially uninteresting. The monotonicity of the evaluation curve going from \\theta_c to \\theta_{nc} should be expected since the transition is going from a model trained on that dataset to one trained on a different dataset and vice versa. Could the authors clarify this issue? Perhaps this reviewer has made an error in understanding Figure 6. A more interesting result would be a \"u\" shaped accuracy barrier. But on most plots in Figure 4, this does not appear, and most curves are the expected monotonic trend.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well-written. The figures could use some clarification:\n\nFigures with grids of plots were a bit dense to parse easily (e.g., Figure 4 and Figure 5).\n- Figure 3 diagrams are hard to understand, especially the grid of similarity descriptions. Unclear what the rows and columns of the grid of rectangles are illustrating.\n- Figure 4 is difficult to parse because column labels like \"Linear Permuted\" are not defined in the main text.\n\nReproducibility is unclear, but the authors mention they will release it during the rebuttal phase.",
            "summary_of_the_review": "There needs to be more work in understanding mode connectivity's functional utility, which this paper meaningfully explores. As it stands, mode connectivity (linear or non-linear) is an empirical phenomenon that is not well understood. This work explores possible avenues for improving model generalization based on this phenomenon. The main caveat to this work is the clarity is made somewhat opaque by the presentation of grids of experiments. The idea would be communicated much better if those results were condensed down.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5987/Reviewer_fq5i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5987/Reviewer_fq5i"
        ]
    },
    {
        "id": "yUa-KT6_vD",
        "original": null,
        "number": 5,
        "cdate": 1669292494555,
        "mdate": null,
        "ddate": null,
        "tcdate": 1669292494555,
        "tmdate": 1671099899910,
        "tddate": null,
        "forum": "NZZoABNZECq",
        "replyto": "NZZoABNZECq",
        "invitation": "ICLR.cc/2023/Conference/Paper5987/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is primarily concerned with **identifying spurious attributes** learned by NNs when learning a specific dataset and **correcting them** during fine-tuning. To this end, they propose mechanistic similarity, a concept that checks whether two pre-trained models with low loss are functionally similar. Based on this concept, they develop connectivity-based fine-tuning, a fine-tuning technique that can modify the mechanism of a pre-trained model.",
            "strength_and_weaknesses": "* Strength\n\n  * From a theoretical point of view, their contributions are largely twofold. First, authors **applied the concept of invariance** to explain the underlying mechanism of the models. (Section 3) Second, a **fascinating conjecture** (Conjecture 1) was presented by the authors about how mode connectivity, an abstract concept in loss-landscape, will affect real-world applications. (Section 4)\n  * On the application side, the authors proposed Connectivity-Based Fine-Tuning as a way to **intentionally editing** the pre-trained model's underlying mechanism. (Section 5)\n  * These contribution points are novel and interesting for scientists studying the loss-landscape of NNs.\n\n* Weakness\n\n  However, readers who wish to apply these findings to their problems may encounter some difficulties for the following reasons. Considering this is discussion phase 2, it is impossible to revise the paper, but I believe the first and third issues should be addressed.\n\n  1. It is unclear how to get rid of spurious attributes in Sections 3 and 4. For example, the authors only explain how the proposed notion of mechanistically similar confirms the functional equivalence of the two models in Section 3. **There is no discussion of what mechanstically similarity corresponds to spurious attributes**. This issue also applies to Section 4 (i.e., There is no explicit connection between spurious attributes and Connectivity-Baed Fine-Tuning).  As a result, readers may feel as if there is no natural connection between sections. **Since the content of each section is interesting enough, I believe the connectivity between sections is the greater concern.**\n\n  2. Although the code was submitted during the discussion period, Section 5's Connectivity-Based Fine-Tuning remains vague in their main paper. For example, how do you pick a minimal dataset $\\mathcal{D}_{NC}$ that does not contain attribute C? Also, why does the average representation matching term used in (ii) of equation (2) make an invariant prediction? For example, what happens if the term is the average of multiple augmentations of a single image rather than the average of multiple images of a single class? **Since Connectivity-Based Fine-Tuning is not the only (or optimal) algorithm that achieves the authors' purpose, these questions are quite natural, and the authors should have explained them appropriately.**\n\n  3. There are some mathematical expressions that seem to be abused.\n\n     * Although an inverse mapping is required for isomorphism, $\\mathcal{A}^{\\alpha_i}_i$ has no inverse mapping because it fixes the value of the $i$-th component of the latent variable to $\\alpha_i$.\n\n     * While authors denote the domain of $\\mathcal{E}$ as $\\mathcal{X} \\times \\mathbb{R}^{m}$, $\\mathbb{R}^{m}$ is incorrect because $\\hat{\\mathcal{A}}$ is a concatenation of interventions.\n\n     * Authors used square brackets both to represent lists (e.g., $[K]$ in Section 2) and to represent closed intervals ($[0,1]$ in Section 5). \n\n     * Furthermore, some mathematical expressions are used without any definition. For example, \u3145they did not define exactly what $[K]$ means. This can only be inferred from context. This issue also applies to the Truncate function in (ii) of equation (3). The exact mathematical expression of $f_\\gamma$ in Section 5 is not specified. Similarly, $\\gamma_{\\theta \\rightarrow \\theta_C}(t)$ is not a path, but rather a point on a path. \n\n       **Combined with the first weakness, these misuses of mathematical expressions will present a huge barrier for readers unfamiliar with the field.**",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this is a well-written paper. In contrast to the contribution of each section, the connection between the sections is insufficient.\n\n* Clarity: The Sections, except for Section 5, are clear, but their connections are lacking.\n* Quality: Mechanistic similarity (Section 3) and Mechanistic mode connectivity (Section 4) are great, but Connectivity-Based Fine-Tuning lacks motivation. (See the second issue of Weakness.)\n* Novelty: The novelty and contribution points of this paper are clear.\n* Reproducibility: Although I have not run it, I believe the submitted code will ensure reproducibility.",
            "summary_of_the_review": "This paper proposes a novel solution (Connectivity-Based Fine-Tuning) to an interesting problem (removing spurious attributes). While the concepts of Mechanistic Similarity and Mechanistic Mode Connectivity are exciting and inspiring, no discussion has been given on how to remove the spurious attribute. The paper will be helpful to readers in many fields with revisions for natural connections between sections.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5987/Reviewer_X6v6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5987/Reviewer_X6v6"
        ]
    }
]