[
    {
        "id": "M2a4-2Ao8V",
        "original": null,
        "number": 1,
        "cdate": 1666667949533,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667949533,
        "tmdate": 1666667949533,
        "tddate": null,
        "forum": "UmHG2bD7X3w",
        "replyto": "UmHG2bD7X3w",
        "invitation": "ICLR.cc/2023/Conference/Paper4581/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work presents two techniques to solve the exposure bias. The first one, dynamic scheduled sampling, is to sample the generated token conditioned on the accuracy. The second one, imitation loss, is to regularize the training. The authors conduct experiments on machine translation and robust text generation to evaluate their methods.",
            "strength_and_weaknesses": "Strength:\n\n- The paper is well written and easy to follow.\n\n- The experiments are comprehensive, and the results are promising\n\n- The idea that using the signal of training process is kind of novel. \n\nWeakness:\n\n- The design of the sampling strategy, especially the Eq. (4), is heuristic. More motivation and analysis of Eq. (4) is required.\n\n- Forwarding a decoder again with modified sequences then computing a loss is not novel because it\u2019s already been used before in similar tasks (e.g., contrastive loss in [1] and consistency loss in [2]). The authors should discuss the differences with them.\n\n[1] BRIO: Bringing Order to Abstractive Summarization, ACL 2022\n\n[2] Target-Side Input Augmentation for Sequence to Sequence Generation, ICLR 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good, the paper is clear and easy to understand.\n\nQuality: the design of sampling strategy fair, the experiments are good.\n\nNovelty: the idea that using the signal of training process is kind of novel, the imitation loss is not novel.\n\nReproducibility: good, the code is provided and should be easy to reproduce.\n",
            "summary_of_the_review": "Overall, this work tries to solve an important problem of sequence generation and has lots of potential. However, I still have some concerns about the design.\n\n1. Low accuracy comes from two cases: (1) the model is not well trained (e.g., in early stage), (2) the output is actually correct but different from the reference. In case (1) it is reasonable to not sample too much from $\\tilde{y}$, but not in the case (2). \n\n2. The accuracy is computed without any alignment. Therefore, we will have a low accuracy when there is just one token deleted/added.\n\n3. The accuracy is only based on single token, not considering the longer sub-sequences such a bi-grams or tri-grams.\n\n4. The accuracy is not considered during sampling. Therefore, when the accuracy is high, there is a high chance that the sampled tokens are identical to reference, making the method less effective.\n\nTherefore, I suggest the authors conduct more theoretical and empirical studies of this part, to make the work more persuasive.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4581/Reviewer_8LdF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4581/Reviewer_8LdF"
        ]
    },
    {
        "id": "PW-NT3U-JGU",
        "original": null,
        "number": 2,
        "cdate": 1666672173570,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672173570,
        "tmdate": 1666672173570,
        "tddate": null,
        "forum": "UmHG2bD7X3w",
        "replyto": "UmHG2bD7X3w",
        "invitation": "ICLR.cc/2023/Conference/Paper4581/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a simple modification of scheduled sampling for transformers that allows them to get better performance on a variety of tasks, including NMT and text generation. Importantly, they analyze why exactly their method does better and produce pretty conclusive quantitative results of this.",
            "strength_and_weaknesses": "This paper does a good job of analyzing why using DySI lets the model produce better, more robust output. They perform the requisite ablations of their method to show it helps. One thing they don't do is check whether repetitions occur with the same frequency as when using regular SS. That is, is their method just a training time optimization (specifically the KL term addition), or does it produce qualitatively different results when you use DySI.\n\nI don't think the NMT results are very impressive. The BLEU scores are < 1 point better, and not even an improvement on the state of the art but rather a Transformer baseline. In my opinion these results are not good enough to publish on.\n\nThe abstract has a broken link to the code, so I couldn't review it.",
            "clarity,_quality,_novelty_and_reproducibility": "Paper was well written but not reproducible because their link is broken. I don't think adding a KL term on top of scheduled sampling is very novel. ",
            "summary_of_the_review": "Authors apply a minor tweak to scheduled sampling and it works a little better than before.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4581/Reviewer_tF2J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4581/Reviewer_tF2J"
        ]
    },
    {
        "id": "Uvy2AYg3QSX",
        "original": null,
        "number": 3,
        "cdate": 1666672437141,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672437141,
        "tmdate": 1666672437141,
        "tddate": null,
        "forum": "UmHG2bD7X3w",
        "replyto": "UmHG2bD7X3w",
        "invitation": "ICLR.cc/2023/Conference/Paper4581/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Dynamic Scheduled Sampling with Imitation Loss (DySI), which combines knowledge distillation and scheduled sampling for text generation.",
            "strength_and_weaknesses": "## Strengths\n1. This paper proposes a new decoding training method that combines scheduled sampling with knowledge distillation. \n1. The performance looks good and the authors also run a significance test.\n1. The method is well-motivated and reasonable.\n\n## Weaknesses\n1. Isn't imitation loss knowledge distillation? I see the explanation in Appendix D (probably it's added because it was once asked by reviewers of another venue) -- I'm not convinced by the explanation there. It makes people confused since it hints about the use of reinforcement learning, which is not the case. It simply adds a KD loss to learn from the teacher-forcing distribution.\n2. Applying KD to decoding is nothing new. Non-autoregressive decoding uses knowledge distillation to learn from autoregressive decoding. This is very similar to what's proposed in this paper while there isn't a discussion about it. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear but lacks novelty. The authors have provided code though I haven't looked at it closely.",
            "summary_of_the_review": "This paper applies KD to dynamic schedule sampling but lacks novelty.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4581/Reviewer_MTWV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4581/Reviewer_MTWV"
        ]
    }
]