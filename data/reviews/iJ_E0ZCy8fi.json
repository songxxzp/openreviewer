[
    {
        "id": "ujgz481pmKA",
        "original": null,
        "number": 1,
        "cdate": 1666357790195,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666357790195,
        "tmdate": 1666357870583,
        "tddate": null,
        "forum": "iJ_E0ZCy8fi",
        "replyto": "iJ_E0ZCy8fi",
        "invitation": "ICLR.cc/2023/Conference/Paper4178/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a text-guided diffusion model for style transfer. To achieve that, a text-guided sampling scheme i.e., patch-wise contrastive loss fine-tuning was used. Compared with previous related works, the proposed method can preserve the semantic content of the source image well. Besides, the proposed method requires no additional training on the diffusion model.",
            "strength_and_weaknesses": "Strength\n\n+Maintaining content well\nIn contrast to the previous methods, the proposed can preserve the content well. However, it is not clear if the proposed method outperforms previous CNNs- or GANs-based style transfer methods. These methods commonly preserve content well. It also means that more methods should be included for comparison, not just comparing with StyleCLIP, StyleGAN-NADA, and VQGAN-CLIP.\n\n+Insight on style transfer with the unconditional diffusion model.\nThis work presents some insights into how to implement style transfer with the unconditional diffusion model. Moreover, it does not require retraining the diffusion model. \n\n+Well-written\nThis paper is well-written and easy to understand. Especially, Figure 2 clearly presents the motivation and difference of the proposed method. \n\n\nWeaknesses\n\n-Limited style\nIn spite of using the CLIP model, it seems the proposed method can only obtain limited text styles such as Pixar, Pop art, Ukiyo-e, etc. All the testing results use these texts. How about other styles? More diverse results should be included to show the effectiveness of the proposed method.\n\n-Limited style transfer quality.\nAs shown in the main paper and appendix, the quality OF transferred styles is not much promising. The reviewer knows that the proposed outperforms other compared methods in the figures shown. However, the quality is still not satisfactory. \n\n-Limited novelty. \nAs the core of this work, the conservative loss with diffusion model is similar to CUT which is used in condition image synthesis. The differences or modifications are not clear. \n\n-Insufficient analysis and experiments\nBesides the limited texts and examples, the computational complexity comparison is missing. This work highlights that the computational complexity is much low. Some analysis and quantitative results are expected. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and can be easily reproduced.\nThe reviewer found this paper was accepted by ECCV 2022 Workshop on Uncertainty Quantification for Computer Vision. The authors should explain this issue seriously.",
            "summary_of_the_review": "The paper has some novelty and experiment flaws, which are the most important factors in my rating. The authors can provide comments based on the points I mentioned in the weaknesses. After that, I will consider my final decision. \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4178/Reviewer_n6KZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4178/Reviewer_n6KZ"
        ]
    },
    {
        "id": "uhFF8L5YV-",
        "original": null,
        "number": 2,
        "cdate": 1666772452450,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666772452450,
        "tmdate": 1666772452450,
        "tddate": null,
        "forum": "iJ_E0ZCy8fi",
        "replyto": "iJ_E0ZCy8fi",
        "invitation": "ICLR.cc/2023/Conference/Paper4178/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to do text-guided style transfer using diffusion models with a combination of a few losses, including two CLIP losses and Contrastive Unpaired Translation (CUT) loss. The CLIP global loss and directional loss guide the image generation towards the desired style specified by the prompt, and the CUT loss encourages the generated image to retain the content of the source image. The results look not bad. But the novelty seems to be limited.",
            "strength_and_weaknesses": "Strengths:\nThe empirical results seems not bad. The generated images faithfully keep the contents of the original images, and the styles are consistent with the text prompt.\n\nWeaknesses:\n1. The method proposed in this paper is basically a combination of a few losses proposed before, i.e., the CLIP global loss and directional loss proposed in StyleGAN-NADA, and the CUT loss proposed in (Park, ECCV 2020). I feel that the novelty is rather limited.\n2. Although the paper is easy to follow overall, some parts are a bit difficult to understand, see the section below.\n3. Some grammar/format errors:\n * Parenthetical citations and citations within the text are incorrectly used in Second paragraph, Introduction.\n * (Missing \"The\") CLIP model is trained... (text before Eq.6)\n * the global loss has shortcomings of mode collapse => the global loss suffers from mode collapse\n",
            "clarity,_quality,_novelty_and_reproducibility": "Some parts are a bit difficult to understand:\n1. I feel difficult to understand the text describing how the MLP is fine-tuned below Eq.10. \n2. \"The DDIM strategy as the forward noising process and DDPM method as the reverse sampling\", what's the benefit of doing so? \n3. Network $F_l$ (before Eq.10) could be illustrated with a diagram.\n4. In the bottom of Page 6, \"with T' = 50, we forward the source image x0 to latent image x_t0 where t0 = 25\", what does this mean?\n\n",
            "summary_of_the_review": "The paper investigates a synergetic combination of a few losses that both guide with the desired new style and enforce the existing contents.  It's a useful work, but the novelty is quite limited, as it's a straightforward combination of existing losses, and there's little insight shed onto the generation process.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4178/Reviewer_VPPF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4178/Reviewer_VPPF"
        ]
    },
    {
        "id": "DWplQZFKYev",
        "original": null,
        "number": 3,
        "cdate": 1666795875142,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666795875142,
        "tmdate": 1666795875142,
        "tddate": null,
        "forum": "iJ_E0ZCy8fi",
        "replyto": "iJ_E0ZCy8fi",
        "invitation": "ICLR.cc/2023/Conference/Paper4178/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new text-guided Diffusion-based style transfer method. The proposed method uses the CLIP loss for style guidance and the CUT loss for content guidance. The proposed method also uses the fine-tuning strategy according to the abstract and introduction (but I didn't find any words about it in the method section). The overall results seem good. The CUT loss and CLIP loss work well.",
            "strength_and_weaknesses": "Strength:\nThe results are very good.\n\nWeaknesses:\nI think the writing and the degree of novelty are questioned, at least from the current version of the paper. But I suspect that the shortcoming of novelty may also be due to writing.\n\nWriting:\n1. I am very confused about the fine-tuning method. I only see this word in the abstract and introduction. And I find nothing about it in the method section (please correct me if I'm wrong). This shows that in the writing of the method, the author still needs to make a lot of efforts to improve. I understand that telling about the diffusion model in such a limited space is difficult, but it shouldn't confuse readers like me very much. \n2. For Eq (11), are these three loss functions weighted? What are the weights?\n3. Many typos. And many references are wrong. lease check them one-by-one.\n\nNovelty:\nAccording to the paper, the losses are mostly borrowed from previous works. That is fine, but I don't see \nenough insights in the paper, e.g., a good understanding of the problem and interpretation or ablation studies. I think this problem is partly caused by the writing. It might be better if the authors could explain more clearly how their approach differs from previous approaches and what motivates them.",
            "clarity,_quality,_novelty_and_reproducibility": "I don't think the current version is clear enough in presentation, and I'm still confused by the details of its approach.",
            "summary_of_the_review": "Good results are as Plus. But the writing is not ready for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4178/Reviewer_JYQf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4178/Reviewer_JYQf"
        ]
    }
]