[
    {
        "id": "5ZiSYi3uJDu",
        "original": null,
        "number": 1,
        "cdate": 1666297338066,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666297338066,
        "tmdate": 1670612302894,
        "tddate": null,
        "forum": "lYZZl2hp1gp",
        "replyto": "lYZZl2hp1gp",
        "invitation": "ICLR.cc/2023/Conference/Paper4058/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this work, the authors define what it means for an encoder to be robust and describe metrics for measuring robustness.  Using these metrics, they then evaluate existing SOTA representation encoders with and without adversarial finetuning (based on the proposed metrics).",
            "strength_and_weaknesses": "Strengths:\n- Definitions for breakaway risk and overlap risk are well-motivated and easy to understand.\n- Interesting problem setting (measuring robustness without labels)\n- Well-motivated metrics\n- Show that fine tuning based on some of the proposed metrics can greatly improve robustness of encoder\n- Writing is clear\n\nWeaknesses:\n- The proposed metrics could be used to analyze encoders trained using any technique: supervised learning, self-supervised learning, and unsupervised learning, but the experiments are restricted to unsupervised encoders.  I think it would be interesting to perform evaluations on encoders of adversarially trained models (supervised learning but drop the final classifier layer) and see if these learned representations actually satisfy the definitions proposed.  Additionally, it would be interesting to see whether the pretraining approaches proposed by works for adversarially robust self-supervised learning (ie. Kim et al 2020) lead to more robust encoders.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I think the writing and presentation is very clear.  I also think that the work is novel as it is the first work to rigorously define what properties we would expect a robust encoder to follow and propose metrics for measuring the robustness of an encoder.  Overall, I think that the metrics are well-motivated and can be very useful for assessing the robustness of current training techniques for adversarial ML.",
            "summary_of_the_review": "I find this paper very interesting as it rigorously defines what it means for an encoder to be robust, and I think that the proposed metrics will be very useful to the adversarial ML research community especially since recently there have been works proposing methods for robust pretraining.  I am a little disappointed by the experiments though, not that they are bad or uninteresting, I just feel like the authors introduced a very powerful toolkit but did not use this toolkit to its full potential.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4058/Reviewer_HYfP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4058/Reviewer_HYfP"
        ]
    },
    {
        "id": "jO5iawtqU1",
        "original": null,
        "number": 2,
        "cdate": 1666601076606,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666601076606,
        "tmdate": 1666601532726,
        "tddate": null,
        "forum": "lYZZl2hp1gp",
        "replyto": "lYZZl2hp1gp",
        "invitation": "ICLR.cc/2023/Conference/Paper4058/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes two unsupervised adversarial risks, i.e., breakaway risk and overlap risk, that evaluate the adversarial robustness without requiring labels. Further, this paper generates unsupervised adversarial attacks via FGSM and PGD maximizes or minimizes the distance between benign and adversarial data. The authors also propose to use adversarial training based on the unsupervised FGSM or PGD to improve the robustness. Then, this paper provides a series of robustness measurements and empirically shows that adversarial-trained models are more robust than standard-trained models.",
            "strength_and_weaknesses": "Strength\n+ This paper proposes two interesting unsupervised adversarial risks based on the distance between benign and adversarial data. The risks indeed solve the problem that what value of the distance being \u201csmall\u201d.  \n+ This paper gives several robustness metrics, including quantiles for (un)targeted attacks, estimation of breakaway risk and overlap risk, nearest neighbour accuracy, adversarial margin, and certified robustness.\n\nWeaknesses:\n- This paper seems to have minor technical novelty. The main techniques of adversarial attacks and adversarial training are based on PGD or FGSM and even the loss function is simply the conventional distance.\n- The relationship (such as differences and similarities) between breakaway risk and overlap risk is not very clear. I am confused about which risk really evaluates unsupervised adversarial robustness. \n- It is somewhat difficult to understand how the metric \u201cuniversal quantiles for untargeted attacks\u201d can evaluate robustness. This metric closely depends on the sampling procedure. It is hard to say a model is non-robust if the fraction of distance between $x\u2019$ and $x\u2019\u2019$ is smaller than the distance between $\\hat{x}$ and $x$ is smaller since it could be incurred by the sampling procedure.\n- The claim that \u201cthe breakaway risk can be very small\u201d is somewhat confusing. Due to this claim, the author proposes nearest neighbour accuracy. However, Table 2 seems to show that breakaway risk can better differentiate the robustness of each model than nearest neighbour accuracy since nearest neighbour accuracy is almost very low and the same among different models.\n- It seems that different unsupervised adversarial metrics do not provide a consistent evaluation. For example, in Table 2, ResNet50 has a lower breakaway risk while a higher overlap risk and PixPro has a higher breakaway risk while a lower overlap risk. Therefore, it is hard to compare the unsupervised robustness between these models using these two proposed metrics.\n- It is not very clear the relationship between unsupervised robustness evaluation and supervised robustness evaluation. Will the unsupervised robust accuracy and supervised robust accuracy be positively correlated between them?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The organization of this paper is not very good. It is somewhat weird to suddenly illustrate adversarial training which is a defensive strategy in Section 4 of unsupervised attacks. This paper proposes two unsupervised adversarial risks and realizes them based on PGD and conventional distance metrics. But it seems that the metric is not consistent. Therefore, the technical quality seems to be not very good and the novelty is somewhat fair. The author provides algorithm and experimental details. Thus, this paper has good reproducibility.  ",
            "summary_of_the_review": "This paper proposes two unsupervised adversarial risks and realizes them based on PGD and conventional distance metrics. However, I have several concerns that have been illustrated in Weaknesses section to be solved. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4058/Reviewer_TA8W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4058/Reviewer_TA8W"
        ]
    },
    {
        "id": "6yX9Rfwa4jO",
        "original": null,
        "number": 3,
        "cdate": 1666679685345,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679685345,
        "tmdate": 1666679685345,
        "tddate": null,
        "forum": "lYZZl2hp1gp",
        "replyto": "lYZZl2hp1gp",
        "invitation": "ICLR.cc/2023/Conference/Paper4058/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the problem of quantifying and improving the robustness of representation models in a task-agnostic fashion. Being one of the first papers to approach the problem, the authors also motivate and provide a mathematical definition for unsupervised robustness. To evaluate the unsupervised robustness of the representation models, the authors propose a generalized attack framework and use it to propose a set of metrics representation-agnostic metrics that can be used to measure the relative robustness of various representation models. Using the attacks, the authors also train adversarially robust representation models and provide a detailed empirical analysis of the relative performance of different representation models with respect to each other and adversarially trained models.",
            "strength_and_weaknesses": "Strengths\n- The paper brings up an essential issue of the robustness of unsupervised representation models and identifies some novel task-independent metrics to measure their robustness.\n- The quantile-based metrics proposed in the paper give a representation-agnostic view of robustness that allows users to compare multiple representation models.\n- The authors also propose some adversarial training methods that improve the randomized smoothing-based certified robustness of the trained models. \n\nWeaknesses\n- Although the empirical attack evidence suggests that the adversarially trained representation models require more PGD iterations, this can also be explained by gradient masking. Moreover, these benefits are also not present for MOCOv3 models. Using a different attack method for the evaluation would resolve this issue.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper addresses an important problem and proposes a novel approach to it. As some of the metrics are presented in a new format, the authors need to take special care to remind the reader of the quantile version of the bounds. It is sometimes hard to follow the paper, especially the certified robustness bounds presented as quantiles in the tables. The authors do provide all the required details to reproduce the work. ",
            "summary_of_the_review": "The paper points out and addresses an important problem for representation models. The paper approaches the problem by considering two possible sources of risk: breakaway risk and overlap risk. Then the authors use this to motivate various metrics for measuring the unsupervised robustness of the classifier. However, the later analysis does not use the estimated values of the two risks. So, I feel some of the empirical investigations are not well motivated. Some of the presented empirical results also need further investigation. Especially the impersonation bounds need to be recalculated with better attacks. The paper has some great ideas which could benefit the community, but I think it could use some rewriting.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4058/Reviewer_v4fr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4058/Reviewer_v4fr"
        ]
    },
    {
        "id": "l902qlnji3",
        "original": null,
        "number": 4,
        "cdate": 1666699203900,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699203900,
        "tmdate": 1666699203900,
        "tddate": null,
        "forum": "lYZZl2hp1gp",
        "replyto": "lYZZl2hp1gp",
        "invitation": "ICLR.cc/2023/Conference/Paper4058/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes several evaluations of unsupervised robustness which are model agnostic and tasks agnostic. This paper examines recent unsupervised models with proposed unsupervised robustness evaluations. Specifically, the paper proposes universal quantiles for untargeted attacks and relative quantiles for targeted attacks as an evaluation metric for unsupervised robustness. The former metric represents the percentage of paired samples that are closer than the clean and adversarial pairs. The latter metric represents the ratio that the targeted attack moves the original image to the target image.",
            "strength_and_weaknesses": "**Strength**\n- This paper first tackles the problem that previous unsupervised adversarial methods only evaluated the classification task as a downstream task. To overcome such limitations, this paper proposes evaluation metrics that do not use any class labels.\n- The idea and the approach to making the evaluation metric for unsupervised robustness are quite novel to me.\n\n**Weakness**\n- I am not quite sure why proposed universal quantiles and relative quantiles are able to represent the robustness of the models. Intuitively, robust models could have a relatively low ratio of universal quantiles but these metrics could represent the vulnerability that induces wrong decisions or unintended actions in downstream tasks. I think the authors could describe how we can interpret these metrics in terms of robustness.\n- I think there is no big difference between U-PGD and L-PGD since the distance function is also a loss function. Further, KL divergence as a distance function could not be used in unsupervised models since there is no class probability in the unsupervised models. I might have missed but what kind of distance function is used for evaluation metric? Moreover, it seems that the L-PGD is already proposed in the previous unsupervised adversarial learning which seems to lack novelty.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** The paper is easy to understand.\n\n**Quality:** The representation of the paper seems fine. However, I hope the authors describe which dataset is used in each table in the caption.\n\n**Novelty:** The problem and the metric that the paper demonstrates are novel to me.\n\n**Reproducibility:** The paper has well reproducibility which elaborates well on the details.",
            "summary_of_the_review": "Overall, I recommend marginally below the acceptance threshold. Because whether the proposed evaluation metric could represent robustness seems unclear I hope the authors could resolve my concerns in the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4058/Reviewer_rApq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4058/Reviewer_rApq"
        ]
    }
]