[
    {
        "id": "-3iiEXx9cMp",
        "original": null,
        "number": 1,
        "cdate": 1666681371605,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681371605,
        "tmdate": 1666681371605,
        "tddate": null,
        "forum": "26aAV_wjoc",
        "replyto": "26aAV_wjoc",
        "invitation": "ICLR.cc/2023/Conference/Paper3514/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new vision-language pre-training (VLP) method for local feature alignment. The key idea is to use a graph optimal transport mechanism to align image patches and text tokens. This paper also extends BT to VLP by introducing an intra-modal objective and an inter-modal objective. Finally, a gated cross-attention is designed to eliminate the fusion encoder that is widely used in previous work.\n",
            "strength_and_weaknesses": "Strength:\n1. Using graph optimal transport (GOT) in multimodal problems is interesting, although the advantage of GOT vs other alignment methods is not investigated (see more details in weakness).\n2. The paper is well-organized and easy to follow.\n3. Thanks for providing the code which can help readers to reproduce the results.\n\nWeakness:\n1. This paper extends BT to VLP problems given the fact that BT does not require large batch size. What if contrastive-based methods use the same batch size as BT, will it degrade the performance significantly? If not, then we can simply use small batch sizes.\n2. How was GOT applied to image features and text features? This is not clearly discussed in the Method section.\n3. On page 5: WD can only match nodes in the graph, and will treat all \u201cmen\u201d entities as identical and will ignore neighbouring relations like \u201cin blue shirt\u201d and \u201cholding the scissors\u201d. Is there any evidence to support this statement?\n4. Intra-modal objective and inter-modal objective share the similar idea of TCL [1], except that TCL is built upon contrastive losses. Furthermore, TCL is a very important work in VLP.\n5. In Equation 2 and 3, what is $k\u2019$ in $C_{i,i}^{k\u2019}$?\n6. What does \u201cweakly-supervised\u201d represent in this paper?\n7. On page 7: \t\u201cFor MLM, we randomly mask 15% text tokens\u201d. Have you replaced the masked tokes with other special tokens?\n8. What\u2019s the advantage of using GOT compared with existing local alignment methods such as OT?\n9. The performance improvement is marginal by using the same backbones.  \n\n[1] Vision-Language Pre-Training with Triple Contrastive Learning, CVPR 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-organized but misses some technical details (e.g., how GOT is applied). The technical novelty and contribution is not significant enough. Since the code is provided, the reproducibility is guaranteed.\n",
            "summary_of_the_review": "Although this paper extends existing methods to the VLP field, the motivation is not clear. Furthermore, the advantages of the proposed modules are not investigated or empirically proved. The technical novelty/contribution and performance improvement is quite marginal.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3514/Reviewer_PoWP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3514/Reviewer_PoWP"
        ]
    },
    {
        "id": "lBsjMslpT1u",
        "original": null,
        "number": 2,
        "cdate": 1666765761019,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666765761019,
        "tmdate": 1666765761019,
        "tddate": null,
        "forum": "26aAV_wjoc",
        "replyto": "26aAV_wjoc",
        "invitation": "ICLR.cc/2023/Conference/Paper3514/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose VoLTA, a unified Vision Language Pre-training paradigm for image-level and region-level applications, in which only image-caption pairs are used. They use graph optimal transport for weakly-supervised feature-level and region-level alignment. They conduct a wide range of vision- and vision-language downstream experiments to demonstrate the effectiveness of VoLTA.",
            "strength_and_weaknesses": "Strength:\n1.\tMany experiments on downstream tasks are conducted.\n2.\tPaper writing and structure are easy to understand.\n3.\tThe idea two views of image and text in GOT part is interesting.\n\nWeakness:\n1.\tThe figures are not clear. For example, in figure 2, it\u2019s confused for the relation of 3 sub-figures. Some modules are not labeled in figure, such as CMAF, L_BT, VoLTA.\n2.\tThe experiments results are not significant.\n3.\tThree steps for training are shown in VoLTA, a) switching off CMAF, b) switching on CMAF, c) keep CMAF and random sampling for training. The ablation study on these parts should be conducted.\n4.\tThe key point of this paper is GOT, but no ablation on this part. The authors are encouraged to verify which parts works.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of this paper is interesting, but the figures, evaluation of the proposed methods are not clear.",
            "summary_of_the_review": "This paper proposes a new VLP framework using GOT, but the evaluation is in-sufficient and the effectiveness of the proposed module is not clear.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "no",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3514/Reviewer_mig1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3514/Reviewer_mig1"
        ]
    },
    {
        "id": "BM5_kGn0wVl",
        "original": null,
        "number": 3,
        "cdate": 1667559435870,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667559435870,
        "tmdate": 1667559435870,
        "tddate": null,
        "forum": "26aAV_wjoc",
        "replyto": "26aAV_wjoc",
        "invitation": "ICLR.cc/2023/Conference/Paper3514/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new Vision-language pre-training (VLP) paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the use of expensive box annotations. Existing end-to-end  VLP methods use high-resolution image-text-box data to perform well on finegrained region-level tasks, such as object detection, segmentation, and referring expression comprehension. However, it comes at high annotation cost. As a solution to this problem, the authors proposed VoLTA which uses adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. As a result, VoLTA pushes multi-modal fusion deep into the uni-modal backbones during pre-training and removes fusion-specific transformer layers, further reducing memory requirements.  Extensive experiments on a wide range of vision- and vision-language downstream tasks demonstrate the effectiveness of VoLTA on fine-grained applications without compromising the coarse-grained downstream performance. ",
            "strength_and_weaknesses": "Strength -\n\n(+) The concepts and higher level overview of the problem is somewhat well defined. \n\n(+) The idea of using Barlow-Twins for both the Image and Text modality and thereby performing inter-modal and intra-modal redundancy reduction is well appreciated and interesting. \n\n(+) The idea of using Graph Optimal Transport instead of Optimal Transport for patch-word alignment seems intuitive and novel for this vision-language domain. \n\n(+) Extensive experiments have been performed on multiple standard benchmarks often surpassing supervised approaches.\n\nWeakness - \n\n(-) The reviewer is very curious about the textual augmentations. The amount of augmentations done on the textual caption looks like a strong data augmentation and in such cases Barlow-Twins may not perform well. Can the authors shed some light on this issue ? This might affect the overall model performance while doing the intra-modal alignment. \n\n(-) The cross-modal attention fusion block part appeared a bit unclear to this reviewer. How is this CMAF module trained ? When alpha=0, only BT and GOT losses are computed , however when non zero then MLM and ITM is computed, does that mean , this is trained in a 2-stage fashion ? i.e first the BT and GOT loss is computed to get a good initilaization point and then CMAF is activated to do the fusion ? This needs some clarification. \n\n(-) This reviewer is also curious to know how is the inference on dense downstream task like object detection and instance segmentation performed ? After VolTa pre-training, is it fine-tuned on the target dataset or the decoders are attached to the CMAF only for finetuning ? This needs some clarification. \n\n(-) What is the purpose of such pre-training in general ? Since the method performs very well on uni-modal downstream tasks but lags behind in multi-modal downstream tasks, it is hard to understand the motivation for such pre-training. The reviewer agrees that annotations are costly, and such weak supervision provides an alternate solution, but only surpassing uni-modal downstream tasks using multi-modal pre-training looks to this reviewer a slightly weaker claim. \n\n(-) Why is the gating based attention is better than traditional cross-attention transformer. Such ablations should be done to justify the design choice which is missing in current form. \n\n(-) The paper has minor grammatical errors which should be fixed for better readibility",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper has good clarity on a higher level as well as technical level. The diagrams and writing is neat and crisp for the majority of the draft. \n\n2. The paper is tending towards high quality but needs some modifications/clarifications ( see weakness ) \n\n3. The proposed solution is novel in most of the parts \n\n4. The paper is easy to read and implement",
            "summary_of_the_review": "Although i think the whole idea is interesting and worthy of discussion in the research community, i am a bit concerned at the use-case of this pre-training as compared to let say off-the-shelf GLIPv2. However, I agree with the authors that such pre-training is indeed necessary in cases where annotations are not available and can provide some useful insights in the near-future. Regarding the draft, the reviewer is slightly confused in how the fusion module is trained and what is the imapct of the textual augmentations in Barlow Twins. Some minor issues like grammatical errors and missing ablations also need some attention. Therefore, at this stage, I will vote for marginal accept and re-evaluate based on authors and other reviewer response.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3514/Reviewer_RqXJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3514/Reviewer_RqXJ"
        ]
    }
]