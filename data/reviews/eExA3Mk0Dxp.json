[
    {
        "id": "8yv9TGpMn2",
        "original": null,
        "number": 1,
        "cdate": 1666555362500,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666555362500,
        "tmdate": 1670454947814,
        "tddate": null,
        "forum": "eExA3Mk0Dxp",
        "replyto": "eExA3Mk0Dxp",
        "invitation": "ICLR.cc/2023/Conference/Paper3606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the adversarial robustness of MARL, with a focus on observation attacks. The proposed attacker can decide both which agent to attack and how to perturb its observation, via a hybrid-action attacker. Then the paper proposes a robust training approach by alternately training the attacker and the agents. An additional defense module is proposed which reconstructs the attacked observation based on the observations of teammates. Experiments in StarCraftII and Predator-Prey have shown the effectiveness of the proposed attacks and defenses.",
            "strength_and_weaknesses": "Strengths:\n\n1. The problem of observation attack in MARL environments is relatively new and interesting.\n2. The paper provides attack and defense results in various scenarios, and the visualizations are good for understanding. \n3. The paper is in general well-writte and easy-to-follow.\n4. The idea of the defense module is interesting.\n\nWeaknesses:\n\n1. My major concern is on the novelty of the paper. \n- The MARL attack problem is a straightforward extension of the single-agent attack problem, as the attack just need to learn an agent id in addition to the observation perturbation. Such an additional action output is easy to achieve with DNN parametrization. It would be more interesting and less trivial if the attacker attacks more than one agents simultaneously (e.g. perturb the observations of up to K agents). \n- The alternate training idea is commonly used by many prior works [1,2,3] to improve the agent robustness, which makes the novelty of the paper limited. What is more, prior works focus on the robustness of a single victim agent, whereas this paper aims to improve the robustness of an MARL system. In this case, I would be worried about the efficiency of alternate training, given that training multi-agent policies is usually more difficult and less stable than training a single agent. \n\n2. The experiment section looks interesting, but not adequate to justify the significance of the work. For example, what is the clean performance of the proposed training method (i.e., the reward of the ROMAO models when there is no attack)? This is an important metric for practical use of the robust training approaches [1,2]. It is possible that the model overly adapt to attacks while not achieving good natural performance.\n\n3. The defense module is interesting, but the details are not illustrated much in the paper. This method is reasonable when the observations have redundant information. For examples, agents look at the same object from different angles. However, if the observation information of each agent does not overlap, such a reconstruction may not be achievable. In addition, since the agent does not know who is attacked, it is possible that the other agents' observations are corrupted. How do they deal with this case? More details of this defense would be appreciated.\n\n\n\n\n[1] Huan Zhang, Hongge Chen, Duane S. Boning, and Cho-Jui Hsieh. Robust reinforcement learning on state observations with learned optimal adversary. ICLR, 2021.\n[2] Yanchao Sun, Ruijie Zheng, Yongyuan Liang, and Furong Huang. Who is the strongest enemy? towards optimal and efficient evasion attacks in deep RL. ICLR 2022.\n[3] Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. ICML 2017.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: this paper is clearly presented. \n\nQuality: the quality of this paper is okay but limited.\n\nNovelty: the novelty of this paper is relatively limited.\n\nReproducibility: code and instructions are provided, although I did not run it.",
            "summary_of_the_review": "This paper studies an interesting and important problem, but the proposed method is of limited novelty and may suffer from efficiency issues. The presentation of the paper is good, while the completeness of experiments can be further improved.\n\n\n---- After rebuttal ---\n\nThank the authors for the responses. I think this line of research is indeed interesting. But I think the current work has relatively limited contribution, so I will maintain my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3606/Reviewer_8Gkv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3606/Reviewer_8Gkv"
        ]
    },
    {
        "id": "m5r3fyQe8P",
        "original": null,
        "number": 2,
        "cdate": 1666584397932,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584397932,
        "tmdate": 1666584397932,
        "tddate": null,
        "forum": "eExA3Mk0Dxp",
        "replyto": "eExA3Mk0Dxp",
        "invitation": "ICLR.cc/2023/Conference/Paper3606/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies observation perturbation attacks in cooperative multi-agent reinforcement learning (MARL), where the attacker can choose one agent to attack and inject noise with a bounded norm into its observations to minimize the long-term return of the system. The paper considers a test stage attack where a MARL policy has already been trained, and the attacker can observe both the underlying model and the MARL policy. The paper adopts an alternate training approach to train a robust defense policy and evaluates the approach in two MARL environments. ",
            "strength_and_weaknesses": "Recent work has focused on state perturbation attacks against a single RL agent. It is interesting to study the multi-agent setting. The alternate training approach for robust RL was proposed by Pinto et al. (2017) and has recently been applied to train a robust policy for single-agent state perturbation by Zhang et al. (2021). Due to its generality, the application of this approach to the multi-agent setting considered in the paper is straightforward. That being said, I am still surprised that the paper only provides a short description of the approach without providing any details.\n\nThere are two problems with the alternate training approach, none of which has been addressed in the paper. First, it has a very high training complexity due to the alternate training of the MARL policy and the attack policy, and it is unclear if the approach will converge. Second, the policy thus trained can be conservative. As shown in the experiments, the robust policy obtained this way is often weaker than the policy trained using adversarial training against random attacks. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The problem studied is almost identical to the single-agent setting since (1) a fully cooperative MARL environment is considered; (2) the attacker is external to the system and can only target a single agent; (3) the attacker has full knowledge about the MDP model and the joint MARL policy. With these simplifications, the attacker's problem (when the MARL policy is fixed) is again single-agent MDP. The only difference compared with previous work on single-agent state perturbation is that now the attacker also needs to decide which agent to attack in addition to the perturbation action. The hybrid action space does not introduce significant new challenges, and the paper simply adopts the HyRL approach of Li et al. (2021) to address that. \n\nThe paper mentions very briefly a defense component where an agent reconstructs its observation from the observations of its neighboring teammates, which sounds interesting. However, no details are provided. \n\nIt looks like the paper only considers random attacks in the experiments according to the three attack modes defined in Section 5.2, which is inconsistent with the three attack types mentioned in Section 3. In particular, it is unclear why the paper does not consider the worst-case attack that responds optimally to the fixed MARL policy. ",
            "summary_of_the_review": "The paper considers a state perturbation attack that targets a single agent in a cooperative multi-agent reinforcement learning (MARL) system and adopts alternative training to obtain a robust policy. Due to the simplification of the threat model considered, the problem is similar to state perturbation against single RL. The proposed attack and defense methods thus follow known techniques with little adaptation and do not provide new insights into MARL security. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3606/Reviewer_gLvj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3606/Reviewer_gLvj"
        ]
    },
    {
        "id": "u76s8fhtbQ",
        "original": null,
        "number": 3,
        "cdate": 1667204365068,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667204365068,
        "tmdate": 1667204365068,
        "tddate": null,
        "forum": "eExA3Mk0Dxp",
        "replyto": "eExA3Mk0Dxp",
        "invitation": "ICLR.cc/2023/Conference/Paper3606/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new training framework for multi-agent reinforcement learning (MARL) to improve the robustness of MARL agents by generating state/observation perturbation on strategically selected agent during training. They additionally propose a defense module to  prevent the MARL agent from being attacked by observation perturbation. Experiments on SMAC and predator-prey (PP) environmenst are presented to illustrate the effectiveness of the adversarial training framework.",
            "strength_and_weaknesses": "Strengths:\n- The new method employs a hybrid action space to select agent to attack along with the perturbation. The authors also incorporate encoder-decoder structure so that the learning is conducted on a latent repsentation space.\n- The approach is shown to make the MARL agent robust to various attacks in SMAC and PP environments.\n\nWeaknesses:\n- My major concern is that the approach presented here is somewhat heuristic. For example, there is no discussion on the convergence guarantee when performing adversarial training. The authors presents an alternating training approach between the attacker and victim agents. I wonder if they happen to select this or the authors have tried other approach and this seems to work well. More details about this are welcome.\n- Another concern is about how the attacker is trained. The authors provide 3 attack mode but do not clearly describe how attackers are trained under those modes. More details should be provided.\n- The defense module is vaguely described and it is not easy to understand the detailed steps without going through the code. It would be great to have more details of this procedure.\n- In a multi-agent setting, there is possibility that two or more agents get attacked simultaneously which is not considered in the paper. I wonder what the authors think about extending the current approach to this setting.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is decent but there are still key details missing. There are not many adversarial training framework for MARL and this paper aims to advance this research direction which is good. T",
            "summary_of_the_review": "I think the current paper is more like a technical report on what the authors did to perform adversarial training on a MARL setting. I do not see a systematic study into why the authors select such framework or any discussion on the convergence guarantee of the adversarial training framework. Some details are missing such as the training of attacker and the defense system.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3606/Reviewer_G76o"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3606/Reviewer_G76o"
        ]
    }
]