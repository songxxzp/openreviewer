[
    {
        "id": "546Kq14BEE",
        "original": null,
        "number": 1,
        "cdate": 1666656784852,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656784852,
        "tmdate": 1666656784852,
        "tddate": null,
        "forum": "Us5in-h2Dp",
        "replyto": "Us5in-h2Dp",
        "invitation": "ICLR.cc/2023/Conference/Paper3518/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on multi-task pretraining via supervised data in NLG area. They collected 77 NLG datasets and applied different variants of multi-task pretraining on top. The pretrained models are evaluated on both seen and unseen tasks, comparing to the prior STOAs and BART finetune baselines.  ",
            "strength_and_weaknesses": "Strengths:\n- A collection of data mixtures over 77 NLG datasets, which are publicly released.\n- A exploration effort on studying multi-task pretraining on the NLG area.\n- Code are released and could be easy to reproduce the results.\n\nWeaknesses:\n- The idea of multi-task pretraining is not novel, as it mostly follows Aribandi et al. (2022).\n- Results are mixed. On Table 2, it seems the multi-task pretraining has minor gains over BART finetuning or Single; while on Table 9, the improvements are indeed significant. \n- The proposed variants of pretraining stages (e.g., MVP+S, MVP+R, MVP+M) don't seem effective as MVP (the results are on-par on most datasets from Table 2 & 3)",
            "clarity,_quality,_novelty_and_reproducibility": "- On table 10, it seems there are a large improvement against ExT5 on the GEM benchmark. Since the ExT5 paper doesn't release their model checkpoints, and the evaluation script of GEM benchmark can make a big difference on the results, it would be good to add BART finetune baseline on Table 10 too -- so that we can exclude the effect of evaluation difference. ",
            "summary_of_the_review": "Overall, the paper makes a significant contribution on the collection of a large amount of NLG datasets and study the multi-task pretraining behaviors for NLG. Their code are released and the results can be reproduced. The idea presented in this paper is only marginally novel and mostly follow the prior work. The results of their proposed method also seems mixed in different datasets.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3518/Reviewer_4XxP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3518/Reviewer_4XxP"
        ]
    },
    {
        "id": "pkH4NajyEGO",
        "original": null,
        "number": 2,
        "cdate": 1666672632419,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666672632419,
        "tmdate": 1666672632419,
        "tddate": null,
        "forum": "Us5in-h2Dp",
        "replyto": "Us5in-h2Dp",
        "invitation": "ICLR.cc/2023/Conference/Paper3518/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Considers supervised pre-training as an alternative to unsupervised pre-training, which is more common with state-of-the-art NLP models. Many (77) NLG supervised datasets are curated to create MVPCorpus which is used for supervised pre-training. However, the training setup is not designed in a way that allows us to compare unsupervised vs supervised pre-training since the backbone model is initialized with BART parameters. Thus the paper cannot answer the question which is posed, and instead shows that continued supervised pre-training using related tasks after starting with an unsupervised pre-trained model helps with downstream tasks. \n\nThe model is initialized with BART and is further 'pre-trained' using supervised/multi-task learning. The main baseline is BART of the same architecture and some variants are compared.\n\nFine-tuning is done in two settings: full fine-tuning, and with prefix tuning. They show that MVP does better in both settings compared to BART and achieves some SoTA results in NLG in the FT setting. Finally, it is shown that zero-shot performance is better than BART as well.\n",
            "strength_and_weaknesses": "# Strengths\n- Improvements over BART baseline,  which has the same architecture, in some tasks. \n- In some NLG cases achieves SoTA, although I've not verified every task.\n\n# Weaknesses\n- There is a major mismatch in the way the paper is written and the experimental setup. It is written to suggest that supervised pre-training can replace unsupervised pre-training, but does not have experiments to back this up. Consequently it has highly misleading claims on merits of supervised pre-training since the models are primarily pretrained in an unsupervised way.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: It should be emphasized from the beginning (including abstract) that this is a study of continued multi-task supervised pre-training, and not a replacement for unsupervised pre-training.\n- Reproducibility: Missing details about how different tasks are mixed in supervised pre-training.\n- Novelty: continued pre-training with related tasks is not new, and is sometimes called intermediate fine-tuning. It is already known to improve performance in certain tasks.\n",
            "summary_of_the_review": "This paper should be re-written as a study of how to continue pre-training with multi-task supervised learning or properly study supervised pre-training by initializing the model from random parameters. At best, if re-written this way the paper shows how to achieve slightly better results on certain NLG tasks compared to BART with intermediate fine-tuning. The novelty is limited and the claims are incorrect.\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3518/Reviewer_AMUT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3518/Reviewer_AMUT"
        ]
    },
    {
        "id": "HSABvCXx9Mt",
        "original": null,
        "number": 3,
        "cdate": 1666696434148,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666696434148,
        "tmdate": 1666696434148,
        "tddate": null,
        "forum": "Us5in-h2Dp",
        "replyto": "Us5in-h2Dp",
        "invitation": "ICLR.cc/2023/Conference/Paper3518/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper propose to continue pre-train a pre-trained seq2seq model using annotated data for generation tasks. The authors collected 77datasets over 11 diverse NLG tasks for the continued train step. Evaluation shows good results.\n",
            "strength_and_weaknesses": "### Strengths\n- Collect a significant amount of training data for seq2seq tasks.\n- Reconfirm empirically that using more data is always useful\n\n### Weaknesses\n- The contribution of the paper is too thin in my opinion (i.e., showing that using more data is useful)\n\n- On *Generaliability* study: I don\u2019t really understand this setup. The authors call it unseen tasks but then the  model is fine-tuned using the annotated data of the task. How is this different from taking any pretrained model and finetuning it on the same data? My understanding here is that the \u201cunseen\u201d part means that the tasks and data weren\u2019t in the continued train step. But calling that _generaliability_ seems to be an overstatement to me. Is that what we all do with pretrained models?\n- There are no human evaluation results. Evaluating language generation is tricky, using automatic metric alone is not sufficient.\nWhile measuring by automatic metric, it shows that supervised pre-training with  more data is useful. There are many datasets with different characteristics. Does it hurts the model in a way that we don\u2019t know yet? What type of hallucination in language generation when the model is trained on diverse datasets? Does the nature of informal text in the Personal Chat dataset have negative influences on formal text CNN/Daily Mail. I think there are many important questions about mixing random datasets in the context of language generation that have not been answered. The small improvement in automatic metrics do not fully justify the approach.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- There is some part quite hard to follow. How do you train  (MVP+R) model? \n- the originality is marginal\n",
            "summary_of_the_review": "In summary, the contribution of the paper is thin. The evaluation is not convincing (lack human evaluation). There are potential issues of concatenating multiple language generation datasets in the supervised pretraining step.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3518/Reviewer_VRii"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3518/Reviewer_VRii"
        ]
    }
]