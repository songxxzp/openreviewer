[
    {
        "id": "0djXzsgrLzC",
        "original": null,
        "number": 1,
        "cdate": 1666317407332,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666317407332,
        "tmdate": 1666317407332,
        "tddate": null,
        "forum": "gOZ_pKANaPW",
        "replyto": "gOZ_pKANaPW",
        "invitation": "ICLR.cc/2023/Conference/Paper1478/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an approach of combining unsupervised metrics to realize an effective selection of anomaly detection models for time-series data. The unsupervised are highly correlated with standard supervised anomaly detection performance metrics. Therefore, the proposed approach can detect anomalies in an unsupervised manner. Using datasets of various domains, the paper performed experiments to show the effectiveness of the proposed approach.",
            "strength_and_weaknesses": "Strength:\n- Simple model to be easily implemented.\n- General approach to accommodate various anomaly detection approaches.\n- Extensive experiments are performed in the paper. \n\nWeakness:\n- The proposed approach is a combination of existing methods. \n- Technical depth of the proposed approach is not so deep.\n- The advantage of the proposed approach is not so significant compared to other approaches.",
            "clarity,_quality,_novelty_and_reproducibility": "Basically, I like the paper's motivation; it is quite a fundamental research problem to select the most accurate model for performing anomaly detection for time-series data. Besides, this paper is well-structured, and easy to follow. I appreciate that the paper conducted experiments based on many real datasets of various domains, such as medicine, sports, and entomology. \n\nIn terms of technical depth, the proposed approach seems to be a combination of the previous approach; it is technically somewhat shallow. \n\nI am concerned about the computational cost of the proposed approach. It needs to compute several models, and the proposed approach itself has user-defined parameters needed to be tuned. The impact of the user-defined parameters for the proposed approach is unclear from the descriptions of the paper. \n\nAs shown in the experimental result, such as Figure 3 and 4, the effectiveness of the proposed approach is not high compared to the previous approaches, although it is not so worse than them. Since the proposed approach requires higher computational costs than the previous approach, it could not be so useful in performing anomaly detections.",
            "summary_of_the_review": "More theoretical analyses are required for the proposed approach. \nThe computational time of each approach should be shown in the paper. \nThe superiority of the proposed approach in terms of accuracy is not so high to accept the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1478/Reviewer_kY2N"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1478/Reviewer_kY2N"
        ]
    },
    {
        "id": "NuKBOZgVkj",
        "original": null,
        "number": 2,
        "cdate": 1666572315511,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666572315511,
        "tmdate": 1666572315511,
        "tddate": null,
        "forum": "gOZ_pKANaPW",
        "replyto": "gOZ_pKANaPW",
        "invitation": "ICLR.cc/2023/Conference/Paper1478/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a solution for selecting models for time-series anomaly detection. Starting from the assumption that no single model performs best across all datasets, the paper argues for the importance of AutoML solutions in that area. As a result, the paper first studies surrogate measures that correlate with the accuracy of anomaly detection methods. Then, it combines these measures using a rank aggregation technique. In the end, the paper demonstrates the effectiveness of the proposed solution.",
            "strength_and_weaknesses": "Strengths\n\n1. Time-series anomaly detection is a well-studied, timely, and important problem\n2. Well-written and easy to follow\n3. AutoML for anomaly detection for time series is not sufficiently studied and hence this study fills a gap\n\nWeaknesses\n\n1. Missing baselines from the AutoML area in general\n2. Missing baselines from the AutoML area for time series\n3. Omitted new benchmarks and evaluation strategies\n\nComments:\n\n- AutoML is a very well-studied area. The paper claims it's the first work regarding time-series anomaly detection, which is also not true. A simple query reveals multiple solutions in that area that are unfortunately not mentioned and not compared against. It's very difficult to assess the importance of this work and if it advances state of the art without covering and comparing the relevant work appropriately\n\nhttps://scholar.google.com/scholar?hl=en&as_sdt=0%2C36&q=model+selection+time-series+anomaly+detection&btnG=\n\nExisting AutoML solutions for non-time-series data can very well be adopted for this problem and needs to be compared. Time-series specific AutoML solution for anomaly detection or other ML tasks should also be used in comparison.\n\n- The work relies on specific recent papers to justify reasoning/choices (e.g., Schmidl et al.) while ignoring works appearing at the same time/conference, with different findings. Schmidl et al limited the time a method could run during their evaluation and used methods \"as is\" (i.e., not reimplemented), which resulted in model failures, which were not taken into consideration but instead decreased the overall performance of specific methods. This is truly problematic as their findings differ from other studies in this area, e.g., Paparrizos et al., which focused on significantly more data (12000+ time series) but less methods and identified several methods significantly outperforming current SOTA. Even though the claim that no method performs well across every single dataset still holds, several of the choices in the current papers are problematic (use of baselines, use of datasets, use of evaluation measures). Recent developments suggest using different evaluation measures in the first place, proving that the F measure is flawed.\n\n\"Tsb-uad: an end-to-end benchmark suite for univariate time-series anomaly detection.\" Proceedings of the VLDB Endowment 15.8 (2022): 1697-1711.\n\n\"Volume under the surface: a new accuracy evaluation measure for time-series anomaly detection.\" Proceedings of the VLDB Endowment 15.11 (2022): 2774-2787.\n\n\n\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is reasonable. Novelty and quality are somewhat low, considering missed recent advances as well as many missed AutoML baselines.",
            "summary_of_the_review": "See above. The missed AutoML baselines is a major flaw of this work. In addition, many recent advances in the time-series anomaly detection literature are not mentioned/ignored, which may change the outcome of this work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1478/Reviewer_E7ki"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1478/Reviewer_E7ki"
        ]
    },
    {
        "id": "6mrpaWFqxp",
        "original": null,
        "number": 3,
        "cdate": 1666616644066,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616644066,
        "tmdate": 1666616785678,
        "tddate": null,
        "forum": "gOZ_pKANaPW",
        "replyto": "gOZ_pKANaPW",
        "invitation": "ICLR.cc/2023/Conference/Paper1478/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This manuscript tried to answer the question in the community of time series anomaly detection: given an unlabeled dataset and a set of candidate anomaly models, how can we select the most accurate model? Therefore, this paper proposed a robust rank aggregation method with theoretical justifications. This unsupervised model selection method has been verified on multiple real-world datasets.",
            "strength_and_weaknesses": "Pros:\n1. The proposed model selection criterion is simple and flexible to implement.\n2. Extensive experiments using various anomaly detection criteria have been performed.\n\nCons:\n1. In the numerical experiments, the proposed rank aggregation strategy is only applied to five pre-selected anomaly injection metrics. It is suggested to run the rank aggregation strategy on all the studied surrogate metric in order to fully showcase the capability of the proposed anomaly detection model selection criterion.\n2. The superiority of the proposed method in comparison to other metrics is not that significant.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea is interesting and easy to follow, with extensive evaluations on multiple real-world time series anomaly detection datasets.",
            "summary_of_the_review": "Overall the idea of using rank aggregation for model selection for time series anomaly detection offers new insights, and the evaluation is solid.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1478/Reviewer_VFqz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1478/Reviewer_VFqz"
        ]
    },
    {
        "id": "rKJN59g_G0",
        "original": null,
        "number": 4,
        "cdate": 1666806930475,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666806930475,
        "tmdate": 1670859336213,
        "tddate": null,
        "forum": "gOZ_pKANaPW",
        "replyto": "gOZ_pKANaPW",
        "invitation": "ICLR.cc/2023/Conference/Paper1478/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on anomaly detection model selection metrics in the unsupervised setting where a user does not have access to labeled anomalies in order to select the best model. They identify three classes of these surrogate metrics (prediction error, synthetic anomaly injection, and model centrality) and also propose a way to combine rankings from multiple surrogate metrics with some theoretical guarantees. They show experiments comparing surrogate metrics from the three classes with their combined rankings method against an oracle, random baseline, and partially labeled baseline. ",
            "strength_and_weaknesses": "Strengths:\n- Very interesting concept, does not seem well studied prior in the literature\n- The proposed Borda rank aggregation has theoretical justifications\n- Overall well presented paper\n\nWeaknesses:\n- Is minimizing empirical influence, essentially throwing out \"bad\" surrogate metrics as \\sigma_i are the rankings of surrogate metric i ? If so, in the empirical results it would be interesting to see if the surrogate metrics that are \"bad\" for a dataset, align with that metrics performance on the dataset. I believe the surrogate metrics being aggregated are the same ones used individually?\n\n- Under the assumptions of Theorem 1 the probability that Borda aggregation makes a mistake in ranking is an exponentially decaying in the number of surrogate metrics M. The experimental results never seem to empirically support this though. While it would be potentially computationally expensive to show it asymptotically, it would be good to see a plot of the performance of Borda aggregation increasing as the number of surrogate metrics increase, even if it is just from 2 to 17.\n\n- None of the methods are able to have significant wins against random on all 10 datasets? Additionally Pred. Error MSE has significant losses against Random in 4 datasets; but it is used as one of the surrogate metrics in Borda? For those 4 datasets, wouldn't that metric basically violate the assumptions of Theorem 1 that Borda is aggregating surrogate metrics that are better than random?",
            "clarity,_quality,_novelty_and_reproducibility": "No issues, everything is well presented, good quality, and novel",
            "summary_of_the_review": "This paper is presented well, proposes an interesting novel concept, and has strong justifications. The only weakness is that empirical results seem somewhat mixed in supporting their claims. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1478/Reviewer_Qvoy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1478/Reviewer_Qvoy"
        ]
    }
]