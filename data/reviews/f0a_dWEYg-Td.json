[
    {
        "id": "KxiDCjHz-q",
        "original": null,
        "number": 1,
        "cdate": 1665758130488,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665758130488,
        "tmdate": 1666084478957,
        "tddate": null,
        "forum": "f0a_dWEYg-Td",
        "replyto": "f0a_dWEYg-Td",
        "invitation": "ICLR.cc/2023/Conference/Paper3897/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper poisons unlabeled training data of contrastive learning (CL) to reduce the test accuracy of linear probing. The imperceptible poisons are iteratively optimized to minimize the CL objective function. After the poisons are generated, the authors add them to the training set and run the target CL algorithm from scratch. When the entire training set is poisoned, there is a clear drop in the linear probing accuracy, e.g., from ~90% to ~50% on CIFAR-10.\n\n\nThe poison generation proceeds as follows. At each iteration, the model parameters are first updated to minimize CL loss and then the poisons are also updated to minimize CL loss. This follows the basic idea in Huang et al (2021) although Huang et al (2021) minimize classification instead of CL loss. One worth mentioning difference is the authors show data augmentations of common CL algorithms are differentiable. They take the gradients of data augmentation into consideration when updating the poisons.\n\nThe authors analyze why the poisons work. They show the poisons work by shortcutting the instance-wise CL task, whose objective is to produce similar representations of two different augmented views. They show the poisoned embeddings of two augmented views are close while the clean embeddings are far.\n",
            "strength_and_weaknesses": "**Strength**\n\n1. This paper gives the first attack against contrastive learning. It is interesting to know that one can find shortcuts for the instance-wise CL task.\n\n2. Several interesting tricks, e.g., letting the gradient back through the momentum encoder and differentiable data augmentations, are proposed to improve the attack.\n\n3. The authors show several defenses, including a newly proposed matrix completion, that can be used to defend against contrastive poisoning.\n\n**Weaknesses**\n\n1. How important is using differentiable data augmentations? I do not see an ablation study on this. Will the poisons be useless without this trick?\n\n2. In Figure 4 you explain why contrastive poisoning works but only use class-wise poisons. It would be better if you could include the results of sample-wise poisons.\n\n3. Most of the experiments assume the attacker has full access to the training data. Although most of previous works also use this assumption, it does not change the fact that it would probably be unrealistic.\n\n4. The explanation of why CP works is that it provides shortcuts for the model to produce similar representations of different augmented views. If this is true, it is possible to directly optimize the poison so that it remains unchanged when the data augmentation is applied. For example, a circle is invariant to rotation. This data/model independent approach is more efficient than the optimization procedure.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please see Strength And Weaknesses. I hope the authors would reply to my first two concerns to further improve this work.\n\nThe paper is well written in general. There is a missing reference in Appendix C.2.",
            "summary_of_the_review": "Given that this paper is the first to show it is possible to design shortcuts for instance-wise CL task, I recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3897/Reviewer_vT93"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3897/Reviewer_vT93"
        ]
    },
    {
        "id": "Kq4r49wyJd",
        "original": null,
        "number": 2,
        "cdate": 1666656728278,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656728278,
        "tmdate": 1669259837419,
        "tddate": null,
        "forum": "f0a_dWEYg-Td",
        "replyto": "f0a_dWEYg-Td",
        "invitation": "ICLR.cc/2023/Conference/Paper3897/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper considers an adversarial setting for contrastive learning: the attacker is allowed to add imperceptible perturbation to all training samples of CL, which will reduce CL's effectiveness as a feature extractor. Experiments show that downstream task's performance will be lowered by the poisoning.",
            "strength_and_weaknesses": "**Strength**\n\nThis paper is technically sound. The experiments considers a wide range of baselines. I appreciate the effort.\n\n**Weaknesses**\n\nAs the authors have self-identified in the limitation, the attack setting seems to be slightly artificial: the attacker has control over *all* training data is a very strong assumption... The attack mechanism is not too surprisingly novel either. I feel that the paper has presented much empirical evidence of what effect the attack is able to bring, but fails to provide in-depth analysis of its implication. For example, on Page 8, the authors have shown that noise in SL attack is sometimes linear separable, while that in CL attack is not. It is a very interesting observation. However, does that mean linear separable noise is easier to detect? or, what does this linear (non)separability suggest? In future edition or work, the authors may dig further the implication of such phenomena to contrastive learning or more general learning problem. Such insights could be more helpful than the attack/defense itself, given that the adversarial setting is not very common in practice.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear in describing settings, technical details and empirical finding. Its novelty and impact is somewhat hindered by the less realistic attack setting.",
            "summary_of_the_review": "Overall, this is a paper with solid empirical evaluation but slightly lacking novelty. To me, it is on the borderline. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3897/Reviewer_Cwex"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3897/Reviewer_Cwex"
        ]
    },
    {
        "id": "Xv8l5FMIIuF",
        "original": null,
        "number": 3,
        "cdate": 1666670100399,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670100399,
        "tmdate": 1669973996012,
        "tddate": null,
        "forum": "f0a_dWEYg-Td",
        "replyto": "f0a_dWEYg-Td",
        "invitation": "ICLR.cc/2023/Conference/Paper3897/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies clean-label availability attacks against unsupervised contrastive learning. The authors first observe that previous poisons, designed to compromise supervised learning, fail to harm contrastive learning algorithms, such as SimCLR and BYOL. Then, the authors propose a new attack method tailored to compromise contrastive learning. Surprisingly, the proposed attack is not only effective for contrastive learning, but also for supervised learning. ",
            "strength_and_weaknesses": "Strengths:\n\n- This paper is the first to study availability attacks against unsupervised contrastive learning.\n- A dual branch scheme is proposed to mitigate the optimization obstacles and thus boost the attack performance.\n- Experiments provide several novel observations:\n  - It is an interesting observation that sample-wise CP mostly performs better, while class-wise CP can transfer better.\n  - While almost all previous poisons in this area are linearly separable, the proposed poison is not. This would make availability attacks more undetectable.\n  - A very simple data augmentation method based on Matrix Completion is proposed to mitigate poisons. The authors find that this simple method can perform better than adversarial training.\n\nWeaknesses:\n\n- Given that Matrix Completion is very effective in defending against the proposed attack, it is natural then to ask whether this simple method can defend against previous poisons, such as unlearnable examples and adversarial poisoning.\n- Table 8 tests the defense power of AdvCL (i.e. a type of adversarial training) against the proposed poisons. Is the perturbation radius of adversarial training $\\epsilon=8/255$? Prior work (Tao et al., 2021, Figure 5) has shown that adversarial training with small $\\epsilon$ could perform better. Is this also true in the context of unsupervised contrastive learning? By the way, adversarial training was first identified by Tao et al as a promising defense against those poisons. Thus, the last sentence of the introduction needs to polishing.\n- There are availability attacks that can work by injecting dirty labels into the training data (Biggio et al., 2012). That is to say, not all availability attacks are imperceptible. Thus, the first sentence of the introduction needs to polishing.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, well-organized, and easy to follow. ",
            "summary_of_the_review": "The paper explains why previous poisons fail and shows how to effectively compromise contrastive learning. This paper observes many novel empirical findings. These constitute a thorough study of clean-label availability attacks against unsupervised contrastive learning. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3897/Reviewer_gmqR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3897/Reviewer_gmqR"
        ]
    }
]