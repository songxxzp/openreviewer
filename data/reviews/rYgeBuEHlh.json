[
    {
        "id": "qE_wv_ycrf",
        "original": null,
        "number": 1,
        "cdate": 1666639406311,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639406311,
        "tmdate": 1666639406311,
        "tddate": null,
        "forum": "rYgeBuEHlh",
        "replyto": "rYgeBuEHlh",
        "invitation": "ICLR.cc/2023/Conference/Paper4487/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This study proposes an innovative strategy to attack reinforcement learning algorithms. Importantly, the attack requires minimal, information about the victim, in contrast to many other approaches that assume much more extensive inside information. Such black box approaches are necessarily less effective but ultimately much more dangerous because they can be applied in a generic fashion. ",
            "strength_and_weaknesses": "Strengths\n\nWhile there has been extensive work on adversarial attacks in pattern recognition tasks, less work has been devoted overall to the problem of adversarial attacks in reinforcement learning. RL is particularly interesting for adversarial attacks because there are multiple potential attack points, beyond the input to the system itself, such as the policy, value computation, etc. \n\nMost approaches in RL attacks require significant knowledge about the algorithm or the victim. This study focuses on the more challenging (and arguably realistically more critical) form of black box attacks.\n\nFig. 5 is impressive in showing results that are near the oracle predictions for the proposed adversaries. \n\nWeaknesses\n\nRather than a weakness, this is a strength and a question. If I interpret the results correctly, the results for the proposed algorithm in the minatar breakout are particularly devastating. I was somehow expecting the results to be weaker than those in Fig. 1 given the increased complexity of the task. I guess that it is hard to compare quantitatively across completely different tasks and reward values, but is there any intuition for why the algorithm should work perhaps better in Fig. 2a? Is it somehow easier to attack minatar breakout? \n\nMost of the remaining figures focus on the simpler tasks. It would be interesting if possible to expand some of the other figures (goal conditioning, comparison to oracle, showing policies) for the more complex breakout task. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written. ",
            "summary_of_the_review": "This paper addresses a timely and interesting problem in ML, namely, how to perform adversarial attacks for RL with minimal, if any, information about the victims. The paper is clearly written and shows very promising results in this direction, surpassing benchmarks and introducing a new methodology for black box RL attacks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4487/Reviewer_fsi4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4487/Reviewer_fsi4"
        ]
    },
    {
        "id": "gYVeojeqVtp",
        "original": null,
        "number": 2,
        "cdate": 1667320262489,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667320262489,
        "tmdate": 1667320262489,
        "tddate": null,
        "forum": "rYgeBuEHlh",
        "replyto": "rYgeBuEHlh",
        "invitation": "ICLR.cc/2023/Conference/Paper4487/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper considers train and test time adversarial attacks on deep reinforcement learning. Here, the attacker can append the state observed by the learning agent with messages given by a deterministic policy. In the threat model, the attacker does not influence transition dynamics or reward functions, introduce non-stationarity, or access agents' actions or parameters. The authors develop both types of attacks based on evaluation strategies (ES). The experimental evaluation considers training with PPO for popular gym environments as well as minatar.  The results show that the training-time attacks can interfere with the learning of the neural network used by the agent while test-time attacks provide direct control over the agent.",
            "strength_and_weaknesses": "Strengths:\n1. There do not exist enough studies on the vulnerability of deep reinforcement learning algorithms against training time attacks. New attacks as designed here can promote the development of more robust algorithms.\n\n2. The attack model presented here makes several black-box assumptions that make the attack more feasible.\n\nWeaknesses:\n1. The threat model does not limit the size of the message appended by the adversary.\n\n2. The evaluation is performed only on PPO. However, better algorithms like TD3, and SAC also exist. It is not clear if the proposed attack can affect learning by these algorithms.\n\n3. Experimental details are not clear (as described below).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality: In general, the paper is easy to understand at a high level. However, several details are not presented which affects my final rating: \n1. Does the attack work for only on-policy learning algorithms or it generalizes to also off-policy algorithms? It would be good to make the scope of the attack clear.\n\n2. What are the useless features of the learning environments (e.g. Cartpole, Pendulum)? Are they part of the state space of the standard agent without attack or new features are added by the authors? If the former, how did the authors determine the attack subset for different environments? If the latter, then why does the addition of these extra features make sense?\n\n3. Since the messages add numerical values to the state, are these numerical values outliers or similar to values for the useful features? Why did you not constrain the length of the message as well as the range of values appearing in the appended messages?\n\n4. For figure 3, the author mention \"shows the cosine distance between these gradient updates.\" how are the gradient updates shown on the x-axis and y-axis different?\n\n5. In Figure 2 (b), the number of updates shown on the x-axis seems small to make any conclusion about the trends for RARL and ACT?\n\n6. Does proposition 1 hold for any tabular RL algorithm?\n\n7. \"is thus represented by a horizontal line in Figure 5.\" Algorithm 2, generates different $\\psi$ for different iterations. Which $\\psi$ is used?\n\n8. Why are the number of timesteps in Fig 1 and 5 for different environments different?\n\n9. What is the runtime of training and test time attacks?\n\n10. Also, the algorithms require training large number of agents for different ES iterations. How realistic is this assumption?\n\nNovelty: The threat model presented here is novel. The algorithms build upon ES which is a known technique, however its use for training and testing time attack on RL is new. The authors should consider providing more details about how ES learns $\\phi$ and $\\psi$ instead of treating it like black-box.\n\nReproducibility: The details about which and how messages are added may prevent reproducibility. The authors do not mention if they will make code available.",
            "summary_of_the_review": "Overall, the paper tackles a less explored but important problem that can promote the development of more robust RL algorithms. However, I found that the threat model and experimental details are not clearly presented due to which I consider the paper below the acceptance threshold for ICLR.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4487/Reviewer_HHmW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4487/Reviewer_HHmW"
        ]
    },
    {
        "id": "8rENQxhKGPm",
        "original": null,
        "number": 3,
        "cdate": 1667415632383,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667415632383,
        "tmdate": 1668709141986,
        "tddate": null,
        "forum": "rYgeBuEHlh",
        "replyto": "rYgeBuEHlh",
        "invitation": "ICLR.cc/2023/Conference/Paper4487/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies adversarial attacks in reinforcement learning under the setting where the adversary can only append to the agent\u2019s observations. The authors propose a meta learning-based Adversarial Cheap Talk (ACT) method to generate such attacks, such that the agent\u2019s training/test-time performance can be improved or attacked depending on the adversary\u2019s objective. The authors also show that using ACT in training can affect test-time performance. Experiments demonstrate the effectiveness of the proposed ACT method.",
            "strength_and_weaknesses": "#### **Strengths**:\n- The appending attack and experiment settings are interesting\n- The authors propose to leverage meta learning for generating attacks\n- The proposed method is straightforward and demonstrates effectiveness\n\n#### **Weakness**:\n- The proposed Adversarial Cheap Talk (ACT) attack match the intuition of appending attack in the RL setting, but it seems the optimization is completely based on evolution strategies (ES), which makes the technical contribution of this work incremental.\n- The two settings of train-time and test-time attacks seem to be less relevant. The former setting trains an adversary along with the victim and the latter setting uses backdoor for test-time manipulation.\n- The test-time attacks introduce backdoor in training for test-time manipulation, however, it lacks details in section 5.3 of how the backdoor is designed and leveraged to fully control the victim.\n- In the train-time influence experiment settings, the selected baseline methods (i.e., random and zero) are too na\u00efve. For the RARL method, authors only present the results in the cartpole experiments, I would like to see more comparison in experiments like pendulum and reacher.\n- In the test-time manipulation experiments (figure 5), why the performance of different settings is not consistent? For example, why does direct oracle reach the maximum return sooner in the reacher experiments?\n- The adversary objective of test-time manipulation is to maximize the goal-conditioned scores, the setting of minimizing the return is not discussed.\n- Finally, I am curious how the proposed attacks perform against simple defense methods as the authors discussed in the last section.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Novelty: This work presents a novel idea of appending adversarial attacks in the RL setting. The proposed setting is novel and well-motivated.\n\n- Clarity: This work is well organized. However, the concepts and methods can be better explained and more details can be helpful for understanding the paper.\n- Quality: This work has certain issues. These include my concerns about the technical contribution and experiment results, lack of baseline comparison etc.\n- Reproducibility: I didn\u2019t find the code of this paper. But I still trust the authors regarding their results.\n",
            "summary_of_the_review": "Overall, I think this work is interesting. Moreover, the authors propose interesting attacks in the RL setting and have discussed train-time and test-time attacks. However, there are certain issues as I mentioned above, I think the paper would be in a better shape after taking the feedback into consideration.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4487/Reviewer_fBNt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4487/Reviewer_fBNt"
        ]
    },
    {
        "id": "nBiLm83Ogux",
        "original": null,
        "number": 4,
        "cdate": 1667459250781,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667459250781,
        "tmdate": 1667459250781,
        "tddate": null,
        "forum": "rYgeBuEHlh",
        "replyto": "rYgeBuEHlh",
        "invitation": "ICLR.cc/2023/Conference/Paper4487/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies adversarial attacks in RL. Unlike previous work, the paper considers adversarial attacks that append messages to the victim state observation. In the attack model, the attacker has very limited knowledge and the attacks don't intervene the the system dynamics, rewards, and remain deterministic during an episode. The contribution of the paper is proposing a new attack model called Adversarial Cheap Talk (ACT) that can achieve the attacker's goal with limited influence in the MDP and the victim.",
            "strength_and_weaknesses": "Strength:\n1. The paper studies a novel and interesting problem, i.e., to craft adversarial attacks that require limited knowledge of the attacker and apply limited influence in the underlying MDP. And the crafted attacks can still achieve the adversary's objective. \n2. The paper is nicely presented and easy to follow. Experiments are illustrative.\n\n\nWeaknesses:\n\nThere are several places in the problem formulation that don't make sense, which needs further justification. \n\n1. Previous work that studies adversarial attacks in the state observation (e.g., [R1]) models the attacks by $s'=s+\\sigma$, where $s$ is the actual state, $s'$ is the observed state, and $\\sigma$ is the attack crafted. Here, the ACT adversary append a message $m(s)$ to the actual state and assume that the victim use $(s,m(s))$ as the observation to generate its actions, which does not make too much sense from a security perspective. (a) Why the victim choose $(s,m(s))$ as the input of its strategy to choose its action? What is the motive? \n(b). Of course, a RL agent does not know the model (the environment dynamics and the rewards). But the dimension of its state space is known beforehand. Appending a message is like changing the dimension of its state space, which would be easy to detect. \n\n2. Does the assumption of stationarity benefit the attacker (to achieve its goal), the victim (to avoid non-convergence), or the authors (to show experimental results that can converge)? The authors did not justify the purpose of requiring the attacker not to introduce non-stationarity. What is the incentive for the attacker to do so If the attacker can achieve its goals by introducing non-stationarity to the learning process?\n\n3. From the figure 1, when the adversary is in the \"Ally\" mode, the authors observe and state that the victim achieves higher rewards in training. But the authors did not explain why this happens? What is the root causes of it? How the attacker or the victim can leverage this phenomenon to do things? \n\n\n\n[R1] Zhang, Huan, et al. \"Robust reinforcement learning on state observations with learned optimal adversary.\" arXiv preprint arXiv:2101.08452 (2021). ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well organized and clearly stated. There are a few places where improvements can be made.\n\n1. In proposition 1, \"the policy of a tabular Victim initialised uniformly along $\\mathcal{M} is independent from its adversary$\". Please use math in the proposition to give precise description of the quoted sentence. The description is embedded in the proof. But not every reader will read the proof and giving a rigorous and precise state is important.\n\n2. In section 6.1, \"Random Adversary: randomly initialise and fix the Adversary\u2019s parameters\". The description of random adversary is very vague. Adding \"randomly\" without specifying how the random initialization is done is misleading. Correct me if I missed any content.\n\nI did not check the code. So I am not the right person to judge the reproducibility of the paper.",
            "summary_of_the_review": "The paper studies a novel and interesting problem. But there are caveats in the problem formulation that needs to be filled (Details are provided in section \"Strength And Weaknesses\").",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4487/Reviewer_c6pu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4487/Reviewer_c6pu"
        ]
    }
]