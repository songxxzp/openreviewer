[
    {
        "id": "FC4YMc5hNTN",
        "original": null,
        "number": 1,
        "cdate": 1666577744360,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577744360,
        "tmdate": 1666577744360,
        "tddate": null,
        "forum": "O5PXo5Y0csVi",
        "replyto": "O5PXo5Y0csVi",
        "invitation": "ICLR.cc/2023/Conference/Paper4301/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the relaxed attention method on the Transformer architecture, across a variety of tasks including automatic speech recognition, lip reading, machine translation, image classification. The technical contributions include exploring relaxed attention on the self-attention module, utilizing it on both training and inference, and introducing variation into image classification where the sequence length is fixed. ",
            "strength_and_weaknesses": "**Strengths:**\n\n1. The paper studies various tasks including on text, speech and image, which guarantees that the conclusions are general and robust.\n2. The paper is generally well-written and easy to follow.\n\n\n**Weaknesses:**\n1. The contribution is somewhat incremental. Extending relaxed attention from cross-attention to self-attention is straightforward, the paper shows that it will bring some improvements to the model performance, but lacks showing the novelty of doing so. For example,  what it brings to the training/inference of the model? Does it solve some problems that the original attention has? And how the relaxed attention achieves that? In my experience, the sparsity in self-attention is natural and provides clear local context information to the model, then what the smoothing regularization term brings to the model?\n2. Another contribution, the matched inference, does not show consistent improvement across different tasks. The paper does not provide a detailed analysis of the results which also limits the novelty. If it cannot bring improvements and no in-depth explanation is conducted, then why take it as one of the contributions?\n3. The method section is too short to provide the details and insights of the proposed method.\n4. The experimental settings lack some details. For example, what is the difference between the clean and other settings in the ASR task? In the \u201cApproach\u201d column in each table, is the relaxed self-attention added on top of relaxed cross-attention or they are separately enabled? In the machine translation tasks, have you conducted experiments on larger benchmark datasets such as WMT which will make the conclusions more faithful?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The formulation of the problem is clear and the paper is well-written. However, the contribution is quite limited. ",
            "summary_of_the_review": "I believe the novelty of the paper is straightforward, but the contribution is limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4301/Reviewer_4P4f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4301/Reviewer_4P4f"
        ]
    },
    {
        "id": "b4mptTl3He",
        "original": null,
        "number": 2,
        "cdate": 1666683644491,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683644491,
        "tmdate": 1666683644491,
        "tddate": null,
        "forum": "O5PXo5Y0csVi",
        "replyto": "O5PXo5Y0csVi",
        "invitation": "ICLR.cc/2023/Conference/Paper4301/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed to use relaxed attention in both self attention and cross attention part of transformer models. In details, the authors proposed to add a smoothing term on the cross attention module with a fuzzy realaxation coefficient drawn from a Gaussian distribution. Extensive experiments were done on automatic speech recognition, lip-reading, machine translation, and image classification.",
            "strength_and_weaknesses": "Strength:\n1. Simple techniques apply to wide range of application.\n2. Extensive experiments.\n\nWeakness\n1. Novelty. The method was first proposed in [1] as the authors cited in paper.\n2. The experimental results are not strong enough as the improvement seems marginal. And also lack of variance on all results. For the results with less significant improvement, it would better to also show the variance among runs.\n\n[1] TimoLohrenz,PatrickSchwarz,ZhengyangLi,andTimFingscheidt.RelaxedAttention:ASimple Method to Boost Performance of End-to-End Automatic Speech Recognition. InProc. of ASRU, pp. 177\u2013184, Cartagena, Colombia, December 2021. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall clear. \n\nIt introduces a simple idea with a lot of experimental results. The significance of the proposed model is concerning as the improvement is marginal.\n\nIt is somewhat novel, although similar idea was proposed in literature, extending the application to wider range is still valuable.\n\nThe code is provided in supplementary. ",
            "summary_of_the_review": "Generally I think relaxed attention is an interesting idea, as it is simple and easy to apply in multiple applications. However, the efficiency of the technique is supported by experimental results, which is not significant enough as the improvements are marginal and metrics' variance between runs are not provided.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4301/Reviewer_2CCe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4301/Reviewer_2CCe"
        ]
    },
    {
        "id": "G6K07Fs1rE",
        "original": null,
        "number": 3,
        "cdate": 1666685405100,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685405100,
        "tmdate": 1666685405100,
        "tddate": null,
        "forum": "O5PXo5Y0csVi",
        "replyto": "O5PXo5Y0csVi",
        "invitation": "ICLR.cc/2023/Conference/Paper4301/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores relaxed attention in transformer models. The relaxed attention interpolates the attention weights with a uniform distribution. This method was proposed by previous work, and this paper extends its application from cross attention to all attention use cases (cross, self, causal). The paper further proposes a variant that randomly draws the interpolating coefficients at training time.\n\nThe experiments cover a variety of tasks, including automatic speech recognition, lip reading, machine translation, and image classification. On some, the results are mixed; on others, the improvements are marginal. \n\n",
            "strength_and_weaknesses": "Strength\n- Clear presentation\n- The method is straightforward\n- Experiments cover a variety of tasks\n\nWeaknesses\n- Extending an existing technique to self and causal attention is thin in terms of technical contribution\n- No evidence is provided to support the significance of the improvements (if any)\n- Lacks in-depth analysis and discussion of the method\n- An important baseline is missing: smoothing the attention weights using a larger temperature\n- The \u201crelaxed attention\u201d naming is unfortunate\u2014it leaves me the impression that it is lifting some constraints in conventional attention (e.g., simplex). I suggest changing it to something related to \u201csmoothed\u201d or \u201cflat.\u201d\n\nDetails\n- The improvements in the MT experiments seem very marginal\u2014the ~0.2 BLEU delta is definitely reachable with a lucky random seed; I suspect this might be the case in other tasks too (MT is the only one I\u2019m familiar with). The paper can benefit from training multiple models with different random seeds, and/or presenting significance tests.\n- It is great to cover a diverse set of tasks, but the paper favors width over depth. I would really appreciate it if the paper could include some in-depth analysis and discussion of the method. For example, how does the smoothing term affect the \u201cflatness\u201d of attention? Comparing the entropy of the attention weights w/ and w/o the uniform term is a good starting point. Performance change over different choices of gamma is another good one. Some discussion/analysis on why smoother attention is better is also interesting. ",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clearly written. \n- It falls short in terms of technical novelty. \n- Without carefully checking the appendix, my educated guess is that the results are fairly easy to reproduce. ",
            "summary_of_the_review": "The paper did a good job in terms of covering a variety of tasks. But it lacks depth at the same time. Given that the technical contribution is thin compared to average ICLR papers, I suggest that the authors should dive deeper into the core assumption of the paper: smooth attention is better in some applications.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4301/Reviewer_WiHX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4301/Reviewer_WiHX"
        ]
    },
    {
        "id": "G-7jhQDIaXa",
        "original": null,
        "number": 4,
        "cdate": 1666723042835,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666723042835,
        "tmdate": 1666794222319,
        "tddate": null,
        "forum": "O5PXo5Y0csVi",
        "replyto": "O5PXo5Y0csVi",
        "invitation": "ICLR.cc/2023/Conference/Paper4301/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper conducts detailed study on relaxed attention for transformer. The relaxed attention mechanism is applying smoothing to the attention weights. Previous work which proposed relaxed attention only applied it to cross-attention in the decoder. This work studies applying relaxed attention to self-attention layer in the encoder. The authors suggest two benefits: 1. the relaxed attention provides regularization when applied to the self-attention layer. 2. the relaxed attention applied to the cross-attention layer suppresses the intrinsic language model and allows better integration of an external language model. The experiments are conducted in automatic speech recognition (ASR), lip-reading, machine translation, and image classification. The work shows better or on par performance with state-of-the-art models on lip-reading LRS3 benchmark (-0.6% in word error rate) and machine translation IWSLT14(DE->EN) (+0.18 BLEU). ",
            "strength_and_weaknesses": "Strength:\n1. The experiments are detailed and cover multiple tasks/domains (speech, translation, and image).\n\nWeakness:\nMinor sentence fluency:\n1. page 2: \"Very early once ...\"\n2. page 9 in Conclusions: \"thereby regularizing already in the encoder\"\n\nQuestion:\n  - Have you tried applying the relaxed attention in the pretraining stage?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, with minor places where the fluency of the sentence could be improved.\nThe paper conducts experiments that introduces new understanding into a previously proposed method.\nI believe the experiments can be reproduced.",
            "summary_of_the_review": "The thorough experiments confirm the usefulness of an previously proposed technique that smooths the attention weights.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4301/Reviewer_ztHw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4301/Reviewer_ztHw"
        ]
    }
]