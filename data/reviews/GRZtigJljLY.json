[
    {
        "id": "xGPg-YVjTew",
        "original": null,
        "number": 1,
        "cdate": 1666565017499,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666565017499,
        "tmdate": 1666597351808,
        "tddate": null,
        "forum": "GRZtigJljLY",
        "replyto": "GRZtigJljLY",
        "invitation": "ICLR.cc/2023/Conference/Paper4976/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a batch-mode active learning method for deep Bayesian Neural Networks. The sampling is based on the intuition of partitioning the equivalent classes optimally and puts less weight on the hypothesis that does not disagree with the true data distribution much. \n",
            "strength_and_weaknesses": "Strength\nThe paper is well-written and well-motivated. I agree that the uncertainty estimation quality of deep neural networks, especially BNN poses a big challenge for modern active learning. I think the proposed method provides a new point of view for sampling based on the concept of equivalent classes. According to my understanding, the sampling strategy has the flexibility to put less focus on differentiating the hypothesis with little disagreement when the uncertainty estimation is not accurate in the early stage of active learning.\nThe proposed method is practical to real-world problems. The proposed active learning strategy avoids listing the full set of equivalence classes exhaustively by leveraging MC sampling.\n\nWeakness\nThe paper lacks some theoretical discussion to draw the connection between the sampling strategy and learning performance. For example, what are the expected gain results from adding a new annotated sample selected by the proposed method? How fast would eq2 converge to eq1 as K increases?\nThe partition of the ECs is intuitive and lacks validation. Why is the Hamming distance used as a measurement and what are the alternative measures?\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the proposed AL method for BNN is novel and the motivation, intuition, as well as methodology, are clearly described in the paper. I have some concerns listed as follows:\nThe demonstration in figure 1 is not very clear to me. How the probability mass in figure b is computed given the EC is a set of N posterior distributions? How can we tell BALD samples are suboptimal from figure b? \nEven though the proposed method does not require to construct ECs explicitly, I still think the number of ECs (i.e, k) is critical to the active sampling performance. The threshold \\tau seems to reflect the number of k indirectly. Could the author further elaborate on the relationship between k and \\tau and how should \\tau be set intuitively?\n",
            "summary_of_the_review": "The proposed method is novel and sound. But adding some theoretical underpinnings is prefered.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4976/Reviewer_CBRc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4976/Reviewer_CBRc"
        ]
    },
    {
        "id": "0f4npB5CYW",
        "original": null,
        "number": 2,
        "cdate": 1666878821946,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666878821946,
        "tmdate": 1669779439145,
        "tddate": null,
        "forum": "GRZtigJljLY",
        "replyto": "GRZtigJljLY",
        "invitation": "ICLR.cc/2023/Conference/Paper4976/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a novel active learning (AL) algorithm for deep Bayesian neural networks (BNN).\nTowards this goal, the paper proposes a novel acquisition function that is aimed at selecting data points that can improve the differentiation of models that belong to different equivalence classes.\nBy defining a decision-theoretic acquisition function that focuses on predictive accuracy and incorporating diversity sampling across equivalence classes, the paper shows that the resulting AL algorithm, referred to as Batch-BALANCE, is scalable and leads to AL performance improvement compared to existing alternatives.\n.\n",
            "strength_and_weaknesses": "OVERALL COMMENTS\n\nOverall, the paper is well-written in a clear and logical manner.\nThe authors provide a brief yet informative review of the existing state-of-the-art relevant to the proposed batch AL algorithm, motivate and present their proposed approach, and assess the performance of the resulting AL algorithm Batch-BALANCE using several widely used benchmarks.\nEvaluations based on different models and benchmarks with various batch sizes show that Batch-BALANCE generally outperforms other popular batch AL methods, including BatchBALD, BADGE, and PowerBALD.\nBatch-BALANCE is shown to work well in both small and large batch regimes, providing effective and scalable means for Bayesian active learning.\n\n\nDETAILED COMMENTS\n\n1. The authors mention the downside of MIS (most informative selection) methods as \"MIS tends to select data points that reveal the maximal information w.r.t. the sampled distribution, rather than guiding the active learner towards learning high accuracy models.\"\nWhile this is a fair statement, the authors do not mention other AL schemes that focus on data acquisition that is expected to reduce model uncertainty that affects the model accuracy.\nA well-known category is ELR (expected loss reduction), where an ELR-based Bayesian AL method tries to select unlabeled data points whose labeling is expected to reduce the loss most effectively.\nIt would be useful to briefly review representative and/or recent ELR-based Bayesian AL methods and discuss their relevance to the proposed algorithm, since at a high-level, they both take a \"decision-theoretic\" approach that focuses on the model accuracy.\n\n2. Although the overall technical presentation in the paper is good, there are some minor suggestions for improving it further. For example:\n\n- Please define ECED (page 4) before using the term, since it only appears in the appendix\n\n- Please describe how the \"Hamming distance\" is computed between different hypotheses (or model parameters). Since Hamming distance is typically used to compare the difference between two symbol sequences by counting the number of locations with different symbols, it is not entirely clear how it is being used in this work for measuring the distance between two BNN parameters w and w'.\n\n- While the authors present the definition of d_H in the appendix, it is still unclear how it applies to computing the distance between BNN parameters. Furthermore, it would be better if the definition of h_H is referred to in the main text.\n\n- While the pseudocode of Algorithms 1-3 is helpful, it would make the pseudocode more readable for future readers if the code is (at least briefly) commented.\n\n- Instead of calling I_Deltah_Balance(x,y) as a combinatorial information measure, it would be better to conceptually relate it to mutual information\n\n\n3. The acquisition function of Batch-BALANCE is obtained through sampling using either MC dropout or  Stochastic gradient Markov Chain Monte Carlo (SG-MCMC).\nHowever, there is currently no discussion on any pros/cons of the respective methods when used with Batch-BALANCE.\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is clearly written making the proposed Bayesian AL method easy to follow and understand.\nHowever, further improvement could be made as suggested in \"Strength and Weakness\".\nThe proposed method is relatively novel, rationally building on existing work on decision-theoretic AL schemes that adopt diversity sampling across equivalent classes and taking advantage of MC sampling schemes used for posterior estimation of BNNs.\nThe evaluation results clearly demonstrate the practical advantages and potential scalability of the proposed Batch-BALANCE algorithm.\n\n\n",
            "summary_of_the_review": "This is a well-written paper that proposes a novel AL method that builds on a decision-theoretic principle and diversity sampling and aims to improve the performance and scalability of AL for deep BNN. The authors provide a brief review of relevant existing work to provide the right context for the proposed method, clearly motivating the work. Mostly, the technical details are elaborated clearly and the performance assessment results demonstrate the merits of the proposed Batch-BALANCE algorithm under a number of different AL scenarios.\n\n. . .\n\nThe reviewer confirms that (s)he has reviewed the authors' rebuttal, which has been reflected in the above review comments and the overall evaluation scores\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4976/Reviewer_xKhG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4976/Reviewer_xKhG"
        ]
    },
    {
        "id": "gv5iDEy00FS",
        "original": null,
        "number": 3,
        "cdate": 1666899470133,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666899470133,
        "tmdate": 1670941055790,
        "tddate": null,
        "forum": "GRZtigJljLY",
        "replyto": "GRZtigJljLY",
        "invitation": "ICLR.cc/2023/Conference/Paper4976/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors presented a scalable batch Bayesian active learning algorithm and its different variations, which works equally well on large and small batches. The authors support their results with experiments on image datasets as well as with tabular data. By combining Hamming distance and combinatorial information measure, the algorithm is able to select samples for labelling based on both uncertainty and diversity, while spending a reasonable amount of computational time.",
            "strength_and_weaknesses": "Strengths:\n- understandable and relevant motivation\n- well and neatly written, consistent presentation of theory and results\n- significant and detailed elaboration of related work, each used claim is supported\n- the quality is presented in several metrics\n- experiments are done in several domains\n\nWeaknesses:\n- Not clear why the particular criterion is used, i.e. the motivation behind the log-likelihood ration is not clear\n- the usage of the particular distance (Hamming) is not explained - multiple distance can be considered\n- importance sampling of configurations y_{1:b} was used and described in BatchBALD paper to calculate joint mutual information, so the novelty of adaptation of previous work on this step is questionable",
            "clarity,_quality,_novelty_and_reproducibility": "The article is written in a clear and consistent manner; the proposed method is an amalgamation of previously published works, but uses their results as an adaptation. Experimental study in different domains looks promising.\n\nIt is interesting to see how the proposed algorithms will work on CIFAR-100, which is considered a rather challenging dataset and requires large batches for active learning. It is also interesting to see how the proposed algorithms will behave when using deep ensembles to estimate uncertainty. While this is a more resource-intensive approach, it also yields more successful results compared to MC-dropout (Beluch et al. 2018).\n\nRather peculiar figure of the running time of the algorithms. We can see from Table 2 that PowerBALD shows top results with respect to alternatives with respect to computational complexity.  At the same time, in figure 2 this advantage is almost not visible due to the scale of the y-axis. I would suggest adding more points to this graph, or at least a zoom for the existing graph, similar to graph 4 (c). I think it is important to reflect this difference correctly, since at first glance PowerBALD on the graphs in most cases has comparable quality to the BALanCe algorithms, spending less computational resources.\nMoreover, if on small datasets like Repeated-MNIST the difference in performance is noticeable, then on SVHN and CIFAR-10 it is practically not visible. It would also be interesting to see a comparison of BALanCE with PowerBALD and on small batches.\nAlso, it would be interesting to see a more extensive discussion of the results presented. Namely, why Power BALD has a comparable performance despite its disadvantages?\nThe same applies to the analysis of the complexity of the algorithms in Table 2 and the discussion on this.",
            "summary_of_the_review": "The paper is well written but the main technical part is too rushed - the design choices should be better explained. The results presented seem convincing. Generally, the paper in the current state is borderline, I current put it above the threshold, though very interested to see the authors response.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4976/Reviewer_wGy4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4976/Reviewer_wGy4"
        ]
    },
    {
        "id": "1JbAnGKwuEv",
        "original": null,
        "number": 4,
        "cdate": 1667128213158,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667128213158,
        "tmdate": 1667128213158,
        "tddate": null,
        "forum": "GRZtigJljLY",
        "replyto": "GRZtigJljLY",
        "invitation": "ICLR.cc/2023/Conference/Paper4976/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Active learning  is widely used for selecting data efficiently for machine learning models. Performance of existing methods is largely dictated by the quality of uncertainty estimates of the model which may make it challenging to scale up to large batches. The authors propose a technique known as Batch Balance based on decision-theoretic active learning to characterise and distinguish between equivalence classes with similar predictions. They subsequently use a combinatorial information measure to define an acquisition procedure that enables the procedure to be scaled to large batches. Through several experiments the authors  try show their algorithm can effectively handle multi-class classification tasks, while having reasonable performance on both low and large batch settings.",
            "strength_and_weaknesses": "Strengths: Overall I think the paper describes the problem of focus in a lot of detail in the introduction and clearly provides algorithms for each of the strategies used ie stochastic batch selection and greedy selection. The authors also compare the proposed approach to several active learning algorithms and variants of the balancing procedure in small and large batch settings.\n\nWeaknesses: The overall aim of the paper is to use make active learning scalable by learning equivalence classes in settings where large batches of queries are common like many applications in reality. In the experiments, though the authors present several baselines on MNIST, EMNIST, CIFAR etc we never get a sense of how the approach would work on a large scale real application or what the challenges would be in a real setting. This is severe limitation. In fact,  the authors don't describe in any detail what the limitations of the proposed approach is.\n\nMoreover, the focus is on learning and characterising equivalence classes with similar predictions but there is no analysis presented on what these equivalence classes between hypotheses are in the experimental section nor a sense of why these equivalence classes make sense. I would have liked to have seen some qualitative analysis of these.\n\nA minor comment would also be to clearly label all axes. The authors state the axis labels in the caption but not consistently for all images which is confusing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper is fairly clearly written and the approach is fairly novel. Yet the experiments are not very convincing. ",
            "summary_of_the_review": "Overall I recommend a weak reject of the paper mainly because there are many claims made in the motivation and introduction of the paper that are not demonstrated empirically. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4976/Reviewer_RVjP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4976/Reviewer_RVjP"
        ]
    }
]