[
    {
        "id": "pt57YkKrJp",
        "original": null,
        "number": 1,
        "cdate": 1666265336301,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666265336301,
        "tmdate": 1666636046814,
        "tddate": null,
        "forum": "BgoOPulkznY",
        "replyto": "BgoOPulkznY",
        "invitation": "ICLR.cc/2023/Conference/Paper5803/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new framework for 3D object centric learning by leveraging the 2D object mask extracted from the motion information. The proposed approach then trains separate object and background nerf using different network structure. Experiments on data with more complex texture demonstrates the performance of the method.",
            "strength_and_weaknesses": "#Strength \n+ The paper leverages the motion information for 3D object centric learning. It can handle images with complex textures\n\n+ The object nerf and background nerf are trained based on global object feature and pixel-wise features.\n\n \n#Weakness\n+ Novelty of the paper. The proposed approach highly depends on the 2D segmentation mask which provides significant information for 3D object centric learning. Moreover, the 2D object masks are obtained from an existing work and the formulation for the object compositional nerf is not new [R1], which renders the proposed method lacks novelty. Please refer to the following missing reference for the compositional nerf [R1, R2].\n\n +  [R1] Unsupervised Discovery of Object Radiance Fields, ICLR2022\n + [R2] Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering, ICCV2021.\n\n+ The 2D mask segmentation framework highly depends on the quality of the optical flow. Then it cannot handle the images of pure color.\n\n+ Eq. (4). Should ${\\bf c}$ be ${\\bf c}_i$? \n\n+ It would be great to explain why results from the proposed approach cannot recover sharp boundaries and the reconstructed objects are larger than the real one (see Fig. 4)\n\n+ It is hard to interpret the scene editing results. It looks like all views are different from the ones shown on the top? It makes it difficult for the reader to understand whether the images are rendered from changed objects\u2019 positions or just the changed camera view point.\n",
            "clarity,_quality,_novelty_and_reproducibility": "+ The paper is well-written in general. The reviewer is concerned with the novelty of the paper by considering it took a few components from existing works.\n\n+ It would be great to explain the advantage of using the object level feature.\n\n+ The method is reproducible.\n",
            "summary_of_the_review": "This paper tackles a challenging problem, unsupervised 3D object-centric learning. The proposed framework is convincing. However, it largely depends on the 2D mask information obtained from a 2D image segmentation framework and the object compositional nerf is not new. Thus, the reviewer is concerned that there is not sufficient novelty in the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I don't have ethics concerns about this paper",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5803/Reviewer_kstW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5803/Reviewer_kstW"
        ]
    },
    {
        "id": "WHipvDOyr-",
        "original": null,
        "number": 2,
        "cdate": 1666341303541,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666341303541,
        "tmdate": 1666341303541,
        "tddate": null,
        "forum": "BgoOPulkznY",
        "replyto": "BgoOPulkznY",
        "invitation": "ICLR.cc/2023/Conference/Paper5803/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a Movable Object Radiance Fields, by using the EISEN method to generate object masks and processing object and background separately. Comparing with uORF and PixelNeRF methods on three self-generated datasets, the authors show that the proposed method can extract accurate object geometry and has higher performance in scene reconstruction and editing.",
            "strength_and_weaknesses": "Strength:\nThis paper proposes a Movable Object Radiance Fields, which separate 3D scenes into objects and background before reconstruction. The authors claim that this method can better handle complex and diverse multi-object 3D scenes.\n\n\nWeakness:\n1.The overall structure takes advantage from many advanced algorithms, e.g.. object mask generation, object latent code computation, compositional rendering. I think the authors should better clarify the novelty and unique contributions of their method. \n\n2.The authors carry out their experiments only on self-generated datasets. I think that for better comparison, they should test on more public datasets, e.g. the ShapeNet dataset, the DTU dataset, etc.\n\n3.The authors use a long description to introduce the EISEN segmentation method. I think it would be better if they explain why EISEN, not other methods are chosen to for mask generation in the proposed method. Besides, they should provide the evaluation results for segmentation quality of EISEN; maybe even apply some more segmentation methods to show the influence of object masks (right now they only compare EISEN with GT masks without quantitative mask qualities).",
            "clarity,_quality,_novelty_and_reproducibility": "The ideas are clear and reproducible, with limited novelty.",
            "summary_of_the_review": "The proposed algorithm is not well compared and demonstrated.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5803/Reviewer_NgLD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5803/Reviewer_NgLD"
        ]
    },
    {
        "id": "gCajvwHY05J",
        "original": null,
        "number": 3,
        "cdate": 1666677192020,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677192020,
        "tmdate": 1666677457266,
        "tddate": null,
        "forum": "BgoOPulkznY",
        "replyto": "BgoOPulkznY",
        "invitation": "ICLR.cc/2023/Conference/Paper5803/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper extends previous work on 3d scene decomposition and reconstruction from a single rgb image. Being able to reconstruct the 3d structure of a scene from a single rgb enables a host of new functionality such as re-rendering multiple views of the scene, editing the scene, and potentially generating images for training 2d and 3d inference models. This work is novel in that it generalizes previous work to arbitrary \"movable\" objects rather than relying on a supervised model trained on a fixed set of objects.  This is a step function improvement in capability enabled by this work. ",
            "strength_and_weaknesses": "The strengths of this paper are as follows:\n* Allows for scene rendering from a single image and scene editing. \n* Enables a new level of capability by generalizing previous work to handle multiple and arbitrary classes of objects in a scene rather than specific objects trained in a supervised fashion.\n* Convincing quantitative and qualitative results demonstrating that the approach works as expected. \n\n\nThe weaknesses of this approach are as follows:\n* All results are demonstrated only on synthetic data.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, is high quality, and the authors state that they will release code and data on acceptance, so reproducibility  should not be a problem. \n\nThis work does seem to be a fairly straightforward combination of  EISEN (Chen et al, 2022) for detecting 2d masks of arbitrary objects and uORF (Yu et al, 2022) for object-based scene reconstruction.  Their Loss function is almost identical to Yu with the addition of the adversarial loss component.  There is no ablation to indicate how important that additional loss is so it's hard to know how significant that addition is.  I would love to see this added in future versions. \n\nThe similarity to the work of Yu is not called out very clearly. They mention Yu in the related work section and is one of the primary baseline methods used, but during the discussion of the method implementation there is little indication of the similarity to Yu, and it is left to the reader to put this together. \n\n",
            "summary_of_the_review": "This is a straightforward combination of two techniques that enables a step-function capability in 3d scene reconstruction from rgb images. I think this is a solid contribution and worth accepting at ICLR. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5803/Reviewer_dyX9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5803/Reviewer_dyX9"
        ]
    },
    {
        "id": "CP0ioHoVfM",
        "original": null,
        "number": 4,
        "cdate": 1666688797882,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688797882,
        "tmdate": 1666688797882,
        "tddate": null,
        "forum": "BgoOPulkznY",
        "replyto": "BgoOPulkznY",
        "invitation": "ICLR.cc/2023/Conference/Paper5803/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new model for unsupervised object-centric 3D scene understanding, called MORF. Unlike previous methods, it leverages 2D image segmentation masks from the pretrained optical flow method EISEN. These are used to initialized a slot based representation augmented by pixel features (as in PixelNeRF), which are used to condition NeRF decoders for objects and backgrounds. On a novel synthetic 3D dataset called Playroom, the model yields better reconstructions and 3D geometry results when compared to prior work. Improved scene editing capabilities are demonstrated, as well.",
            "strength_and_weaknesses": "Strengths:\n - The paper addresses a relevant and interesting problem, namely, unsupervised object-centric 3D scene understanding.\n - It is well written and easy to follow.\n - The idea of incorporating motion cues for better segmentations makes sense and has not been explored for 3D models so far.\n - MORF convincingly outperforms uORF on the new dataset.\n\nWeaknesses:\n - The characterization of prior datasets (\"They only demonstrate simplistic scenes with a single object category, and the objects in the scene are in uniform colors.\") is not accurate. While they are synthetic and might indeed by called simplistic, e.g. Multishapenet from ObSuRF and MSN-hard from OSRT use objects from multiple ShapeNet categories, and their standard, multi-colored textures.\n - Consequently, I disagree that the new dataset represents the jump in complexity that the paper suggests it does. To me, it appears of a similar level of complexity as ObSuRF's Multishapenet, and much simpler than OSRT's MSN-hard.\n - Increased performance comes at the cost of requiring video data for training, which the baseline models don't.\n - The fact that the processing of video data is outsourced to an existing 2D model decreases MORF's technical novelty, and doesn't make use of the full potential of combining 3D with video observations.\n - Given the emphasis on novel view synthesis in the evaluation, the state of the art model SRT (Sajjadi et al., 2021) should be included as a baseline, even though it will not yield segmentations or 3D geometry in its default configuration.\n - Some additional information could be added for clarity (see below).",
            "clarity,_quality,_novelty_and_reproducibility": "__Clarity__: In general, the paper is well written and easy to follow. Some additional data would be useful to provide to make it easier to evaluate qualitative results:\n - Dataset characteristics such as resolution, number of video frames, etc.\n - Training time / computational cost of EISEN and MORF.\n - Videos from the dataset, as well as EISEN segmentation results compared with ground truth segmentations.\n - Videos showing novel view synthesis and segmentation results.\n - Extracted 3D meshes.\n\n__Quality__: The experiments are setup in a reasonable way, although the inclusion of an SRT baseline as discussed above would be nice. I think not comparing to ObSuRF or OSRT is acceptable, as they operate in slightly different settings. Even though this is largely provided by EISEN, I think a segmentation comparison between uORF, EISEN, and MORF would still be useful, as the geometric scores blend together segmentation and 3D reconstruction quality.\n\n__Novelty__: The fact that MORF mainly leverages cues from an existing, pretrained model reduces its novelty. That said, it is a first step towards the challenging setting of 3D video data.\n\n__Reproducibility__: The authors have promised to provide code and data, which goes a long way for reproducibility. Neither has been provided for review, however. Taking the paper by itself, many implementation details are missing, such as the architectures of the image encoder, or the NeRF MLPs.\n",
            "summary_of_the_review": "Overall, the paper represents an interesting first attempt at utilizing both video and 3D cues for better unsupervised scene understanding. While I believe its novelty is somewhat overstated, the experiments are competently executed. I am therefore leaning towards acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5803/Reviewer_Kin7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5803/Reviewer_Kin7"
        ]
    }
]