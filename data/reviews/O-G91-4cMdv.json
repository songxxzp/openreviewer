[
    {
        "id": "FLYixTi1fr3",
        "original": null,
        "number": 1,
        "cdate": 1665696242000,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665696242000,
        "tmdate": 1669745761096,
        "tddate": null,
        "forum": "O-G91-4cMdv",
        "replyto": "O-G91-4cMdv",
        "invitation": "ICLR.cc/2023/Conference/Paper3436/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the task of approximating human didactic\nsimilarity judgments over N pairs of (images/audio/text). While high\nquality human judgments over all pairs is ideal, (N choose 2) is too\nbig for large N. The authors consider gathering N descriptions of the\nobjects (tags or captions), extracting text representations of those\ndescriptions, and then computing text-text similarity. Experiments\nacross a large number of models and datasets reveal the surprising\neffectiveness of computing text-text similarities.",
            "strength_and_weaknesses": "Reconstructing pairwise human similarity judgments across N objects is\nan interesting task, and the author's idea of gathering textual\ndescriptions is clever. The performance of the stacked models (which\ndepend on domain-specific pretrained models and text-text\nsimilarities) and text-only models are surprisingly strong across a\nvariety of domains --- this finding has implications for the relative\nsurprising representational strength of text. The annotation process\nthey describe for their tag corpus is interesting --- multiple rounds\nof annotations are undertaken with crowdworkers fixing each-other's\nerrors. The authors consider a large number of models across many\ndomains --- the benchmark experiments are extensive, and the \"size\nvs. accuracy\" plots in the end are cool. Figure 5 is amazing --- few\nworks offer such clear practical guidance.  Finally, I liked the\nhopeful message in the end --- that a combined multimodal approach\nseems to work best, so don't completely discard the domain-specific\nrepresentations.\n\nIt should be noted that the cited work where some of the data is from,\nMarjieh et al. 2022, is quite similar in the sense that it also\nproposes to estimate human similarity judgments. This work appears to\nextend that work by exploring more types of models and their\ncombinations.\n\nMy main concerns are that this set of experiments suggests some clear\nnext setups that I don't nesc. think are out of scope for this work.\nSpecifically:\n\n- If the goal in practice is to reconstruct human judgments --- I\n  would have liked the authors to compare against a supervised setup.\n  How much of the remaining misalignments can be fixed by gathering n\n  << N^2 additional pairwise judgments and then training a supervised\n  model ontop of the models?\n\n- I would have liked to have seen more experiments with even less\n  supervision --- is it possible to gather n < N captions, and train a\n  model on those very few pairs to map to text, and then use the\n  resulting modality --> text model as input to the models shown here?\n\n- I would have appreciated more discussion of tags vs. captions for\n  the images. In figure 3A, I assume that the text only methods use\n  captions unless specified otherwise. But --- because the tags were\n  not fed to, e.g., BERT/RoBERTa, I can't tell if the performance\n  gains are due to the full captions being used, or if the gains are\n  due to the more expressive models. A trivial linearization of, e.g.,\n  \"A photo of a tag1, tag2, and tag3\" handed to a LLM would have been\n  a nice comparison to see.\n\n- Finally, I would have been interested in a fully LLM setup where\n  instead of pairwise cosine similarities, both text pairs are fed to\n  a very large LLM (e.g., OPT or GPT3) and the model outputs a likert\n  rating as text.",
            "clarity,_quality,_novelty_and_reproducibility": "As I mentioned, the novelty is somewhat limited because of the prior human\njudgements work the authors get their data from. But, the extensive set of\nexperiments is interesting/new.",
            "summary_of_the_review": "Overall, this work address an interesting task/approach ---\napproximating pairwise similarity judgments over N objects with only\nO(N) annotation via language models. While this setup has been\nconsidered by Marjieh, this work presents more experiments that show\nthe text representations are surprisingly strong. They consider a very\nlarge number of models, modalities, etc. The released tag sets would\nlikely be useful to someone. But, a handful of missing additional\nexperiments, which are arguably in-scope for the present work, are not\npresented. And, prior work has already made the observation that LLMs\ncan approximate similarity judgments across non-textual modalities,\nwhich appears to somewhat limit the novelty of this work.\n\n\nAfter response:\n\nThe authors seemingly ran all of the experiments I suggested and have incorporated them into the paper. Amazing :-)\nFurthermore, the authors clarified the difference with prior work.\nAll my concerns are addressed, so I raised my score from 5-->8",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3436/Reviewer_AJDz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3436/Reviewer_AJDz"
        ]
    },
    {
        "id": "BnIPXCoBp_s",
        "original": null,
        "number": 2,
        "cdate": 1666597771720,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597771720,
        "tmdate": 1666597771720,
        "tddate": null,
        "forum": "O-G91-4cMdv",
        "replyto": "O-G91-4cMdv",
        "invitation": "ICLR.cc/2023/Conference/Paper3436/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors make the observation that representations learned by DL models produce proximity scores very different from human evaluations. They introduce a simple and scalable technique to make the human and model produced similarity scores closer. Essentially, text descriptions or tags for various input data points (across modalities) are passed through LLMs (or word-frequency methods) and the resulting representations used for proximity scores. Stacked representations combining the existing model and the text description representation is shown to consistently match human similarity metrics better.",
            "strength_and_weaknesses": "Strengths: I think this is a well-motivated problem. Learned representations are often used as features in the small-data regimen or sometimes directly for getting proximity scores in an AI setting. This paper address the human interpretability of these representations by (i) confirming that human similarities and proximity scores from models can vary a lot, (ii) text-descriptions or tags can be leveraged and stacking these representations with the model-learned representations can help. I also appreciate that the technique is scalable and in many cases not that much of an overhead to implement. I appreciate the arguments in the related text that leverage cognitive science literature. In addition, the paper is easy to follow.\n\nWeaknesses: The paper doesn't have too many weaknesses. I was wondering if we could get some numbers on if the stacked representations help in additional downstream tasks like say classification (i.e. does the performance on imagenet improve if you use imagenet + text). However, I understand that this can be significant undertaking and do not want to base my review on this experiment but it is a potential future direction.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper presents a straightforward but well-motivated idea. Having representations match human similarity judgement is indeed useful.\n\nImplementing this technique would require (i) ability to acquire free-text or tag values which are domain-dependent but not prohibitively expensive, (ii) ability to obtain LLM representations which are straightforward since high quality implementations and libraries exist for these now. Reproducibility is not an issue.\n\n",
            "summary_of_the_review": "- Well motivated problem \n- Clearly described technique that is scalable, easy to implement\n- Techniques like these that are easy to implement and help with interpretability are of great use in the small-data regimen (where the bulk of us are). I would like to see this paper at ICLR.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3436/Reviewer_kd7y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3436/Reviewer_kd7y"
        ]
    },
    {
        "id": "YNKRxEZYKke",
        "original": null,
        "number": 3,
        "cdate": 1666621991379,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666621991379,
        "tmdate": 1666621991379,
        "tddate": null,
        "forum": "O-G91-4cMdv",
        "replyto": "O-G91-4cMdv",
        "invitation": "ICLR.cc/2023/Conference/Paper3436/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new class of similarity approximation methods based on language. To collect the language data required by these new methods, the authors also developed and validated a new adaptive tag collection pipeline, which is significantly cheaper than the classical methods. Finally, the authors also develop \u2018stacked\u2019 methods that combine language embeddings with DNN embeddings, and find that these consistently provide the best approximations for human similarity across all the three modalities. ",
            "strength_and_weaknesses": "Strength:\n1. The paper is overall well-written and easy to follow.\n2. Multiple datasets are evaluated for the proposed method.\n3. Multiple modalities are explored and evaluated on the proposed method.\n\nWeakness:\n1. The technical contribution of the proposed method is weak.\n2. No baselines are compared to the proposed method in the experiment section.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well-written. The authors consider multiple modalities and datasets to evaluate their proposed method. But the novelty of the proposed method is weak. ",
            "summary_of_the_review": "The main issue of the paper is the novelty of the proposed method. Based on technical novelty and insufficient experiments, I don\u2019t think the current version meets the standard of the ICLR. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3436/Reviewer_hUBS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3436/Reviewer_hUBS"
        ]
    },
    {
        "id": "yUP72x_OyqI",
        "original": null,
        "number": 4,
        "cdate": 1666761420198,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666761420198,
        "tmdate": 1666761451753,
        "tddate": null,
        "forum": "O-G91-4cMdv",
        "replyto": "O-G91-4cMdv",
        "invitation": "ICLR.cc/2023/Conference/Paper3436/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "To approximate human judgements without increasing cost and losing reliability, the authors proposed propose a new class of similarity approximation method based on language, namely captions and stacked words. Their method achieved good approximation with only O(N) computation cost. They also developed the novel overall flow to compute representational similarity. They also claim they are going to make the human judgement dataset publicly available.",
            "strength_and_weaknesses": "[Strength]\nThe concept of the paper is nicely presented for the next AI abundant society where demand for human judgments is more increased. They proposed the novel similarity approximation method with a smaller number of judgements.  \nThey provide the evaluation data in the supplemental material, which is beneficial.\nOverall, I believe what the authors done is quite systematic and very beneficial for the future refinement of the DNN model.\n\n[Weakness]\n\n-\tRepresentational similarity\nFor beginners, the word of representation similarity is not easy to follow. At least, authors should define the terminology in the introduction.\n\n\n-\tThe title\nIn particular, the linkage between title and abstract seems not clear at the first glance.\nPlease consider to update the title and abstract so that readers can easily understand the topic theme.\n\n\n-\tIs English best?\nThere are many languages. In my opinion, I view that authors selected English language to represent the basis of all things in nature. Why English(e.g., language system such as polysemous, grammar, dataset availability\u2026)? Justification of English employment would make the paper better. \n\nRelated to above topic, the paper report \u201cN = 1,492 US participants for the new behavioral experiments reported in this paper.\u201d Are they all Americans and share the same contexts? (e.g., cultures\u2026) \n\n-\tCrowd scouring\nHow long does it take to STEP-Tag? It is good that measure actual time that participants consumed. Comparing with that with the time used for writing captions may strengthen your claim. \n\n-\tDiscussion\nI feel strange because the paper end with \u201cDiscussion\u201d. Please consider adding conclusion section or rename the last section as \u201cDiscussion and Conclusion\u201d.\n",
            "clarity,_quality,_novelty_and_reproducibility": "-\tQuality\nGood. Issues for human judgment cost have been solved with the proposed idea. \n\n\n-\tClarity\nGood. In the experiment, model they used is sufficient. \n\n\n-\tOriginality\nGood. The idea of methods themselves are not very novel. However, I believe the exhaustive amount of examination is valuable, which increase the originality, thus it should be highly evaluated.\n",
            "summary_of_the_review": "I think this paper should be accepted for the \u201cHuman like\u201d evaluation point of view as DNN becomes more sophisticated. I believe their finding is also beneficial for the community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3436/Reviewer_ZARH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3436/Reviewer_ZARH"
        ]
    }
]