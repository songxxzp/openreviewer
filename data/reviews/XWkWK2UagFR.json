[
    {
        "id": "nlJtUkR4ir",
        "original": null,
        "number": 1,
        "cdate": 1666681843827,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681843827,
        "tmdate": 1666681843827,
        "tddate": null,
        "forum": "XWkWK2UagFR",
        "replyto": "XWkWK2UagFR",
        "invitation": "ICLR.cc/2023/Conference/Paper971/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a framework for neural gauge fields in the context of radiance fields and experimentally shows that the proposed regularisation is superior to alternative ones. This work also introduced the idea of a top-k gauge that gradually alleviates the need for regularisation.",
            "strength_and_weaknesses": "Strengths\n-----------\n\nOverall I believe the idea of exploring to learn a gauge transformation in an end-to-end manner is an important research direction, and this work makes contributions in this regard.\n\nResults show better performance than state-of-the-art methods.\n\n\nWeaknesses\n--------------\nOverall, this paper was hard to evaluate because I am not an expert in the neural radiance field and because I believe the presentation requires improvement. The elaboration on existing methodologies and new ideas is not in a way that presents the reader with an abstraction of the theory behind recent advances to then clearly and concisely show their contributions.\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Although I believe this work could present new ideas, I feel the presentation requires significant improvement to evaluate novelty as not an expert in the topic.",
            "summary_of_the_review": "Despite they could be a significant theoretical contribution in trying to present a new framework for gauge fields, I could not really judge the impact and novelty in the way this paper is written. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper971/Reviewer_mQvE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper971/Reviewer_mQvE"
        ]
    },
    {
        "id": "G8iKGkbf2r",
        "original": null,
        "number": 2,
        "cdate": 1667463583947,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667463583947,
        "tmdate": 1667520227860,
        "tddate": null,
        "forum": "XWkWK2UagFR",
        "replyto": "XWkWK2UagFR",
        "invitation": "ICLR.cc/2023/Conference/Paper971/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses the feature index or coordinate mapping function in radiance field reconstruction as a gauge transformation problem. The core idea of the manuscript is to optimize the transformation by maximizing the mutual information and encouraging uniform coordinate distribution with an EM distance regulation term. Extensive experiments on syntheses and real object datasets demonstrate that the proposed method achieves better performance in terms of rendering quality and compactness compared to the recent SOTA fast radiance field reconstruction approaches (instant-NGP and TensoRF ) and space wrapping baed radiance field reconstruction methods (Non-Rigid Neural Radiance Fields and NeuTex).",
            "strength_and_weaknesses": "What's good:\n1) The idea of transforming coordinate learning into a distribution regulation problem is interesting and elegant.\n2) The usage of KL divergence and EM distance loss for coordinate space mapping seems novel and interesting.\n3) The paper provides a detailed mathematical derivation of the relationship between mutual information maximization and the proposed KL divergence loss, but I could not read over the derivation due to this emergency review.\n4) The proposed method shows better quantitative rendering scores than the previous approach.\n\nTo be improved:\n1) The overall pipeline is unclear to me, a pipeline figure could really help to better understand the proposed method.\n2) The paper lacks a key highlight that motivates me to use it in practice, it's good to perform better than the other two space-wrapping-based approaches, but the scores still have a big gap with SOTA and suffer from high computation cost. Would be better if the authors can figure out some unique properties in the proposed method. How does it perform for radiance field editing and rendering speed? Can it combine with the traditional rasterization rendering framework for fast rendering since the features are distributed in 2D space?\n3) Table 3 is tricky to me, why not use the default configuration in the TensoRF and Instant-NGP? Seems the total parameters in Instant-NGP are significantly smaller than TensoRF under your configuration but the model size in instant-NGP is larger than TensoRF: 2^14 * 8 * 2 (even assuming 2^14 entries for each level) vs. 64 * 64 * 27 * 3, please correct me if any misunderstanding to the counting. And could be better to list the training time to have a better side-by-side comparison.\n4) The reference to the convolutional occupancy field is missing, which is early work that also uses space wrapping.\n5) Lacking the reference, discussion and comparison with \"Variable Bitrate Neural Fields\", which also learn a coordinate wrapping function.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear mostly.\n\nNovelty: Yes as commented above\n\nReproducibility: not sure since the implementation section is not clear to me.",
            "summary_of_the_review": "The proposed regulation terms are novel and interesting to me, and happy to see the authors can clarify the questions listed in the \"to be improved\" section in the rebuttal period.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper971/Reviewer_Bfkm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper971/Reviewer_Bfkm"
        ]
    },
    {
        "id": "G-ybuQXDz0j",
        "original": null,
        "number": 3,
        "cdate": 1667514774686,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667514774686,
        "tmdate": 1670648349375,
        "tddate": null,
        "forum": "XWkWK2UagFR",
        "replyto": "XWkWK2UagFR",
        "invitation": "ICLR.cc/2023/Conference/Paper971/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a way to learn gauge transformer for neural fields applications (especially modeling neural radiance field for novel view synthesis). The main contribution of the work is to propose a unified framework to reason about different NeRF paradigms in the language of gauge transformation and proposes a regularization to enable learning such transformation without suffering from collapsing in local minimum. Empirical results suggests that the proposed regularization indeed improve performance.",
            "strength_and_weaknesses": "Strength:\n1. The paper provides a novel perspective that unifies the coordinate transform in instant-NGP, tensorRF, and triplanes under the framework of gauge transform. \n2. The proposed regularization shows improvement over existing regularization methods provided by prior works.\n3. The paper shows that with the novel Top-4 Gauge, the method is able to perform comparably with SOTA.\n\nWeakness:\n1. Evaluation. It's not very clear to me that the using downstream novel view synthesis is the best way to evaluate Neural Gauge Fields. I believed that it's essential to show that the proposed method doesn't lag behind SOTA performance, but it's also important to show case what this new formulation should allow. I will encourage the authors to include other applications, potentially in the field of learning texture, materials, BRDF etc.\n2. Writing. In the introduction and method section, it's hard for me to follow the motivation why is it beneficial to think of instant-NGP, tensorRT in the framework of gauge transform. This is not clear to me what's the theoretical benefit (cleaner? easier to reason about different mappings?) or the empricial advantages. I would also encourage the authors to include a background section about gauge transform as it can benefit the readers without enough backrgound.\n3. The authors compared to instant-NGP / tensorRT, both of these two methods are very memory efficient and fast to optimize. I wonder if Neural Gauge Field subsumes these formulation, does the same benefit carries through? It would be nice to see time profiling and memory profiling if possible.",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the strength & weakness section.",
            "summary_of_the_review": "My main concern about the paper is about the evaluation. The paper mainly evaluate the performance of Neural Gauge Field using novel view synthesis, without reporting optimization time, memory, or other applications (which I believed the true use of Neural Gauge Field can lay). The secondary concern is regarding it's not clear why we need the unified language in terms of gauge transform to describe different formulation such as instang-ngp, triplanes, and/or tensorRF. \n\n\n-------\nUpdate: I decided to raise my score since the authors address a number of concerns I raised.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper971/Reviewer_hK9t"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper971/Reviewer_hK9t"
        ]
    },
    {
        "id": "Sz1IapXgs8",
        "original": null,
        "number": 4,
        "cdate": 1667526357308,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667526357308,
        "tmdate": 1667526357308,
        "tddate": null,
        "forum": "XWkWK2UagFR",
        "replyto": "XWkWK2UagFR",
        "invitation": "ICLR.cc/2023/Conference/Paper971/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the author proposes a general framework for the neural gauge field, including continuous and discrete cases. To solve the collapse problem, the author proposes a regularization term - InfoReg. In practice, the regularization term regulates the target distribution to a uniform distribution. The author also proposes a new top-k gauge mechanism that achieves a trade-off between model collapse and computational cost. Experiments show the method outperforms other regularization methods and gauge transformation methods. The method also outperforms SOTA in rendering tasks, but with higher computation costs. ",
            "strength_and_weaknesses": "Strength:\n+ The proposed method proves to be effective and has the potential to be adopted by future \"gauge transformation\" frameworks. \n\n+ The paper is well-written and easy to follow for me. \n\nWeakness:\n+ The performance of the rendering tasks at inference time sacrifices computational cost. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n+ Top-K strategy helps with distribution collapse. The paper claims the Top-K strategy helps with distribution collapse. The experiment shows the strategy improves rendering quality but doesn't provide evidence that it helps with distribution collapse. Also, it will be better if the paper can use a quantitative metric to evaluate the degree of \"collapse\". \n\n+ Training Time. I'm wondering what the training time of this method compares with other methods. As the EMD calculation takes a long time, will the regularization terms make the training time longer than other methods? \n\n+ Equation 1: i_x \u2208 R^1 => i_x \u2208 N^1\n\n+ Uniform Distribution Range: For the continuous case, the range of q(y) is R^2 or [0, 1]^2?\n\n+ Other manifolds: Tha paper assumes the target distribution to be a uniform distribution. Does the method also apply to other distributions? It will be interesting if these results can be provided, to show the generalizability of the method. \n\nQuality\n+ Fair.\n\nNovelty \n+ Fair.\n\nReproducibility\n+ Fair\n\n",
            "summary_of_the_review": "This paper proposes a general method for gauging transformation, which is intellectually interesting and could be potentially applied to other frameworks and applications. My major concern is the higher training and inference time for high-quality results. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper971/Reviewer_2BXg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper971/Reviewer_2BXg"
        ]
    },
    {
        "id": "tQA50qIxpr",
        "original": null,
        "number": 5,
        "cdate": 1667575251221,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667575251221,
        "tmdate": 1669829171917,
        "tddate": null,
        "forum": "XWkWK2UagFR",
        "replyto": "XWkWK2UagFR",
        "invitation": "ICLR.cc/2023/Conference/Paper971/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": " This paper studies the problem of learning the gauge transformation in an end-to-end manner within learning the neural scene representation setting.  In this context, gauge transformations in  continuous and discrete cases are learned. The developed learning paradigm, which is generic,  maps a 3D point to a continuous coordinate or a discrete index in the target gauge, respectively.\nThis work also introduces  the concept of Information Regularization (InfoReg) from the principle of information conservation during gauge transformation. This regularization is performed by maximizing the mutual information between gauge parameters.  The paper also claims that the proposed top-k gauge is useful in achieving a tradeoff between learning collapse and computation cost.\n",
            "strength_and_weaknesses": "++ The paper is generally well written and easy to follow. The presented method is intuitive and meaningful. Graphical illustrations of Fig. 1, 2 , and 4 are particularly helpful.\n\n++ Sections 3.2 and 3.3 are well motivated and offer new theoretical insights.\n\nThe empirical discovery and analysis on top-k gauge, reported in Section 3.3 and Fig. 7 are particularly interesting.  \n\n++ The supplementary material with additional pseudo code and discussion regarding the method\u2019s limitations is helpful.\n\n\n-- Comparison with SOTA method on Table 3, although highlights the benefit in terms of model size, is limited to only two scenes. It is difficult to conclude the benefit of the proposed method over SOTA without more exhaustive experiments.\n\n--  The claim of  \u201ctrade-off between collapse and cost\u201d on the basis of Figure 7 (in Section 4.2.2) is somewhat misleading.  The reported PSNR for the \u201coriginal\u201d method may be poor but cannot be said to collapse. As such, the reported gap in PSNR (which is not very significant) may not generalize well across scenes.\n\n--  Qualitative results reported in Fig 8 and 9 (in the supplementary materials) are difficult to understand.\n\n--  The limitation on \u201coutput layer dimension ranging from 2^14 to 2^24\u201d is indeed of a serious concern.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please refer the strengths above.",
            "summary_of_the_review": "\nI generally like the addressed problem and the approach made in the paper. However, it is currently difficult to know the benefit of the proposed method. This is mainly due to the limited experimental results. I suggest authors report more exhaustive experiments highlighting the contribution empirically. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerning ethical issues as far as can be seen.\n",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper971/Reviewer_HA8S"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper971/Reviewer_HA8S"
        ]
    }
]