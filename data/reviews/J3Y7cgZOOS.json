[
    {
        "id": "XfB8wdoIDk-",
        "original": null,
        "number": 1,
        "cdate": 1666434560792,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666434560792,
        "tmdate": 1666434560792,
        "tddate": null,
        "forum": "J3Y7cgZOOS",
        "replyto": "J3Y7cgZOOS",
        "invitation": "ICLR.cc/2023/Conference/Paper4096/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Deep graph networks (DGNs), especially those following the message passing paradigm, cannot effectively capture long-range dependencies in graph datasets and usually suffer from exploding and vanishing gradients.\n\nThis paper takes an ordinary differential equations (ODE)-perspective and proves that using anti-symmetric parameter matrices can be effective in mitigating the aformentioned issues.\n\nThe effectiveness of the resulting model, termed Anti-symmetric-DGN (A-DGN), is demonstrated on benchmark graph property prediction and node classification datasets.\n___",
            "strength_and_weaknesses": "\\\n**Strengths**\n\n\\+ The paper is well-organised and Figure 1 clearly explains the key ideas of the proposed method.\n\n\\+ The use of anti-symmetric weights for node's own features (Equations 4 and 5) to effectively deepen DGNs is theoretically grounded and interesting.\n\n\n\\\n**Weaknesses**  \n\n\\- There are no experiments to support the claim that A-DGN can specifically alleviate/mitigate oversquashing.\n\n\\- There are no experiments to support the claim that A-DGN can effectively handle long-range dependencies specifically on graph data requiring long-range reasoning.\n\n\\- The poor long-range modelling ability of DGNs is attributed to oversquashing and vanishing/exploding gradients but the poor performance could also be due to oversmoothing, another phenomenon observed in the context of very deep graph networks [Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning, In AAAI'18].\n___",
            "clarity,_quality,_novelty_and_reproducibility": "\\\n**Clarity**\n\nThe paper is generally well-organised.\n\nThere are a few typos some of which are listed below.\n\n* Abstract: yields to -> leads to\n* First line in Section 2: networks architectures -> network architectures \n* Inconsistency in the limits of the index i (d in the discussion leading upto Proposition 1 and n in Proposition 1)\n* Last line in Section 2: despite we designed -> despite designing or although we designed\n\n\\\n**Quality**\n\nExperiments on oversquashing are needed to support the claim that A-DGN can alleviate/mitigate oversquashing, e.g., TreeNeighboursMatch problem in prior work [2] (Section 4.1 in the paper).\n\n[2] On the Bottleneck of Graph Neural Networks and its Practical Implications, In ICLR'21.\n\nFor real-world datasets, the authors could consider datasets from relevant graph rewiring based methods to mitigate oversquashing [3] which are relevant baselines for A-DGN.\n\n[3] Understanding over-squashing and bottlenecks on graphs via curvature, In ICLR'22.\n\nMore experiments are needed to confirm that A-DGN can capture long-range dependencies, e.g., the authors could consider Code2 dataset used in prior work [4] (Section 5.4 in the paper).\n\n[4] Representing Long-Range Context for Graph Neural Networks with Global Attention, In NeurIPS'21.\n\n\\\n**Novelty**\n\nThe novelty can be strengthened by positioning the contributions with prior work on oversmoothing since oversquashing, vanishing/exploding gradients and oversmoothing are associated with DGNs.\n\nSpecifically, adding/subtracting identity matrices (as in Equation 5) has been previously employed [5] for DGNs with many hidden layers in the context of oversmoothing.\n\n[5] Simple and Deep Graph Convolutional Networks, In ICML'20.\n\nSome of the datasets used in GRAND [6] (which has been cited) and in this paper are the same (e.g., PubMed, Coauthor CS, Photo) but GRAND has explored oversmoothing as well.\n\n[6] GRAND: Graph Neural Diffusion, In ICML'21.\n\n\\\n**Reproducibility**\n\nThe code is submitted; additionally the main part and the supplementary part include enough material, e.g., dataset details, hyperparameters, for an expert to replicate the results of the paper.\n___",
            "summary_of_the_review": "While the proposed method is interesting, more empirical evaluation in support of the claims and positioning with relevant prior work are needed to strengthen the contributions.\n___",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4096/Reviewer_Cm1J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4096/Reviewer_Cm1J"
        ]
    },
    {
        "id": "2MRZS7MWd88",
        "original": null,
        "number": 2,
        "cdate": 1666535595989,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666535595989,
        "tmdate": 1669203320258,
        "tddate": null,
        "forum": "J3Y7cgZOOS",
        "replyto": "J3Y7cgZOOS",
        "invitation": "ICLR.cc/2023/Conference/Paper4096/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an anti-symmetric GNN to tackle the oversmoothing phenomenon in GNNs. \nThe authors suggest an ODE based approach and analyze their model A-DGN and show that it is non-dissipative, i.e., feature/energy preserving and therefore suggest that it can alleviate the oversmoothing phenomenon. Several experiments with a variable number of layers is offered showing slight improvement over the considered methods.",
            "strength_and_weaknesses": "Positive points:\n- The work continues the research in the field of neural networks as ODEs that enables to interpret neural networks as dynamical systems.\n- The authors provide nice theoretical understanding of their method.\n- The paper is well organized.\n\nNegative points:\n\n- The concept of non-dissipative systems in GNNs is not novel and was previously proposed in the following papers:\n\n\"PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations\"\n\n\"Graph-Coupled Oscillator Networks\"\n\n- It is not clear to me why the authors chose to use a layer shared W matrix and not to learn it per layer, given their claim that the theoretical analysis is also correct with such a formulation. Does it yield worse results? In any case it needs to be examined and reported.\n\n-What is the difference between the proposed aggregation in equation (6) and the one in GraphConv (Morris et al, 2019). It seems to be the same to me.\n\n-It is not explained in section 4.1 why did the authors choose to change the experimental settings from the original ones (number of nodes) in Corso et al.\n\n- In the context of alleviating oversmoothing , a discussion of existing method is lacking. For example, a comparison with GCNII (Chen et al), EGNN (Zhou et al), pathGCN (Eliasof et al).\n\n-The experimental evaluation is lacking proper comparison with recent GNN methods, for example with GraphCON (Rusch et al), GCNII (Chen et al), GRAND (Chamberlain et al).\n\n- The authors run their experiments for up to10,000 epochs, which is a large number of epochs compared to typical methods in GNNs. Can the authors explain why? What is the effective number of epochs required to get the best results? What is the accuracy when using a smaller number of maximal epochs, like 500 or 1000 epochs?\n\n- The scope of experiments is quite narrow. Ideally the authors should report their results on more benchmarks such as Cora/Citeseer/Pubmed and heterophilic datasets as proposed in Geom-GCN (Pei et al).\n\n- In the context of oversmoothing evaluation the authors report the accuracy with only up to 30 layers. I think that showing the accuracy (and also possibly the energy of the node features) with more layers (e.g. 64 layers) will be more convincing. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow and very clear.\n\nThe evaluation of the method is lacking both with comparison to existing methods and with respect to itself (e.g., an ablation study, more datasets and settings)\n\nThe work is an extension of existing method and not completely novel. \n\nThe method is sufficiently described to be reproduced.",
            "summary_of_the_review": "The paper proposed an interesting extension of existing ODE based GNNs but lacks a discussion of them, and the experimental evaluation is lacking. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4096/Reviewer_Hf3x"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4096/Reviewer_Hf3x"
        ]
    },
    {
        "id": "dyfQhsb4l_O",
        "original": null,
        "number": 3,
        "cdate": 1666584025125,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584025125,
        "tmdate": 1666584025125,
        "tddate": null,
        "forum": "J3Y7cgZOOS",
        "replyto": "J3Y7cgZOOS",
        "invitation": "ICLR.cc/2023/Conference/Paper4096/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed an anti-symmetric deep graph neural network (A-DGN) to solve the over-squashing problem of GNNs. First, this paper gave a sufficient condition that an ODE corresponding to a given GNN is stable and non-dissipative (Proposition 1). Then, to satisfy the sufficient condition, they proposed to make the GNN weights anti-symmetric (Proposition 2), deriving A-DGN. Finally, numerical experiments about the graph property prediction problems and the graph classification problems were conducted to verify the usefulness of the proposed method.",
            "strength_and_weaknesses": "Strengths\n- The proposed method provides a new approach to the over-squashing problem by associating a GNN with the properties (stability, non-dissipative) of corresponding ODEs.\n- The proposed method is easy to implement and can be applied to many GNNs.\n- The proposed method can effectively mitigate the accuracy degradation caused by deepening GNNs (Figure 2).\n\nWeaknesses\n- The mathematical statement is somewhat informal.\n- Comparison with baseline methods that addressed the over-squash problem or performance degradation of deep is preferable.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nI would say that the statement of Proposition 1 is not rigorous. For example, $\\approx 0$ is an informal statement. Also, non-dissipative is not mathematically defined. Therefore, I would suggest writing the formal statement of Proposition 1. However, if there is a risk that it would be too complicated to include in the main text, it would be acceptable to include only informal results in the main text and a precise statement in the appendices.\nNevertheless, this paper is clear overall, and I can understand the main point of the paper.\n\n\nQuality\n\nOne of the objectives of this paper is to solve the over-squashing problem (and the performance degradation of GNN caused by the problem). However, the numerical experiments only considered the GNNs proposed prior to [Alon & Yahav, 2021], which pointed out the over-squashing problem. Therefore, I would suggest comparing the proposed method with existing methods that addressed these problems (e.g., [Topping et al., 2022]). Also, the performance degradation of deep GNNs is the subject of interest for the over-smoothing problem. Therefore, I suggest comparing the proposed method with methods that mitigate the over-smoothing problem (e.g., GCNII [Chen et al., 2020]).\n\n[Topping et al., 2022]: https://openreview.net/forum?id=7UmjRGzp-A\n[Chen et al., 2020] https://proceedings.mlr.press/v119/chen20v.html\n\n\nNovelty\n\nSeveral existing studies have analyzed GNNs by considering an ODE obtained as its continuum limit. However, to the best of my knowledge, few studies have effectively utilized the benefit of converting GNNs to ODEs. This study related the problem of over-squashing to the properties of the corresponding ODEs and derived a desirable property of GNNs, clarifying the connection between GNNs and ODEs.\n\nReproducibility\n\nOK. This paper provides the code for the numerical experiments as supplement material. Although I have not run the code, I expect that it is possible to reproduce the same experiments as the authors.",
            "summary_of_the_review": "This paper provides theoretically grounded solutions to the problem of accuracy degradation due to the deepening of GNNs from the standpoint of ODE stability. The method is simple and applicable to a wide range of GNNs. On the other hand, the numerical experiments have not been compared with recent GNN models, and comparing the proposed method with them is desirable.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4096/Reviewer_fEsH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4096/Reviewer_fEsH"
        ]
    },
    {
        "id": "3DkAlt9py1q",
        "original": null,
        "number": 4,
        "cdate": 1666692773968,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692773968,
        "tmdate": 1666692773968,
        "tddate": null,
        "forum": "J3Y7cgZOOS",
        "replyto": "J3Y7cgZOOS",
        "invitation": "ICLR.cc/2023/Conference/Paper4096/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a new formulation of GNNs based on ODEs. The main goal of the method is to ensure stability and non-dissipation properties, thus allowing the model to preserve long-range dependencies with deep architectures that also avoid vanishing gradient problems. The paper provides theoretical arguments about the properties of the proposed A-DGN model. Moreover, experiments on several tasks support the benefit of the A-DGN over several baseline models.",
            "strength_and_weaknesses": "Strengths:\n* The paper provides an elegant formulation of GNNs based on ODES. More importantly, several message aggregation schemes can be incorporated into the model.\n\n*  Providing theoretical arguments about the properties of A-DGN is a key part of the paper. Even if some properties can be derived from previous studies (including works in control theory), they still contribute to the understanding of the model.\n\n* Good experimental pipeline, considering different important tasks.  \n\nWeaknesses:\n* The main limitation of the paper is the choice of baseline models. While the paper focuses on addressing the over-squashing effect, none of the baselines has specifically been designed to do so. Therefore, to this extend, the part of the evaluation does not favor traditional GNNs. \n\n  Moreover, while training deep traditional models (e.g., GCN, GAT), the problem of over-smoothing occurs. From the discussion, this has not been taken into account in the evaluation. For instance, methodologies like PairNorm could be used to prevent over-smoothing.\n\n  At the same time, no prior work on neural ODEs is considered. I would expect that some of these models (e.g., DGC, GRAND) should have been part of the evaluation\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I enjoyed reading the paper. The different concepts are clearly presented. At the same time, the paper keeps a good balance between theoretical contributions and empirical evaluation.  Despite building on existing models that first studied the relationship between GNNs and ODEs, the paper still presents several novel ideas. \n\nNext, I highlight some of my concerns, in addition to the main limitation above, that need further clarification.\n\n* To what extend do the datasets used contain long-range dependencies? I understand that this is the case while predicting the diameter of the graph, but this is not obvious in the real-world datasets used in the paper.\n\n* The paper does not discuss the running time of the proposed methodology. I understand that the focus is on capturing long-range dependencies, but having an image of the time complexity would be helpful for the reader.\n",
            "summary_of_the_review": "Overall, I believe it\u2019s an interesting approach that further increases our understanding of the relationship between GNN models and ODEs. Some aspects related to the experimental evaluation and the choice of baseline models should be addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4096/Reviewer_29BN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4096/Reviewer_29BN"
        ]
    }
]