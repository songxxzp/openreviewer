[
    {
        "id": "u2COOjvug1",
        "original": null,
        "number": 1,
        "cdate": 1666340586294,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666340586294,
        "tmdate": 1666574983569,
        "tddate": null,
        "forum": "1tfGKiwnJRJ",
        "replyto": "1tfGKiwnJRJ",
        "invitation": "ICLR.cc/2023/Conference/Paper2952/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies safe reinforcement learning (RL) where safety violation must be bounded during training. They propose a new approach that balances the trade-off between efficient progress in exploration and safety guarantee. Specifically, the proposed approach updates Dirichlet-Categorical models of the state transition probabilities that describes the agent's behavior within the environment via Bayesian inference. They then approximate the agent's belief in terms of risks. They provide theoretical guarantees on the convergence on as well as empirically showing the performance of the proposed approach.",
            "strength_and_weaknesses": "### Strength\n**Problem Settings.** The problem settings are important and interesting.\n\n**Theoretical Analysis** I think Theorem 3.1 is indeed a nice theorem though it is not clear to me how novel this theorem is.\n\n### Weakness\n**Paper writing.** First of all, the presentation of this paper should be improved in terms of notations. For example, $P$ or $Q$ are used in two meanings\n+ P: Transition probability (in Section 2) and lowest risk level (in Section 3.3)\n+ Q: finite set of states (in Section 2) and Q-function (in Section 3.4).\n\nSuch inconsistency makes this paper quite hard to follow. Also, the notations are far from standard as an RL paper, which gives the reader unnecessary and unessential burden. As an instance, I personally want the authors to avoid using $Q$ in other meanings than Q-function. State space is typically denoted as $S$ or $X$ in RL papers.\n\n**Related work.** It would be better to write related work section in more organized manner. The current related work section is just a list of relevant papers, and it is hard to follow the story of the related work.\n\n**Missing citations.** As a particularly relevant papers, I would like to list the following paper. I think it may be better to compare the authors' method with this existing work (the source code is released).\n- As, Yarden, et al. \"Constrained Policy Optimization via Bayesian World Models.\" International Conference on Learning Representations. 2021.\n\n**Experiments.**  I don't think the experiment has been fully conducted. The benchmark task is very simple (i.e., grid world and PacMan), and the baseline methods are vanilla Q-learning. There are a lot of existing safe RL algorithms or benchmark tasks, so I think the authors should have compared with recent ones. For example, representatives of the benchmark tasks for safe RL is Safety-Gym or MuJoCo, and the notable safe RL baselines are PPO-Lagrangian (Here are just examples). At least, I do not think that the authors' claims are supported by the current experiments.\n\n### Minor comments\n- Since Moldovan and Abbeel (2012) is on safe exploration without ergodicity assumption, position of its citation seems weird to me. It would be more reasonable to cite it in the end of the next sentence.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity.** This paper is hard to follow due to inconsistency of mathematical notations. Also, related work section should be rewritten in an organized manner.\n\n**Quality.** The quality of the theoretical analysis seems ok to me (though I am not fully confident), but that of experiments should be improved in terms of the selection of both benchmark problems and baselines.\n\n**Novelty.** As far as I know, the proposed method is new. However, compared to recent work, I am not convinced whether the proposed method is better than existing ones. I don't think this paper has not supported the advantages of the proposed method neither empirically nor theoretically\n\n**Reproducibility.** The source code is not attached and experimental conditions are not fully written in the paper (including appendix), I need to say that the reproducibility is low for now.",
            "summary_of_the_review": "Though the problem setting and proposed method is interesting, I have several concerns as I listed in the Weakness. Hence, I recommend rejection for now.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2952/Reviewer_BCMN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2952/Reviewer_BCMN"
        ]
    },
    {
        "id": "SERPdk8qQz",
        "original": null,
        "number": 2,
        "cdate": 1666827789878,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666827789878,
        "tmdate": 1666827789878,
        "tddate": null,
        "forum": "1tfGKiwnJRJ",
        "replyto": "1tfGKiwnJRJ",
        "invitation": "ICLR.cc/2023/Conference/Paper2952/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work introduces Risk-aware Cautious RL (RCRL) algorithm that allows agents to be trained and keep them safe. By keeping a Dirichlet-Categorical model for each environment state, PCRL approximates the risk associated with the agent\u2019s behavior originating from local action selection. To evaluate the performance of the PCRL algorithm authors have conducted experiments at a small grid-world environment ($20 \\times 20$) and a simplified version of PacMan (one ghost, 2 foods). \n",
            "strength_and_weaknesses": "**Strength**\n\n- The problem of keeping agents safe during the training is of high interest and challenge. RCRL is able to estimate the risk associated with the agent\u2019s behavior.\n- A theoretical analysis is also provided.\n\n**Weaknesses**\n- The idea of keeping a separate  Dirichlet-Categorical model for each environment state is quite limited. The scalability of the RCLR algorithm is under question.\n- Despite its merits the novelty of the RCLR algorithm is limited.\n- Some parts of this work are hard to be followed by the reader, especially due to the notation.\n- Empirical analysis is limited as experiments have been conducted in a small grid-world environment and on an over-simplified version of PacMan. ",
            "clarity,_quality,_novelty_and_reproducibility": "The first point that should be addressed by the authors is the scalability of the proposed RCLR algorithm. Is it feasible the application of the RCLR algorithm to environments with large discrete state spaces or even continuous environments? Also, the complexity of the RCLR algorithm should be presented, and its connection to the hyper-parameter $m$ (back-propagation steps). \n\nThe main novelty of the proposed work should be also highlighted. Is the main contribution the keeping of a separate  Dirichlet-Categorical model for each environment state or the estimation of the risk associated with the agent\u2019s behavior? In an abstract point, the way under which RCLR estimates the risk associated with the agent\u2019s behavior can be seen as an MCTS. \n\nAs regards the clarity of the paper, some parts should be revised. Especially, the notation makes it hard for the reader to understand easily the idea behind the RCLR algorithm.\n\nThe empirical analysis is another limitation of this work. Specifically, it would be really interesting the examination of the RCLR algorithm on more challenging high-dimensional tasks. Also, comparisons should be conducted with more baselines apart from the vanilla Q-learning algorithm. The standard deviation should be also provided in Table 1. Finally, it is not clear the number of runs at PacMan (I guess that the presented results are coming by only one run).",
            "summary_of_the_review": "As aforementioned, the paper presented some interesting ideas but it has many weaknesses in its current version.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2952/Reviewer_Fqdj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2952/Reviewer_Fqdj"
        ]
    },
    {
        "id": "SPuVk9a5hxz",
        "original": null,
        "number": 3,
        "cdate": 1666970054599,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666970054599,
        "tmdate": 1666970212680,
        "tddate": null,
        "forum": "1tfGKiwnJRJ",
        "replyto": "1tfGKiwnJRJ",
        "invitation": "ICLR.cc/2023/Conference/Paper2952/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors investigates a well founded probabilistic approach to safe RL, with primary contributions pertaining to safe training. Transition between states in the MDP is (during learning) modelled explicitly using a Dirichlet-Categorical model. Given a prior on transition probabilities (which can be fully informative) then the RL agent updates the belief of transition probabilities based on observed transition using Bayesian inference. The authors further derive suitable approximations of the risk associated with selecting action a in state q, based on the probability of transition and the cost of bad states. With the reasonable assumption that the agent can observe unsafe states m steps away (as transitions in the MDP), the approach is evaluated in slippery Gridworld and slippery PacMan showing impressive results. The paper ends with a well suited discussion on what the consequences for a learning agent when (proper) probabilistic safety guarantees are imposed, e.g. when the prior of the MDP transitions is weak. ",
            "strength_and_weaknesses": "Strength:\n* A proper probabilistic treatment on prior, safety and risk for RL given a Markov Decision Process.\n* Tractable and useful approximations of expected risk (similar to traditional expected reward). Furthermore, a clear probabilistic safety requirement is used to bound the risk (using Cantelli Inequality), with a high interpretability.\n* An excellent analysis and discussion on the results, and what safe RL entails.\n* Good exposition of related works.\n* The approach can be added on top of other RL methods (with discrete state action spaces?).\n\nWeaknesses:\n* No comparison to any other approach to safe RL. (However, given how formally well founded the presented approach is, this is more a valuable addition than a necessity.)\n* Figure 2 (which I think is great and really important) can be improved:\n  * I suggest that you draw red and yellow boundaries around where respective areas are in (b)-(d).\n  * Make sure that all four figures have the same xtick and font size.\n  * When referring to the figure, (y,x) is used which was confusing at first. E.g. the stand-still state in (b) is referred to as (13,1). I suppose that what is used is (row, column), but for figures/images (x,y) is standard.\n  * For clarity, I suggest that you invert the yticks, without changing the figure, such that the starting location is (0,0). A $(-\\hat{y},\\hat{x})$ coordinate system for figures is only standard within UX/GUI/Graphics. It is better to use a $(\\hat{x},\\hat{y})$ coordinate system.\n  * I understand that it does not fit, but I would have liked to have this figure in the main paper.\n* The slippery PacMan experiment could be extended (different $P_\\text{max}$, two ghosts, a little bit larger).",
            "clarity,_quality,_novelty_and_reproducibility": "High clarity and quality. High novelty, as far as I can tell, but I might have missed some parts of the safe RL literature.\nMost details is in the paper to reproduce, I think. But I strongly encourage the authors to release source code upon acceptance.",
            "summary_of_the_review": "The presented approach is (as far as I know) well suited in the literature, well founded, well presented, a highly suitable approach to safe RL (for training and otherwise) (in the applicable context of the approach) and sufficient empirical evaluation with interesting results which also motivate the method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2952/Reviewer_HHjA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2952/Reviewer_HHjA"
        ]
    },
    {
        "id": "2Ofyvi_Qj8b",
        "original": null,
        "number": 4,
        "cdate": 1667468574105,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667468574105,
        "tmdate": 1667468574105,
        "tddate": null,
        "forum": "1tfGKiwnJRJ",
        "replyto": "1tfGKiwnJRJ",
        "invitation": "ICLR.cc/2023/Conference/Paper2952/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a risk-aware bayesian reinforcement learning method to tackle the problem of safe exploration. Specifically, the authors assume that the agent maintains a Dirichlet-Categorical model of the MDP, and propose a method to derive an approximate bound on the confidence that the risk is below a certain level. Experiments in tabular environments demonstrate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strengths:\n\n1. The problem of safe exploration is important. \n\n2. The proposed method is theoretically sound. The authors leverage a method from [1] to approximate the expected value and variance of the agent\u2019s belief about the risk, and prove its convergence. \n\nWeaknesses:\n\n1. The novelty of the proposed method is unclear to me. Bayesian RL and risk-aware safe exploration have been widely studied in previous work. The authors may want to provide a detailed discussion on the novelty of the proposed method.\n\n2. The experiments are insufficient. First, some important baselines are missing. The authors may want to compare to more baselines. Second, it would be more convincing if the authors could evaluate their proposed method on continuous control tasks, such as Mujoco [1].  \n\n[1] Todorov et al. \"Mujoco: A physics engine for model-based control.\" international conference on intelligent robots and systems. IEEE, 2012.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Reproducibility: This paper is well-organized and easy to follow.\n\n Quality and Novelty: The novelty of the proposed method is unclear to me. Moreover, the experiments are insufficient.",
            "summary_of_the_review": "The proposed method is theoretically sound. However, the novelty of the proposed method is unclear to me, and the experiments are insufficient.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2952/Reviewer_qaTz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2952/Reviewer_qaTz"
        ]
    },
    {
        "id": "4Qpeo3z0cn",
        "original": null,
        "number": 5,
        "cdate": 1667471397345,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667471397345,
        "tmdate": 1667471397345,
        "tddate": null,
        "forum": "1tfGKiwnJRJ",
        "replyto": "1tfGKiwnJRJ",
        "invitation": "ICLR.cc/2023/Conference/Paper2952/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the safe reinforcement learning problem where safety constraint violations are bounded. Therefore, the agent's goal is to manage exploration and safety maintenance efficiently.\n\nTo achieve that, the authors propose a cautious RL scheme that uses Dirichlet-Categorical models of transition probabilities. They also empirically validate the different performance aspects of the proposed algorithm.",
            "strength_and_weaknesses": "**Strengths of paper:**\n1. The problem of safety constraint violations in RL is important and has many real-world applications.\n\n2. The authors empirically validated the performance of the proposed algorithm.\n\n**Weakness of paper:**\n1. The assumption of knowing which states are safe and which are unsafe (even locally) is very strong. \n\n2. The motivational example is unsuitable for the considered problem setup in the paper as the state space can be very large or even continuous for the robotic problem.\n\n3. The proposed approach will become computationally inefficient (or even infeasible) for a large number of states or actions (continuous state and action space).\n\n4. It is unclear why authors do not use tighter concentration results for variance like Bernstein inequality.\n\n5. No theoretical guarantee: No theoretical guarantee of how the proposed method will work compared to the optimal policy. \n\n\n**Question and other comments.** \n\nPlease address the above weaknesses. \n\nI have a few more questions/comments:\n1. What is the initial belief used for transition probabilities?\n\n2. Page 3, the line before Section 3: Change $0$ in $\\alpha_a^{i0}$ to something else as it gives the impression that transition is happening from state $i$ to state $0$.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\nThe paper is well organized, but the presentation has minor details that could be improved, as discussed earlier in Strength And Weaknesses.\n\n**Quality:** \nThe experimental evaluation is the bare minimum to support the main claims.\n\n**Novelty:** \nThe main ideas of the paper have limited novelty.\n\n**Reproducibility:** \nThe code is unavailable, which makes it difficult to reproduce the empirical results.",
            "summary_of_the_review": "This paper significantly overlaps with my current work, and I am very knowledgeable about most of the topics covered by the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I do not find any ethical concerns.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2952/Reviewer_G8rL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2952/Reviewer_G8rL"
        ]
    }
]