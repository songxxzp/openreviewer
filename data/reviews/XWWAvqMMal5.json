[
    {
        "id": "5ul3A8Rs0k",
        "original": null,
        "number": 1,
        "cdate": 1666265042518,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666265042518,
        "tmdate": 1666265042518,
        "tddate": null,
        "forum": "XWWAvqMMal5",
        "replyto": "XWWAvqMMal5",
        "invitation": "ICLR.cc/2023/Conference/Paper2836/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The proposed approach corresponds to an RL based acquisition strategy in a NAS search space, that is basing its trajectory on both the observed and predicted (surrogate) evaluations. This is called a mixed batch, whose share in either is a relevant hyperparameter to the success of the model by balancing the prediction error from a surrogate with the computational cost of a full evaluation. This trade-off is identified by a theoretic decomposition of the RL-based acquisition error, comprising of the generalization error of the predictor - to which an unmixed-ground-truth policy may overfit - and the policy error induced by a reward signal of the predictor on the other hand.",
            "strength_and_weaknesses": "### Strengths:\n\n1. The theoretic analysis on the sources of error is nice and basing the method off-of that is a good idea.\n1. I presume, that formulating the archtiecture as a policy (albeit I\u2019d strongly encourage to elaborate on that) is a beneficial way to express the architecture, as an incremental buildup on the architecture and attributing reward to model components and their sequential composition is akin to how humans would go about finding new architectures and may be helpful to reduce overal cost. The upside of this is that this heuristic is performance driven and the compositional strategy is being optimized.\n1. Having an ablation on the introduced hyperparameter.\n\n### Weaknesses:\n\n1. Considering https://arxiv.org/pdf/1904.02642.pdf, the proposed approach corresponds to an RL based acquisition strategy in a NAS search space, that (just like the linked approach) is allowed to update the surrogate model in its sequential acquisition process. The main difference is that for the policy update, both ground turth architecture evaluations and predictor based evaluations are mixed. The novelty is limited to the construction of the batch and the theoretic analysis. I do not see a clear benefit of having a mixed batch (which is also very costly due to the truth evaluation) over said paper other than speeding up the training of the acquisition function (RL agent) by making use of predicted reward signals as cheap proxy. Just like in the linked paper, the surrogate becomes a moving target by virtue of newly collected trajectories, that takes into account all what can be known in terms of ground truth and is guaranteed to be informative eventually by the sequential process.\n1. Fitting a supervised model to the observed rewards as a surrogate model is basically nothing new (as surrogates are doing that). Also the method is rather simplistic in the sense that it merely introduces a hyperparameters for the composition of \u201ctrue\u201d & \u201cpredictor\u201d datapoints which is arbitrarily set albeit having some Ablation on that hyperparameter for their specific problem instances.\n1. There are still quite a few language issues (regarding declination etc.) that disturb the flow of reading.\n1. At least in the BO literature, one would rather prefer \u201csurrogate\u201d over predictor. In RL one would consider the predictor to be a \u201cmodel\u201d. This is just a rephrasing but makes it clearer. In the same sense, \u201cpredictor-RL-based NAS\u201d should rather be reduced to model-based NAS using an RL controller or something similar. To facilitate readers from other fields the entry to your paper, this might be helpful footnote.\n\n#### Specifically:\n\n1. (Abstract) \u201cUnfortunately, even a promising performance predictor may suffer from the accuracy decline due to long-term and  continuous  usage,  thus  leading  to  the  degraded  performance  of  the  search strategy\u201d seems to be a somewhat wrong or at least insufficient description of what you seek to improve upon. Reading this the first time, it was absolutely unclear to me what exactly you were set out to do (what your issue is). Instead - from my understanding, you could rephrase it like this: \nUsing the fixed target predictor to an RL controller necessarily will cause performance degradation on new datapoints, as it does not allow for observations updating the predictor and is limited by its precision and mislead by its generalization error. This can be observed in k=0 (4.4). On the other hand k=N is the other extreme case where we only ever collect new datapoints - making it prohibitive to train brittle and expensive RL methods on it.\n1. (2.1.) The distinction between training-based & training-free predictors is poorly disambiguated. In particular, any of the methods described here can be used as feature space to a predictor, which then learns how to relate the feature space to a performance measure. After all both categories aim at linking their \u201cstatistics\u201d of the architecture - be it the graph or cell representation or the e.g. zero-shot statistics to a performance score.\n1. I do not see the difference between a predictor-based NAS (having an encoder & regressor) to a surrogate model such as a GP, that defines a specific kernel (encoder).\n1. (3.2) Stating an MBRL problem is not sufficient on its own, but a specification of all its components in your setup-interpreation would bring value in understanding how the model is constructed. \n1. (3.2) I would very much appreciate a more detailed outline of how a trajectory corresponds to a description of a neural architecture. \n1. (4.4) k=all is informal. Make it k=N, Figure 2. The variation over seeds would be more indicative of the actual performance.\n1. At least to me, the depiction of Algorithm 1 is not required.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty. The novelty in light of the linked paper is rather limited. The mixed batch construction and the introduced hyperparameter as well as the (short) theoretic analysis are what is really novel about this paper.\n\nReproducibility. I am convinced that the results are reproducible. I very much appreciate the analysis of the natural edge cases of the hyperparameter.\n\nClarity. There are quite a few things in the Weakness-Section, that should be addressed to have a more convincing argument and placement of the paper.\n\nQuality. The scientific procedure is sound.",
            "summary_of_the_review": "Considering the limited novelty and the insufficiently precise formulation of the problem & problem setting I would recommend not accepting the paper in its current form.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2836/Reviewer_9HrM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2836/Reviewer_9HrM"
        ]
    },
    {
        "id": "7beb2SfmsSW",
        "original": null,
        "number": 2,
        "cdate": 1666682123815,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682123815,
        "tmdate": 1666682123815,
        "tddate": null,
        "forum": "XWWAvqMMal5",
        "replyto": "XWWAvqMMal5",
        "invitation": "ICLR.cc/2023/Conference/Paper2836/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a predictor-based neural architecture search with N-sized mixed batch, in which the performances of k architectures are evaluated by training from scratch and the performances of the rest N-k ones are predicted by the predictor. Besides, it studies the impact of predictors on NAS theoretically and empirically. ",
            "strength_and_weaknesses": "Strengths:\n1.\tThe paper is well written and easy to understand.\n2.\tThe theoretical and empirical study of predictors on NAS is good.\nWeaknesses:\n1.\tThe first question is that the evidence of the motivation is not direct. Since the problem to be solved is that \u201ca predictor suffers from the accuracy decline due to long-term and continuous usage\u201d, the authors need to plot a figure about the decline in accuracy of a predictor over time (search steps) in different settings to support their claim. \n2.\tAnother question is why choose k = 2, 5, 2 in cifar-10, cifar-100, imagenet-16-120 in Table 1, while the result in Table 3 shows that the best k should be 5, 8, 2 ? The best results of the two tables do not seem to match.\n3.\tIs there any related work about the mixed-batch method? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is good in clarity, novelty and reproducibility. But I am not sure about its quality due to its motivation, which is not supported by sufficient evidence. ",
            "summary_of_the_review": "To summarize, this paper is good in clarity, novelty and reproducibility. However, I am concerned about the weaknesses mentioned above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2836/Reviewer_qrWv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2836/Reviewer_qrWv"
        ]
    },
    {
        "id": "6b7tg7p3Iw",
        "original": null,
        "number": 3,
        "cdate": 1666699255616,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666699255616,
        "tmdate": 1666699255616,
        "tddate": null,
        "forum": "XWWAvqMMal5",
        "replyto": "XWWAvqMMal5",
        "invitation": "ICLR.cc/2023/Conference/Paper2836/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "During the search, the neural architecture search (NAS) algorithm uses the validation accuracy of the trained candidate model for feedback. As training a candidate every time is expensive, performance predictors, which take the architecture as input and output the validation accuracy, are used. This paper proposes a trick to improve performance predictor based RL search algorithm. \n\nThey use a batched search where the policy samples N architectures, the validation accuracy of all the architectures are obtained to   The RL policy first samples S architectures and the predictor is trained on them. Rather than using this trained predictor to estimate the accuracy of all the sampled architectures at every iteration to perform batch-wise updates of the policy,  the predictor is used to obtain the accuracy of only N-K architectures. The rest of the architectures are trained from scratch. They claim that this alleviates the error contributed in the search owing to the predictor component. They were able to  empirically demonstrate that their technique works.\n\n",
            "strength_and_weaknesses": "Strength:\n1. They are able to find well performing architectures in the neural architecture search\n\nWeakness:\n1. Is your boost actually owing to your LSTM based RL agent, your batch-wise update policy of the agent or the entire PNASM algorithm? Can you do an ablation study on that? (1) Run the search without a predictor with just the LSTM agent, but update the policy after every candidate. (2) Update the policy after a batch of candidates are found, still without the predictor. (3) Use the agent and the predictor but update after every architecture is found. (4) Use the agent and the predictor but perform batchwise update of the policy but without evaluating any additional architectures from scratch. Rely completely on the predictor performance.\n\n2.  In the PNASM algorithm, the performance predictor is trained on fewer architectures to begin with and is trained as new architectures are encountered. So the contribution of error due to performance predictor is high initially. Including the accuracies of architectures trained from scratch would alleviate it. But if one trained the same performance predictor with more architectures initially, it would lead to a better Kendall Tau score and hence a lower predictor error component. Given that we know the accuracies of the architectures in the NASBench 201 search space, is there a way to analyze these two paths?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and is easy to follow.  The contribution is very limited as it applies to surrogate model based RL NAS.",
            "summary_of_the_review": " We need further justification to bolster the claim that this algorithm reduces the error better than just training the predictor with a lot more data. Also, they need to provide the results of the ablation study that I requested.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2836/Reviewer_tJna"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2836/Reviewer_tJna"
        ]
    },
    {
        "id": "bw3MDQE8Up",
        "original": null,
        "number": 4,
        "cdate": 1666806651805,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666806651805,
        "tmdate": 1666806651805,
        "tddate": null,
        "forum": "XWWAvqMMal5",
        "replyto": "XWWAvqMMal5",
        "invitation": "ICLR.cc/2023/Conference/Paper2836/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper evaluates model based performance predictors in NAS inside a model-based reinforcement learning (MBRL) framework. The authors show theoretically that as long as the expected predicted reward by the performance predictor improves by a certain factor during the optimization, one can guarantee improvements under the expected true reward. The efficacy of the method is demonstrated on the NAS-Bench-201 benchmark, where it outperforms most of the other black-box and predictor based NAS algorithms.",
            "strength_and_weaknesses": "The motivation of the paper to investigate the usage of model-based performance predictors for NAS inside a MBRL framework seems plausible. The paper is in general easy to follow and well-structured. There are some interesting theoretical guarantees adopted from the MBRL literature, which the authors built their algorithm, which seems simple and effective. Nevertheless, I have the following criticism:\n\n- **Not enough empirical evaluations**. While it is useful to evaluate a new proposed NAS method on tabular benchmarks such as NAS-Bench-201 used by the authors, I also find it necessary to evaluate on other tabular benchmarks, such as the ones in NAS-Bench-Suite [1], and on a real NAS benchmark. \n\n- **Novelty and limitations**. The proposed algorithm is based on a simple modification of model-based RL, derived from theoretical guarantees adopted from the RL literature. The theoretical justification is interesting but the novelty in the method itself is slightly incremental. Moreover, RL nowadays is not the off-the-shelf choice for NAS, as there are much more efficient methods existing.\n\n-- Questions --\n\n- What is the motivation behind using the specific proposed adaptive method in Section 4.4?\n\n- Would the same Mixed batch method be used for other black-box methods such as evolutionary strategies, or does it work only for MBRL because of the theoretical guarantees?\n\n-- Minor --\n\nLine 3 in Algorithm 1 seems unnecessary.\n\n-- References --\n\n[1] NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy. Mehta et al. ICLR 2022",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is in general clearly written and it covers most of the necessary background in and relevant literature in NAS and MBRL.\n\n- The proposed algorithm is relatively simple and the novelty aspect is marginal. \n\n- The authors provide the codebase to reproduce the results ",
            "summary_of_the_review": "Despite the theoretical guarantees and the motivation, the paper is lacking more empirical evidence  that the method works on a wide range of benchmarks, therefore it needs more work before it is ready for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2836/Reviewer_k5Nf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2836/Reviewer_k5Nf"
        ]
    }
]