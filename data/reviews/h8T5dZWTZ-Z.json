[
    {
        "id": "RZ6QCRxorok",
        "original": null,
        "number": 1,
        "cdate": 1666626836544,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626836544,
        "tmdate": 1669740014677,
        "tddate": null,
        "forum": "h8T5dZWTZ-Z",
        "replyto": "h8T5dZWTZ-Z",
        "invitation": "ICLR.cc/2023/Conference/Paper1546/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper evaluated some essential design choices of existing binary neural networks for image restoration tasks. The authors further propose a network design and evaluate it on three different tasks. Experimental results show that the proposed binary network outperforms all the reference methods.",
            "strength_and_weaknesses": "**Strength**\n\n- The paper is easy to follow.\n- Experimental results seem to be strong. The proposed method outperforms all reference methods across all three tasks.\n\n**Weaknesses**\n\n- Limited novelty: The presented findings regarding residual connection, BN and the position of RPrelu are not supervising. Similar observations have already been reported and discussed in Bireal, Real2binaryNet and ReActNet papers. In terms of network design, there are only minor adaptions on top of ReActNet block design making the proposed BBCU block incremental. The current design lacks significant innovation.\n\n- Current design choices are more based on empirical attempts and lack of refinement of optimization methods specified on IR tasks. For example, BNN-specific training methods such as two-stage training (real2binaryNet), progressive sign function ([1]), knowledge distillation training, etc., have not been verified in this work. However, these methods show significant improvement in the image classification task.\n\n- If we want to demonstrate the practical effectiveness of BNN-based IR methods, it will be better to also compare with more compression methods, such as pruning-based methods and compact model designs. Also, is it possible to use a binary inference engine like larq or Dabnn to verify the real inference speed on Arm cpu? We know that the test results on hardware can sometimes be quite different from the theoretical computational complexity OPs. Arms cpu is also an important computing platform for IR tasks.\n\n[1] Gradient matters: Designing binarized neural networks via enhanced information-flow",
            "clarity,_quality,_novelty_and_reproducibility": "No further concern in this category.",
            "summary_of_the_review": "The paper is well structured, and the experimental result seems to be strong. But, I still have several concerns regarding novelty, a more complete comparison, and adding inference speed evaluation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1546/Reviewer_zzdZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1546/Reviewer_zzdZ"
        ]
    },
    {
        "id": "3S4hrGGIym",
        "original": null,
        "number": 2,
        "cdate": 1666630892946,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630892946,
        "tmdate": 1666630892946,
        "tddate": null,
        "forum": "h8T5dZWTZ-Z",
        "replyto": "h8T5dZWTZ-Z",
        "invitation": "ICLR.cc/2023/Conference/Paper1546/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Since the different properties in CNN and BNN, it hardly applies the experience of designing CNN to develop BNN. In this paper, the author rethinks and analyzes the actual function of components in binary convolution, such as residual connection, BatchNorm, activation function, and structure, for IR tasks. Based on their observations and analysis, they designed a general basic binary convolution unit for binarized image restoration networks. Extensive results can support their claimed contributions.",
            "strength_and_weaknesses": "Strengths: The paper is easy to follow and provides many insightful analyses, and the results are impressive and show promising potential for\u00a0BNNs.\n1. IR model compression is an important and urgent research direction, and BNN is a promising compression method. This paper tackles this problem and thus potentially can have a big impact.\n2.\u00a0This paper rethinks and analyzes the component of BNN. It is interesting and practical. The visualizations, experiments, and analyses are sound and interesting.\u00a0\n3. The experiment results are impressive and general. The proposed model can significantly improve performance on different low-level vision tasks compared with SOTA methods.\u00a0\n4. The ablation studies are insightful. It shows the binarization benefit of different parts and the performance of CNN and BNN to the residual connection, etc.\u00a0\n5. The paper is well written and clearly organized.\n\nWeakness\uff1a I have the following questions about the method and result details.\n1. About the method and experiments,\u00a0do you use the BBCU-V4 in Fig.2 to arm the IR networks? It is better to clarify this point.\n2.\u00a0In Tab.6, are the OPs computed based on 320\u00d7180 LQ images?\u00a0 Besides, I hope the author provides OPs in Tab.4.\u00a0\u00a0\n3. Typos: In Fig.3 (b), the \u201cBBCM-U\u201c seems not correct.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-organized and easy to follow. IR model compression is an important and urgent research direction and BNN is a promising compression method. It is worth exploring BNN on low-level vision. This article deeply analyzes the components of BNN and proposes a practical basic binary convolution unit for binary IR networks. It provides insightful and interesting analyses and conclusions in experiments to guide later BNN design and the results are impressive and general. Besides, the author provides demo codes to reproduce the results in the paper, which makes the paper more reliable and convincing.",
            "summary_of_the_review": "The paper is well written. The paper deeply analyzes the components of BNN and designs a BBCU. The ablation studies, like the visualization of feature distribution, demonstrate the effect of the method. The main comparison results with others are impressive and general. Overall, this paper is interesting, insightful, and practical.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1546/Reviewer_2H4s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1546/Reviewer_2H4s"
        ]
    },
    {
        "id": "gv8Pr5BIQ1D",
        "original": null,
        "number": 3,
        "cdate": 1666656915242,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656915242,
        "tmdate": 1666656915242,
        "tddate": null,
        "forum": "h8T5dZWTZ-Z",
        "replyto": "h8T5dZWTZ-Z",
        "invitation": "ICLR.cc/2023/Conference/Paper1546/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, the authors study the components in binary convolution, such as residual connection, Batch Norm, activation function, and structure, for image restoration (IR) tasks. The aim is to conduct systematic analyses to explain each component\u2019s role in binary convolution and discuss the pitfalls of such mechanisms.",
            "strength_and_weaknesses": "Strengths::\n1) The motivation case study in section 1 is good.\n2) The experimental section is good but can be improved based on the point mentioned in the weaknesses section.\n\nWeaknesses::\nThe paper can be improved based on the following points.\n1) The section 1 can be rearranged and written better. For example, analysis are provided in numerate bullet points but the paragraphing for each analysis is not consistent. Please rectify that for better readability.\n2) It would be better to include some form of content table for better readability at the end of section 1 to describe the progression of the paper texts. Readability of the paper is not good and needs to be worked on.\n3) Please specify each variables in Xjf\u2297Wjf in section 3. Additionally, please specify what each avertible in the section is and why are they important. Please justify the design choices.\n4) Section 4, though has a lot of experimental evaluation but lacks justification and reproducibility. Please explain why such evaluation are chosen and how can be reproduced successfully. Explanation is lacking.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper lacks clarity and needs a lot of work to improve. Please note the weaknesses section in the review for further points on this. Novelty can't be determined with certainty as reproducibility of the work is lacking.",
            "summary_of_the_review": "1) The section 1 can be rearranged and written better. For example, analysis are provided in numerate bullet points but the paragraphing for each analysis is not consistent. Please rectify that for better readability.\n2) It would be better to include some form of content table for better readability at the end of section 1 to describe the progression of the paper texts. Readability of the paper is not good and needs to be worked on.\n3) Please specify each variables in Xjf\u2297Wjf in section 3. Additionally, please specify what each avertible in the section is and why are they important. Please justify the design choices.\n4) Section 4, though has a lot of experimental evaluation but lacks justification and reproducibility. Please explain why such evaluation are chosen and how can be reproduced successfully. Explanation is lacking.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns as of yet.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1546/Reviewer_WV2K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1546/Reviewer_WV2K"
        ]
    },
    {
        "id": "Z0ti5kLrru",
        "original": null,
        "number": 4,
        "cdate": 1666821956095,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666821956095,
        "tmdate": 1670344835222,
        "tddate": null,
        "forum": "h8T5dZWTZ-Z",
        "replyto": "h8T5dZWTZ-Z",
        "invitation": "ICLR.cc/2023/Conference/Paper1546/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a method to improve upon binary neural network for image restoration (IR). It shows that batch normalization can help to obtain a good accuracy performance when binarizing IR models. Based on this observation, the authors introduce a basic binary convolution unit that benefits from batch norm properties using residual alignment. It has been shown that IR binary models outperform existing models across different tasks and datasets.\n",
            "strength_and_weaknesses": "Strengths:\n\n-- The basic binary convolution unit is very simple and effective. IR models equipped with such units outperform existing works.\n\n-- The binary IR models were tested on a wide range of tasks and datasets.\n\n-- The paper is well-motivated, easy to read and well-structured.\n\nWeaknesses:\n\n-- The weight of technical novelty of this work is much lower than that of its empirical novelty. The proposed unit is obtained based on empirical experiments rather than a concrete mathematical foundation.\n\n--  It is not very clear if the improvement comes from RA or RPReLU. What I am curious to see is an ablation study for BBCU-V4 without RA.\n\n-- There is a small degradation in performance when binarizing body and upsampling parts in image super-resolution. I am wondering why it was not explored by prior works. If it was, then please compare.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and its results can be reproduced. In terms of novelty, I believe its empirical novelty is more significant.",
            "summary_of_the_review": "In general, this paper introduces a simple and effective binary unit that further closes the gap between the binary model and its full-precision counterpart. The reasoning and development of the idea is based on empirical results, which its technical novelty is limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1546/Reviewer_SdPY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1546/Reviewer_SdPY"
        ]
    }
]