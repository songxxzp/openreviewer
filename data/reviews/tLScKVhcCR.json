[
    {
        "id": "cNOMq8X16r6",
        "original": null,
        "number": 1,
        "cdate": 1666558367314,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558367314,
        "tmdate": 1666795254051,
        "tddate": null,
        "forum": "tLScKVhcCR",
        "replyto": "tLScKVhcCR",
        "invitation": "ICLR.cc/2023/Conference/Paper1165/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces a new method to preserver first integrals in neural network modeling called first integral-preserving neural differential equation (FINDE) with the claim that FINDE can preserve and discover first integral in data without needing explicit information about the underlying structure. The authors propose two versions of FINDE: cFINDE (continuous integration) and dFINDE (discrete integration). In its introduction, the paper outlines the importance and challenges related to finding first integrals from data and which first integrals can be especially relevant for computer simulations (Energy, momentum, mass, etc). Next, the authors formally define first integrals and describe related work of first integrals in numerical analysis and neural networks that preserve first integrals, including Neural Ordinary Differential Equations, as well as works that have built on top of it and different kind of architectures (holonomic neural networks - HNN). The paper then introduces and defines the two versions of FINDE: cFINDE and dFINDE with relevant theorems explaining important preservation properties related to first integrals for both formulations.\n\nIn their experiments, the authors evaluate cFINDE and dFINDE on relevant datasets that contain different kinds of first integrals. In their implementation, the authors leverage an HNN,  apply related loss formulations to train cFINDE and dFINDE, and define valid prediction time (VPT) as an evaluation metric. The first set of experiments investigate whether first integrals are preserved in FINDE via a mass spring example. The second set of experiments show results on finding known integrals with FINDE compared to using vanilla HNN with the results indicating that FINDE methods generally perform better. The third set of experiments deal with finding first integrals in unknown systems using a set of examples (KdV, pendulum). The results generally indicate that FINDE methods are able to reproduce the ground truth compared to neural ODEs.",
            "strength_and_weaknesses": "**Strengths**\n\n* The paper introduces a novel formulation for a relevant problem in applying neural network to physical systems and computer simulations thereof. The FINDE methods are described and derived with detail and their properties tested as part of the experiments.\n* The paper provides a relevant set of experiments to assess the properties and performance of FINDE, both cFINDE and dFINDE and seems to achieves superior performance compared to other methods.\n\n**Weaknesses**\n\n* The paper could be improved by having more detailed Figure captions across all figures, especially the one outlining the results (e.g. Figure 1, Figure 4 and Figure 5). More detailed captions would provide greater clarity on what the data show and how to interpret the results.\n* The authors could further clarify potential sources of error in FINDE. The current version explains how dFinde resolve the time discretization error in cFINDE, but it would be good to have a more information on other sources of error created by the approximation and they can potentially be resolved. E.g. In any finite difference scheme there is approximation error, such as truncation errors coming from the discretization of the equation.\n\n**Additional Questions**\n\n* You mention that FINDE preserves multiple quantities at the end of Section 2. Could you explain more how that occurs? Specifically how does preserving multiple quantities (e.g. energy and mass) differ from preserving just one quantity (energy) related to your method?\n* Given the additional advantages of dFINDE, in what situations would someone choose cFINDE over dFINDE?\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe paper is generally well-written with relevant details described. Figures could be improved with more detailed captions to guide the reader.\n\n**Quality**\n\nThe quality of the writing and experiments is generally strong.\n\n**Novelty**\n\nThe novelty is generally strong and tackles a relevant problem.\n\n**Reproducibility**\n\nThe authors provide relevant detail and code for their experiments, as well as a reproducibility statement.",
            "summary_of_the_review": "The paper introduces a novel method and provides a relevant set of experiments to showcase the abilities of the method. Improvement can be made to further strengthen the paper in subsequent revisions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1165/Reviewer_vr6Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1165/Reviewer_vr6Y"
        ]
    },
    {
        "id": "8iPqdf-Fth",
        "original": null,
        "number": 2,
        "cdate": 1666684505855,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684505855,
        "tmdate": 1669707300359,
        "tddate": null,
        "forum": "tLScKVhcCR",
        "replyto": "tLScKVhcCR",
        "invitation": "ICLR.cc/2023/Conference/Paper1165/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes first integral-preserving neural differential equation (FINDE) that can learn a dynamical system and its invariant quantities (i.e., first integrals) from observed data *without* prior knowledge (e.g., conservations of energy and momentum) or assumed geometric structures (e.g., Hamiltonian or Lagrangian) on the target system. FINDE consists of two neural networks; a neural network $\\hat{f}(u) \\in \\mathbb{R}^N$ for modeling the target dynamics (i.e., neural ODE) and another neural network $V(u) \\in \\mathbb{R}^K$ that outputs $K$ first integrals. Then, FINDE projects the neural vector field $\\hat{f}(u)$ to the directions in which $V(u)$ is preserved, by using the projection method. Thus, it guarantees the state $u$ evolves on $N-K$ dimensional sub-manifold (level set) which preserves first integrals. The optimization procedure of the projection method is well-formulated by using Lagrange multipliers, and it yields a novel type of neural ODE (continuous FINDE, cFINDE) that can learn conservative dynamics with repsect to the modeled frist integrals. The authors also present a discrete version of the proposed FINDE (dFINDE) to preserve the learned first integral under the discretization of numerical integrators. The authors theoretically show cFINDE and dFINDE exactly preserve the modeled first integrals $V(u)$ under the continuous and dicrete cases, repectively. The authors validate their approach with four dynamical systems including simple two-body problem and complicated KdV system.",
            "strength_and_weaknesses": "Strength:\n\n1. The motivation of this paper is clear. Extracting underlying invariant quantities as well as learning dynamics *without* prior knoweldge on the target system is definitely important and challenging problem in physics. This paper provides a solid contribution to such an important problem. It might be one of good references for automated scientific discovery via machine learning tools.\n\n2. The paper is technically sound. The used projection method seems to be a principled way to preserve arbitrary first integrals. The authors properly modifies the projection method optimization as a learnable model (FINDE) based on neural ODE framework. \n\n3. The paper is clearly written and easy-to-follow.\n\n4. This work is highly reproducible. All used model architecture and hyper-parameters are well summarized in the main text of this paper. Experimental codes were made available.\n\nWeakness:\n1.  $K$, the output dimension of $V(u)$, is very important hyper-parameter because it directly determines the number of invariant quantities. Inappropriate setting of $K$ rather deteriorates the predictive performance of the model as depicted in the experimental results. In the current version of this paper, the authors do not clearly mention how one can determine such a critical hyper-parameter for unknown dynamical systems. It will be nice if the authors elarborate it.\n\n2. I found a prior work [1] that seems to be very relevant to the proposed method. [1] uses PCA to find $N-K$ dimensional sub-manifold of trajectories and extracts $K$ invariant quantities. It will be nice if the authors cite [1] and compare pros and cons of the proposed method with [1] briefly.\n\n***\n[1] Liu, Z., & Tegmark, M. (2021). Machine learning conservation laws from trajectories. Physical Review Letters, 126(18), 180604.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe paper is well-written with sufficient details on the proposed algorithm.\n\nQuality:\n\nThis paper tackles an important problem of physics and proposes a simple solution to resolve it. However, I have a concern regarding a hyper-parameter $K$ which might be very critical.\n\nNovelty:\n\nThe use of the projection method for learning invariant quantities is straightforward (principled) yet novel to me. While the authors provide a thorough list of related literatures, however, [1] might be an important yet uncited work that very relevant to this work.\n\nReproducibility:\n\nThe authors provide a runnable code as well as a detaild description on the experimental setting.",
            "summary_of_the_review": "Overall, I believe the paper is very interesting, tackles an important problem, and thus is worthy of publication. However, the concern on $K$ tuning prevents me from recommending a clear accpetance of the paper in its current form. Currently, my evaluation is weak accept, however, I would like to increase my score if my concern is well addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1165/Reviewer_RgJp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1165/Reviewer_RgJp"
        ]
    },
    {
        "id": "MbCqgjZtxA",
        "original": null,
        "number": 3,
        "cdate": 1666724963021,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666724963021,
        "tmdate": 1666724963021,
        "tddate": null,
        "forum": "tLScKVhcCR",
        "replyto": "tLScKVhcCR",
        "invitation": "ICLR.cc/2023/Conference/Paper1165/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper asks a very pertinent question as to how we can incorporate knowledge of conserved quantities when solving PDEs via nets. The suggested method is quite unclear. ",
            "strength_and_weaknesses": "The main strength of the paper is that it raises and important question and that it has tried some serious benchmark tests. Also interestingly this paper tries to marry the ideas of symbolic regression with ideas of neural training to discover conserved quantities. Definitely a worthwhile attempt. \n\nPrima facie there seem to be major weaknesses in the paper.  \n\nFor a start V seems to be a neural net whose output coordinates encode the conserved quantities. But is V being trained? If yes, how? If I understand correctly -- the function f in Theorem 1 is also a neural net. Is that being trained? Its entirely unclear to me as to how these nets are getting determined. If there is a loss function somewhere that is getting minimized then that is extremely unclear. The paper is in dire need of a pseudocode! \n\nThe testing of preservation of the conserved quantities looks very ambiguous to me. While the authors want to use this as one of the main tests it doesn't seem to have a clear definition till Appendix A (Equation A13?) - and even then it seems to be a function of time while the Table 3 seems to suggest that this is a number that characterizes the whole solution.  And if we focus on Table 3 then it seems that at no value of K is the new method beating the benchmarks in this 1-step criteria for this 2-body example. Isn't that a strong argument against the new method? \n\nFurther, in Appendix D.2 the authors seem to clearly state that their approach of symbolic regression is not leading to a correct identification of the conserved quantities for their 2-pendulum and the FitzHugh-Nagumo datasets. So what exactly is the claim of the paper at all? Doesn't this beat the central claim in the paper? \n\nUsing KdV as a test-case is a quite tricky. Firstly there seems to have be a typo in equation A18 when quoting the KdV. Further KdV has infinite number of conserved quantities. The authors claim to have cut that down to some finite number by some discretization. This is entirely unexplained in the paper - and potentially a very fatal issue since that sounds like trying to entirely change the target from trying to solve KdV to something else! ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is extremely low. \nI am unable to figure out even basic things here like explained in the above point. \n\n  \n",
            "summary_of_the_review": "Given the weaknesses described above, I feel there are major gaps here - either very fundamental issues have gotten overlooked from the study or the presentation has failed to communicate them to the reader.  The paper needs a major rewrite in the very least.  ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1165/Reviewer_pYdf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1165/Reviewer_pYdf"
        ]
    },
    {
        "id": "5PgnAW9Rjq",
        "original": null,
        "number": 4,
        "cdate": 1667224527675,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667224527675,
        "tmdate": 1667224527675,
        "tddate": null,
        "forum": "tLScKVhcCR",
        "replyto": "tLScKVhcCR",
        "invitation": "ICLR.cc/2023/Conference/Paper1165/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a neural network based approach to preserve and discover first integrals in the underlying target systems. In this paper, authors propose two instances of the approach, namely cFINDE and dFINDE, which works in continuous-time and discrete-time respectively. The proposed FINDE method is a great exploration in the area of preserving and/or discovering invariant quantities when learning dynamical systems with deep learning methods, and it is a generalization of existing deep learning based methods. As I see, the FINDE method makes a solid contribution.  ",
            "strength_and_weaknesses": "The main strengths of this paper are \n\n$\\mathbf{1}.$ on the proposal of enhancing the neural network based approach to learn dynamical systems by proposing an additional structure/equation to preserve and/or discover invariant quantities. \n\n$\\mathbf{2}.$ This paper is very well organized, written and illustrated with graphic results. The background knowledge is adequate for readers (relevant in deep learning fields) to understand the rationale and the intuition driving the proposal, and the paper is very readable.  \n\nThere are a few weakness, which could make the paper better but should not overshadow the strength\n\n$\\mathbf{1}.$ The numerical results can be explained and presented better. For example, the switch between dFINDE and cFINDE could be explained better as they often make a bit confusion in the text; the plots can be improved with better presentation: for instance, Figure 3 - it can be shown as four subplots, each of which can compare HNN vs HNN+cFINDE; the plots could confuse audience a bit: for instance, it is hard to see the ground truth. \n\n$\\mathbf{2}.$ It is very interesting to see the big difference induced by using different values of $K$. As discovering the invariant quantities is one of the cores of this approach and also as author addressed in the conclusion, conducting more comprehensive and in-depth numeric investigation on the role of $K$, with more detailed discussion, along with more analysis in theory, would benefit a lot in helping audience understand and the field leverage the proposed methods.    ",
            "clarity,_quality,_novelty_and_reproducibility": "$\\cdot$ This paper is very well clearly written and presented, with a great deal of details in introducing the background, literature review, intuitive explanation, theoretical analysis and numerical study. \n\n$\\cdot$ The novelty of this paper is obvious as it proposed novel approach to generalize and enhance the existing literature to capture and discover first integral of the neural-network based dynamic system learning methods. The implementation is straightforward.\n\n$\\cdot$ This paper has detailed explanation of theoretical analysis, implementations, and experiments. And, it also listed most (if not all) of the used libraries that author used in the implementation.  ",
            "summary_of_the_review": "As stated in the above comment sections, this paper has a solid contribution with a novel approach. It is a good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "There is no ethics concerns.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1165/Reviewer_USE3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1165/Reviewer_USE3"
        ]
    }
]