[
    {
        "id": "IaWw8JqRWm",
        "original": null,
        "number": 1,
        "cdate": 1665968284624,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665968284624,
        "tmdate": 1668625189860,
        "tddate": null,
        "forum": "kjPLodRa0n",
        "replyto": "kjPLodRa0n",
        "invitation": "ICLR.cc/2023/Conference/Paper1857/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors explore different pretraining objectives for tabular datasets, apply them to vanilla and embedding-based MLP architectures, evaluate their relative downstream predictive performance, and compare their performance with gradient-boosted decision trees (GBDTs, namely CatBoost and XGBoost). Specifically, the authors explore unsupervised pretraining objectives, target-aware pretraining objectives, and combinations of both.\n\nExperiments on 11 datasets (6 classification and 5 regression tasks) suggest several findings: 1) deep learning (DL) models generally benefit from pretraining in terms of downstream performance (this includes both simple MLP and transformer-based architectures, although there was no improvement of transformer-based models over simple MLP architectures); 2) target-aware pretraining objectives performed better than unsupervised objectives; 3) combining target-aware and unsupervised pretraining objectives tends to work best, and perform similarly or better than GBDTs on downstream tasks.",
            "strength_and_weaknesses": "Strengths\n---\nPretraining is an important part of the significant success of DL models on downstream vision and NLP tasks, and thus exploring various approaches of pretraining for tabular data is an important and potentially significant endeavor.\n\nThe insight of self-prediction generally outperforming contrastive learning is interesting and significant.\n\nExperiments are performed on a wide range of tabular classification and regression datasets.\n\nWeaknesses\n---\nThere should be a more detailed comparison to GBDTs. The performance of the best pretraining methods are often similar or marginally better than the GBDT methods. An empirical runtime comparison of the training and inference times between GBDTs and the MLPs would provide significant value. Runtime results in addition to model size comparisons would give readers and machine learning practitioners a better sense of the tradeoffs between these methods and ultimately decide if the marginal increase in predictive performance is worth the additional complexity of different pretraining methods.\n\nNo comparisons to AutoML methods such as AutoGluon, which often outperform simple GBDT methods for tabular tasks.\n\nThe writing could be more precise in places. For example, what percentage of the inputs are corrupted in the description of the contrastive approach described in Section 3.2? Also, in Tables 3 and 4, the caption describes the bolded results as statistically significant across all models; however, there are multiple bolded numbers in the same column, thus I'm not sure what the bolded results are significant to. There are also a number of grammatical errors; consider using a service like Grammarly to fix these issues.\n\nI think a more detailed discussion of the differences between this work and the work by Gorishniy et al. (2021) would greatly benefit readers. Also a table summarizing all of the pretraining methods discussed in this paper would also help readers significantly.\n\nMinor Weaknesses\n---\nThe definitions of \"standard ensemble\" and \"efficient ensemble\" in Table 5 are not clearly stated. There are also no standard errors or significance tests for Table 5.\n\nConsider using \"\\cite\" when passively citing previous work.\n\nWhen open quotes are used, they are facing the wrong direction.\n\nTables 1, 6, and 7 are shown in the middle of a paragraph, consider moving them to either before or after the paragraph, or to the top or bottom of the page.\n\nMissing period in the captions of Tables 1, 2, and 5.\n\n\"MLP-PLR\" should be \"MLP-P-LR\" in the \"Models\" paragraph of Section 3.1\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper can be improved, but the paper provides a nice overview and evaluation adapting existing pretraining objectives from vision and NLP tasks to tabular tasks.",
            "summary_of_the_review": "The paper builds on work by Gorishniy et al. (2021) and expands the evaluation of pretraining objectives for tabular data. However, the paper lacks a more thorough comparison to GBDTs and AutoML methods, and the clarity of the paper could be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1857/Reviewer_MXDS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1857/Reviewer_MXDS"
        ]
    },
    {
        "id": "Jnhlkf15qk",
        "original": null,
        "number": 2,
        "cdate": 1666591949301,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591949301,
        "tmdate": 1666591949301,
        "tddate": null,
        "forum": "kjPLodRa0n",
        "replyto": "kjPLodRa0n",
        "invitation": "ICLR.cc/2023/Conference/Paper1857/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Deep learning has become an important method for tabular data. Even though pretraining based DL has been demonstrated successful in computer vision and NLP problems, it is challenging to make reliable conclusion about pretraining efficacy in tabular DL. This work provides a fully labeled tabular datasets to understand if pretraining helps tabular DL in a fully supervised setting and compare pretraining methods to the strong supervised baselines. \n\nUsing the target dataset, the method pretrain the model parameters with pretraining objectives, including generative positive pairs in contrastive-like objective or to corrupt the input for reconstruction in a self-prediction based objectives. In the second stage, the model is finetuned on the downstream classification or regression task. ",
            "strength_and_weaknesses": "Strengths:\n- The paper generates some useful insights around how to pretrain a MLP model for tabular data and finetune the model to achieve stronger results on downstream tasks. There are also interesting discussions round pretraining objective. Self reconstruction and mask prediction are useful pretraining objectives, as compared to contrastive loss. \n\n- Combing supervised loss and unsupervised loss in the pretraining helps further increase the downstream performance. \n\nWeaknesses:\n- The paper is not clear about its experimental setup: what is the pretraining dataset and what is the downstream datasets? Are they the same but with different losses? \n\n- As the paper pointed out that adding supervised loss and unsupervised loss during pretraining enhances downstream task performance. But the paper does not provide any evidence if adding target task prediction loss during pretraining would benefit generalization? Whether adding the prediction of some tasks would hurt the performance on some other tasks with different loss functions?\n\n- If there is not universal solution between self-prediction objective, when applied in real, how should we pick the optimal objective, given some mixture of downstream tasks?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and is of decent quality. \nHowever, the paper is not novel. \nAs the method is simpler, it should be pretty straightforward to reproduce. ",
            "summary_of_the_review": "The paper is insufficient for a publication in ICLR, as explained in the weakness section. \n\nPlease provide more detailed explanation on the experimental setup, particularly:\n- The pretraining dataset, dataset size.\n- Downstream tasks. tasks losses, labeled dataset size, etc. \n\nPlease also formulate the problem properly with the pretraining losses and downstream task prediction losses. \n\nIf the downstream tasks have different prediction losses, how should we pick the pretraining recipe?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1857/Reviewer_S291"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1857/Reviewer_S291"
        ]
    },
    {
        "id": "T0DvYavT-kE",
        "original": null,
        "number": 3,
        "cdate": 1666673708408,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673708408,
        "tmdate": 1668691350508,
        "tddate": null,
        "forum": "kjPLodRa0n",
        "replyto": "kjPLodRa0n",
        "invitation": "ICLR.cc/2023/Conference/Paper1857/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the pretraining practice on tabular data. It shows that using the object target labels during the pretraining stage is beneficial for the downstream performance. Several target-aware pretraining objectives are proposed. Experiments on various datasets validate the claim.\n",
            "strength_and_weaknesses": "The paper compares various supervised pretraining objectives on tabular data. Some helpful conclusions are drawn from the experimental results. \n\nHere are some suggestions:\n1. The main assumption of the paper is that \"Pretraining is always performed directly on the target dataset and does not exploit additional data\", which is not always the case in the deep learning field. It seems the authors need to point out revisiting the \"supervised\" pretraining. The reviewer has two questions:\n    1.1 There are some self-supervised learning strategies on tabular data, such as SubTab. Will they help get better performance?\n    1.2 Could we pre-train on another dataset and then apply the pre-trained model to the target dataset? \n2. The authors need to make the formulation of the pretraining and fine-tuning stages clear. Which learnable parameters are shared between the two stages? Some readers may not be familiar with the tabular model.\n3. The pretraining objectives introduce a lot of augmentation strategies. Will they also help GBDT methods?\n4. If we use different deep models like ResNet or FT-Transformer, will we get similar results?\n5. The authors validate that pretraining is a good trick to boost the ability of deep tabular models. Does it mean that pretraining becomes a basic step when we use deep models on tabular data? Then how to compare various deep models in a fair manner? Different methods may need different pretraining strategies.",
            "clarity,_quality,_novelty_and_reproducibility": "Although the novelty of the paper is a bit limited, the empirical results show some good practice for tabular data training. The experiments are convincing.\n",
            "summary_of_the_review": "The paper is clear and well-written. Several pretraining strategies are investigated in the paper. Experiments support the claim of the paper. There are some suggestions, and please see the weakness part.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1857/Reviewer_QuBL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1857/Reviewer_QuBL"
        ]
    },
    {
        "id": "FDTUdEiOmv",
        "original": null,
        "number": 4,
        "cdate": 1667062824936,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667062824936,
        "tmdate": 1667062824936,
        "tddate": null,
        "forum": "kjPLodRa0n",
        "replyto": "kjPLodRa0n",
        "invitation": "ICLR.cc/2023/Conference/Paper1857/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper deals with tabular data, specifically examining in detail the question whether deep learning models can be trained to compete with traditional methods based on decision trees. In particular, the paper studies the role of pretraining, a training modality that is peculiar to deep learning and has been crucial in establishing its dominance in NLP and vision.\nThrough a extensive battery of experiments on several tabular datasets, the paper evaluates various pretraining methods and architectures, showing that specific types of pretraining allow deep learning models to be competitive against decision tree models on tabular dataset.\nThe most successful type of pretraining combines mask denoising and label noise, and the paper proposes an explanation of the success of label noise based on studying a synthetic dataset which allows to control the relevance of individual features to predict the target. This study reveals that masked and label noise denoising pretraining allows to better predict features that are less relevant to predict the target.\nFinally, the paper shows that the same pretrained deep learning model can be finetuned multiple times, and all the multiple finetuned models combined to increase performance even further.",
            "strength_and_weaknesses": "Strengths:\n- Well-motivated and timely study that shows that deep learning models can consistently beat decision trees on tabular datasets\n- Extensive study on multiple datasets and invesigating multiple pretraining procedures \n- Interesting synthetic dataset construction to control feature relevance to study the effect of combine masked and label noise denoising pretraining\n- Careful comparison of different types of MLPs and transformer models\n- Useful study of computation and time cost due to pretraining\n\nThe paper has a few minor weaknesses, which can mostly addressed by clarifying some points.\n- In he synthetic experiment, is importance rank in Fig. 1 given by the influence of the vector p? Or is that derived from the feature importances?\n- During finetuning in the efficient ensembling case, is the whole architecture including the backbone trained end-to-end, or is just the heads that are being trained on a fixed backbone? This makes a great difference in terms of compute and memory: the difference between keeping around 15 different backbones and only one.\n- The paper is rather complete in terms of citations of previous works, but is however missing some notably very connected papers. It would be for instance beneficial to cite the paper Padhi et al. \"Tabular Transformers for Modeling Multivariate Time Series\", ICASSP 2021, which also uses masked denoising pretraining (although in tabular time series as opposed to simple tabular datasets).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is very clearly motivated and presented\nQuality: High-quality systematic study on the effect of different pretraining techniques on deep learning for tabular data\nNovelty: The paper's strength is not innovation, but this type of systematic study comparing traditional ML and deep learning for tabular data is very timely\nReproducibility: The paper's results seem reasonably reproducible thanks to the detailed description of the experiments and hyperparameters used",
            "summary_of_the_review": "The paper presents a timely exhaustive study on the effect of pretraining for deep learning on tabular data, and how this can be used to beat traditional machine learning techniques like decision trees.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1857/Reviewer_9GeL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1857/Reviewer_9GeL"
        ]
    }
]