[
    {
        "id": "XKWr6J14BtT",
        "original": null,
        "number": 1,
        "cdate": 1665999621085,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665999621085,
        "tmdate": 1665999621085,
        "tddate": null,
        "forum": "Ix4Ytiwor4U",
        "replyto": "Ix4Ytiwor4U",
        "invitation": "ICLR.cc/2023/Conference/Paper3417/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an offline imitation learning (IL) method supported by a word model. The world model is obtained by training on the transitions collected in a dataset, without any direct access to the actual environment. A intrinsic reward is also designed that measures the divergence between expert and agent in the trained word model. The approach is well motivated. Empirical results shows that the proposed method outperforms baselines adapted to the world-model, namely GAIL and BC.",
            "strength_and_weaknesses": "Strength:\n1. The paper is well written and easy to follow. All elements of the proposed model appear well-motivated. The proposed method is sound, and appears a novel combination of various techniques.\n\n2. Empirically, the evaluation shows the proposed method outperforming the baselines appropriately adapted to the world-model setting.\n\nWeakness:\n1. The empirical evaluation, while carefully designed, is limited in scope and does not explicitly highlight/explain why the proposed method outperforms the baselines. For instance, my understanding is that the only difference between GAIL and DITTO is that GAIL uses adversarial reward. However, from this singleton comparison it is difficult to assess if adversarial reward is worse than intrinsic reward. Related to this, there are various recent adversarial formulation that outperforms GAIL and they should also be considered in the evaluation to more broadly evaluate the impact of the reward formulation. As an example, PWIL [1] is fairly similar to DITTO in terms of how reward is computed.\n\n2. The quality of the world-model and reward functions are not independently analyzed. In the current experiments, the size of the dataset directly affects both the quality of the world model and of the reward. However, it is important to separate these two effects, as the authors also argued that learning the world model can be done using transitions of any quality. Potential ablation may include separate the datasets used for world model and reward computation to isolate the two effects.\n\n3. The method appears directly applicable to Mujoco environments. It would be nice to see if world model is still beneficial in a vector-based observation/state setting.\n\nOverall, any additional experiments to improve our understanding of the proposed method and why it outperforms existing ones would be beneficial.\n\n[1] Primal Wasserstein Imitation Learning, Dadashi et al. ICLR 2021",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and motivated. The proposed method appears to be a novel combination of various existing components. The practical algorithm is described in details with source code included for reproducibility.",
            "summary_of_the_review": "The paper proposes a fully offline IL method by training a world model and leverage standard RL for imitation learning. The method is well motivated and shows improved performance over baselines. Additional ablation and experiments are needed to better understand the advantages of the proposed method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3417/Reviewer_pKRJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3417/Reviewer_pKRJ"
        ]
    },
    {
        "id": "st9uss75Dq",
        "original": null,
        "number": 2,
        "cdate": 1666663973154,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663973154,
        "tmdate": 1668907447978,
        "tddate": null,
        "forum": "Ix4Ytiwor4U",
        "replyto": "Ix4Ytiwor4U",
        "invitation": "ICLR.cc/2023/Conference/Paper3417/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces DITTO, a method for fully offline imitation learning in POMDPS high dimensional observation spaces. The idea progresses by first learning a dynamics model or 'world model'. Then, an RL agent is trained in this latent space to minimize the divergence to the learned state-action density to the expert state-action density. ",
            "strength_and_weaknesses": "Strengths\n+ The proposed method is very neat, and explained well. The improvement obtained from an explicit model-based approach is great to see, and encouraging for model-based RL in general.\n+ The theory is nice to have, as is the discussion of the fundamental challenges facing imitation learning. \n+ The experimental results seem promising to start with. \n\nWeaknesses\n+ The authors write 'to the best of our knowledge we are the first to study completely offline imitation learning without behavior cloning in high-dimensional observation environments'. In fact, there are several prior works which deal with this setting. They include ValueDice [1], IQ-Learn [2], EDM [3] AvRIL [4], all of which achieve decent performance. There are almost certainly more, of which you can probably identify most by looking at the papers that cite [3] .\n\n  [1] Kostrikov et al, Imitation Learning via Off-Policy Distribution Matching, ICLR 2019\n\n  [2] Garg et al, IQ-Learn: Inverse soft-Q Learning for Imitation, Neurips 2021\n\n  [3] Jarrett et al, Strictly Batch Imitation Learning, Neurips 2020\n\n  [4] Chan et al, Scalable Bayesian Inverse Reinforcement Learning, ICLR 2021\n\n  Several of these methods are non-adversarial and so do not suffer from the (very reasonable) issues raised by the authors with regards to GAIL. \n\n+ I think I may be misunderstanding, but is there a reason that RL is required to learn the actor in the world model? It seems that if everything is controlled and latent, we can just differentiate through the policy directly (using a Gumbel-softmax) for issues with discrete distributions.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThe paper is clear enough, but a bit verbose in places (eqns 11-14 could be cut, for instance)\n\nQuality\n\nThe paper is relatively high quality, and the work is quite polished. \n\nNovel \n\nAs far as I am aware, this approach is novel. There does seem to be a bit of similarity to the EDM approach, which might be worth clarifying.\n\nReproducibility\n\nCode is provided, so the approach seems reproducible.\n",
            "summary_of_the_review": "The paper proposes a new method for explicitly model-based offline imitation learning. The method is exciting and shows promise. However, the experimental results are weak, not evaluating on the state-of-the-art modern offline imitation learning baselines. This is understandable since the authors were not aware of these methods. However, I don't think that a paper which lacks comparison to current benchmarks should be recommended for acceptance. I would be very happy to reconsider if the authors could provide comparisons to the methods mentioned above, and demonstrate how DITTO compares.\n\nUpdate after rebuttal:\n\nThanks to the authors for the replies and additional baselines. Based on these, I have increased my recommendation to 6. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3417/Reviewer_ZmXh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3417/Reviewer_ZmXh"
        ]
    },
    {
        "id": "5ilkK57o2FH",
        "original": null,
        "number": 3,
        "cdate": 1666670284920,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670284920,
        "tmdate": 1666670284920,
        "tddate": null,
        "forum": "Ix4Ytiwor4U",
        "replyto": "Ix4Ytiwor4U",
        "invitation": "ICLR.cc/2023/Conference/Paper3417/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies imitation learning in partially observable environments. The proposed method is to learn a world model using a dataset of any quality, unroll the agent's policy in the latent space of the world model to simulate the agent's trajectory, and train the policy with a reward function to make the agent's trajectory close to the demonstration trajectory in the latent space.\n\nSpecifically, the world model follows the recurrent neural network to predict future latent states, and the world model is trained with ELBO objective, consisting of the KL divergence between the prior and posterior distributions of latent states, and reconstruction error for the raw observation. The agent is trained via the on-policy actor-critic algorithm, where the policy is executed in the latent space of the world model. The dense reward signal penalizes the distance on the latent state densities between the agent's trajectory and the expert demonstration trajectory.\n\nExperiments are conducted on five Atari games and the proposed is compared with behavior cloning, and generative adversarial imitation learning, showing advantages over the baselines in three out of five environments.",
            "strength_and_weaknesses": "Strengths:\nThe method is well-motivated to study the covariate shift issue in imitation learning, which is an important and interesting problem.\nThe proposed method is technically solid. The learning of the world model and on-policy actor-critic is well executed to make the relatively complicated pipeline work in Atari games. \nThe paper writing is mostly clear and easy to follow.\n\nWeaknesses:\nThe performance improvement in only three out of five Atari games is not very impressive.\nThe statement \"address the problem of covariate shift\" in the abstract is not well-supported by the experiments. Regarding the experiments, it is unclear whether the bottleneck for baselines is covariate shift, and how much covariate shift can be addressed by the proposed method. \nThe evaluation and analysis can be improved. For example, how does the quality of the dataset affect the quality of the world model, and how does the quality of the world model affect the final performance of imitation learning? I doubt \"the quality of the policy used to generate world model training episodes did not affect final performance\". See below for more detailed questions and comments. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe method is described clearly except for several details.\nIn Equation (5), \\hat{M} is undefined before. I guess it is a typo and it should be M?\nAbout the reward design, it is mentioned that \"unlike L2, the formulation is independent of the scale of the underlying feature space\". However, it is also stated that \"maximizing the max-cosine objective is equivalent to minimizing the L2 norm between two vectors\". So the relation between your reward formulation and L2 distance is confusing. It will be better to explain this paragraph in the Appendix with the corresponding mathematics formulas and derivations.\n\nQuality:\nBecause the proposed method uses both datasets of any quality (for world model learning) and expert demonstration data (for policy learning), I'm curious how the proposed method compares with model-based offline RL algorithms, given the same dataset with trajectories of different qualities.\n\nThe statement that \"the quality of the policy used to generate world model training episodes did not affect final performance\" seems overclaiming to me. For example, in Atari games Montezuma's Revenge, the environment is composed of 24 rooms in the first levels. When the quality of datasets for world model learning is moderate, many rooms will not be covered in this dataset. Thus, the world model never sees some rooms during training. With great expert demonstrations covering all these 24 rooms, will the world model fail in the unseen rooms? How does the proposed imitation learning approach perform in these rooms?\n\nIt will be great to see more analysis of the proposed method. As for world model learning, how do the number and quality of datasets affect the final performance? As for policy learning, how does the agent training horizon H affect the performance? It seems surprising that the policy can unroll in the world model for H=15 steps to predict future value.\n\nRegarding the reward design for intrinsic reward, could you compare it with some existing reward designs based on latent space distance, such as https://proceedings.neurips.cc/paper/2018/file/35309226eb45ec366ca86a4329a2b7c3-Paper.pdf? The reward in this work is to calculate the step-by-step distance, which might be too restrictive to encourage the agent exactly follow the demo. When the environment is stochasticity (not sure whether the environment is deterministic or stochastic in the current experiment), will this kind of reward design hurt the final performance?\n\nNovelty:\nThe novelty is okay. Although learning the world model and reward design according to distance in latent space is not novel ideas, it is novel to combine them in the model-based imitation learning setting.\n\nReproducibility:\nCode for experiments is provided in Appendix. So the reproducibility is okay. The main issue is to apply the proposed method further for more environments because there is no ablative study about hyper-parameters and we have no idea how to tune the method in new environments.",
            "summary_of_the_review": "Overall, this paper proposed a technically solid method for imitation learning, but the evaluation can be better if conducted in more environments, with more datasets, and more design choices for the reward function.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3417/Reviewer_RnpT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3417/Reviewer_RnpT"
        ]
    },
    {
        "id": "l5Gr0wMMR-",
        "original": null,
        "number": 4,
        "cdate": 1667195163400,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667195163400,
        "tmdate": 1667197308316,
        "tddate": null,
        "forum": "Ix4Ytiwor4U",
        "replyto": "Ix4Ytiwor4U",
        "invitation": "ICLR.cc/2023/Conference/Paper3417/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces an offline imitation learning algorithm. The proposed algorithm consists of two parts: (1) to train a world model using demonstration of any quality and (2) to train a policy using $\\lambda$-TD where the reward function is computed as Equation (7) (i.e. intrinsic reward). Finally, the authors empirically show that the proposed method outperforms baselines in offline setting.",
            "strength_and_weaknesses": "## **Strength**\n\nDITTO shows good empirical results.\n\n## **Weakness**\n**Algorithm**\n\n- DITTO uses an intrinsic reward function. However, there are no justification to use this reward function. At least there should be some consideration of the ideal case (e.g. in theory, DITTO is equivalent to a distribution matching between current policy and expert policy)\n\n**Experiments**\n\n- In Figure 3. D-BC always achieves near-expert performance when the number of expert trajectories is large enough. Compared to D-BC, DITTO shows slightly better performance. Therefore, it seems that more experiments are needed to show the superiority of DITTO.\n- I am not sure if it is a good thing to outperform the expert in (offline) imitation learning. Why DITTO exceeds average expert performance?\n\n**Related Works**\n\n- One of the simplest ways to solve offline imitation learning with learned world model is to use HIDIL [1], which aims to solve an offline imitation learning problem given a misspecified simulator.\n- In addition, existing (model-free) offline IL algorithms [2-4] can also be used for policy learning on learned world model.\n\n[1] Jiang, Shengyi, Jingcheng Pang, and Yang Yu. \"Offline imitation learning with a misspecified simulator.\" NeurIPS. 2020.\n\n[2] Kim, Geon-Hyeong, et al. \"DemoDICE: Offline imitation learning with supplementary imperfect demonstrations.\" ICLR. 2022.\n\n[3] Xu, Haoran, et al. \"Discriminator-weighted offline imitation learning from suboptimal demonstrations.\" ICML. 2022.\n\n[4] Ma, Yecheng Jason, et al. \"SMODICE: Versatile Offline Imitation Learning via State Occupancy Matching.\" ICML. 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "Using intrinsic rewards as in Equation (7) is interesting and novel.\n\nHowever, theoretical justification seems necessary.\n",
            "summary_of_the_review": "Solving offline imitation learning is an important, but the proposed algorithm is too empirical.\nThis paper uses only 5 Atari domains and there is no theoretical analysis.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "- ",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3417/Reviewer_Nmum"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3417/Reviewer_Nmum"
        ]
    }
]