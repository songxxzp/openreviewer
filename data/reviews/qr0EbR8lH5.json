[
    {
        "id": "EqCG22g6mu",
        "original": null,
        "number": 1,
        "cdate": 1666550902112,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666550902112,
        "tmdate": 1666550902112,
        "tddate": null,
        "forum": "qr0EbR8lH5",
        "replyto": "qr0EbR8lH5",
        "invitation": "ICLR.cc/2023/Conference/Paper2391/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies Personalized Decentralized Bilevel Optimization (PDBO). Based on Stochastic Gradient Push (SGP) method, the authors propose Hyper-Gradient Push (HGP) method to estimate the hypergradient and analyze the estimation error of its variance-reduced version (VR-HGP). On top of the hypergradient estimation, they conduct experiments to solve PDBO problems and compare its performance to some existing algorithms.\n",
            "strength_and_weaknesses": "Pros:\n1. This paper considers the general setting in decentralized training, i.e., the network is stochastic and directed.\n\n 2. The formulation of PDBO is more general than previous papers, as discussed in Section E and Table 3 of the appendix.\n        \n3.  HGP method does not require computing exact Hessian or Jacobian, which is an improvement comparing to previous CDBO papers.\n\nCons:\n\n1. Reformulating Eq. (5) as Eq. (6) is reasonable in practice (and is true under Assumption 2). However, in theory technically speaking the condition $y_i^* = y_i^{(T)}$ in Assumption 2 is unusual. It would be good if the reformulation (Eq. (6)) of Eq. (5) can be theoretically justified (e.g., via some error estimation) without assuming $y_i^*=y_i^{(T)}$ in Assumption 2.\n\n2. (Assumption 2) The authors claim \"To apply implicit differentiation, we introduce the following assumption as in Grazzi et al. (2020)\", but is \"$y_i^* = y_i^{(T)}$\" assumed in Grazzi et al. (2020)? It seems that in Grazzi et al. (2020) they only require $y^*$ to be unique instead of accessibility in $T$ steps. \"$y_i^*=y_i^{(T)}$\" seems to be a strong assumption that is not included in papers about non-asymptotic convergence of bilevel optimization. Besides, carefully analyzing the error incurred by $\\|y_i^* -y_i^{(T)}\\|^2$ is not a trivial problem in bilevel optimization literature. For example, Chen et al. (2022) uses a novel way to deal with this error (see Lemma 3 therein and the order of $T$ in Theorem 1 and Proposition 2) and successfully reduce the sample complexity comparing previous works on bilevel optimization.\n\n3. (Theorem 1) This theorem characterizes the hypergradient estimation error. How can we apply this in real PDBO algorithms to solve (5)? Should we just use $v^{(\\lambda)} + c^{\\lambda}$ as the hypergradient and then use single-agent gradient descent, or should we still run some decentralized algorithms to solve (5)? It is unclear how we should use the estimation (i.e., $v^{(\\lambda)} + c^{\\lambda}$) mentioned in Theorem 1 to run PDBO. It would be better if there is another theorem characterizing the convergence results (e.g., sample complexity) of PDBO using this estimation.\n\n4. (Mini-batch) We need to use mini-batch method (i.e., setting $b$ in Theorem 1 to be large enough) for HGP. However, some of the previous CDBO papers (e.g., Yang et al. (2022)) do not require mini-batch method.\n\nQuestions: \n\n (\"Suppose the i-th client is responsible for computing the i-th block of $u^{(m+1)}$ and $v^{(m+1)}$, ...\", Section 4.2.) What happens after all the agents finish the computation of their own blocks? Is there a central node collecting all the blocks to obtain $v^{(M)}$?\n\nReferences:\n\n[Chen et al., 2022] T. Chen, Y. Sun, and W. Yin. Closing the gap: Tighter analysis of alternating stochastic gradient\nmethods for bilevel problems. Advances in Neural Information Processing Systems, 34, 2021.\\\\\n\n[Grazzi et al., 2020] R. Grazzi, L. Franceschi, M. Pontil, and S. Salzo. On the iteration complexity of hypergradient\ncomputation. In International Conference on Machine Learning, pages 3748\u20133758. PMLR, 2020.\\\\\n\n[Yang et al., 2022]S. Yang, X. Zhang, and M. Wang. Decentralized gossip-based stochastic bilevel optimization over\ncommunication networks. arXiv preprint arXiv:2206.10870, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality, clarity and originality of the work need to be improved. See Strength And Weaknesses.",
            "summary_of_the_review": "Overall, I feel that the proposed method has several benefits than previous ones, but there are quite a few limitations and questions left. I tend to reject this time, but am open to increase the evaluation based on the other reviewers' comments and the authors' response. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2391/Reviewer_TFeg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2391/Reviewer_TFeg"
        ]
    },
    {
        "id": "i_5vgGOQUD4",
        "original": null,
        "number": 2,
        "cdate": 1666647048591,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647048591,
        "tmdate": 1666647048591,
        "tddate": null,
        "forum": "qr0EbR8lH5",
        "replyto": "qr0EbR8lH5",
        "invitation": "ICLR.cc/2023/Conference/Paper2391/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a general formulation for learning personalized models in a fully-decentralized context. The problem is formulated as a bi-level optimization problem, where the outer-objective updates the local hyper-parameters given the optimal local parameters of the inner objective. The algorithm for optimizing such problem in a decentralized way is then derived, supposing that the communication graph is fully connected but only a few directed communication edges are active at each iteration.",
            "strength_and_weaknesses": "The paper is very hard to read, and hence it is difficult to assess the contributions and correctness of the derivations.\n\nThese are some of the reasons that make the paper difficult to parse:\n1. Some notation and formulations are inexact, confusing or undefined:\n- In Problem 1 and Problem 5, the index $i$ is used for two different thing: first for indicating the client's parameters to be optimized and then for indexing the terms of the sum. Also the size of $\\xi_i$ is not provided and $p(\\xi_i)$ is not formally defined.\n- Right before Equation 2, it is said that \"The i-th client in SGP updates its weight $\\omega_i$\" but Equation 4 do not update this parameter.\n- Equation 4, the operation $[a \\quad b]$ is not defined. I assumed it was a vector concatenation. Also $p()$ denotes here the message distribution, while it was used before for the local data distribution.\n- (minor) In page 3, end of Section 2, $p_{ij}$ should be replaced by $p()$ ($p_{ij}$ cannot be a doubly stochastic matrix as it is defined as a scalar function).\n- In Assumption 2, the stationary point is denoted by $y^\\star$ while later one is referred to as $y^\\star(\\lambda)$.\n- Overall, the equations are very hard to read because the notation is heavy. The paper would benefit from a simplification. For instance, the update matrices that depend on $y$ (resp. $\\lambda$) should be called in a way that remind the reader of this dependence. The Jacobian is called $A$ and its estimate $u$.\n\n2. It is not clear which criteria are used for determining if a set of parameters should be optimized in the outer or inner loop of the bi-level problem. For instance, why is the label weight vector of PDBO-DA considered a hyper-parameter, hence to be optimized in the outer problem, and not the parameters of the neural network? In the end, all the parameters of the model are specialized to the agent. \n\n3. Little intuition or explanation are provided for supporting the various derivation choices. For instance, why are truncated Neumann series and recurrent back-propagation needed to compute the outer gradient?\n\nFinally, the interplay between agent similarity (usually referred to as mixing matrix) and model performance is never discussed in the paper. No assumptions are then made on the model for drawing directed communication edges, as the paper does not try (or guarantee) to recover the true mixing matrix. However, the performance of a personalized model depends on how often/well an agent communicates with similar agents. If existing works make more assumptions on the graph model, it is to guarantee a good estimation of the true mixing matrix, hence a good estimation of true local models.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very hard to read. Because of its low clarity, I cannot judge the quality and the novelty of the results.\nThe code for reproducing the results was not submitted and in the empirical section the chosen model architecture is not described.",
            "summary_of_the_review": "The paper would require a major revision in order for the reader to appreciate its contributions. At this stage, the significance and potential impact of the work are limited because of the poor clarity of the paper and because of the lack of discussion or guarantees on the recovery of the true mixing matrix depending on the choice of random communciation model.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2391/Reviewer_3KYv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2391/Reviewer_3KYv"
        ]
    },
    {
        "id": "rEryshqS7_",
        "original": null,
        "number": 3,
        "cdate": 1666653872512,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653872512,
        "tmdate": 1666654284011,
        "tddate": null,
        "forum": "qr0EbR8lH5",
        "replyto": "qr0EbR8lH5",
        "invitation": "ICLR.cc/2023/Conference/Paper2391/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a gradient-based bilevel optimization that reduces most personalization approaches to the optimization of client-wise hyperparameters. Then the authors propose a decentralized algorithm to estimate gradients with respect to the hyperparameters, which can run even on stochastic and directed communication networks. Empirical results demonstrated that the gradient-based bilevel optimization enabled combining existing personalization approaches which led to state-of-the-art performance, confirming it can perform on multiple simulated communication environments including a stochastic and directed network.",
            "strength_and_weaknesses": "Strength: \n- This study proposed a gradient-based PDBO, which reduces most personalization approaches to the optimization of hyperparameters possessed by each client. \n\nWeaknesses: \n- Assumption 2 is a bit too strong to require $y_i^*=y_i^{(T)}$ make it unpractical. The authors should provide some bound estimation for it.\n- Empirical results using the settings of EMNIST classification is a bit too simple.  \n ",
            "clarity,_quality,_novelty_and_reproducibility": "- paper is hard to follow. the authors better to provide some intuition or explanation for some derivations \n\n- unable to verify. code is not provided but the authors claimed in the paper to release code after review process ",
            "summary_of_the_review": "This study proposed a gradient-based PDBO, which reduces most personalization approaches to the optimization of hyperparameters possessed by each client. The approach is not practical since the assumption is too strong, and the experiment using EMNIST classification is a bit too simple. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2391/Reviewer_Mrqc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2391/Reviewer_Mrqc"
        ]
    },
    {
        "id": "edVNI1HOT2K",
        "original": null,
        "number": 4,
        "cdate": 1666667874200,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667874200,
        "tmdate": 1666667874200,
        "tddate": null,
        "forum": "qr0EbR8lH5",
        "replyto": "qr0EbR8lH5",
        "invitation": "ICLR.cc/2023/Conference/Paper2391/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of personalized bilevel optimization in a decentralized distributed learning system over stochastic and directed underlying communication networks. The major contribution of this paper is to propose an effective scheme termed HGP to compute the hyper-gradient, i.e., the gradient of hyperparameters in the outer-problem, in a decentralized way. Specifically, this paper first shows the failure of standard backpropagation in hyper-gradient computation on a directed network, then introduces a push-sum communication strategy to tackle the biasness of the hyper-gradient estimate. Moreover, the variance reduction technique is introduced to handle the large variance in the push-sum communication. This paper further derives an upper bound for the estimation error of the proposed method and evaluates it effectiveness empirically. ",
            "strength_and_weaknesses": "**Strengths:**\n+ The discovery of the failure of standard backpropagation for hyper-gradient computation over directed networks is interesting, and the use of push-sum strategy sounds novel.\n+ This paper is well-written in general and easy to follow.\n+ The derivation of the estimation error bound is technically sound.\n\n**Weaknesses:**\n- Some assumptions in this paper need to be justified. \nI wonder if the assumption of knowing the receiving frequency in advance is reasonable. Although this paper said that it can be estimated by simulating several rounds, it will bring estimation error to the computation of hyper-gradients. Does the main theorem take such an error into consideration? \nIn addition, please justify Assumption 5, which said that the edges are \u201cundirected\u201d in expectation. I wonder if this assumption still holds for standard (deterministic) directed networks.\n- The relevance to some related work should be clarified.\nSince the proposed method relies on the previously proposed recurrent backpropagation method, HPG should be compared with Grazzi\u2019s work in detail, especially from the theoretical aspect. Specifically, what is the difference between using the sending edges and the receiving edges in principle? Will it bring any difficulty in theoretical analysis? Besides the error bound for VR-HGP, can you provide a bound for the standard HGP and compare it with Grazzi\u2019s? \nAnother related line of work is the push-sum method. It seems that the core innovation of HPG is to introduce such a method to the optimization of the outer-problem. Hence, I wonder what is the main difference from applying the method to the inner-problem as in previous works (perhaps more complex theoretical analysis)? \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written in general. The comparison with related work on decentralized bilevel optimization is clear, though the relevance to previous hyper-gradient estimation methods and push-sum methods need to be clarified in more detail. In addition, several assumptions in theoretical analysis need to be further justified (please refer to the above **Weaknesses**). ",
            "summary_of_the_review": "This paper contributes an effective scheme for hyper-gradient computation in PDBO over stochastic and directed networks, where the use of push-sum communication to make an unbiased estimate is interesting. However, the relevance to previous hyper-gradient computation methods and push-sum communication methods should be explained in more detail, and several assumptions in theoretical analysis need to be justified.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2391/Reviewer_b9iT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2391/Reviewer_b9iT"
        ]
    }
]