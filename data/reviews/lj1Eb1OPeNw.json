[
    {
        "id": "n8D6KYZ1b1d",
        "original": null,
        "number": 1,
        "cdate": 1666695521247,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695521247,
        "tmdate": 1669452434253,
        "tddate": null,
        "forum": "lj1Eb1OPeNw",
        "replyto": "lj1Eb1OPeNw",
        "invitation": "ICLR.cc/2023/Conference/Paper2261/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a spatio-temporal entropy score to conduct training-free neural architecture search for efficient video recognition. They use a simple network search space with a carefully designed performance estimation score. The searched results achieve state-of-the-art performance on action recognition.",
            "strength_and_weaknesses": "## Strength\n\n1. The derivation of the entropy score is thorough.\n1. The visualization of the searched architectures in Figure 1 reveals interesting properties of 3D CNNs.\n\n## Weakness\n\n1. As the search space is the most important part of a NAS method, the authors should give more details such as the lower bound / upper bound performance of the search space.\n1. The search happens without considering auxiliary modules like BN, Reslink, and so on. Will the entropy score transfer well with such a gap?\n1. Why is Table 2 only compared with most 2D CNN-based methods with ResNet50? Also, the baselines are outdated.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and the theorems are carefully proven. However, the experiments are not sufficient and the baselines seem outdated.",
            "summary_of_the_review": "Baselines are most 2D CNNs-based methods and outdated. So I recommend to weak reject this paper until results on stronger baselines are provided.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2261/Reviewer_Jkpo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2261/Reviewer_Jkpo"
        ]
    },
    {
        "id": "rC91lzOYho",
        "original": null,
        "number": 2,
        "cdate": 1666749861762,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666749861762,
        "tmdate": 1666749861762,
        "tddate": null,
        "forum": "lj1Eb1OPeNw",
        "replyto": "lj1Eb1OPeNw",
        "invitation": "ICLR.cc/2023/Conference/Paper2261/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to find a good 3D CNN architecture for video recognition. This study proposes to use a novel proxy task, maximizing a designed entropy value, to search for the effective architecture. The experiments show the effectiveness of the proposed algorithm.\n",
            "strength_and_weaknesses": "Strength\n1. The experiments are solid, showing a good alignment between the proxy task and the video recognition ability.\n2. The results on Sth-Sth are good.\n\nQuestion\n1. \u201cTable 9 shows three instantiations of E3D with varying complexity, including E3D-S (1.9G FLOPs), E3D-M (4.7G FLOPs), and E3D-L (18.3G FLOPs).\u201d Wondering the details about how to get E3D-S/M/L \u2013 are they searched separately? Or search a model and then manually scale up.\n2. Is it possible to get an even larger model (maybe the name would be E3D-XL)? If it is possible, would we expect higher performance?\n3. Is it possible to extend this entropy idea to other architectures (eg transformer)?\n4. Is it possible to trivially apply other training-free NAS on 3D task? I am a little bit concern that it is not convinced the proposed method is better than previous training-free NAS method.\n\nWeakness\n1. The results on Kinetics400 are not as good as the transformer based method (although I understand the transformers are computationally expensive)\n2. See question 4: It seems mainly compared with manually designed architectures, but don't compare with other NAS methods.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: very clear\nQuality: high\nNovelty: Okay. Training free and entropy idea is explored on 2D, but haven't explore on 3D.\nOriginality: I didn't see similar works before.",
            "summary_of_the_review": "This paper is clear. The story makes sense. The results of this paper is good, and the author shows that the proposed method is better than random search. However, it makes the paper less convincing if don't compare with other NAS based methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2261/Reviewer_SqC4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2261/Reviewer_SqC4"
        ]
    },
    {
        "id": "4yumpVQ0Y_",
        "original": null,
        "number": 3,
        "cdate": 1666831396093,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666831396093,
        "tmdate": 1666831396093,
        "tddate": null,
        "forum": "lj1Eb1OPeNw",
        "replyto": "lj1Eb1OPeNw",
        "invitation": "ICLR.cc/2023/Conference/Paper2261/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a spatio-temporal entropy score (STEntr-Score) with a refinement factor to handle the discrepancy of visual information in spatial and temporal dimensions, through dynamically leveraging the correlation between the feature map size and kernel size depth-wisely. Then the proposed entropy-based 3D CNNs (E3D family), can then be efficiently searched by maximizing the STEntr-Score under a given computational budget, via the existing evolutionary algorithm without training the network parameters. Comprehensive comparisons on the popular benchmark datasets show the advances of the proposed method. ",
            "strength_and_weaknesses": "The strengths of the paper are: \n+ This paper tried to exploit the important direction for the video action recognition, or generally the video intellengent analysis. \n+ The authors proposed to estimate correlation between feature map and kernel size in different depths, and furthermore, they proposed the Spatio-Temporal Entropy Score. \n+ The proposed STEntrScore is positively correlated with Top1 accuracy which indicates that the proposed spatio-temporal refinement can handle the discrepancy of visual information in spatial and temporal dimensions.\n+ The performance on the benchmark datasets is promising for the research in this area.\n+ Inference and training speed are appealing \n\nThe weaknesses of the paper are: \n- No limitation discussion in the paper \n- I would like to see some discussions on the affect of the temporal resolution on the performance, such as if we increase or decrease the number of frames we consider in the searching and final architecture, what the performance? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is presented clearly. Although the search method is not new, however, the consideration of the feature maps, kernel size in different depths and refined by using the proposed Spatio-Temporal Entropy Score shows promising experimental results. I think it will provide useful insights to the community. ",
            "summary_of_the_review": "Overall, I think this is a good paper which will have positive contributions to this field. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2261/Reviewer_RjKM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2261/Reviewer_RjKM"
        ]
    }
]