[
    {
        "id": "el8f7jLSvWt",
        "original": null,
        "number": 1,
        "cdate": 1666234207527,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666234207527,
        "tmdate": 1668712315966,
        "tddate": null,
        "forum": "ih0uFRFhaZZ",
        "replyto": "ih0uFRFhaZZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1891/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper attempts to perform unsupervised disentanglement with continual representation learning methods. Specifically, they designed a variational autoencoder (VAE) that exploits the relationships between datapoints based on their latent factors. The model learns the latent representations of data by applying a self-organizing mixture of spike-and-slab priors (which is topologically informed) to the VAE. The proposed method is validated on many disentanglement datasets (3DShapes, variants of MNIST, and celebA), where the proposed method showed superior disentanglement performance when compared to other continual learning algorithms.\n",
            "strength_and_weaknesses": "Strength\n- It is interesting to utilize sample-2-sample relationship for disentanglement, and it is interesting to see how it can be applied in the continual learning regime, where semantic factors can possibly be expanded. \n- The paper presented solid mathematical/theoretical contributions by formulating the variational inference process with Bayesian-SOM and spike-and-slab distribution. The presented method seems to have several unique advantages because of the selected priors.\n\nWeaknesses\n\n1. Major concern: experiments and methods for benchmarking aren't sufficient for demonstrating the strength of the method.\n- The proposed method seems to have relatively weak disentanglement performance when compared with previously existing methods. Specifically, [1] reported several models\u2019 MIG scores on 3Dshapes (the non-splitting version), where FactorVAE has an average of 0.3, AnnealedVAE has an average of 0.4. They also reported relatively high model variance, where a naive VAE can achieve a MIG score of as high as 0.8. It is understandable that the paper reported relatively low variance as each model is run 5 times and there might exist different settings for randomness. However, it is not convincing that the model actually disentangles well, as it does not outperform naive VAE by a large margin, and some other VAE methods that are designed for disentanglement study can easily outperform the proposed method.\n- The benchmarking models are selected with certain bias. Only model2 (beta-VAE by Burgess) and model4 (life-long disentanglement by Achille) are specifically designed for disentanglement (correct me if I am wrong). All other methods are continual learning methods. It is understandable to include many continual learning methods, as the proposed method is also based on continual learning techniques. However, as the method is motivated to address the disentanglement problem, it should consider more methods (e.g. VAEs), and especially state-of-the-art methods, that are specifically designed for disentanglement (e.g. FactorVAE, TC-VAE, DIP-VAE) to have a fair comparison.\n- Although the evaluation of disentanglement has been a long-standing problem [1, 2], using the mutual information I(z_{past}; factors_{new}) seems to be a particularly biased one as the proposed method is optimized on similar components.\n\n2. Related work related to disentanglement learning is lacking\n- There are many different variants of VAE that are designed for disentanglement that are not considered and mentioned. The original beta-VAE paper [3] isn\u2019t cited. [4-6] are VAEs that are mentioned above. [7-9] are more recent unsupervised methods that consider sparse coding or separatable latent factors.\n- When doing unsupervised disentanglement learning, it is also recommended to clearly demonstrate the assumptions [1].\n\n3. Motivation isn\u2019t strong enough\n- It isn\u2019t totally convincing to me why one would like to perform disentanglement under the continual learning setting. I think it would be beneficial if the authors could stress less about how it is the first work that evaluates the disentanglement scores for continually learned latents, but stress why it would be important instead.\n\n4. Minor comments:\n- The details of the Split-3DShapes dataset aren\u2019t clear. How many data points are within each subset? Why it is separated in this particular way?\n- How are the hyper-parameters selected? I did not find any details about the model selection procedure. What are the training/validation/testing sets? (also for the benchmark models)\n- I am not sure if the important details for reproducing the models are included. What is the model learning rate?\n- Does the network architecture apply to all benchmark models?\n\n\n[1] Locatello, Francesco, et al. \"Challenging common assumptions in the unsupervised learning of disentangled representations.\" international conference on machine learning. PMLR, 2019.\n\n[2] Mathieu, Emile, et al. \"Disentangling disentanglement in variational autoencoders.\" International Conference on Machine Learning. PMLR, 2019.\n\n[3] Higgins, Irina, et al. \"beta-vae: Learning basic visual concepts with a constrained variational framework.\" (2016).\n\n[4] Kim, Hyunjik, and Andriy Mnih. \"Disentangling by factorising.\" International Conference on Machine Learning. PMLR, 2018.\n\n[5] Chen, Ricky TQ, et al. \"Isolating sources of disentanglement in variational autoencoders.\" Advances in neural information processing systems 31 (2018).\n\n[6] Kumar, Abhishek, Prasanna Sattigeri, and Avinash Balakrishnan. \"Variational inference of disentangled latent concepts from unlabeled observations.\" arXiv preprint arXiv:1711.00848 (2017).\n\n[7] Rhodes, Travers, and Daniel Lee. \"Local Disentanglement in Variational Auto-Encoders Using Jacobian $ L_1 $ Regularization.\" Advances in Neural Information Processing Systems 34 (2021): 22708-22719.\n\n[8] Liu, Ran, et al. \"Drop, swap, and generate: A self-supervised approach for generating neural activity.\" Advances in Neural Information Processing Systems 34 (2021): 10587-10599.\n\n[9] \u200b\u200bHoran, Daniella, Eitan Richardson, and Yair Weiss. \"When Is Unsupervised Disentanglement Possible?.\" Advances in Neural Information Processing Systems 34 (2021): 5150-5161.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The originality of the work is relatively good, there are many interesting innovative components of the proposed methodology.\n\nThe quality is not satisfying enough, as there are concerning issues with experiments (see above).\n\nThe clarity could be further improved. Many details about model training are lacking.\n",
            "summary_of_the_review": "I suggest rejection for this paper. Although there are many interesting and innovative components of the proposed approach, the evaluation of the proposed method isn\u2019t convincing as many benchmark models that should be considered are not considered. The introduction (motivation) and the related work section could also use major editions to improve clarity. I am willing to increase the score for this paper if sufficient revision is made accordingly to address my major concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1891/Reviewer_ARpk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1891/Reviewer_ARpk"
        ]
    },
    {
        "id": "NRHULgPGhXu",
        "original": null,
        "number": 2,
        "cdate": 1666633258103,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633258103,
        "tmdate": 1666633258103,
        "tddate": null,
        "forum": "ih0uFRFhaZZ",
        "replyto": "ih0uFRFhaZZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1891/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to achieve continual, disentangled, and unsupervised representation learning where the representations can be reused and expanded upon newly arriving data. For this, the authors propose to model the relational structure of continuously arriving data based on their \"active semantic factors\", which allows for the discovery of new semantic factors while recycling (the old) shared ones. The two pillars of the methodology are (i) self-organizing maps (SOMs) to maintain a relational data structure and (ii) sparse spike variables to model the underlying active semantic factors in each mixture component. The method is shown to achieve outstanding continual disentanglement of latent semantic factors.",
            "strength_and_weaknesses": "### Strengths\n- It is a thoughtful and careful approach to designing the model architecture. Authors start with a well-defined problem, identify a problem in existing literature (the independence assumption) and propose a creative solution. The use of specific components is well-motivated, and the intuitions are verified through ablation studies. The loss formulation is principled and well-argued.\n- The paper is very well-written. The sentences are crystal clear and the ideas are communicated in a great level of detail. The transitions are also very smooth. I really enjoyed reading the work! \n\n### Weaknesses\n- The final loss term (11) is rather complicated, involving many moving parts. Although each term is individually motivated, ablation studies displaying their impact are needed.\n- Qualitative evaluation can be improved. The authors provide quantitative evidence that their method performs continual disentanglement, but it would be informative to dig deeper into the results shown in Figure 3. In particular, introducing novel factors (not limited to floor colour) in sequence would shed more light on the learning dynamics and make a more convincing argument that the model discovers, reuses, and expands latent semantic factors. Another extension (arguably out of the scope of the submission) would be to use the model as a generative replay module in existing continual learning methods and check if the disentangled representation provides benefits for downstream tasks like classification or semantic segmentation.\n\n\n### Questions\n- I don't immediately see why sparse coding enables the discovery of \"active semantic factors\" in an auto-encoder. In particular, why are these \"semantic factors\" instead of, e.g., \"active features\"? This could be elaborated more.\n- I'm not sure how exactly $\\alpha$'s are inferred for streaming data. Precisely, why do we maintain a set? Here, does each \"semantic factor\" correspond to one latent dimension? How does this connect to Figure 1?\n- Why do we need to address the fact that \"generative replay lacks mechanisms to constrain newly arrived data\"?\n- Does the method maintains a fixed number of SOM snapshots? How are they updated as learning proceeds?\n- If eq.8 is optimized perfectly, then would we still need to optimize eq.9? It seems to me that both objectives serve the same purpose? ",
            "clarity,_quality,_novelty_and_reproducibility": "### Quality\nThe submission is of high quality. The problem of continual unsupervised disentangled learning is well-defined and well-motivated. The architecture is derived in a principled manner and justified by an ablation study. The evaluation takes into account the most relevant competing methods. As mentioned before, the manuscript would benefit from another round of editing.\n\n### Novelty \nWhile the individual components of the proposed architecture are well established, the particular configuration is novel to the best of my knowledge. Shifting the focus towards unsupervised learning is a valuable contribution to the field of continual learning, dominated by supervised discriminative learning.\n\n### Reproducibility\nThe appendix contains details on generating the datasets, a blueprint of the network architecture, and values of the relevant hyperparameters. This information should be enough to reproduce the results. The authors promise to release the code, but the URL in the appendix is a placeholder.\n\n### Clarity\n- The paper is well structured and formatted, but grammatical mistakes, typos, and inconsistent punctuation make it occasionally hard to follow (see minor points). \n- What is meant by \"environment\" in the second paragraph?\n- \"The latent variable z from the k-th mixture component\" <--- Is this correct wording?\n\nMinor:\n- The keyword \"disentanglment\" should be corrected\n- The discussion on \"generative-replay\" in the first paragraph seems unrelated to the rest of it. \n- \"Expanding learned semantic factors, in part, *results* naturally from\"\n- \"While the common strategy of generative replay teaches a model what semantic factors to use on the replayed data\" <--- I'm not sure if the previous paragraphs clearly explain this.\n- \"learning of progress in continual unsupervised representation learning is relatively limited\" sounds strange.\n- Using the notation $p(z|w_k=1)$ in eq.1 might be better for consistency.\n- Shouldn't the first equation in eq.2 be $q(z,w|x) = q_\\phi(z|x)p_\\psi(w|z)$?\n- \"We follow the theory in (Gepperth & Pf\u00a8ulb, 2021)\" <--- This can be explained with a few words\n- Including more explanation in Fig2 caption would be better\n- Typos\n  - determie ---> determine\n  - \"In existing works, reusing semantic factors **are** mainly attempted by a teacher-student like approach (...)\"\n  - \"To overcome this, we argue that the model needs to learn **two critical knowledge** (...)\"\n  - \"toplogoically\" (page 3), \n  - \"stick-and-slab\" (page 4)\n  - \"CUODS\" (page 8)\n  - \"we presents\" (page 16)\n ",
            "summary_of_the_review": "The paper presents a novel, well-motivated method that tackles an important and under-explored aspect of continual learning. While the qualitative evaluation could be more thorough, the submission in the current form would already be a valid contribution to the continual learning and representation learning community. I recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1891/Reviewer_z1ce"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1891/Reviewer_z1ce"
        ]
    },
    {
        "id": "tQ9FiGjgNfb",
        "original": null,
        "number": 3,
        "cdate": 1666679502185,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679502185,
        "tmdate": 1666679502185,
        "tddate": null,
        "forum": "ih0uFRFhaZZ",
        "replyto": "ih0uFRFhaZZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1891/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the author shows that an overlooked key ingredient to continual unsupervised learning of representations is to exploit the relational structure of data based on their underlying active semantic factors. This paper proposes a novel VAE with self-organizing spike-and-slab mixtures called CUDOS. ",
            "strength_and_weaknesses": "Strength:\n\n- The CUDOS can treat continually-arrived data independently, without knowing how they are related based on the underlying semantic factors. \n\n- This paper has conducted a large number of experiments to testify to the disentangling ability of CUDOS.\n\n- This work mitigates the catastrophic forgetting from a novel perspective of generative replays from SOM-mixture. \n\n\nWeakness:\n\n- More visualization like TSNE can further clarify the disentangling ability of CUDOS.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "I am satisfied with the clarity, quality, and novelty.",
            "summary_of_the_review": "This paper presents CUDOS, a novel VAE with self-organizing spike-and-slab mixtures. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1891/Reviewer_ZXCc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1891/Reviewer_ZXCc"
        ]
    },
    {
        "id": "Hu0xThKUm2",
        "original": null,
        "number": 4,
        "cdate": 1667064357121,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667064357121,
        "tmdate": 1667064357121,
        "tddate": null,
        "forum": "ih0uFRFhaZZ",
        "replyto": "ih0uFRFhaZZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1891/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for disentangling semantic factors (a.k.a. generative factors) in a continual setting where the observed data from different environments come in a stream. This paper studies continual disentanglement in unsupervised learning, an under-explored area. This paper proposes principled solutions to reusing, expanding, and continually disentangling latent dimensions for each data environment coming in a continual fashion. The overall method is called the Continual Unsupervised Disentangling of self-Organizing representations (CUDOS). CUDOS learns the relationships among active latent dimensions for each data environment and reuses appropriate latent dimensions while training on new data environments. Latent space is modeled as a mixture of self-organizing maps (SOM) where each mixture component is modeled as a spike-and-slab distribution to encourage sparse representation in latent space. This paper proposes separate objective functions for each desideratum of continual disentanglement. Empirical studies show that CUDOS performs better than existing methods on various datasets w.r.t. multiple metrics.",
            "strength_and_weaknesses": "Strengths:\n------------\n+ **Motivation:** Each of the issues related to continual disentanglement: catastrophic forgetting, expanding, and reusing latent dimensions are dealt with separately using three objective functions that are added to the traditional ELBO objective in a Variational Auto Encoder. Such isolation gives more control over the latent space to achieve better disentanglement.\n+ **Methodology:** When training on new data environment, CUDOS expands the latent dimension by using new semantic factors that are not entangled with existing active semantic factors (Equation 7). Such expansion is a good addition to the CUDOS methodology to improve the disentanglement score in continual learning.\n+ **Experiments:** Experimental results are promising in confirming to the theoretical claims of the paper.\n\nWeaknesses:\n----------------\n- **Readability:** The paper is challenging to understand in some places. Especially, Sub-Section 3.2 could be more detailed. For example, does the line\u00a0\u2018Generative replay lacks mechanisms to constrain newly arrived data.\u2019 mean CUDOS wants to generate a replay buffer using old semantic factors? Also, some preliminaries on existing work, such as how SOMs are used in VAEs, would make the paper more self-contained. \n- **Methodology:** Why is it essential to expand latent dimensions while training in new environments? Why cannot existing/already active semantic factors capture new variations in the data? For example, in Figure 3.b and page 7 of the results section, while I agree that CUDOS uses previous inactive dimensions for learning new semantic factors, naive VAE also generate variations w.r.t. one semantic factor while keeping other semantic factors fixed. That is, naive VAE and CUDOS both produce the desired behavior of disentangling semantic factors. What advantage does CUDOS bring by using previously inactive dimensions?\n- **Methodology:** Are semantic factors continually expanded when training in a new data environment? What if active semantic factors of an environment describe a new data environment? Is it guaranteed that CUDOS will not learn/expand the existing semantic factors? How does it affect the training procedure? If it is assumed that a new data environment leads to expanding latent space, the authors should clarify it in the paper and its effect on the training procedure.\n- **Experiments:** In the experiments, the reconstruction loss of CUDOS is not the best compared to baselines. Since good generation is an essential aspect of any generative model, this could be a negative aspect of CUDOS. Is this because of multiple objective functions in Equation 11? What is the effect of weighting hyperparameters of Equation 11 on the final output? \n- A typo: in the results of Moving MNIST & Fashion-MNIST, it should be Fig. 5 instead of Fig. 4.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper makes some novel contributions in continual disentanglement. Readability of the paper could be improved. Code is not available to reproduce the results.",
            "summary_of_the_review": "This paper addresses an important and under-explored idea of unsupervised continual disentanglement of the data coming from different data environments. As discussed in the weaknesses, it is unclear what real-world advantage CUDOS brings by expanding latent dimensions (which is a major contribution of CUDOS) for each data environment. I feel experimental results are sufficient to verify the theoretical claims of the paper. Considering the novel contributions, I provide my rating as \u2018marginally above the acceptance threshold\u2019.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1891/Reviewer_NqT4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1891/Reviewer_NqT4"
        ]
    }
]