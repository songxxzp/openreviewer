[
    {
        "id": "LVigsmflN0",
        "original": null,
        "number": 1,
        "cdate": 1665621275030,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665621275030,
        "tmdate": 1665621275030,
        "tddate": null,
        "forum": "9MO7bjoAfIA",
        "replyto": "9MO7bjoAfIA",
        "invitation": "ICLR.cc/2023/Conference/Paper5517/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a simple new ensemble method for protecting data from someone training on it.  The work shows the empirical success of their method on multiple datasets and compared to strong baselines like adversarial poisons.",
            "strength_and_weaknesses": "The main strength of the paper is in the empirical success compared to strong baselines and defenses.\n\nIn Figure 3, the peak test accuracy of the proposed method is just as high as for adversarial poisons.  Is there a way to reduce this peak?  Otherwise, early stopping might be a highly effective defense.\n\nIt might be worth considering the AR poisons of \u201cAutoregressive Perturbations for Data Poisoning\u201d as they are a recent stronger baseline than adversarial poisons.\n\nHow did you tune the attack hyperparameters (e.g. step size) of adversarial poisons since you are using a smaller perturbation radius than they used in the experiments in their paper?\n\nIn general, the writing is a bit messy and hard to follow, but the overall structure and content is good, and I was still able to understand the writing nonetheless.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear enough, although the writing is hard to follow at times.  I did not notice some of the hyperparameters that are needed to re-produce the experiments, but maybe they were already there and I just did not see them.  Crucially, the hyperparameters for the competitor are important since this work does not use the same constraint space as the paper for adversarial poisons.",
            "summary_of_the_review": "In general, while the writing could use some work and there is an additional baseline that would be nice to include, I currently lean towards acceptance.  The paper provides significant empirical contribution and extensive testing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5517/Reviewer_FTbn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5517/Reviewer_FTbn"
        ]
    },
    {
        "id": "ts3sCuhF8x9",
        "original": null,
        "number": 2,
        "cdate": 1666613414748,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613414748,
        "tmdate": 1669738127442,
        "tddate": null,
        "forum": "9MO7bjoAfIA",
        "replyto": "9MO7bjoAfIA",
        "invitation": "ICLR.cc/2023/Conference/Paper5517/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is about perturbing images such that it is not possible to use them for training a model which has good performance. The authors propose to build upon existing works which use adversarial perturbations to corrupt the training set. As contributions, this work proposes to ensemble the perturbation gradients coming different model snapshots of a same training deep network. The perturbation gradients come from a feature alignment loss such that perturbed samples when passed to the DNN result in feature maps which are close to the mean feature of other classes.",
            "strength_and_weaknesses": "Strength:\n1) The paper is clear and very well written.\n2) Lots of experiments to support the paper claims with interesting ablation studies.\n\nWeakness and suggestions:\n1) Adversarial training easily circumvents the data corruption. The proposed method suffers from the same drawback of other existing methods.\n2) L2 perturbation(not only L-inf) could complete the study. Maybe mixed perturbations could be helpful against adversarial training.\n3) What about exponential moving average as a comparison to the ensemble of snapshots? It would be a way to avoid storing all these snapshots.\n4) Typos in the abstract and other places for the perturbation bounds: with l_inf=8 instead of L_inf=8/255 for examples.",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty lies in using a feature alignment loss and an ensemble of snapshots instead of a single DNN to attack the images. The paper is well written and clear.",
            "summary_of_the_review": "The paper is enjoyable to read but it only marginally improves over existing methods both technically and experimentally. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5517/Reviewer_bFJ9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5517/Reviewer_bFJ9"
        ]
    },
    {
        "id": "xrmQ-Pc3tjv",
        "original": null,
        "number": 3,
        "cdate": 1666675357431,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675357431,
        "tmdate": 1669923275966,
        "tddate": null,
        "forum": "9MO7bjoAfIA",
        "replyto": "9MO7bjoAfIA",
        "invitation": "ICLR.cc/2023/Conference/Paper5517/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed to utilize intermediate checkpoints from a single training process to protect data from unauthorized use. It also proposed a novel feature alignment (FA) technique that improves the accuracy of its proposed self-ensemble protection (SEP) method. FA uses an existing theory called neural collapse to align the last layer feature of a sample for each checkpoint to the feature mean of the target class samples. It has minor improvement in performance. But the method is simple which will make it useful. ",
            "strength_and_weaknesses": "**Strength: **\n\n- The objective is straightforward and the problem of data protection is exciting and vital. \n\n- Improving performance with intermediate checkpoints from a single training is an excellent idea to save time and resources. \n\n**Weaknesses:**\n\n- The feature alignment section could be explained a little better. Especially the neural collapse theory should be explained a little more since it is the primary tool used to develop FA.\n\n- The experiments are limited. More results on different datasets comparing different models should have been shown. Only one model and dataset are tested while the proposed method is compared with the previous methods in Table 1.\n\n- It is unclear what model and dataset are used to produce Table 3.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Sec 3 is not well explained, it could be rewritten for better understanding. The quality is acceptable. They have done minimal experiments to support their claims. It needs more experiments. Their self-ensemble idea is interesting, but it is mainly an extension of previous work.",
            "summary_of_the_review": "The paper attempts to solve a compelling problem and proposes a simple solution. However, it could not support empirically enough. As mentioned in the weakness section, it needs a little more work and rewriting.\n\n***\nThe authors have clarified the raised doubts, updating the ratings accordingly. \n***",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5517/Reviewer_JA31"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5517/Reviewer_JA31"
        ]
    }
]