[
    {
        "id": "74J8euTozOS",
        "original": null,
        "number": 1,
        "cdate": 1666556459685,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556459685,
        "tmdate": 1666862532349,
        "tddate": null,
        "forum": "2_B-eiVbgBs",
        "replyto": "2_B-eiVbgBs",
        "invitation": "ICLR.cc/2023/Conference/Paper5376/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a novel loss function which is tailored specifically for learning A*-heuristics. $L$* captures (1) heuristic monotonicity and (2) optimal A*-search efficiency in terms of #node expansions. The authors motivates theoretically why $L$* overcome known limitations to L_p loss in terms of A* heuristics, and prove relevant theorems of $L$* to this end.\n\n$L$* is evaluated on 3 difficult problems (Sokoban, mazes with teleports and Boxoban levels). $L$* is shown to consistently outperform $L_2$-loss as well as other state-of-the-art methods. Good generalization to problem instances with increased complexity not trained on is shown. Finally, a novel application of curriculum learning for A*-heuristics is demonstrated to be highly effective, both in terms of performance and the small number of SGD-steps necessary. Learning $L$* from random initialization and by only training on solved problems for every epoch, show an amazing performance increase at a small fraction of the cost in SGD-steps, in comparison with what is necessary for competing methods to reach similar performance.",
            "strength_and_weaknesses": "Strengths\n==========\n\nThe theoretical investigation on $L_2$ limitations and the desired and acquired properties of $L$*\n\nThe evaluation is very strong with respect to the selected problem domains.\n* $L_2$-loss is trained using the same training data as $L$*-loss (i.e. off the optimal path as well).\n* The use of an automatic planning method which is optimal (S(ym)BA*) as baseline.\n* Comparison with MCTS and model free methods DRC & I2A.\n* Learning with curriculum learning from random initiation with only training data produced by the method itself.\n\nLearning $L$* does not presuppose a distinction between open and closed sets. That is, the training data can be generated using other methods than search.\n\n\nWeaknesses\n==========\n\nSome parts of the paper, including equations, are not clear.\n* I can see no \\cdot but I interpret the $leq$ expression as an implicit indicator function (equivalent to the use of an Iverson bracket). I do however strongly suggest that you make this clear in the equation. E.g. by introducing an Iverson bracket explicitly, or by using the indicator function. Preferably something that is easy to *swap* out for $L_l(x)$.\n* \"During training, we iterate over many samples of A* explorations which enlarges the scope of L \u2217 .\" - What does this mean? What is \"many samples of A* explorations\" exactly, is it over many different problem instances?\n* Theorem 2 and proof: \\bar should be over \\Gamma, not h_\\theta\n* \"the Iverson bracket is replaced by a logistics loss function $L_l(x) = $ ...\": This should be clarified, see the first bullet point.\n* \"could solve 96% of levels of unfiltered boxoban mazes after seven epochs.\": At epoch 8 then, which is not shown in Table 3? Please include it in Table 3.\n\n\nMinor things\n==========\n\n* \"The zero $L_2$ does not guarantee the optimal efficiency of A*\" -> \"$L_2$ = 0 does not ...\"\n* We propose a $L$* loss function\" -> \"We propose the $L$* loss function\"\n* A*-algorithm: \"go to 4\" -> \"go to 5\"?\n* \"Many prior art on\" -> \"Many prior works on\"\n* Footnote 3: $L_1$ and $L_2$ seem to be reversed in the second part of the sentence?\n* \"the extension to a set of plan is trivial\": Would be clearer for readers if it is fleshed out a bit more.\n* 4) \"the problems identified above in Section 2.3\": Be more specific here, e.g. what problems precisely (summarize).\n* \"DRC (3,3) allowed 1G iterations of SGD\" and other \"allowed\" in the same sentence: \"allowed\" -> \"used\"?\n* Table 3: Using a graph instead (i.e. $L_2$ and $L$* can use the same color for 3000, but one of them use line plot with '+' or similar) would give a better idea of rate of improvement, if shown as a function of epochs.",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned under Strength And Weaknesses, there are some clarity issues. However, the gist of the paper is clear. Most importantly, the theoretical and the empirical results (and methodology) are clear.\nThe quality is good (great if listed issues are fixed), novelty is very high and reproducibility is high.",
            "summary_of_the_review": "The paper propose a simple but highly effective approach to learning good heuristics efficiently for A*-search in complex planning domains. The theoretical motivation is strong and well founded, the contributions significant, and the results well supported by a thorough empirical evaluation. If the demonstrated performance generalizes to other problem domains, then it is highly likely that the paper will have a high general impact.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5376/Reviewer_ytHz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5376/Reviewer_ytHz"
        ]
    },
    {
        "id": "1zckxwmQqF",
        "original": null,
        "number": 2,
        "cdate": 1666610647202,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666610647202,
        "tmdate": 1670766440072,
        "tddate": null,
        "forum": "2_B-eiVbgBs",
        "replyto": "2_B-eiVbgBs",
        "invitation": "ICLR.cc/2023/Conference/Paper5376/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper deals with the problem of learning heuristic function for A*, a popular heuristic search algorithm that can guarantee optimality of solutions and expands the minimal number of states. The paper proposes L*, a loss function tailored for A* which minimizes an upper bound on the number of expanded states. The paper motivates L* by highlighting the problem of using L_2 loss function as done in previous work and present experimental analysis that shows that heuristic functions learned by L* have better coverage (i.e., number of solved instances in time limit) compared to heuristic learned by L_2 loss and by other selected baselines.",
            "strength_and_weaknesses": "Strengths:\n* Approach is well motivated, highlighting problems with using L_2\n* Experiments show improvement in coverage compared to the baselines\n\nWeaknesses:\n* Missing literature and baselines: there are many learning-based approaches for heuristic search that are not based on L_2 and are not cited in the paper [e.g., 1-4]. [1-2] have specifically focused on Sokoban. [3][4] are older works that avoid problems with L_2 by focusing on learning to rank.\n* Optimality: the paper seems to focus on A* and is motivated by the \"false sense of optimality\" in L2, however the proposed approach is, to my understanding, not optimal. Specifically:\n    - The theoretical optimality guarantees (Section 3.1) only hold for instances in the training set (i.e., to guarantee optimality in general, we would have to train on all possible instances).\n     - L* is not differentiable and there is no guarantee that training find optimal solution (even with respect to the training set).\n* Experimental results not sufficient to evaluate the proposed approach: Despite the focus on optimality, the experiments only focus on coverage and not on solution cost. Some of the baselines are not optimal, including Mercury 14, Stone soup, and the RL approach, as well as the proposed approach (to my understanding). It is therefore important to analyze what is the (average) obtained solution quality by each of the methods.\nGiven the lack of optimality and the comparison to non-optimal baselines, it would also be interesting to study the performance of the proposed L* heuristic function in greedy best-first search.\n* Proofs are said to be in the supplementary material, however I could not find such material (neither at the end of the document nor uploaded to OpenReview).\n\n\n[1] Orseau, Laurent, and Levi HS Lelis. \"Policy-guided heuristic search with guarantees.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 14. 2021.\n\n[2] Feng, Dieqiao, Carla P. Gomes, and Bart Selman. \"The Remarkable Effectiveness of Combining Policy and Value Networks in A*-based Deep RL for AI Planning.\" (2021).\n\n[3] Garrett, Caelan Reed, Leslie Pack Kaelbling, and Tom\u00e1s Lozano-P\u00e9rez. \"Learning to rank for synthesizing planning heuristics.\" Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence. 2016.\n\n[4] Xu, Yuehua, Alan Fern, and Sungwook Yoon. \"Learning Linear Ranking Functions for Beam Search with Application to Planning.\" Journal of Machine Learning Research 10.7 (2009).",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is mostly clear and easy to follow.\n\n- The approach seems mostly novel, however there is missing literature (including techniques that do not rely on L_2 loss) that need to be cited and the difference and similarities to proposed approach should be highlighted. Details provided above.\n\n- Reproducibility: the work seems reproducible and the author state that they will make \"Scripts reproducing our\nexperiments together with mazes and solutions\" available upon acceptance. However, I was not able to find the supplementary material that the paper mentioned should include the proofs and additional discussion.\n",
            "summary_of_the_review": "Overall I think it is an interesting research direction and a well motivated approach that demonstrates nice experimental results. However, the paper is missing relevant literature, seem to have a conflict between the extensive focus on optimality and the actual theoretical guarantees the method provides, and lack of experimental results to experimentally support claims on optimality (i.e., analysis solution quality).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5376/Reviewer_SoyE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5376/Reviewer_SoyE"
        ]
    },
    {
        "id": "fALl8uWEOio",
        "original": null,
        "number": 3,
        "cdate": 1666682036929,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682036929,
        "tmdate": 1668446783540,
        "tddate": null,
        "forum": "2_B-eiVbgBs",
        "replyto": "2_B-eiVbgBs",
        "invitation": "ICLR.cc/2023/Conference/Paper5376/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new loss, called L* as opposed to L2 loss for training a heuristic to be used in A* search.\n",
            "strength_and_weaknesses": "I like the general direction of this paper. I could see how since L2 is not directly optimizing the main objective of being a good heuristic, a different objective could do better. \nHowever, I think that the framing does not compare L* to the current state of the art. \nIn particular, one main paper that was not cited is DeepCubeA. In that paper, they use the L2 loss, but use value iteration and curriculum learning to get around the problems laid out with L2 in this paper. So I would really like to see comparisons to DeepCubeA, where the L2 loss is swapped out with L*. \n",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper was well-written. To my knowledge, this is the first paper looking at different losses than L2 for heuristic search, so it is novel in that respect. It should be relatively reproducible. ",
            "summary_of_the_review": "In summary, I am not convinced that L2 loss is bad. In particular, DeepCubeA was able to solve Sokoban and Rubik's cube using L2 loss. To be really convincing, I would like to see a problem where DeepCubeA fails because of the L2 loss, but the L* loss is able to solve the problem. \n\n_____\n\nAfter reading the comment I have bumped my score a bit but still recommend reject because I would like to see direct comparisons with DeepCubeA. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5376/Reviewer_8chY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5376/Reviewer_8chY"
        ]
    },
    {
        "id": "Slfr-RLLYDY",
        "original": null,
        "number": 4,
        "cdate": 1666756950616,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666756950616,
        "tmdate": 1670874727795,
        "tddate": null,
        "forum": "2_B-eiVbgBs",
        "replyto": "2_B-eiVbgBs",
        "invitation": "ICLR.cc/2023/Conference/Paper5376/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper introduces a novel loss function L* for learning heuristic functions that attempts to shrink the size of the the A* search tree. While previous methods for learning heuristic functions relied on minimizing the L2 loss, this paper argues that L2 loss doesn't necessarily reduce the search tree size. Empirical results of Sokoban-like puzzles show that L* might indeed reduce the A* search effort.  ",
            "strength_and_weaknesses": "Strength\n\nThis paper tackles one of the key weaknesses of learning heuristic functions to guide the A* search, which is the lack of a proper loss function. The L* loss seems to be a good step in the direction of having a loss function that is more correlated with the A* search effort.  \n\nWeaknesses\n\nWhile the problem the paper tackles is important and the idea for solving this problem is promising, the paper has several important weaknesses that might prevent it from being published at this point. \n\n1. The paper presents two types empirical evaluations. In the first, one uses classical planners to generate a training set from which a heuristic function is learned. In the second, the algorithm learns from the problems it is able to solve. The first setting is hardly interesting at all because it assumes the existence of a planner that is able to solve the problems. The second setting is the interesting one because it is general and it doesn't assume prior knowledge or a system for generating training data. I would have preferred an extensive evaluation on the second setting than the current mix of evaluations. \n2. The evaluation is somewhat week because the system is evaluated only on grid-based puzzles (Sokoban and Maze with teleports). Instead of showing results on the setting where classical planners generate a training set, I would have preferred to see results on more domains. \n3. The comparison of optimal classical planners with A* with a learned heuristic function is problematic as the planners are finding optimal solutions, while A* is finding suboptimal ones. Even if the solution costs are near optimal, the problems can be substantially easier if you lift the optimality requirement. MCTS and model-free methods aren't good baselines as they tend to be weak. MCTS suffers when planning on long horizons such as those of Sokoban (see [2]). Model-free methods are in disadvantage as they don't use the model to search, so one should expect that they perform rather poorly compared to search algorithms. \n4. A more fair comparison would be with satisficing planners, which don't have the guarantee of finding optimal solutions. Another baseline that should be included is WA* with a learned heuristic function with the L2 loss. WA* is surprisingly effective in the learning setting (see [5]). Levin tree search (LTS) [1][2] uses a policy to guide its search and it defines a loss function that is an upper bound on the size of the tree. One can then learn a policy that minimizes this loss. This is exactly what is done in this paper, but for a policy. It is natural to wonder how A* with the L* loss compares to LTS.   \n5. The paper misses important citations. Whenever talking about optimality of A*, the paper seems to be referring to [3]. The procedure described in Section 5.3 is called Bootstrap [4].\n\nReferences\n\n[1] Orseau, L.; Lelis, L.; Lattimore, T.; and Weber, T. 2018. Single-Agent Policy Tree Search With Guarantees. In Advances in Neural Information Processing Systems 31, 3201\u20133211. Curran Associates, Inc.\n\n[2] Orseau, Laurent and Levi H. S. Lelis. \u201cPolicy-Guided Heuristic Search with Guarantees.\u201d AAAI (2021).\n\n[3] Rina Dechter and Judea Pearl. 1985. Generalized best-first search strategies and the optimality of A*. J. ACM 32, 3 (July 1985), 505\u2013536. https://doi.org/10.1145/3828.3830 \n3.  \n\n[4] Arfaee, Shahab & Zilles, Sandra & Holte, Robert. (2011). Learning heuristic functions for large state spaces. Artif. Intell.. 175. 2075-2098. 10.1016/j.artint.2011.08.001. \n\n[5] Solving the Rubik's Cube with Deep Reinforcement Learning and Search. Forest Agostinelli*, Stephen McAleer*, Alexander Shmakov*, Pierre Baldi. Nature Machine Intelligence, Volume 1, 2019\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nThe paper is mostly easy to understand. However, I do miss a more formal treatment with respect to A*'s optimality. I don't understand what it means to have a \"false sense of optimality\", for some definition of optimality. \n\nQuality\n\nWhile the idea is promising and interesting, the execution is lacking (see my comments above).\n\nNovelty\n\nThe loss function is novel, to the best of my knowledge. \n\nReproducibility\n\nThe idea is simple enough (which is something very positive!) that should be easy for someone to reproduce the results of this paper. ",
            "summary_of_the_review": "The paper introduces a novel and interesting loss function for learning heuristic functions for guiding the A* search. The results are somewhat preliminary and that is why I recommend rejection at this point. I do strongly encourage the authors to resubmit the paper if it is indeed rejected from this conference. The issues related to writing, lack of formalism, and missing important citations should all be easy to fix. The evaluation is the problem. The evaluation will be much stronger once L* is tested on more problem domains and compared with proper baselines. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5376/Reviewer_oNqW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5376/Reviewer_oNqW"
        ]
    }
]