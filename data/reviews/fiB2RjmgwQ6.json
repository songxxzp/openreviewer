[
    {
        "id": "44sTgjJOTzY",
        "original": null,
        "number": 1,
        "cdate": 1666126787349,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666126787349,
        "tmdate": 1666142915097,
        "tddate": null,
        "forum": "fiB2RjmgwQ6",
        "replyto": "fiB2RjmgwQ6",
        "invitation": "ICLR.cc/2023/Conference/Paper675/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper points out that there are two prominent issues in complex multi-speaker separation results: 1) There exist some noisy voice pieces belonging to other speakers; 2) Part of the target speech is missing. A Filter-Recovery Network (FRNet) is hence proposed to solve these problems. The authors also emphasize that their single model can separate voices for a variable number of speakers, which is simply achieved by proper training data setting (i.e., including mixtures with a different number of speakers). Overall, the model design is quite interesting with a good performance improvement.",
            "strength_and_weaknesses": "This paper first points out that there are two issues in complex multi-speaker separation and solve these two issues with the proposed Filter-Recovery Network (FRNet). The model structure of FRNet is reasonable, but there are still some unclear parts that need to be addressed:\n\n1. The loss function used to train the proposed model is only based on e.q. 13 without any constraints, how do you guarantee the Filter Net and Recovery Net work properly as their original goals (removing noise voices and extracting missing voices, respectively)?\n2. Some audio samples should be released in the supplementary material, especially the separated result at each stage: i.e., a) after Basic Audio-Visual Speech Separator, b) after Filter Net and c) after Recovery Network. This can also somewhat solve my first concern.\n3. The inputs of the Filter Net are Vi and Mi, why the mixture X doesn\u2019t need to be fed into the model?\n4. For the experimental parts, how do you calculate the scores (SDR and PESQ) if there are multiple output-separated speeches?\n\nThe proposed method to separate voices for a variable number of speakers with a single model is not very novel, to my understanding, it is mainly based on a proper setting of the training set.\n\ncomparison\nTypo:\n1.\tPage 2, Specifically, Specifically, -> Specifically.\n2.\tPage 6, k-th speaker -> i-th speaker.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper presents the problem and proposed solution quite clearly so that readers may be able to reproduce the results. The proposed model is simple with novelty. ",
            "summary_of_the_review": "The prosed method is motivated by two observed problems in the multi-speaker separation. FRNet can also be used as a post-processing module to improve the performance of different audio-visual separation frameworks is a plus. Overall, the paper is well written with good experimental results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper675/Reviewer_t4UV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper675/Reviewer_t4UV"
        ]
    },
    {
        "id": "bQcHA5Ntfgp",
        "original": null,
        "number": 2,
        "cdate": 1666373592948,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666373592948,
        "tmdate": 1669664441036,
        "tddate": null,
        "forum": "fiB2RjmgwQ6",
        "replyto": "fiB2RjmgwQ6",
        "invitation": "ICLR.cc/2023/Conference/Paper675/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work presents an audio-visual speaker separation method suitable for multi-speaker scenarios. An audio-visual speaker separator is first introduced which is followed by a filter-recovery network which aims to further improve the separation quality. The latter can also be combined with other separator networks and it is shown that it can improve their performance as well. The proposed approach is tested on standard benchmarks and it  outperforms other state-of-the-art methods.  ",
            "strength_and_weaknesses": "Strengths\n\nSeveral results and an ablation study is presented which convincingly show that the proposed approach outperforms other existing approaches.\n\nThe proposed FRNet can improve the performance of other speech separation approaches and this is a nice contribution.\n\nWeaknesses\n\nFirst of all, writing can be improved. Although the text is understandable there are several errors/typos. More details in the next section.\n\nThe impact of facenet/lipnet is missing in the ablation study. It would be good to show how important each of them is. Since most of the information is contained in the lip movements the contribution of facenet might be small.\n\nThe authors emphasise that one of the main contributions is that the proposed method can separate mixtures with a variable number of speakers simultaneously during training. Isn\u2019t this something the existing approaches can already do? And also the number of speakers needs to be known in advance. It\u2019s not clear why this is an important contribution.\n\nIt is not explained why the authors chose a two-step approach instead of an end-to-end approach, i.e., why not integrating the filter and recovery networks in the speech separation model and optimise them jointly?\n\nThe proposed audio-visual speech separation network (without the FRNet) is very similar to other existing audio-visual approaches. Why is this model better? This is also not clear.\n\nSome details are not explained, e.g. the lambda coefficient in eq. 13 is set to 0.5, how is this value chosen? Why is training performed for 19 epochs? Why 2.55 seconds are used for training? It seems these numbers have been chosen via an optimisation stage but it is not explained how this was performed.\n",
            "clarity,_quality,_novelty_and_reproducibility": "It is easy to follow the paper but proofreading is needed. A non-exhaustive list of typos from the introduction  is the following (but can be found in all sections):\nFirstly, Filter module -> Firstly, the filter module\nthe Recovery module use -> uses\nmost of works -> most works\nSpecifically, Specifically -> specifically\n\nFig. 2 is a bit confusing. It\u2019s only understandable after reading the text. It would be better if all the necessary information to fully understand it is contained in the figure or caption.\n\nThe main novelty is the introduction of the FRnet. It is shown that if combined with other existing speech separation networks leads to improved performance.\n\nIt is not possible to reproduce all the results without help from the authors. The test sets are generated by random sampling.\n",
            "summary_of_the_review": "Overall, this is an interesting contribution but there are several weakness as explained above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper675/Reviewer_VNdU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper675/Reviewer_VNdU"
        ]
    },
    {
        "id": "go9iHFRbnm7",
        "original": null,
        "number": 3,
        "cdate": 1666559557370,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666559557370,
        "tmdate": 1669962036616,
        "tddate": null,
        "forum": "fiB2RjmgwQ6",
        "replyto": "fiB2RjmgwQ6",
        "invitation": "ICLR.cc/2023/Conference/Paper675/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies masking-based audio-visual source separation, which predicts a complex spectral mask for the audio mixture for each speaker conditioning on the mixture speech and the video of the target speaker. The authors proposed BFRNet, which is composed of a audio-visual source separation model that predicts a mask for each speaker, and a filter-and-recovery network (FRNet) that refines predicted masks to a) remove non-target residual speech and b) to recover target speech removed from the initially predicted mask.",
            "strength_and_weaknesses": "Strengths\n\n- BFRNet yields better performance on VoxCeleb2, LRS2, LRS3 compared to prior works\n- The mask refinement module, FRNet, is complementary to other masked-based audio-visual speech separation models according to the experiments in Table 2 and 3.\n- Ablation studies confirm that both the filter and the recovery module contribute to the improvement. It also shows video facilitates the filter network, and using the cleaned mask to the recovery module improves.\n\nWeaknesses\n- While the empirical results are strong, this paper can be improved by providing more explanations of why the FRNet further improves the base audio-visual speech separation module. \n  - Why does the filter network remove the non-target speakers\u2019 voice better than the base separation module? The filter network only takes predicted mask and video features as the input. It does not even take the mixture audio as the input. It is surprising that it can tell where the noise is by just looking at a mask.\n  - Similarly, why does the recovery network learn what to keep better than the base audio-visual source separation network?\n  - It is possible that adding FRNet improves because now the entire model is bigger than the one without. To justify the FRNet carries benefits other than increasing the model capacity, the authors are recommended to compare with a stronger base model (e.g., Basic Audio-Visual Speech Separator) that have a deeper encoder/decoder where the parameter counts match roughly that of the BFRNet.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Notation can be improved\n  - The authors describe that \u201c[we] feed $V_i$ and $Au_i$ to AV-Fusion module to obtain a[n] enhanced feature.\u201d Why is the audio feature $Au_i$ speaker dependent? According to the diagram it is the output from the encoder that takes mixture spectrogram X as input.\n  - In Sec 3.3 Recovery Net, why are M_{1, \\cdots, S} referred to as \u201ccoarse\u201d masks? Do they have different temporal or frequency resolution?\n  - The paper can use more proofreading. There are still some typos.\n- Experiments are good but can be improved as suggested in the Weakness section.\n- The idea of refining masks for source separation is novel\n- The proposed method appears to easy to reproduce\n\n",
            "summary_of_the_review": "Experimental results are strong compared to the prior work. The main novelty is a module for mask refinement, which can improve several existing audio-visual speech separation models as demonstrated by the authors. More controlled experiments are required to justify the gain does not simply result from the increase in the number of parameters.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper675/Reviewer_X56m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper675/Reviewer_X56m"
        ]
    }
]