[
    {
        "id": "VDGdPGpskq",
        "original": null,
        "number": 1,
        "cdate": 1666536207495,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666536207495,
        "tmdate": 1666536207495,
        "tddate": null,
        "forum": "gfPUokHsW-",
        "replyto": "gfPUokHsW-",
        "invitation": "ICLR.cc/2023/Conference/Paper1166/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper focuses on the bias in continual learning. According to the bias, this paper designs a novel method, named learning without prejudices (LwP) to discourage malignant forgetting and encourage benign forgetting. The main contributions can be summarized as:\n\n(a) Presenting a novel framework, termed \"continual unbiased learning\", continual unbiased learning benchmarks and an evaluation protocol.\n\n(b) Categorizing the forgetting into malignant forgetting and benign forgetting.\n\n(c) Proposing a novel method, Learning without Prejudices (LwP).",
            "strength_and_weaknesses": "Strengths:\n\n(a) This paper is well-written and easy to read.\n\n(b) The phenomenon found in this work is easy to follow.\n\n(c) I appreciate the method proposed in this paper for its motivations and great performance in extensive experiments.\n\nWeaknesses:\n\n(a) Please describe BYOL in details.\n\n(b) The proposed method splits the classifier into $[f^a,f^b,f_{{L}}]$. Why training $f^a$ by GANg can address malignant forgetting? Why training $f^b$ by BYOL contrastive learning can encourage benign forgetting? \n\n(c) In the final step of Algorithm 1, $f^a$ and $f^b$ will be retrained?\n\n(d) In practice, how to determine $l$ and $L$ in Section 4.2?",
            "clarity,_quality,_novelty_and_reproducibility": "Good quality.\nGood clarity.\nNice originality.",
            "summary_of_the_review": "I recommend accept, good paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1166/Reviewer_ZsCC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1166/Reviewer_ZsCC"
        ]
    },
    {
        "id": "_eymgCfPDc_",
        "original": null,
        "number": 2,
        "cdate": 1666788049453,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666788049453,
        "tmdate": 1666788194778,
        "tddate": null,
        "forum": "gfPUokHsW-",
        "replyto": "gfPUokHsW-",
        "invitation": "ICLR.cc/2023/Conference/Paper1166/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper coins the new problem of Learning without Prejudices in a continual learning (CL) setup. Unlike most CL works that assume catastrophic forgetting, which is considered bad, the paper is the first to point out that there are actually both malignant and benign forgettings and that one must actually encourage the benign forgetting. Benign forgetting happens when existing biases (correlations between the label and attributes) are gradually unlearned. Two key techniques are proposed: a GAN-like feature generation is used to discourage malignant forgetting by reducing dependencies to a few biased attributes and feature-level augmentation and contrastive learning, which encourages benign forgetting by bypassing the label-attribute correlation. Experiments show that the proposed method outperforms various baselines on several real datasets.",
            "strength_and_weaknesses": "Strengths\n- Defines and solves a novel and important problem.\n- The problem is well formulated.\n- The two key techniques are effective in handling malignant and benign forgetting and go well together.\n- Experiments show clear benefits of the proposed method.\n\nWeaknesses\n- In Section 3.2, the explanation right after Equation (1) is a bit cryptic. Perhaps the authors can explain while making direct references to Figures 1 and 2? It also took a while to understand how $\\beta$ plays a role when using the images in Figure 1.\n- How generally applicable is Equation (1)? I don't think it's always the case that accuracy decreases as bias decreases. Usually one would expect lower bias to lead to higher accuracy.\n- In Section 4.1, it is not clear why a GAN training on lower layers of a CNN discourages malignant forgetting only. Expanding the feature space seems to discourage both malignant and benign forgetting. What is the evidence that only malignant forgetting is affected? Also how does one set $l$? \n- Why should contrastive learning be performed on $f_\\theta^b$ instead of $f_\\theta^a$ and $f_\\theta^b$ combined (i.e., $f_{[1,\\ldots,L-1]}$)? \n- The contrastive learning seems effective when bias is defined as a correlation between label and attribute space. Although out of the scope of this paper, if bias is defined as some biased distribution in the attribute space, how would this technique perform? \n- Section 4.2: \"Since augmentations of input image space are not directly applicable in the feature space due to distribution shift\" -> can you elaborate a bit more on why this is the case?\n- In Section 5.2, please briefly explain how the baselines (LfF, EWC, LwF, DGR, GFR, ABD, HAL, DER) work.",
            "clarity,_quality,_novelty_and_reproducibility": "- The problem is very novel.\n- The proposed techniques look very effective.\n- The paper is generally well written.\n",
            "summary_of_the_review": "This paper proposes the novel problem of learning without prejudices by making the key observation that forgetting can be benign. The proposed techniques for encouraging benign forgetting and discouraging malignant forgetting are effective and go well with each other. Experimental results look strong. The presentation can be improved a bit.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1166/Reviewer_kjJH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1166/Reviewer_kjJH"
        ]
    },
    {
        "id": "EURGzukgx",
        "original": null,
        "number": 3,
        "cdate": 1666814908059,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666814908059,
        "tmdate": 1666814908059,
        "tddate": null,
        "forum": "gfPUokHsW-",
        "replyto": "gfPUokHsW-",
        "invitation": "ICLR.cc/2023/Conference/Paper1166/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper introduces concepts of benign forgetting and malignant forgetting in continual learning, and it conducts a study to discourage malignant forgetting and encourage benign forgetting. The performance of the proposed method is compared to previously proposed continual learning algorithms. ",
            "strength_and_weaknesses": "(+) The paper proposes an interesting problem. To categorize forgetting as malignant and benign and propose an algorithm to discourage malignant forgetting and encourage benign forgetting sound interesting. Essentially there is a distribution mismatch between the training and test dataset. \n(-) In CL, the problem is generally defined assuming non-intersecting class labels; however, CUL is studied assuming that the class labels are equal. This restricted problem considered in this paper makes the CUL problem less interesting and very restrictive.  Since the test data in this paper for every task have same distribution, there does not seem to be any catastrophic forgetting in the usual sense.\n(-) The readability of the paper could be improved with examples and figures.\n(-) Comparative experimental results do not include more recent CL algorithms and the datasets used for evaluation are small and not extensive. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally clear; however, the scope of the problem is very narrow as the task share a common class set with the distribution of the test data being identical. The paper does not compare with more recent CUL papers and should compare it. It is not clear whether the experiment results are reproducible. Please refer to strengths and weaknesses.",
            "summary_of_the_review": "The paper is generally clear; however, the scope of the problem is very narrow as the task share a common class set with the distribution of the test data being identical. The paper does not compare with more recent CUL papers and should compare it. It is not clear whether the experiment results are reproducible.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1166/Reviewer_K36S"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1166/Reviewer_K36S"
        ]
    },
    {
        "id": "7nwZSYbLFkE",
        "original": null,
        "number": 4,
        "cdate": 1667282846540,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667282846540,
        "tmdate": 1669135144712,
        "tddate": null,
        "forum": "gfPUokHsW-",
        "replyto": "gfPUokHsW-",
        "invitation": "ICLR.cc/2023/Conference/Paper1166/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the setting of ``continual unbiased learning\" (CUL), where for each task of continual learning, labels are strongly correlated with specific but different input features. It also proposes to use self-supervised learning methods to address the model bias issue during CUL.",
            "strength_and_weaknesses": "Strength:\nThe setting and method are clearly presented. \nThe effectiveness of the proposed method under the proposed setting is demonstrated in several synthetic datasets.\n\nWeaknesses:\nThe setting of the paper seems artificial. Specifically, this work considers the case where strong correlations (large beta) exists between the label space and feature space. And each task contains data with different yet strong correlations. And the training is under continual learning case where old data cannot be re-visited hence strong correlations cannot be regularized using other data. Though I agree that this setting is new, but it is not very convincing to me that it is practical. For example, if we know that each task has strong bias, why would we completely drop previous data? \n\nIn the meantime, the algorithm design seems particularly-biased to the proposed setting, and not very unexpected. Specifically, in the proposed setup, each task has a strong correlation between labels and features, hence using self-supervised learning where the model is regularized without labels seems somewhat natural to me, making it less surprising. Though for this point, I agree that at least to combine GAN and BYOL, there are some technical contributions made in this work.\n\nMoreover, all experiments are conducted with synthetic data, in small scale (MNIST/CIFAR level, with < 10 tasks), and with strong artificial bias. I agree on the motivated examples that maybe frogs are taken frequently with swamp, but I can hardly image that in practice similar cases happens as in the proposed setting, for example, frog sometimes mostly taken with swamp, yet later taken with sky etc. Hence, I believe to better motivate the practicality of the proposed setting and the effectiveness of the proposed methods, two things need to be done:\n1) Show that in practice, the proposed setup actually happens, using real-world datasets.\n2) In practice, it is at least often unknown whether strong biases occurs in all tasks in the sequence. Hence, the proposed method must be also effective when no strong bias is presented in continual learning, a fair comparison against other CL methods is required on such setting (maybe normal CL problems).\n\nAlso, there is no notion of computation/parameter increase during the experiments. It would be clearer if the increase on the model parameters and computations during training can be explicitly reported, since two new branches are required for GAN and BYOL. And maybe for fairer comparisons, one can compare with baseline models with the same model size etc.. But I do understand that it might be hard to compare completely fairly on this point. \n\nFinally, for replay-based methods, only 200 images are used in the buffer, I wonder whether all methods are still much worse than the proposed one when the replay buffer is enlarged? Say at least 10 times larger?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly presented.\nThe quality can be further improved especially on the experiment part.\nThe novelty seems not very strong due to the reasons mentioned in ``Strength And Weaknesses\".\n",
            "summary_of_the_review": "As explained in detail in ``Strength And Weaknesses\". I agree that the paper proposes a new setting and effective method under this setting. However, according to the weaknesses part, I think the paper needs further motivation of the proposed setting and supporting evidence of the method effectiveness in general. And the novelty is somewhat limited.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1166/Reviewer_vbj9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1166/Reviewer_vbj9"
        ]
    }
]