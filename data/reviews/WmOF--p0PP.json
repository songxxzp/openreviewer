[
    {
        "id": "83IqihuAhP",
        "original": null,
        "number": 1,
        "cdate": 1666187981828,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666187981828,
        "tmdate": 1666226369693,
        "tddate": null,
        "forum": "WmOF--p0PP",
        "replyto": "WmOF--p0PP",
        "invitation": "ICLR.cc/2023/Conference/Paper433/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the problem of estimating the covariance function for a random process. The random process functional data is sampled on a finite set of knots. Four algorithms are proposed, namely, RANDOM-KNOTS, RANDOM-KNOTS-SPATIAL, B-SPLINE, and B-SPLINE-SPATIAL. These four algorithms are proposed with the main motivation of reducing computational complexity. The first two algorithms achieve this target by reducing the sampling rate, and the last two algorithms achieve this target by using B-spline interpolation. Convergence analysis of the estimated covariance function to the true covariance function is provided. Convergence of the eigensystem is also derived.",
            "strength_and_weaknesses": "Strength:\ncomprehensive numerical test; theoretical results on convergence.\n\nWeaknesses: Some typos do not belong to weaknesses, and are listed below simply because we do not find a better place to input. Below we also list some mathematical mistakes.\n1. Line 5 of page 3, the notations \"O\" and \"o\" should be defined.\n2. Assumption 2: for minimum value G(t,t') should be G(t,t).\n3. Is \"Figure 2.2\" the one at the top of page 6? I feel that this figure adds confusion. Here is the reason. According to the beginning of Section 2.1, the coordinates of vectors are set to zero randomly, and for Figure 2.2(a), with probability 1-(3/6)=0.5. With such a probability and a dense vector of dim=6, we may get a sparse vector with 3 zeros, 2 zeros, 4 zeros, etc. In Figure 2.2(a), all the three dim=6 vectors are sparsified to vectors with 3 zeros. I feel uncertain about whether each element of a vector is set to zero independently, or we are just randomly select 50% of the coordinates and then set them to zero in a batch.\n4. How is the norm ||G-hat - G-bar|| defined in Theorem 1?\n5. At the end of page 5, it seems to me that the least square regression with B-splines would lead to interpolation. Is that true?\n6. This paper proposes two families of covariance estimators, and according to Remark 2, the B-spline-based approach outperforms the random-knots family. Given such a claim, what is the reason for introducing RANDOM-KNOTS and RANDOM-KNOTS-SPATIAL?\n7. Line 2 of page 12, please define the inner product <.,.>_d.\n8. The first equation in Appendix B: the norm ||.|| is applied to a scalar, and should be changed to absolute value |.|.\n9. The second centered equation in Appendix B could be wrong if j=j'. Please provide treatment for this case.\n10. For the second centered equation in Appendix B, since we have the term \\bar{h}_j which contains h_{1j}, ..., h_{nj}, and they are transformed from x_{1j}, ..., x_{nj} independently as demonstrated in Figure 2.2(a), this centered equation is wrong even if j is not equal to j'.\n11. Line 4 of Appendix C, obviously xi_{ij}=1 and M_j>= 1 are not equivalent. More details are necessary to prove why the two conditional expectations are equal. The claim that event {xi_ij=1 and xi_ij'=1} happens with probability (Js/d)^2 is wrong when j=j'; this mistake may spoil the whole proof.\n12. The proof of Theorem 2 is wrong because the case j=j' is not considered.\n13. In the proof of Theorem 3, we note that the case p=1 is discussed. This causes 0^0 to show up in Theorem 2. Please define the value of 0^0 in such case.\n14. In the proof of Theorem 3, what is the relation between (12) and (13)? In particular, if one expands the target function in (12) as a quadratic function of 1/T(r)^2, then the off-diagonal entry is in general not zero. So we believe that the proof of Theorem 3 is wrong.\n15. Lines 5 & 7 on page 18: there is a factor \"n\" missing (but this is just a typo easy to fix). Besides this, how to derive the finiteness of the r_0 moment of W_i from Assumption 5?\n16. At the bottom of page 20, as the convergence speed is not given for the second term, deriving the rate n^{-1/2} is not proper. At the end of this section there is another place with the same problem.\n17. The proof of Theorem 5 fails when G has duplicated eigenvalues, in which case the estimator for an eigenvector does not converge to the eigenvector in general. Also, in line -4 on page 21, how to guarantee that C is finite?\n18. On page 22, a new assumption on positive eigenvalues is added to the proof. Please also include this assumption in the theorem.",
            "clarity,_quality,_novelty_and_reproducibility": "The main results are stated clearly. I can not say this paper is of good quality because of the problems found above. The results are new, but the six assumptions could together strongly restrict the application scenario and the novelty of the paper. The numerical computation appears to be reproducible.",
            "summary_of_the_review": "The results are new, to my best knowledge, and deserve a publication. However, some proofs are wrong, so I can not recommend a publication of this paper in its current form. I would consider to recommend a higher score after these problems have been resolved (without compromising the significance of the main results), and no more big mathematical mistake is found.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The paper studies machine learning theory, and there is no ethics concern.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper433/Reviewer_BUn8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper433/Reviewer_BUn8"
        ]
    },
    {
        "id": "_RP22Cu88g",
        "original": null,
        "number": 2,
        "cdate": 1666270146871,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666270146871,
        "tmdate": 1666270146871,
        "tddate": null,
        "forum": "WmOF--p0PP",
        "replyto": "WmOF--p0PP",
        "invitation": "ICLR.cc/2023/Conference/Paper433/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors try to estimate the covariance function of sparsified functional data for dimension reduction. They propose four sparsification schemes and give the corresponding nonparametric estimation of the covariance function. The covariance estimators are asymptotically equivalent to the sample covariance computed directly from the original data. Moreover, the estimated functional principal components effectively approximate the infeasible principal components under regularity conditions. ",
            "strength_and_weaknesses": "Strength: The sparsified data costs less for transmission. The construction is simple such that the sparsified data has the similar covariance matrix get by the original data.\nWeakness: The computation of covariance matrix for the sparsified data has the same (even more) complexity as that of the original data, since the mean vector (h bar) may be dense with very high probability.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is poor. For example,\n1. The mu-holder continuous functions in page 3, we cannot get the information of q from their definition.\n2. Origin data set in Page 5\n\nThe problem may not be valuable. It may be better to transmit the principal components than the preprocessed data (sparsified data). ",
            "summary_of_the_review": "In this paper, the authors try to estimate the covariance function of sparsified functional data for dimension reduction. They propose four sparsification schemes and give the corresponding nonparametric estimation of the covariance function. The covariance estimators are asymptotically equivalent to the sample covariance computed directly from the original data. I think it is not a good idea to transmit the original data or preprocessed data if the partner just needs the principal components. In summary, I do not think it is a good paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper433/Reviewer_orUQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper433/Reviewer_orUQ"
        ]
    },
    {
        "id": "JK5LBAd4os",
        "original": null,
        "number": 3,
        "cdate": 1666725259183,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666725259183,
        "tmdate": 1666726215722,
        "tddate": null,
        "forum": "WmOF--p0PP",
        "replyto": "WmOF--p0PP",
        "invitation": "ICLR.cc/2023/Conference/Paper433/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes two classes of sparsification schemes for densely observed functional data with potential spatial correlations. The first class of schemes is random sparsification methods that extend the work of [1] from mean estimation to covariance estimation and the second class involves B-spline interpolation of the original data. Both classes achieve the desired convergence rates as by the originally observed functional data. \n\nReference:\n\n[1] Jhunjhunwala, Divyansh, Ankur Mallick, Advait Gadhikar, Swanand Kadhe, and Gauri Joshi. \"Leveraging Spatial and Temporal Correlations in Sparsified Mean Estimation.\" Advances in Neural Information Processing Systems 34 (2021): 14280-14292.",
            "strength_and_weaknesses": "Strength: the paper is overall well written and the presentation is clear.\nWeakness: the overall novelty of the proposed method is limited and the applicability of the B-spline-based methods may be restrictive.\n\n1. The ideas of random sparsification and random-spatial sparsification follow rather closely to those in [1], except that [1] focuses on estimating the mean vector of the sparsified vectors. The extension to the covariance function estimation is straightforward and less innovative. If there are any nontrivial challenges in the theoretical investigations, please clarify.\n\n2. The ideas of B-spline interpolation rely heavily on the assumption that the observed random trajectories are smooth functions. But in practice, it is commonly the case that these trajectories are contaminated with measurement errors. I am not sure whether the proposed theory can be modified to address this issue, which will be of more practical interest. I don't think this will be an issue for the random sparsifications scheme, but not sure about the B-spline interpretations.\n\n3. I found some of the assumptions are not so standard. For example, (1) in Assumption 2, why would you need to assume $\\min_{t}G(t,t')>0$, which seems to be a strange condition? (2) In Assumption 3,  it is also strange to assume the decaying rate of the eigenvalues depends on the sample size $n$.  The decaying rate is usually only assumed to change with $k$. (3) In Assumption 5, I am confused by the wording \"the number of distinct distributions...is finite\". Please clarify the definition. \n\n4. I am also not so sure about some of the theoretical results regarding the convergence rate. (1) In Theorem 4, why the convergence rate for the first mean function is almost sure convergence while all other threes are convergence in probability? (2) The convergence rate in Theorem 5 appears to be faster than the existing convergence rate in the literature. For example, in Corollary 3.7 of [2], the convergence rates of these quantities are proved to be of the order $O_p(\\sqrt{\\log(n)/n})$, slower than those in Theorem 5. Please double-check.\n\nReference:\n\n[1] Jhunjhunwala, Divyansh, Ankur Mallick, Advait Gadhikar, Swanand Kadhe, and Gauri Joshi. \"Leveraging Spatial and Temporal Correlations in Sparsified Mean Estimation.\" Advances in Neural Information Processing Systems 34 (2021): 14280-14292.\n\n[2] Li, Yehua, and Tailen Hsing. \"Uniform convergence rates for nonparametric regression and principal component analysis in functional/longitudinal data.\" The Annals of Statistics 38.6 (2010): 3321-3351.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the presentation is clear. But I think the novelty is limited.",
            "summary_of_the_review": "I think the paper is well written, but in my opinion, the contributions are incremental at best.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper433/Reviewer_Dxm2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper433/Reviewer_Dxm2"
        ]
    },
    {
        "id": "-ys48Znkmy",
        "original": null,
        "number": 4,
        "cdate": 1666758522544,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666758522544,
        "tmdate": 1666758522544,
        "tddate": null,
        "forum": "WmOF--p0PP",
        "replyto": "WmOF--p0PP",
        "invitation": "ICLR.cc/2023/Conference/Paper433/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the problem of covariance estimation for a 1D time series based on $n$ identically distributed (but possibly correlated) draws of length $d$.  The standard estimator would look at all the data, but the authors consider four sparsified versions of the estimator based on a sample of the entries of the different series.  Two of the estimators use random selection of the data (one taking into account correlation between the different draws), and two use samples at fixed locations and fill in intermediate values using B-splines (again, with one taking into account covariance between the draws).  The authors also state convergence theorems for the estimated covariance and its principle eigenvalues and eigenvectors.",
            "strength_and_weaknesses": "The combination of using correlation among draws and using interpolation via B-splines is an interesting theoretical idea, and the theorems look like the types I would hope for (I have not checked the details).\n\nAt the same time, despite the claimed advantages in terms of computational efficiency, this seems to mostly offer theoretical contributions.  None of the experiments addressed the claimed computational efficiency -- all were about accuracy of the estimators.\n\nI had a hard time understanding what was happening with the application section, both in terms of the actual data to which the estimators were being combined and how that data was used.  It seems like the data was a sequence of embedding vectors; but how is it reasonable to model these as identically-distributed time series with smooth correlation?  And what happens afterward in the clustering?  It would be useful to provide a few more details.\n\nAlso, while accounting for the correlation across draws (or components or spatial locations, as the authors have it) definitely decreases the mean squared error in estimating the standard estimator using all the data, it is less clear that it decreases the mean squared error in the estimator as a whole.  The highly-correlated case is exactly the one in which the standard covariance estimates should be most noisy, I think?  It would again be useful to have a comment on this.",
            "clarity,_quality,_novelty_and_reproducibility": "I found this paper rather difficult to read.  Quite a lot of results are packed into the eight pages (with all proofs in the supplementary materials); but there are so many theorems that there's not so much room left for exposition!  Not even a sketch of the idea of the proof is in the main body.\n\nOn a more minor note: the labels on the figures are too tiny to be at all visible.",
            "summary_of_the_review": "The paper is largely interesting for the theoretical results that it proves about the four proposed estimators.  The claimed efficiency improvements are never really demonstrated, and I found it hard to figure out what was going on in the one real application.  As a piece of mathematics, the results seem plausible and somewhat interesting, particularly in the version that incorporates both temporal and spatial correlations.  At the same time, the paper is so packed with theorems (proved in quite a long additional supplement) that it is not altogether easy to follow.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper433/Reviewer_DGgK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper433/Reviewer_DGgK"
        ]
    }
]