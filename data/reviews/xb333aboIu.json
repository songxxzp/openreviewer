[
    {
        "id": "MFfs97ZN-T",
        "original": null,
        "number": 1,
        "cdate": 1665982914995,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665982914995,
        "tmdate": 1665982947230,
        "tddate": null,
        "forum": "xb333aboIu",
        "replyto": "xb333aboIu",
        "invitation": "ICLR.cc/2023/Conference/Paper3195/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the weight initialization scales/magnitudes for sufficiently wide and sufficiently deep neural networks. Noticing that an inappropriate initialization magnitude may multiplicatively accumulate via forward and backward propagation through the large number of layers leading to non-trainability of the network due to exponential explosion or vanishing, the authors focus on the critical initialization magnitudes in order to keep the trainability. Particularly, the paper shows that, with Layer Normalization, the criticality of initialization holds for a large range of magnitudes (or even all initialization).\n",
            "strength_and_weaknesses": "The main result of the paper is novel, while the other results and discussions are somewhat incremental. \n\n> As the gradient explosion or vanishing had been one of the major issues in training deep neural networks, many researchers have realized the importance of the magnitude of initialization, and proposed methods to solve or mitigate this issue. Many of these attempts were indeed about finding the critical initialization, such as Kaiming initialization, NTK initialization etc.. For example, in the NTK initialization [1], there is a scaling factor $1/\\sqrt{m_l}$ on each layer so that the output of each neuron is at the order of a constant (neither too large nor too small). In addition, in [2], there is an extra factor $c_\\sigma$ to normalize each layer in order to make the initialization critical. Given these prior results, the result and discussion in Section 2 about MLP seem repetitive and not new. \n\n> Theorem 2.7 is more like a definition of $\\zeta$ instead of a theorem. It is not a surprise at all that APJN is exponential to the number of layers, as the network is MLP. \n\n> The major contribution tells an interesting result. Although it has been wide known that both LayerNorm and skip connect mitigate the gradient explosion and vanishing issues, it is still surprising to see that the combination of the two can totally address this issue, in the sense that any initialization does not lead to gradient explosion or vanishing. \n\nOther comments:\n\n> Below Eq.(2), I cannot quite agree that the partial Jacobian is a random matrix. Once the network is trained away from its initialization, I don\u2019t see how the partial Jacobian is still random.\n\n> In Theorem 2.4, Eq. (10), the NNGP kernel is not in a recursive format. \n\nReferences:\n[1] Neural Tangent Kernel: Convergence and Generalization in Neural Networks.\n[2] Gradient Descent Finds Global Minima of Deep Neural Networks\n",
            "clarity,_quality,_novelty_and_reproducibility": "see comments in the 'strength and weaknesses\" section",
            "summary_of_the_review": "Initialization scale has long been an essential topic in training deep neural networks. While many aspects/results of the analysis have been known by the machine learning community, I have never seen the conclusion that, with a proper combination of LayerNorm and skip connection, any initialization scale does not result in gradient explosion and vanishing. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3195/Reviewer_7FZf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3195/Reviewer_7FZf"
        ]
    },
    {
        "id": "b8IVJJFqVpp",
        "original": null,
        "number": 2,
        "cdate": 1666389685376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666389685376,
        "tmdate": 1673037159946,
        "tddate": null,
        "forum": "xb333aboIu",
        "replyto": "xb333aboIu",
        "invitation": "ICLR.cc/2023/Conference/Paper3195/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper investigates criticality conditions in deep networks, using the NNGP (neural network Gaussian process) framework in the infinite width limit, focusing on architectures with LayerNorm and residual connections.\nIt introduces an estimator for trainability or training stability, the Averaged partial Jacobian norm (APJN), related to the scale (number of layers) over which gradients explode or vanish, which can help selecting the scale of parameter initialization.\nIt then shows theoretically and empirically that some architectures are always trainable, as they are critical regardless of the initialization scale, confirming previous experimental findings.",
            "strength_and_weaknesses": "Strengths\n-------------\n1. A formal analysis, building on existing work on criticality, and expanding it to popular architecture choices\n2. Theoretical confirmation of practical observations regarding critical behavior of networks with various non-linearities, skip-connections, and LayerNorm\n3. Practical confirmation on finite-depth and width MLP Mixer networks\n4. The presentation is overall clear and easy to follow, previous work and novel contributions are usually well introduced\n5. Source code is included\n\nWeaknesses\n------------------\n1. Some confusion in notations (see below)\n2. Some clarity concerns: the definition of APJN always refer to an $(l_0, l)$ pair of layers, but it's not clear what the role of $l_0$ is. It looks like a free parameter one could chose, but what are the consequences of picking one? I get that setting $l_0 = 0$ would have challenges as $N_0$ cannot go to infinity, but could one simply set $l_0 = 1$ without loss of generality?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n--------\nGood overall, the paper is easy enough to read despite the formalism. It is well structured, with the essential information in the main article and additional content in the appendix.\n\nMinor points:\n1. Squared terms explicitly repeated in formulas (e.g., derivatives in Eq. 3) would be easier to read with $(...)^2$, as the reader would not need to try and play differences to make sure both expressions are the same.\n2. Is the characterization of the architecture-dependent critical exponent $\\zeta$ novel, or should there be a citation before Theorem 1.3?\n3. The end of Section 4 mentions the tanh activation function and refers to Fig. 2, but tanh is not in Fig. 2.\n4. What is the scale of \"trainability\" in Fig. 2? What does a trainability of 0.5 mean, for instance?\n5. Is there supposed to be a difference between $\\chi_{\\mathcal J}$ and $\\chi_J$?\n\nQuality\n---------\nGood. The theoretical analysis seems solid. The experiments are well designed to show the practical consequences of initialization in different settings, in terms of gradient scale, and of trainability. The bibliography seems complete.\n\nNovelty\n-----------\nThe theoretical analysis is new, and more general than previous works, which are mentioned in Section 1.2, which makes connections with this work.\n\nReproducibility\n--------------------\nExcellent. The description of algorithms is clear and not ambiguous, and source code is provided.",
            "summary_of_the_review": "The paper provides a theoretical analysis of criticality in different variants of deep neural architectures, proposes a practical surrogate for trainability, and provides experiments supporting its usefulness.\nThese are useful, well-supported results, that confirm and justify some previous findings, the presentation is clear enough to understand and reproduce.\nI think this is an overall good paper, and should be accepted.\n\n**Update after discussions**\n\nAfter live discussions with AC and other reviewers, I've come to add additional weight to the following negative points:\n* clarity still leaves to be desired, the reader has to work too much to extract the main points\n* the insight has limited value given the existing best practices: the proposed indicator is not really tested outside of current usual settings.\n\nAccording to that discussion, I'm updating my rating. I still think the paper should be accepted, and would have rated it a 7 (rather than 6) if the option was available.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3195/Reviewer_sMx1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3195/Reviewer_sMx1"
        ]
    },
    {
        "id": "5H77bERcdd",
        "original": null,
        "number": 3,
        "cdate": 1666426869449,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666426869449,
        "tmdate": 1671091676915,
        "tddate": null,
        "forum": "xb333aboIu",
        "replyto": "xb333aboIu",
        "invitation": "ICLR.cc/2023/Conference/Paper3195/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work introduces the concept of Averaged Partial Jacobian Norm (APJN).\nThe APJN can be used to derive criticality measures &mdash;i.e. how trainable a network is&mdash; for arbitrary parts of a neural network.\nFrom the APJN, an empirical estimator can be derived to empirically compute how \"trainable\" a given neural network is.\nFurthermore, the criticality of layer normalisation, skip connections and the MLP-Mixer architecture is analysed.",
            "strength_and_weaknesses": " - [+] An empirical estimate of criticality should be useful to easily tune certain hyper-parameters of neural networks (e.g. initialisation).\n - [-] The quality of the proposed empirical estimate is not properly tested.\n - [-] There are a lot of forward and backward references, making the paper hard to read.\n - [-] Assuming that the new architecture refers to applying Layer Normalisation before the activation function, there is nothing *new* about it (Ba et al., 2016).\n - [-] Most of the results from the criticality analysis are already known.",
            "clarity,_quality,_novelty_and_reproducibility": "### Quality\n\n - There seems to be no motivation for why the APJN should be an interesting value to consider.\n   To be more precise, the exact formulation feels arbitrary:\n    1. the choice for computing the Jacobian over a subset of layers, instead of just one or all layers,\n    2. the use of the Frobenius norm instead of something like the spectral norm,\n    3. the multiplication factor $\\frac{1}{N_l}$, which seems to imply that this is actually an average over squared $L_2$-norms of gradient vectors of individual output entries.\n - Figure&nbsp;1 seems to be the only experiment to verify that the APJN of the last layer can actually be used to estimate criticality.\n   Although these results look promising on first sight, closer inspection reveals a few issues.\n   E.g. for \"erf, $\\mu = 0.9$\" the APJN seems to be quite off in general.\n   Similarly, for \"GELU, $\\mu = 0.0$\", the empirical and theoretical results do not seem to agree for small $\\sigma_b^2$.\n   Without a proper analysis of these failures, it would be hard to rely on these estimates.\n\n### Clarity\n\n - Although I am familiar with most of the related work, I needed multiple read-throughs to distill the actual contributions from this paper.\n   This seems to be a strong indicator that this paper would benefit from a thorough rewrite for it to be useful to the broader ML community.\n - The many references to various parts in the paper, typically break the reading flow.\n   Since the references often span multiple pages, I found myself spending quite some time going back and forth to put everything together.\n - The derivation in eq.&nbsp;(9) appears to be trivial, but I would argue that it can not be that easy.\n   E.g. using the uniform law of large numbers (Jennrich, 1969), I see how $\\frac{1}{N_l} \\sum_{i=1}^{N_l} \\phi(h_i^l)^2 \\overline{\\to}^{N_l \\to \\infty} \\mathbb{E}_{h_i^l}[\\phi(h_i^l)^2]$, from which the result would follow, could hold.\n   However, there are a number of assumptions on $f(x) = \\phi(x)^2$ that would need to be satisfied for this to work.\n - The statement of proposition&nbsp;2.6. is very confusing.\n   The authors mention $l_0$ and $l$ in the text, but the formula only contains $L-2$ and $L-1$, which are both not specified.\n   Also, it is unclear what \"next to the output\" means in this context.\n   I would argue it must be the derivative from pre-activations of the last layer w.r.t. the penultimate layer, but I am not entirely sure whether this is correct.\n\n### Originality\n\n - I would argue that the related work section should include a paragraph to establish the relation of this work to mean field theory.\n   After all, the results in eq.&nbsp;(21),&nbsp;(23) and&nbsp;(25) appear to be the kind of results you could obtain from a mean field analysis.\n   Most of the relevant papers are cited at some point, but I think it would be useful to explicitly highlight similarities and differences with this line of research.\n - I fail to see what is new about Theorem&nbsp;2.5.\n   This seems to be exaclty the same result as eq.&nbsp;(8) in (Schoenholz et al., 2016).\n - The results in eq.&nbsp;(23) seem to be exactly those in Table&nbsp;l ($\\underline{\\boldsymbol{\\chi}}$) from (Yang & Schoenholz, 2017).\n - Applying layer normalisation to pre-activations is exactly what Ba et al. (2016) did,\n   unlike what footnote&nbsp;2 suggests.\n   In eq.&nbsp;(3) of (Ba et al., 2016) normalisation is defined in terms of $a_i^l$, which should be the pre-activations according to eq.&nbsp;(1).\n - Ba et al. (2016) clearly state (see table&nbsp;1) that layer normalisation makes weights scale- and shift-invariant.\n   Therefore, the following excerpt from the manuscript should probably be more explicit about how well this is already known:\n   > One fortunate outcome of both theory and experiment is that when LayerNorm is applied to preactivations, ReLU networks can still be initialized using He initialization (He et al., 2015) which, in our convention, is $(\\sqrt{2}, 0)$.\n - It would probably be useful to point out how the layer-norm results relate to existing analyses of batch-normalisation (e.g. Yang et al., 2019).\n - Section&nbsp;5 states that residual connections are \"commonly\" combined with layer normalisation.\n   To the best of my knowledge, it is more common to use batch normalisation, because that is also what He et al. (2016) used.\n   I think it would be useful to point out a few works that use layer normalisation at this point.\n   It also raises the question why the authors chose to study layer normalisation instead of batch normalisation.\n\n### Other, minor comments\n\n - At some points in the paper, sentences are hard to read due to lack of punctuation.\n   (e.g. first sentence of the introduction).\n - The starting point for the recursion in eq.&nbsp;(1) is not defined.\n - The weight distribution for $w_{ij}^{l+1}$ was probably scaled by $\\frac{1}{N^{l}}$ instead of $\\frac{1}{N^{l-1}}$ (cf. Lecun et al., 1998b and He et al., 2015).\n - Typo in theorem 2.4. \n - Eq.&nbsp;(10) does not seem to be a recursive formula.\n   Also the formulas in eq.&nbsp;(21) are not recurence relations.\n   In eq.&nbsp;(23), only the first formula would be a recurrence relation.\n - A lot of formulas would be easier to read (and shorter) if $\\phi(x)^2$ (cf. eq.&nbsp;21) were used instead of $\\phi(x)\\phi(x)$ (e.g. eq.&nbsp;12).\n - Please, directly refer to the corresponding sections in the appendix instead of just \"the appendix\".\n\n### References\n\n - Jennrich, R. I. (1969). Asymptotic properties of non-linear least squares estimators. The Annals of Mathematical Statistics, 40(2), 633-643.\n - LeCun, Y., Bottou, L., Orr, G. B., & M\u00fcller, K.-R. (1998b). Efficient BackProp. In G. B. Orr & K.-R. M\u00fcller (Eds.), Neural Networks: Tricks of the Trade (1st ed., pp. 9-50). Springer.\n  - Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., & Schoenholz, S. S. (2019). A Mean Field Theory of Batch Normalization. ICLR 7.",
            "summary_of_the_review": "I would argue that the most interesting contribution of this paper would be a practical estimator for criticality.\nHowever, the proposed estimator is not properly tested to be able to assess when/how this could be used.\nMost of the theoretical analyses turn out to be known results from a body of work on mean field theory.\nTherefore, I do not see what added value the APJN has since it basically builds on the same principles.\nFinally, I found the paper very hard to read and I believe that a rewrite would be necessary to really benefit the broader ML community.\n\n---\n\n### update\n\nAfter the discussion with the authors, I see that there were some misconceptions from my side that lead to an overly negative review.\nOn top of the empirical measure for criticality, the effects due to the combination of layernorm and skip connections could be considered a novel (theoretical) contribution.\nHowever, the bias parameter of a linear layer in front of a normalisation layer is typically initialised to all zeros in practice and thus $\\sigma_b^2 = 0$. As a result, everywhere criticality should also be achieved with layernorm only.\nOverall, I would argue that the paper in its current form still does not do a good job of clearly communicating the insights (and their limitations).\nTherefore, I believe that the ideas in this paper would benefit from a more structural rewrite than the rebutttal period would allow.\n\nTherefore, I decided the correctness score from 2 to 3 and my recommendation from 3 to 5.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3195/Reviewer_PgVN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3195/Reviewer_PgVN"
        ]
    },
    {
        "id": "Ki0QJduZ7U",
        "original": null,
        "number": 4,
        "cdate": 1666644118215,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666644118215,
        "tmdate": 1673033740900,
        "tddate": null,
        "forum": "xb333aboIu",
        "replyto": "xb333aboIu",
        "invitation": "ICLR.cc/2023/Conference/Paper3195/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper discusses a notion for assessing the trainability (meaning, do the gradients avoid vanishing or exploding during training) of deep neural networks of various architectures -- the Frobenius norm, squared, of the averaged partial Jacobian (APJN) between the pre-activations at layers $\\ell_0 \\leq \\ell$. The paper applies the APJN as an empirical diagnostic of the criticality (is the network trainable, in the above sense) and also to demonstrate that for deep MLP networks with residual connections and with Lipschitz activations with LayerNorm applied to pre-activations, the initialization scheme does not impact the criticality of the network. This result formalizes a view on deep network architecture design -- either fix a network for which the criticality (and thus, loosely, trainability) is sensitive to initialization, and find the right initialization, or define a network using methods like LayerNorm and residual connections so that the network is trainable regardless of initialization. Additionally, an upper bound on the rate of divergence of the gradients in non-critical settings depending on residual connection strength $\\mu$ and the activation functions is given. Several experiments are performed to validate the theoretical contribution as well.",
            "strength_and_weaknesses": "* Strengths: The paper provides a nice characterization of their proposed measure, the APJN, and suggests that it is a useful diagnostic to assess the trainability of deep neural networks empirically. The paper also more formally makes some connections present in the existing literature, and presents a nice takeaway that more precisely characterizes the importance of LayerNorm and residual connections from the perspective of initialization. In particular I appreciated the development of the correlation length concept, it was clearly explained and provided nice intuition. Finally, several experiments on smaller datasets are conducted to validate the theory and the gap between the infinite width setting and realistic settings.\n\n* Weaknesses: The primary weaknesses in my view are as follows: \n     * I believe that the notion of trainability/criticality in this paper has not been directly shown to imply generalization in the literature, and it is more of a heuristic notion. However, many papers study this notion and so it is not a big issue. It would however be nice to clarify with more detail the relation between criticality/trainability and actual model performance in a more formal sense (or at least highlight that this is unknown). It would also be useful to provide a more formal definition of criticality from the previous works that studied it, and to make the connection between the criticality conditions that APJN satisfies and the existing notions of criticality clearer. It would be good to clarify the meaning (and justification) of \"Very deep networks will not train unless initialized critically.\" at the end of the first section.\n     * I don't see a justification of Proposition $2.6$ anywhere -- is this just a (reasonable) assumption for the empirical results? It would be nice to highlight / include a pointer to Figures 3 and 4 which essentially are justifying this assumption, if I understood correctly.\n     * At least one other paper (https://arxiv.org/abs/1802.05296) studies the notion of partial Jacobians and provides conditions that depend on the norms of these partial Jacobians (see Definition 5, the interlayer cushion, in the mentioned paper) in the setting of giving generalization bounds. It probably makes sense to mention this work and any others, as while I do not think I have seen the APJN specifically in previous works, there are probably quite a few more than just the three papers that were cited in the related work that study it. This issue is not a big deal as long as the APJN has not been introduced before or analyzed, but it does impact a little bit the claims of introducing this notion in my opinion.\n    * I don't understand why one would ever care about $\\ell_0$ other than $\\ell_0 = 1$ or $\\ell_0 = L - 1$ (the latter being a computationally tractable, and apparently accurate, approximation) -- is there any reason to care about other $\\ell_0$ as a diagnostic? If not, then perhaps this could be clarified when the paper mentions that the growth of the APJN only depends on $\\ell_0$ via some constant.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: The paper was clear overall, with a few suggestions highlighted in the weaknesses mentioned above. I have listed some minor points below: \n    * 2nd sentence of abstract: add a comma after \"infinity\"\n    * 7th line of intro: \"Furthermore, for special choice of parameterization...\" choice --> choices\n    * Below Definition $1.1$: \"sqaured\" is misspelled. \n    * I'm not sure why the term \"algebraically\" is used -- shouldn't it be \"polynomially\" to contrast with exponential situations? If I read correctly, this term is referring to the polynomial growth of $\\ell$ with some degree $-\\zeta$.\n    * In the statement of Thm $1.3$, the forward reference to Eq. 24 is a bit jarring. It would be nice to include the definition in the statement.\n    * In Lemma $2.2$, the big-O notation used in Eq. 8 is unclear to me - could it be clarified?\n    * Figure 2 was a bit unclear to me in terms of exactly what is being plotted - is Figure 2 plotting accuracy after training? More details are needed.\n    * Remark $5.2$ -- \"the architecture design\" -- don't need \"the\".\n    * Lemma B.1 -- need to complete the sentence with punctuation. \n    * Before Definition B.2 -- need to complete the sentence with punctuation.\n    * Remark B.7 -- $\\mathbb{E}_{\\theta}\\left[O\\right]$ is not defined?\n    * Before Remark B.8 -- Equation reference missing\n    * Figure $7$ -- formatting on \"log-log\" should be fixed.\n\n\n* Quality: The paper was interesting and the ideas introduced useful and insightful. \n\n\n* Novelty: The paper was sufficiently novel for publication.\n\n\n* Reproducibility: The experiments seem easy to reproduce, though I have not done so myself.",
            "summary_of_the_review": "Overall, the paper introduced a seemingly novel diagnostic for evaluating the criticality/trainability of deep neural networks and provided some thorough characterizations of the trainability in a variety of situations, in the infinite-width limit. The paper backed up its results with experiments, and used the results to highlight and justify the ways methods like LayerNorm and residual connections can be viewed as a method of making the initialization problem easier to solve. The settings studied are limited, and it is not directly clear how these issues (trainability, etc) impact generalization performance, but it seems to be a useful approximation to understand better and to more thoroughly study. The ideas in the paper may also be helpful to further develop both the practice and theory of deep networks.\n\n==== POST-REBUTTAL ====\n\nAfter reading the other reviews and rebuttals, I remain convinced the paper should be accepted.\n\n==== POST-DISCUSSION =====\n\n After the discussion, overall, I think the paper proposes an interesting measure to investigate when deciding how to initialize deep networks, and provides some justification via a combination of existing theory and limited experiments that it should be interesting. My feeling was still that the paper was relatively clear, though perhaps there is some over-claiming in the theorem statements and attribution should be a bit clearer/cleaned up. I think it is reasonable to accept (I would vote something closer to 7 if that were an option), but I do agree that the paper would be significantly improved by a) cleaning up the theorems / equations (probably don't need to write down each case the way they did), b) tweaking the language to avoid some over-claiming, and c) additional experiments validating the measure in practice. However, I view these changes along the axis from being a decent paper to a great paper and still lean towards acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3195/Reviewer_6A97"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3195/Reviewer_6A97"
        ]
    }
]