[
    {
        "id": "BUMYUg4YA2H",
        "original": null,
        "number": 1,
        "cdate": 1666429912904,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666429912904,
        "tmdate": 1666429912904,
        "tddate": null,
        "forum": "nXmU89Rfmgg",
        "replyto": "nXmU89Rfmgg",
        "invitation": "ICLR.cc/2023/Conference/Paper5191/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper extended the HyperTransformer and proposed an Incremental HyperTransformer(IHT). The proposed IHT re-used the old weights as an input to generate the new weights for the incoming task in the continual sequence. This mechanism encourages the new model to utilize the knowledge in the old model. Experiments on class-incremental setting and task-incremental setting demonstrate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength:\n- The architectural innovation seem effective and are easily interpretable when thinking about utilizing the learned knowledge.\n- The prototypical loss is orthogonal to the architecture design.\n- I think even general classification task can benefit from the proposed architecture design and loss design.\n\nWeaknesses:\n- I think it is better to give a clear definition of few-shot class-incremental-learning\n- In my opinion, the re-use of old weights can help the convergence of the new task because the old weights can be treated as an initialization of the new model, but how does this mechanism alleviate the catastrophic forgetting? Does author use the replay buffers? It seems that we have to store all the query sets of each seen tasks.\n- The proposed prototypical loss is similar to the Nearest-Mean-of-Exemplars Classification in iCaRL[1]. More discussion is expected.\n- The experiments lack comparison with other continual learning baselines.\n\n\n[1] Rebuffi S A, Kolesnikov A, Sperl G, et al. icarl: Incremental classifier and representation learning. CVPR 2017.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the definition of the core problem in this paper is missing, and I expect more explanation about why the proposed method can alleviate forgetting.\n",
            "summary_of_the_review": "Based on the comments in the Main Review part, I tend to reject this paper. The main reason is the lack of explanation and experiments of why the proposed method can catastrophic forgetting. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5191/Reviewer_hN2L"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5191/Reviewer_hN2L"
        ]
    },
    {
        "id": "Z9p3QhNsyb",
        "original": null,
        "number": 2,
        "cdate": 1666539628270,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666539628270,
        "tmdate": 1666539628270,
        "tddate": null,
        "forum": "nXmU89Rfmgg",
        "replyto": "nXmU89Rfmgg",
        "invitation": "ICLR.cc/2023/Conference/Paper5191/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In the paper, the authors use HyperTransformer (HT) in a new setting that mixes continual learning and few-shot learning. The authors propose to reuse these generated weights as input to the HT for the next task.",
            "strength_and_weaknesses": "1. The paper is not well written. For me, it is not trivial to extract information about the setting. What authors mean by continual learning in few-shot learning. \n\nFEW-SHOT INCREMENTAL LEARNING is well described in literature and authors should concentrate on such a problem, not few-shot or continual learning.\n\n2. The paper should contain a section that explains what authors understand by continual learning mixed with few-shot learning. How exactly looks FEW-SHOT INCREMENTAL LEARNING in the paper?\n\n3. Chapter 3 HYPERTRANSFORMER ARCHITECTURE starts with the formulation of few-shot learning. But section 4 INCREMENTAL HYPERTRANSFORMER directly starts with a model, without a specification of how the few-shot learning task was generalized. \n\n4. Experimental section is not well done. First of all, it is not clear what the training looks like and how the evaluation is computed. \n\n5. Authors do not specify numerical measures for evaluations\n\n6. Authors evaluate the method only by plots. It is difficult for future investigators to work whit such papers. The charts are nice the authors should give exact numbers in the appendix. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is interesting and in some sense presents valuable novelty. Unfortunately, the presentation and evaluation of the model are not enough for publication.",
            "summary_of_the_review": "The paper is interesting but the presentation and evaluation of the model are not enough for publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5191/Reviewer_XnH3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5191/Reviewer_XnH3"
        ]
    },
    {
        "id": "ZdHV_ol32s",
        "original": null,
        "number": 3,
        "cdate": 1666658762513,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658762513,
        "tmdate": 1666658762513,
        "tddate": null,
        "forum": "nXmU89Rfmgg",
        "replyto": "nXmU89Rfmgg",
        "invitation": "ICLR.cc/2023/Conference/Paper5191/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper makes a few contributions: using a newly published weight generation model HyperTransformer to generate model weights for incremental task/class learning; modifying the classic SCE loss with an existing prototype loss (from a few-shot learning paper). ",
            "strength_and_weaknesses": "Strength:\n\nIt is a good baseline method for using the weight generation model in class-incremental and task-incremental learning. \n\nWeakness:\n\nIts applications of the existing weight generation model and the typical prototypical loss are straightforward. Its overall idea is too similar to many related few-shot learning works (which however not being mentioned in the related work section). Please check some of them in [1,2,3]. It does not explicitly solve the forgetting problems in incremental learning, but simply relies on the performance of the adopted hypernetwork. I can take this work as a good baseline but without clear novelty over the baseline.\n\n[1] Li et al. CVPR 2022. Sylph: A Hypernetwork Framework for Incremental Few-shot Object Detection. It introduced a few-shot hyper network to generate weights and biases for each few-shot detection task.\n\n[2] Przewi\u0119\u017alikowski et al. arXiv 2022. HyperMAML: Few-Shot Adaptation of Deep Models with Hypernetworks. It introduced HyperMAML \u2014 an approach to the Few-Shot learning problem by aggregating information from the support set and directly producing weights updates.  \n\n[3] Sendera et al. ArXiv 2022. HyperShot: Few-Shot Learning by Kernel HyperNetworks. It fused kernels and hyper network paradigm. Its hypernetwork takes the support set as input to predict the classifier weights to do few-shot task adaption.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is good.\nQuality is poor due to the limited novelty.\nNot sure about the reproducibility. No code was given.",
            "summary_of_the_review": "It is a good baseline paper based on existing techniques (or losses), but does not contribute a novel idea or solution to class-incremental learning or task-incremental learning explicitly. The paper is clearly written and easy to follow.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5191/Reviewer_UZsQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5191/Reviewer_UZsQ"
        ]
    },
    {
        "id": "LK_83mIDJI",
        "original": null,
        "number": 4,
        "cdate": 1667157736095,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667157736095,
        "tmdate": 1668957787837,
        "tddate": null,
        "forum": "nXmU89Rfmgg",
        "replyto": "nXmU89Rfmgg",
        "invitation": "ICLR.cc/2023/Conference/Paper5191/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a few-shot continual learning approach that uses a hypertransformer to infer the task-specific CNN weights. These weights are then used as inputs to the hypertransformer thus serving as a state for it. The whole architecture resemble a recurrent neural network but with added feature of assimilating few-shot learning. I liked the idea of extending hypertransformers to continual learning.",
            "strength_and_weaknesses": "### Strengths\n- The approach might seem like simple extension of hypertransformers, but to make it work is not easy. Thus I believe it's novel and substantial contribution.\n- The paper is fairly well-written and easy to follow.\n- The results are convincing but could have been better. I expand on this in the following section.\n\n### Weaknesses\n- I would have appreciated a slightly harder task for tiered imagenet i.e. 20 way, 5 shot.\n- From task incremental learning (fig 3), it seems that adding more tasks does lead to some forgetting as performance for $\\theta_{0}$ and $\\theta_{1}$ is consistently higher than $\\theta_{3}$ and onwards (esp. for tiered imageNet). Could authors explain why is this the case?\n- I would have appreciated if the authors had considered some external few-shot continual learning baselines such as FSCL from [Wang et al. 2021b].\n- Instead of freezing the prototypes, could they be updated with momentum update as it was done in momentum contrastive learning [1]?\n- What  does the x-axis of individual plots in figure 3 refers to?\n- Why is there is no MergedHT baseline for tiered imageNet (fig4) and task incremental learning (fig3)?\n- Since you use task embeddings as a state (like in recurrent neural networks). Do these embeddings get updated in between tasks by some other mechanism? For context, I am trying to understand if there could be an issue like vanishing gradients with the proposed recurrent neural network like approach?",
            "clarity,_quality,_novelty_and_reproducibility": "In my opinion, the proposed method is novel. The paper is well written and easy to follow. No code is provided therefore I can't say much about the reproducibility of the work.",
            "summary_of_the_review": "The paper proposes a new way to assimilate continual learning in hypertransformers architecture. I liked the approach, but have few concerns regarding the evaluations. I have pointed them out in the Weaknesses and would appreciate if the authors could address them.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5191/Reviewer_qvBd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5191/Reviewer_qvBd"
        ]
    }
]