[
    {
        "id": "XVwAYn3VMou",
        "original": null,
        "number": 1,
        "cdate": 1665903573599,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665903573599,
        "tmdate": 1665903573599,
        "tddate": null,
        "forum": "TVY6GoURrw",
        "replyto": "TVY6GoURrw",
        "invitation": "ICLR.cc/2023/Conference/Paper1420/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers a realistic DP notion called Inter-silo Record-level DP (ISRL-DP) which can address the shortcoming of central DP and LDP. An algorithm satisfying ISRL-DP requires that the output of this algorithm cannot be distinguished between two adjacent dataset of any agent, given all other agents\u2019 dataset being fixed. The paper then gives a series of risk bounds for federated learning with ISRL-DP. The loss function is assumed to be convex in this paper. \n\n---\nFirst, under the homogeneous setting (i.e. the data distribution is the same across all agents), the paper shows noisy MiniBatch SGD is ISRL-DP. It then gives upper bounds of excess risk for convex and strongly convex loss function. It also gives lower bounds under the case of full participation (i.e. all agents participate in each round), which almost matches the upper bound. Furthermore, the lower bounds are quite general and can be reduced to existing lower bound for central-DP and local-DP.\n\nSecond, the paper considers a heterogeneous setting with full participation, and proposes an one-pass accelerated noisy MB-SGD algorithm with convergence guarantee under the convex and strongly convex loss function. Notably, the upper bound has a nearly optimal rate when the strongly convex loss function is well-conditioned, and is sub-optimal when the loss function is convex. \n\nThird, this paper considers the empirical excess risk minimization, and proposes an algorithm that achieves optimal rate. \n\nFourth, this paper considers a case with a trusted shuffler that can permute the datasets from all agents. It proposes an algorithm that can achieve optimal rate with central-DP under homogeneous agents. Under heterogeneous agents, nearly optimal rate is achieved under the extra assumption that the loss is smooth.\n\nSeveral experiments are conducted and the results corroborate the theory. \n",
            "strength_and_weaknesses": "Strength:\n\n1. Meaningful problem setting: the ISRL-DP is a reasonable notion of DP with practical applications. This paper also explains this point clearly by listing scenarios under which ISRL-DP is appropriate (i.e. when a client only trust its own silo). Therefore, establishing theoretical guarantees for ISRL-DP is a meaningful problem. \n\n2. Solid theory: under the general setting of convex loss function, this paper considers a various of different settings including a (not complete) combination of the following: homogeneous versus heterogeneous agents; convex and strongly convex loss functions; smooth and general loss functions; existence of trusted shuffler or not. This paper gives algorithms for these settings with convergence rate guarantees and ISRL-DP.\n\nThe solid theory is accompanied by rather extensive experimental results. The performance of the proposed optimization algorithm is comparable to that of FedAvg which has no DP guarantee. \n\n---\nWeakness: \n\nThere is no major technical weakness of this paper in my opinion. Still, I think the following is worth mentioning. \n\n1. The homogeneous setting is over-simplified and not very realistic. Basically, since $f$ is the same for all agents, when the distribution $\\mathcal{D}_i$ is the same for all $i$, it means every agent is facing the same learning problem. This seems not very practical. \n\n2. The main text of this paper is not easy to read. The reason is that there are quite a few theoretical results with different assumptions. So it is a little hard to follow. \n\n---\nMinor comments/suggestions:\n\n1. It is confusing when the authors mentioned in the bottom of page 3 that in section 4 they consider a setting where $F$ is an empirical loss. Since from equation 1 and 2 the definition of $F$ involves an expectation over data distribution, the \u2018empirical loss\u2019 confused me when I read for the first time. \n\n2. I suggest making a table of results (i.e. convergence rate, etc.) and assumptions (i.e. smooth or not; convex or strongly convex, etc.). This table can help readers compare different results and follow the settings. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is clear. The theoretical results are clearly stated with description of assumptions and risk bounds. Experimental details are also provided. \n\nQuality: The theory is solid and covers several different settings, and is corroborated with extensive experiments. I did not check all the proof details but they look credible. \n\nNovelty: There is technical novelty because this paper addresses unique technical challenges during the proof of the lower bound and novel analysis framework is used. The authors discuss this point clearly in the paper. So I think this paper has novelty. Furthermore, although the paper is restricted to the convex loss setting, this is not a big problem in my opinion since the paper provides an analysis of ISRL-DP which is lack from the literature. \n\n\nReproducibility: \nI did not check all the proof details but they do look credible to me. As for the experimental results, details are included in the appendix. Based on these two points, I believe this paper has good reproducibility. \n",
            "summary_of_the_review": "I recommend accept.\n\n---\nMy reasons are the following:\nThe problem considered is meaningful and well-motivated. The ISRL-DP is an interesting topic and a theoretical analysis would be valuable to the community.\nThe theory looks solid and covers a variety of settings, most of which are practical. Although the homogeneous setting is not that interesting, I do not think this is a huge problem because it is only part of the paper, and even for the convex homogeneous case this paper seems to give (one of) the first analysis.\nThe proof is complete and looks credible. The proof (I read some of it) is well-written with plenty of explanations.  \nThere are experimental results to support the theory. \n\n---\nThere is something that needs improvement.\nPresentation: The main text should be organized better to make results and assumptions easier to follow. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1420/Reviewer_beiC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1420/Reviewer_beiC"
        ]
    },
    {
        "id": "shfGjJN6Zuv",
        "original": null,
        "number": 2,
        "cdate": 1666589191984,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589191984,
        "tmdate": 1666589191984,
        "tddate": null,
        "forum": "TVY6GoURrw",
        "replyto": "TVY6GoURrw",
        "invitation": "ICLR.cc/2023/Conference/Paper1420/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies Inter-silo record-level DP in cross-silo federated learning, where all communications to the server should satisfy item-level DP. The authors investigate convex optimization and design algorithms that achieve near-optimal performance for both convex and strongly convex cases. With amplification by shuffling, the error rate matches the optimal rate for central differential privacy. The results are supported by empirical evidence.",
            "strength_and_weaknesses": "Strengths\n- This work provides the first convergence results for convex optimization and ERM under ISRL-DP to the best of my knowledge.\n- Near-optimal rates are obtained for stochastic convex optimization and ERM for convex and strongly convex functions. \n- The practical setting where client data distributions are heterogenous is considered and convergence results are provided. \n- The authors provided empirical results for the proposed algorithms, and the performance is encouraging.\n\nWeakness\n- The lower bound proof uses an indirect approach using privacy amplification by shuffling. However, privacy amplification holds under certain conditions on $\\epsilon_0, \\delta_0$ and may lead to extra log factors, so tight lower bounds may not be obtained for all parameter regimes. What's the main difficulty in directly proving lower bounds for ISRL-DP?\n- Presentation: the table summarizing the results (figure 3) can be made larger and more readable. ",
            "clarity,_quality,_novelty_and_reproducibility": "To the best of my knowledge, this work provides the first convergence results for convex optimization and ERM under ISRL-DP \nThe results, contributions, and techniques are clearly stated. \nThe authors provide code for their algorithms and experiments which helps with reproducing the results claimed in the paper.",
            "summary_of_the_review": "This work provides convergence rates for SCO and ERM with convex losses under ISRL-DP, verified by experiment results. A minor issue is that the lower bound technique is indirectly proved and may not lead to tight rates for all parameter regimes. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1420/Reviewer_SUpG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1420/Reviewer_SUpG"
        ]
    },
    {
        "id": "Az950zvfqIY",
        "original": null,
        "number": 3,
        "cdate": 1666624874178,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624874178,
        "tmdate": 1666624874178,
        "tddate": null,
        "forum": "TVY6GoURrw",
        "replyto": "TVY6GoURrw",
        "invitation": "ICLR.cc/2023/Conference/Paper1420/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper analyzes the convergence of FL with record-level DP. It establishes nearly tight upper and lower bound for Noisy-Distributed-MB-SGD in the i.i.d. setting. For the non-i.i.d. setting, it proposes a new DP-FL algorithm (a modified version of Noisy-SGD), and proves the upper bound of its convergence rate. In particular, the upper bound of the proposed algorithm nearly matches the optimal i.i.d. bound when the loss function is strongly convex. This paper further analyzes the upper bound of the convergence rate when the shuffle model is applied into FL. Finally, it conducts numerical experiments to examine the theoretical results.",
            "strength_and_weaknesses": "Strength:\n\n1. This paper fills some gaps in the field of (record-level) differentially private FL convergence analysis. In particular, the authors present the first tight convergence analysis of FL with record-level approximation DP.\n\t\n2. This work provides the first convergence analysis when the shuffle model is incorporated in FL to achieve DP.\n\nWeaknesses\n\n1. The motivation for Inter-Silo Record-Level Differential Privacy (ISRL-DP) is not clear. If I understand correctly, ISRL-DP is identical to the setting where record-level DP is incorporated into local model training process, e.g., train local models by DP-SGD [1], and then submit local model parameters to the (untrusted) server. In other words, it treats each FL party as a database, and answer the query from the server (i.e., submit local models) in a differentially private manner. Therefore, It is unclear to me what really is the difference between ISRL-DP and record-level DP.\n\n2. The proposed algorithm (Algorithm 1) seems impractical. The Accelerated Noisy MB-SGD algorithm samples batches without replacement. In this way, each individual record is only used exactly once for the gradient computation, and thus parallel composition can be applied to account the overall privacy loss. While this modification can help achieve a convergence rate that is close to the i.i.d. setting, I am afraid that this algorithm will perform poorly in practice, because each record is only used once during the entire FL training process. A more general and practical setting is that each record would be sampled multiple times (e.g., DP-SGD algorithm [1]).\n\n3. For the analysis of the non-i.i.d. setting, I do not understand how this paper quantifies the degree of non-i.i.d. In previous work [2], the non-i.i.d. degree is explicitly characterized by introducing a parameter ($\\Gamma$). \n\n======== Reference ========\n\n[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, \u201cDeep learning with differential privacy,\u201d in CCS, 2016.\n\n[2] X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, \u201cOn the convergence of fedavg on non-iid data,\u201d in ICLR, 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality, clarity, and originality of the paper is good.",
            "summary_of_the_review": "This paper presents some interesting theoretical results. In particular, it proves the tight upper and lower bounds for individual-level DP FL algorithms when silo data is i.i.d. On the other hand, the proposed non-i.i.d. algorithm is impractical, thus the empirical novelty and significance is somehow limited. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1420/Reviewer_Gj6i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1420/Reviewer_Gj6i"
        ]
    },
    {
        "id": "FSg1TN9Bze",
        "original": null,
        "number": 4,
        "cdate": 1666675186005,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675186005,
        "tmdate": 1666677791037,
        "tddate": null,
        "forum": "TVY6GoURrw",
        "replyto": "TVY6GoURrw",
        "invitation": "ICLR.cc/2023/Conference/Paper1420/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a notion called Inter-Silo Record-Level Differential Privacy that differentiates from the well studied central DP, client-level DP, and local DP.  Very detailed analyses for convex losses in various cases are conducted, and experiments are provided.",
            "strength_and_weaknesses": "Strength: \n1. The theoretical analysis of this paper is very thorough and informative, most cases with convex losses are considered and both upper bounds and lower bounds are derived. \n2. The considered privacy notion is of high practical relevance. \n\nWeakness:\n1. It could help audience to appreciate the significance of this work better if the authors can highlight the implications of ISRL-DP, and maybe give some examples instead of pure mathematical languages to illustrate this notion.\n2. It can be better if the authors can highlight the technical contributions of these analyses, like are there any technical challenges in obtaining these results comparing with other privacy notions, what make these results different from repeating the machinery of derivations in other DP notions. \n3. For the secure shuffle part, can the authors provide some more discussions with existing works such as [1]?\n\n[1] Cheu, A., Joseph, M., Mao, J., & Peng, B. (2021, September). Shuffle Private Stochastic Convex Optimization. In International Conference on Learning Representations.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is very well written and organized. The main body contains most information but some results are informal due to the abundance of results.",
            "summary_of_the_review": "This paper conducts thorough analysis the privacy notion ISRL-DP in federated learning, the results are solid. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1420/Reviewer_aKXR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1420/Reviewer_aKXR"
        ]
    }
]