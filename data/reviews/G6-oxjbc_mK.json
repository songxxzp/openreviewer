[
    {
        "id": "vqDwCcoWR82",
        "original": null,
        "number": 1,
        "cdate": 1666383423494,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666383423494,
        "tmdate": 1666383423494,
        "tddate": null,
        "forum": "G6-oxjbc_mK",
        "replyto": "G6-oxjbc_mK",
        "invitation": "ICLR.cc/2023/Conference/Paper2882/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": " This paper studies the convergence and generalization of shallow ReLU networks with trainable bias\u2019s initialized to a constant value.  They provide a global convergence guarantee and a local Rademacher complexity based generalization bound.\n",
            "strength_and_weaknesses": "**Strengths:**\nThis paper attempts to relate sparsity of activation patterns to the optimization and generalization of ReLU networks.  I do believe that sparsity of activation patterns in relation to the optimization dynamics and generalization is indeed an interesting topic that deserves study.\n\n**Weaknesses:**\nWhile I do think sparsity of activations is an interesting topic in relation to optimization and generalization, I think the sparsity studied in this present work is unnatural. Initializing the bias\u2019s to some large constant value is not done in practice, and this work does not make an effort justify this form of initialization to induce sparsity.  While they cite [1] which considers this setting, I think this work should try to justify this to the reader on its own.  The number of neurons required by Theorem 3.1 is super-exponential in the bias initialization value $B$, and the convergence rate and step size decay super-exponentially with $B$.  Thus inducing sparsity by setting $B$ to be large comes at a high cost, and this must at least be explained.\n\nIn Theorem 3.1 the requirement on the width $m$ has quadratic dependence on $n$ similar to Theorem 2.3 in [2] (the arxiv version is the one I\u2019m referencing).  The main difference I can see is that in [2] the network does not have bias\u2019s.  My question to the authors is aside from training the bias\u2019s what is the novelty in Theorem 3.1?\n\nTheorem 3.8 appears to be an adaption of Theorem 5.1 in [3].  I do not see what is different here aside from the modification to the NTK that comes from the specific bias initialization.  Also it does not seem that [3] is properly referenced in Theorem 3.8 (also note that the result in [3] is an instantiation of Lemma 22 in [4]).\n\nTheorem 3.11 is essentially outlining a sufficient condition on the labels so that the quantity $y^T (H^\\infty)^{-1} y$ is well bounded.  However this data dependent region they define seems quite arbitrary.  $y^T (H^\\infty)^{-1} y$ can be viewed as the discrete RKHS norm of the target function.  Improved optimization and generalization for target functions that have small RKHS norm has been investigated in [5] and [6].\n\nIn sum, I am not convinced about the contribution of the results of this work in relation to previous works.  The authors emphasize repeatedly that they improve over the results of [1], however I am not convinced that they significantly improve over the works I outline above.  If the authors can provide strong responses to my objections specifying the improvements of their works in relation to these works I will consider changing my recommendation.  However, for now I am inclined to issue a reject recommendation for this submission.\n\n**References:**\n\n[1] Zhao Song, Shuo Yang, and Ruizhe Zhang. Does preprocessing help training over-parameterized neural networks? Advances in Neural Information Processing Systems, 34:22890\u201322904, 2021a.\n\n[2] Samet Oymak and Mahdi Soltanolkotabi.  Towards moderate overparameterization: global convergence guarantees for training shallow neural networks.  Published in: IEEE Journal on Selected Areas in Information Theory, 1(1):84\u2013105, 2020.\nPreprint: https://arxiv.org/pdf/1902.04674.pdf\n\n[3] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 322\u2013332. PMLR, 09\u201315 Jun 2019a. URL https://proceedings.mlr.press/v97/arora19a. html.\n\n[4] Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. J. Mach. Learn. Res., 3(null):463\u2013482, mar 2003. ISSN 1532-4435.\n\n[5] Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HygegyrYwH.\n\n[6] Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approximation perspective. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and \u00b4 R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 253f7b5d921338af34da817c00f42753-Paper.pdf.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nI do have some small questions/comments about the clarity of the writing.\n\nIn the abstract: \n* perhaps make the following edit \u201ctowards zero in linear rate\u201d \u2014> \u201ctowards zero at a linear rate\u201d\n\n* You write \u201cand same-as-previous (ignoring logarithmic factors) generalization bound\u201d.  I think by \"same-as-previous\" you mean that you recover prior bounds.  Please perhaps rephrase as this sentence is a bit confusing to read.\n* Consider rephrasing \u201cUp to our knowledge\u201d as \u201cTo our knowledge\u201d\n\npage 1: Consider making the following edit \u201cThe literature of sparse neural network\u201d \u2014> \u201cThe literature of sparse neural networks\u201d\n\npage 2: \u201cand same as-previous (up to logarithmic factors) generalization bound.\u201d Maybe this sentence can also be rewritten.\n\npage 4: \u201cour study handles the trainable bias in the convergence analysis (which is the first of such a type).\u201d  Can you specify exactly what you mean by this sentence?  There are existing works that train the bias's in convergence analysis.  Which aspect of your work is the first in this regard?\n\npage 4: You state \"To elaborate, our bound only requires $m \\geq \\tilde{\\Omega}(\\lambda_0^{-4} n^4 \\exp(B^2))$, as opposed to the bound $m \\geq \\tilde{\\Omega}(\\lambda_0^{-4} n^4 B^2 \\exp(2B^2)$ in (Song et al.,\n2021a, Lemma D.9).  If we take $B = \\sqrt{0.25 \\log m}$ (as allowed by the theorem), then our lower bound yields a polynomial improvement by a factor of $\\Theta((n / \\lambda_0)^{8/3})$ , which implies that the neural network width can be much smaller to achieve the same linear convergence\u201d.  Can you explain how you reach this conlusion?  The ratio of the two bounds is $B^2 \\exp(B^2)$ which after plugging in $B = \\sqrt{0.25 \\log m}$ becomes $\\frac{1}{4} m^{1/4} \\log(m)$.\n\n**Novelty**\n\nMy concerns about novelty were outlined in the \"Strengths and Weaknesses\" section.\n\n**Reproducibility**\n\nSince this is primarily a theoretical work with proofs I do not find reproducibility applicable here.\n\n",
            "summary_of_the_review": "This work attempts to address an interesting topic (the relation between sparse activations and optimization/generalization).  However the type of sparsity they study (initializing bias's to a large contant) feels contrived and is not justified.  Furthermore, I have doubts about the contribution of their work in relation to prior works.  If the authors can provide strong responses to my reservations which show that I missed essential components of their work I will consider raising my recommendation.  However, for now I am issuing a reject recommendation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2882/Reviewer_neAa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2882/Reviewer_neAa"
        ]
    },
    {
        "id": "AL5rebH-ML2",
        "original": null,
        "number": 2,
        "cdate": 1666799055442,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799055442,
        "tmdate": 1666799055442,
        "tddate": null,
        "forum": "G6-oxjbc_mK",
        "replyto": "G6-oxjbc_mK",
        "invitation": "ICLR.cc/2023/Conference/Paper2882/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers optimization and generalization properties one-hidden-layer ReLU networks trained in the NTK regime, with an initialization of the bias that is fixed to a non-zero constant $B \\geq 0$. This leads to networks with sparse activation patterns, which may be useful for sparsifying network parameters after training. The authors also provide improved bounds on width and generalization compared to previous works studying the NTK regime in similar settings, and complement the theory with simple experiments illustrating the sparsity of activations.",
            "strength_and_weaknesses": "Strenghts: The tighter bounds on width and generalization seem to apply regardless of the sparsity level and could be a useful addition to the NTK literature.\n\nWeaknesses:\n\nI have trouble understanding the motivation behind this work: in the NTK regime, using a different initialization of the bias basically corresponds to using a different activation, namely a shifted ReLU $\\sigma(\\cdot - B)$ instead of $\\sigma$. While this may lead to sparser activation patterns, the width required to achieve similar guarantees to the $B = 0$ case (in the kernel regime at least) needs to be much larger, basically compensating the possible sparsity benefits.\n\nIt is also unclear if the use of a different activation requires an entirely new analysis, as opposed to using previous results. For instance, the bounds on least singular values of the kernel matrix have been tightly characterized in various settings with generic activations, e.g. [here](https://arxiv.org/abs/2007.12826) in certain high dimensional regimes. In particular, such statistical properties typically depend on Hermite coefficients of the activation, and for the shifted ReLU it should be easy to see that these are more or less scaled down by a factor $\\sim e^{-B^2/2}$ compared to the ReLU coefficients.\n\nThat said, I might have missed the motivation of the authors, and encourage them to explain this further. If the key contribution is about improving the bounds, then I'm not sure activation sparsity should be part of the story?",
            "clarity,_quality,_novelty_and_reproducibility": "Well written, but somewhat incremental (see above).",
            "summary_of_the_review": "The technical contributions are potentially useful, but the motivation is unclear.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2882/Reviewer_crhr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2882/Reviewer_crhr"
        ]
    },
    {
        "id": "NYQuN4Knze",
        "original": null,
        "number": 3,
        "cdate": 1666830296415,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666830296415,
        "tmdate": 1666830296415,
        "tddate": null,
        "forum": "G6-oxjbc_mK",
        "replyto": "G6-oxjbc_mK",
        "invitation": "ICLR.cc/2023/Conference/Paper2882/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This is a theoretical paper on an NTK analysis of two-layer ReLU network with a trainable nontrivial initial bias. The motivation of study is that by using an appropriately initialized bias term, the network produces *sparse* activation at initialization; and if one considers the kernel regime then the sparsity is maintained throughout training, hence may be leveraged for efficiency purposes (but this application is not the focus of this paper). \n\nThe main contributions are:\n\n- A convergence analysis to show that GD drives training error towards zero;\n- A generalization analysis to show generalization bound via Rademacher complexity;\n- (On a technical level) A new bound on the smallest eigenvalue of limiting NTK, by resorting to a \"data-dependent region\".",
            "strength_and_weaknesses": "# Strength\n\nSparsity in activation is becoming increasingly important as the size of the models we are training are becoming very large and resource-consuming to use. Existing theoretical study on networks with sparse activations seem rare, and this work may motivate many subsequent studies along this direction, potentially providing justification or guidance for introducing sparse activation in large models in more principled ways.\n\nThe paper is very well written and is easy to follow despite that it contains a lot of theoretical formulations and claims. Though certain relevant discussions seem missing.\n\nThe theoretical results on generalizatoin, convergence, and smallest eigenvalue seems interesting. (Though I am not in a position to understand their significance / novelty on a technical level)\n\n# Weakness\n\nTwo major questions I have:\n\n- Section 3.1 is structured as first presenting the main result (i.e., Theorem 3.1), then explains the key ideas of proving this result and some lemmas along the way. Here, I am not understanding how these lemmas lead to Theorem 3.1.\n\nSpecifically, Thm 3.1 is about training loss decaying to zero. Lemma 3.3 is about bounding activation flipping probability when the change in network parameter is not too large. However, I don't see a discussion on 1) why we expect the change in parameter is not too large, and 2) what is the benefit of bounding the activation flipping probability. Then, Lemma 3.4 provides an error bound on initial loss L(0), but there is no discussion on why we need to bound it given that we only need to bound the ratio L(t) / L(0).\n\n- The paper is about sparsity in activation maps, but there is in general a lack of discussion on 1) how / whether sparsity helps or provides benefits with proving the convergence / generalization results, and 2) how / whether sparsity facilitates convergence / improves generalization by their theoretical results, and whether such results aligns with what can be observed in practice (e.g. faster convergence and better generalization).\n\nOther comments:\n\n- Theorem 3.1: Maybe I missed it but I did not find where delta on the RHS of the inequality is defined.\n\n- Remark 3.2: Is the improvement over Song et al. obtained because this paper considers trainable bias while Song et al. considers non-trainable bias?\n\n- Definition 3.10: Data-dependent Region. This assumption is the key for the third contribution, but it seems quite arbitrary in the sense that we don't know if or how likely it is going to be satisfied in practice.\n\n# Additional Comments\n\nThere are some references on sparse activation in existing network architectures. These are particularly relevant as their sparsity emerges automatically from regular training.\n\n- Rhu, Minsoo, et al. \"Compressing DMA engine: Leveraging activation sparsity for training deep neural networks.\" 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2018.\n- Andriushchenko, Maksym, et al. \"SGD with large step sizes learns sparse features.\" arXiv preprint arXiv:2210.05337 (2022).\n- Li, Zonglin, et al. \"Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers.\" arXiv preprint arXiv:2210.06313 (2022).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is very well-written in general though some discussion seems missing. \n\nQuality: The paper provides solid theoretical study. On a technical level, the significance seems a bit limited as the analysis are mostly based on adapting existing analysis (for the convergence) or making strong assumptions (for bounding the smallest eigenvalue).\n\nNovelty: I believe the theoretical study of a two-layer NN with a trainable and nontrivially initialized bias is novel.",
            "summary_of_the_review": "The paper studies the important problem of sparse activation using a testbed of two-layer neural network in the NTK regime. The presentation is mostly good but there may be some missing discussions. The technical novelty seems a bit weak (though I am not familiar with related studies).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2882/Reviewer_dzEj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2882/Reviewer_dzEj"
        ]
    },
    {
        "id": "IRmY3SFR-A",
        "original": null,
        "number": 4,
        "cdate": 1667111435052,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667111435052,
        "tmdate": 1667111435052,
        "tddate": null,
        "forum": "G6-oxjbc_mK",
        "replyto": "G6-oxjbc_mK",
        "invitation": "ICLR.cc/2023/Conference/Paper2882/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper analyses 1 layer ReLU nets with trainable biases under a fixed bias init using the NTK framework. It polishes the proof techniques used in various other papers and gives better bounds for the overparameterisation required to be in the NTK regime. ",
            "strength_and_weaknesses": "Strengths:\n\nThe paper generalises classic NTK results to the trainable bias setting which is the standard setting for several training protocols. \n\nThe paper gives short and simple proof sketches to get the gist of the proof.The proof sketches do a good job of capturing the main technical novelty/contribution in the paper.\n\nCareful analysis and development of new proof techniques to get better bounds.\n\n\nWeaknesses:\n\n1. Trainable biases with non-constant/random init can easily be modelled with no-bias results using modified data with a concatenated constant to the feature vector. What advantage does constant bias init have over this init? This needs to be made clear.\n\n2. The advantage of a non-zero B initialisation seems non-existent in all the theoretical results -- Theorem 3.1 requires a larger net, and achieves slower convergence with a non-zero B when compared to B=0. The first (and more dominant) term of Rademacher complexity in Theorem 3.8 is independent of B as mentioned in the para after the theorem. Similarly in Theorem 3.11 using a non-zero B seems to only reduce the smallest eigen value. So why should one bother with this initialisation at all?\n\n3. The definition of \"data-dependent region\" is confusing. What is the Probabilitiy in definition of P_{ij} over? \n\n4. The experimental results for different initial biases is underwhelming and incomplete. Why not give results for positive B that would actually increase the number of inactive neurons?  In the experiments here the non-zero biases seem to increase the active neurons, while actually reducing the accuracy. \n\n5. It is also not clear which of the three main theorems the experimental result demonstrates. Perhaps it demonstrates a key property exploited by the Theorems, in which case this needs to be stated as a separate theorem.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly.",
            "summary_of_the_review": "Despite my issues listed in the weakness section, I think the paper makes enough technical contributions and proof technique ideas to merit an acceptance. \n\nThe paper would do very well to motivate why any practitioner would want a constant non-zero bias init both empirically and theoretically.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2882/Reviewer_D4Qh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2882/Reviewer_D4Qh"
        ]
    }
]