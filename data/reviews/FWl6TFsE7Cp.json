[
    {
        "id": "zVRfXYtWgY",
        "original": null,
        "number": 1,
        "cdate": 1666785876159,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666785876159,
        "tmdate": 1666785876159,
        "tddate": null,
        "forum": "FWl6TFsE7Cp",
        "replyto": "FWl6TFsE7Cp",
        "invitation": "ICLR.cc/2023/Conference/Paper914/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new architecture to satisfy the mini-batch consistency (MBC) property, which is an important property required by set functions in the streaming fashion. Specifically, given a mini-batch consistent function $f$ (e.g., slot set encoder, SSE), the authors prove that for an arbitrary set function $f^*$, the composition $F = f^* \\circ f$ is also mini-batch consistency, resulting in the proposal architecture, universal MBC set function (UMBC). Additionally, a Monte Carlo dropout strategy is proposed to cast the UMBC module more robust on out-of-distribution data. Experimental results show promising results on amortized clustering and point cloud classification.",
            "strength_and_weaknesses": "### Strengths\n- The problem raised in this paper is interesting. The attached code is helpful and reproducible.\n- The proposed MC dropout method is nice. Despite being straightforward, it is exactly the first time to consider uncertainty estimation and model calibration in the set function communities.\n### Weaknesses\n- The UMBC method proposed in this paper seems to be an extended version of SSE, which might limit the technical novelty.\n- It seems that the proposed UMBC architecture just composes the SSE ($g \\circ f$) and a permutation invariant/equivariant set function ($f^*$) on the top. It is unclear how the additional set function (i.e., $f^*$) contributes to the expressiveness of mini-batch consistency modeling. I am afraid that if we stack multiple layers of SSE [Bruno et al. 2021], we could achieve the same goal of using one layer of SSE and a set transformer on the top.\n\n- Proposition 4.1 seems to be an important contribution, as it releases the sigmoid constraint in SSE and shows that the softmax attention also satisfies mini-batch consistency. However, it is unclear about the contribution of softmax attention compared to the sigmoid attention. Actually, we can apply arbitrary kernel functions to calculate the attention weights between the inducing points $S$ and the data $X$. One could just replace the sigmoid function with an exponential kernel.\n\n\nBruno, Andreis, et al. \"Mini-Batch Consistent Slot Set Encoder for Scalable Set Encoding.\" *Advances in Neural Information Processing Systems* 34 (2021): 21365-21374.\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe writing has some space to be improved. I list some comments bellow: \n\n- In property 3.3, the notation $f(X)=Z$ is redundant. I think you may want to define the aggregation function as $ g: \\{Z_j \\in \\mathbb{R}^{d^\\prime}\\}_{j=1}^P \\rightarrow \\mathbb{R}^{d^\\prime}$  instead of $g: \\{f(X_j) \\in \\mathbb{R}^{d^\\prime}\\}_{j=1}^P \\rightarrow \\mathbb{R}^{d^\\prime}$, or you could discard the notation $f(X)=Z$ avoid confusion.\n\n- In the line following property 3.3, the definition of $\\sigma(\\cdot)$ is missing. I think it should be a sigmoid function with some normalization constraints. Moreover, it would be helpful to understand if you could explicitly write out what $g$ and $f$ stand for in $\\mathrm{Attention}(S,X) = \\sigma(SX^T) = \\sum_{j=1}^P \\sigma(SX_j^T) X_j$.\n\n- Lemma 4.1 & Theorem 4.1 seems to be a bit redundant. It\u2019s better to merge them together. Moreover, it would be great if there exists an algorithm to show how UMBC works. The equation $X\\in \\mathbb{R}^{N\\times d} \\rightarrow f(X) \\rightarrow \\Phi \\in \\mathbb{R}^{K \\times d} \\rightarrow f^* (\\Phi) \\rightarrow Y$  is helpful for readers to understand. However, it is confusing for me as the MBC aggregation function $g$ is missing in this equation. I think an algorithm (like alg.1 in the SSE paper) would be a better and more direct way to show how UMBC works.\n\n- In lemma 4.1, what does an arbitrary set function stand for? If I understand correctly, it should be \u201carbitrary S2V function\u201d, as a set function is the one that takes a set as input, and the S2V function is a kind of set function which satisfies permutation invariance/equivariance according to definition 3.1.\n\n- I think a better expression in theorem 4.1 could be: \u201cLet $g$ and $f$ be mini-batch consistent, $f^*$ be \u2026. By Lemma 4.1, the composition \u2026.\u201c  Otherwise, it is quite confusing to say $f$ satisfies property 3,3 and at the same time say $g$ is mini-batch consistent.\n",
            "summary_of_the_review": "Overall, although the problem is interesting and the method works well in some cases, I think the work needs to be improved. The major concern is the unclear contribution of the additional permutation invariant/equivariant set function $f^*$ on the top of SSE architecture. In this regard, I think the proposed method is somewhat incremental, and most claimed contributions are similar  as SSE.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper914/Reviewer_SNvt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper914/Reviewer_SNvt"
        ]
    },
    {
        "id": "CRWXl61TGx5",
        "original": null,
        "number": 2,
        "cdate": 1666831460736,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666831460736,
        "tmdate": 1666831460736,
        "tddate": null,
        "forum": "FWl6TFsE7Cp",
        "replyto": "FWl6TFsE7Cp",
        "invitation": "ICLR.cc/2023/Conference/Paper914/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a framework for converting any non-Mini-Batch-Consistent (non-MBC) models to an MBC model. Specifically, for any non-MBC set function, $f^\\star$, we can convert it to an MBC function by plugging in an MBC function before it. This framework also enables incorporating uncertainty estimation methods, such as MC-Dropout for neural set functions. The authors also conducted extensive experiments and ablation studies for demonstrating the effectiveness of their algorithms.\n",
            "strength_and_weaknesses": "Strengths:\n- The paper is mostly well-written, though there are some parts I feel are confusing (detailed later). \n\n- The proposed framework for converting any non-MBC functions to MBC functions is simple and general. \n\nWeakness:\n- I am a bit confused with Lemma 4.1 and Theorem 4.1. As in Proposition 3.3, the Mini-Batch-Consistency (MBC) is defined for a set function f and an aggregation function g, instead of just a single function. So, what do you mean by F satisfy property 3.3? What's the corresponding aggregation function for F?\n\n- Monte Carlo Slot Dropout: why does the composition of the functions $f^\\star$ and the dropout mask still a set function? Once the dropout mask is given, shouldn't the permutation of the slots affect the output? \n\n\n- Figure 4: It seems that UMBC + Diff. EM performs worse than Diff. EM alone, and this is also the case for UMBC + set transformer vs. Set transformer? I.e., UMBC + Diff. EM gives a larger negative log-likelihood, hence a lower log-likelihood. \n\n- Table 1: Why does Oracle perform the worst in terms of NLL? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear. The authors also provide the code for reproducibility.",
            "summary_of_the_review": "Overall, I think the idea in this paper is interesting and novel, though it's not quite hard to derive. However, I have some doubts about the Lemmas and theorems in the paper as well as the empirical results, as detailed in the Strength&Weakness section. Also, I don't work in this area, so I am not able to assess the potential impact of this work. \nI will consider to increase the rating if the authors can address my concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper914/Reviewer_9w5z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper914/Reviewer_9w5z"
        ]
    },
    {
        "id": "_AmWWHUQM0",
        "original": null,
        "number": 3,
        "cdate": 1666863424682,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666863424682,
        "tmdate": 1666863424682,
        "tddate": null,
        "forum": "FWl6TFsE7Cp",
        "replyto": "FWl6TFsE7Cp",
        "invitation": "ICLR.cc/2023/Conference/Paper914/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "**Background**. This work focuses on the problem of so-called *neural set* functions, where the input is a set, the task could be clustering, classification etc., and the map from a set to an output is a neural network.  \nSince the input is a set, there are certain invariances that are relevant for this case, e.g., the output should not change if the order changes, etc.\nAmong these, most relevant to this work is the so-called *Mini-Batch Consistency* (MBC) property, which is relevant for the case when we cannot pass the entire input to the model and we have to partition the input.\nThe MBC property requires that the final output of an auxiliary function that we use should be the same as if we passed the entire (non-partitioned) input.\nFor example, consider we want to take the max element of a large vector: we may partition the input, apply max, and then apply max on top of that, which gives overall an MBC map.\n\n**Summary**.\nLet us say that the overall map is a composition of two functions, say feature and prediction part, where the input of the latter is the output of the former. This paper points out that it suffices that the feature map is an MBC map so that the overall map is MBC (and the prediction map can be an arbitrary function). This insight simplifies the architectural constraints of the prior work of Bruno et al (2020), and allows for handling inputs of arbitrary and varying size/dimension. \nIn addition, dropout is considered on the output of the feature function, as well as several comparisons with some existing prior works.\n\n\n\n",
            "strength_and_weaknesses": "## Strengths\n- This work seems to identify an important problem that having an input of varying size may be hard to handle for neural set functions\n- the paper does a lot of empirical evaluations including comparison with baselines, dropout techniques, and ablation studies\n\n## Weaknesses\n- While this work seems to identify a relevant problem, I do not think it resolves it, please see the next part on motivation\n- although there are numerous experiments, I do not see these as conclusive, that is, it is still unclear to me if the proposed solution is outperforming the baselines, and moreover, how it performs against naive implementations when the input is of variable length, see part below on experiments\n- I find that the theoretical results are relatively simple arguments, see part below on theoretical results \n- [minor] writing: see the section below\n\n\n\n### Motivation\nIf I understand well, there are two main motivations to focus on this problem:\n1. memory issue: when having varying input sizes, if we can set a maximum size of the input we can always use padding with zeros for example, or with some negative number if we use max-pooling etc. However, this naive strategy may result in very large memory requirements. \n2. technical aspect: when we can not specify the maximum input size, the question that arises is how to technically deal with such inputs. \n\nRegarding the former, this paper does not actually resolve the problem. See for example the part on self-attention starting from Proposition 4.1. To perform the normalization one needs to first do a pass to compute the individual normalization constants and then run in mini-batches. However, when backpropagating during training, the resulting model will be the full graph (as if the entire input was passed). Please describe how your method works on self-attention models, and provide wall clock and memory footprint comparisons with the baselines.\n\nRegarding the latter, a baseline would be to simply pre-define a fixed input size and sample randomly that many samples. For example, if the input size is 1000, we can sample say pairs of elements, and we can fix to sample say 10000 such pairs (each time forming the pairs at random). This would increase significantly the dataset size, and the overall model (assuming the task is not simple to just do max-pooling) may perform well. Moreover, at inference time such a procedure would directly yield a confidence interval since multiple samples will be yielded from a full test input, thus the different predictions could be used for confidence intervals.\nPlease include such a baseline in your comparisons.\n\nLastly, I do not understand how, given an arbitrary aggregation function (not necessarily MBC), the herein proposed framework specifies that one proceeds  (as the abstract alludes to)? I understand that if that aggregation function is MBC then the entire map will be MBC as well. Nonetheless, often in practice, one does not know what is a suitable aggregation function (and standard ones may not be).  Hence I do not understand the claim of how *any* function can be transformed into an MBC map -- e.g. if there is prior knowledge that standard MBC functions are not suitable for the problem, but some other non-MBC one is, how could one proceed?\n\n\n### Experiments\n\nIt seems like UMBC does not always improve upon the baseline, e.g. Diff. EM + UMBC in Fig. 4. Similarly, in Tab. 3 sometimes it does not provide improvement.\nCould you provide more insights into this, e.g. additional datasets, or running UMBC with Deep Sets and Slot Set Enc?\n\nPlease also provide comparisons with a naive approach to solve this problem, see the part above on motivation.\n\nRegarding the MCDropout, a natural question that arises is if the baseline methods significantly improve when it is used. Could you provide such experiments for completeness?\n\n\n### Theoretical results\nI find that Lemma 4.1. is relatively obvious since it is easy to see that if we aggregate the results in a way that satisfies the MBC property, the overall output will not change with respect to if we were to pass the full input). \nI do not understand why Theorem 4.1. is not a simple corollary of Lemma 4.1, that is, since in Lemma 4.1  $f$ and $f^\\star$ are arbitrary maps, the statement in theorem 4.1 follows immediately as a special case of it (otherwise, a proof of it would be needed).\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n**Clarity.**\nIn my opinion, the writing and clarity could be significantly improved.\nFor example, by reading the introduction, it is hard to understand what the motivation is, and it only becomes clearer in later sections. Please provide a specific example for a set function, and build on it to motivate focusing on MBC etc.\nSome parts have very long sentences that are hard to understand, e.g., the paragraph on parallel UMBC Heads.\n\n**Reproducibility**. The source code is provided.\n\n### Questions\n- Fig. 2: could you provide some intuition why Set Transformer performs that poorly here (given that in Fig. 4 it does well)? \n- I do not understand why at the end of section 4 you have that dropout may give faster training. Note that dropout still does the same computation\n- I am not sure what the last paragraph of section 4 means. Could you explain?  \n\n### Minor\n- Lemma 4.1. It -> it;  and satisfies -> to  satisfy\n- Fig. 2: it would be helpful to fix the $y$-axis to be the same range everywhere\n- Table 3: seems like Set transformer should be in bold in the first column with the results",
            "summary_of_the_review": "The paper points out a relevant abstract problem of neural set functions. It appears to me that the provided solution does not fully address the problem and the empirical results are insufficient (inconsistent benefit of the herein proposed UBMC, and lack of comparison with the simplest approach to address this).\nThis paper also considers dropout approaches on some parts of the model, but I do not think that these contribute to its novelty, since MCDropout is a general approach, especially since here it is applied to the remaining standard part of the neural net (after the aggregation). \n\n\nMy recommendation is only temporary, and I would be happy to raise my score if the authors address the major concerns or explain if I misunderstood.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper914/Reviewer_BZo6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper914/Reviewer_BZo6"
        ]
    },
    {
        "id": "Ujbs4CKxES",
        "original": null,
        "number": 4,
        "cdate": 1666887021057,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666887021057,
        "tmdate": 1668722155698,
        "tddate": null,
        "forum": "FWl6TFsE7Cp",
        "replyto": "FWl6TFsE7Cp",
        "invitation": "ICLR.cc/2023/Conference/Paper914/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the problem of training neural networks which operate on sets.  The main focus is on the mini-batch consistency (MBC) property, which roughly states the following: Let $X$ be a set, and $(X_1, \\dots, X_n)$.  Given a map $f$ from sets to vectors and an aggregation function $g$, the pair $(f,g)$ are MBC if $g(f(X_1), \\dots, f(X_n)) = f(X)$.  Informally, this means that the aggregated output from processing the partition of $X$ should give the same output as if $X$ was passed directly through $f$.\n\nThe authors show that non-MBC functions can be made MBC by pre-processing the data with an MBC function.  They then introduce a so-called \"universal\" MBC (UMBC) function.  They then present several experiments using their UMBC.",
            "strength_and_weaknesses": "## Strengths\n\n**Writing.**  The paper is well-written.  Of my batch of 7 papers, I would rank this as the second strongest in terms of writing.  The ideas are clearly articulated, and the flow of the paper makes sense.\n\n**Experiments in different settings.**  This paper conducts a range of experiments, from clustering to ablation studies to distribution shift robustness.  This is commendable, and a strong point of the paper.\n\n## Weaknesses\n\n**Defining notation and key terms.**  There were a few points in the paper where I had trouble parsing terms introduced by the authors.  For example, the authors talk about consistency: \n\n> \"Subsequent work has highlighted the utility of Mini-Batch Consistency (MBC), the ability to sequentially process any permutation of a set partition scheme (e.g. streaming chunks of data) while maintaining consistency guarantees on the output.\"\n\nAt this point in the paper (only a few lines in), the reader has no way of knowing what \"consistency\" means here.  It becomes somewhat inferable by the time the reader gets to Section 3, but I fear that some readers will be lost.  It would be worth explaining the main ideas of the paper in a way that does not require technical definitions in the abstract.\n\nOther feedback regarding similar weaknesses:\n\n* The authors seem to use MBC to refer to the term \"mini-batch consistency\" in the abstract and \"mini-batch consistent\" in the intro.  It would be clearer if the authors stuck to using MBC to refer to one or the other of these two phrases.\n\n* It's unclear what the authors mean by a \"valid\" set in Section 3.  \n\n* The presentation of set functions is confusing.  At the beginning of Section 3, set functions $f$ are said to map input sets $X$ to output sets $Y$.  However, later on, Property 3.1 tells us that set2vector functions map sets to one or several vectors.  And from this point onward, the authors seem to assume that all set functions are set2vector (e.g., in Thm. 4.1 -- the main result in this paper).  Given this, I think there needs to be clarification regarding what the input and outputs to the set functions are.  If we assume that $f$ maps sets to sets, then Thm. 4.1 seems to not apply.  If we assume that $f$ maps sets to vectors, then $g\\circ f$ is vector valued, and therefore the composition $f^\\star \\circ g\\circ f$ doesn't make sense if $f^\\star$ takes sets (not vectors) as input.\n\n* The notation in Property 3.2 is confusing.  The equation $f([x_{\\pi(i)}, \\dots x_{\\pi(N)}]) = [f_{\\pi(1)}(X_1), \\dots, f_{\\pi(n)}(x_n)]$ doesn't make sense to me.  Why does the permutation on the LHS go from $\\pi(i)$ to $\\pi(N)$ and why does the permutation on the RHS go from $\\pi(1)$ to $\\pi(n)$.  Should we assume that $N=n$?  What is $i$?\n\n* What do the authors mean when they say that MBC \"[added] a new dimension to the original view of Property 3.1?\"  In what way does this add a dimension?  Are we to interpret this mathematically or intuitively?  If the former, how does dimension come into play?  If the latter, it's not clear *how* this adds dimension, and it would be helpful if the authors could expand on this point.\n\n* Sets are often written as *elements* of $R^k$ for some $k$ or of $R^{n\\times k}$ for some $n$ and $k$.  I find this relatively confusing, because there are also various vectors in play which are truly elements of Euclidean space.  Is there a better way to denote which objects are sets and which are vectors?  For example, is it correct to say that $\\{1,2,3\\}\\in\\R^3$?  I would argue that the answer is no, because $\\{1,2,3\\}$ denotes the same set as $\\{3,2,1\\}$, but clearly as vectors $[3,2,1]$ is not the same as $[1,2,3]$.  So there is an identifiably issue.\n\n* The function Attention$(S,X)$ is used almost a full page before it is defined (on page 4).  This confused me as I was reading.\n\n* In the definition of Property 3.3, what are $n_i$, $d$, and $d'$?  Should they be inferable from context?\n\n* What is a slot-normalized activation function?  In general, after reading the paper, it was not clear to me what a \"slot\" is.  As this seems relatively important, it would be helpful if the authors could give intuition here.  One could certainly read the paper of Bruno et al., but for this paper to be self-contained and so as not to confuse readers, giving intuition for the most relevant related works seems important here.\n\n* What is $\\hat{\\sigma}$, i.e., what does the hat denote?\n\n* What do the authors mean when they say \"arbitrarily hard MBC constraints.\"  What makes one constraint harder than another?  In what sense can this get \"arbitrarily hard?\"\n\n* Why are MBC models like Deep Sets not able to leverage pairwise relationships? \n\n* What do the authors mean by \"simple\" $f^\\star$s?\n\n* In the intro, the authors mention infinite set sizes.  So it's reasonable to ask: How does all of this extend to settings where the cardinality of the input is infinite (countably? uncountably?)?  How does one even construct a partition?\n\n**Experiments.**  The experiments do not seem to support the conclusions made by the authors.  Indeed, I'm concerned that I missed something fundamental here, and if so, I hope that the authors will explain further.  However, it seems to me that baselines like Deep Sets ofter outperform the UMBC approach outlined by the authors.  For example, in Figure 4, the authors say that panels (e) and (f) show the best performance.  However, Diff. Em and Set Transformer (panels (c) and (d)) seem to reach lower values of the NLL for larger test set sizes.  Even for small test set sizes, (c) and (d) seem to do better.  Similarly, in Table 3, Deep Sets gets higher accuracy than all of the other methods on test sets of size 1000 and 2048.  Somewhat confusingly, these numbers are not bolded, despite the fact that (i) they perform better and (ii) (from my understanding) Deep Sets *is* MBC.  In the same categories, Deep Sets also outperforms UMBC on the NLL score.  And in Figure 6, it's not clear that UMBC does much better than the baselines.  So to summarize, the results seem relatively inconclusive as to why UMBC should be used when architectures like Deep Sets seem to offer strong performance.\n\n**Theory.**  The theory is also not a particular strength of this paper.  It's relatively strong to call the results lemmas and theorem in my opinion.  Thm. 4.1 says that if you preprocess the data in an MBC way, any function will be MBC which seems self-evident.  That is, if I have a set function $f^\\star$, if the input is in chunks, I can just use a separate MBC architecture $(g,f)$ to ensure that the output of the $g(f(X))$ is the same regardless of whether I chunked or not.  This holds directly by definition of MBC.  \n\nMore broadly, the question is: What is the impact of this theory?  Does it change our understanding or result in significantly better empirical results?  And based on my understanding of the experiments (see the discussion above), I'm not sure that it does.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity.**  The paper is clearly written.\n\n**Quality.**  Everything seems to be correct.\n\n**Novelty.**  This paper has limited novelty in my opinion.  The theory seems to hold as a sole result of the definition of MBC, and the empirical results seem inconclusive.  So on the theoretical and empirical side, it's not clear what the novelty is here.\n\n**Reproducibility.**  This seems reproducible.  It looks like the authors will release the code.",
            "summary_of_the_review": "To summarize, this paper is well-written and it has a broad array of experiments.  However, there are quite a few points of confusion in the notation and description of the setting.  Furthermore, I would argue that empirical results are inconclusive and that the theory does not constitute a significant contribution.  Therefore, I recommend that this paper not be accepted.\n\n**Post-rebuttal.**  The authors fixed a number of typos, clarified the notation, reworded some parts of the text, and re-captioned the plots of some of the experiments.  All of this has improved the paper, and therefore I will raise my score from 3 --> 5.  However, as discussed in my response, I think that there are still some fundamental issues which make me lean toward reject.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper914/Reviewer_cnQu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper914/Reviewer_cnQu"
        ]
    }
]