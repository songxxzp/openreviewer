[
    {
        "id": "HDF3m703qzp",
        "original": null,
        "number": 1,
        "cdate": 1666386607122,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666386607122,
        "tmdate": 1666386607122,
        "tddate": null,
        "forum": "eLgK35G3A5d",
        "replyto": "eLgK35G3A5d",
        "invitation": "ICLR.cc/2023/Conference/Paper4340/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents the annealed fisher implicit sampler which trains an implicit sampler (i.e GAN style model) to sample from an unnormalized probability distribution. The model is trained to minimize the Fisher divergence (i.e the score matching objective) between the model samples and the unnormalized distribution. This is an appealing objective for unnormalized distributions as the normalizing constant of the target is not necessary. However, we must be able to compute the the score function (i.e log-probability gradient) of the implicit model which is not tractable. The authors propose to use score matching techniques to learn a score network to approximate this quantity. They then propose a surrogate objective involving the target distribution, the score network, and the implicit sampler to minimize the fisher divergence.\n\nThe authors advocate for an annealed training objective which anneals the target from a simple distribution to the desired distribution which improves performance. Also the authors advocate to use a few steps of Langevin MCMC on top of the implicit sampler to correct for errors which they claim improves performance.",
            "strength_and_weaknesses": "Strengths:\n\nThe method presented in the paper in interesting and the provided theory is detailed and appears correct. I was intrigued but the surrogate objective and its equivalence to the original goal of score matching. The theory was pretty easy to follow as well. \n\n\nWeaknesses:\n\nOverall the paper has 2 main weaknesses. First, I do not think the potential issues of the approach are explored in enough detail. Second the empirical evaluation is insufficient and lacking baselines.\n\nPotential Issues:\n\nI am concerned by utilizing an approximate to the score function of the model. This incorporates bias into the training objective. The authors show that when the score network is perfect then we are minimizing FD. How much error (in terms of FD maybe) is tolerable? Is it possible to bound the bias using the FD between the score network and implicit model? I am worried as well that this optimization may run into saddle point issues like with GANs or the neural stein sampler. It might be interesting to see how this approach works in a setting where we can get the exact score function (such as with a normalizing flow model). We could then compare how the normalizing flow performance works when trained with reverse-KL, true Fisher Divergence, and approximated Fisher Divergence (using the score network). Right now its not so clear how these things compare. \n\nEvaluation: \n\nOnly two tasks are used for evaluation and these are relatively small scale in nature. Existing sampling approaches often present results on distributions with thousands of dimensions. I would be more convinced if the authors included at least one such distribution such as a pre-trained normalizing flow or a log gaussian cox process (two benchmarks which are becoming more standard in the field). Next the, paper is severly lacking baselines. In fact, there are no baselines beyond methods presented in this paper. The authors compare their method to the neural stein sampler. Why did they not compare with it? As well, the direct competitor to neural samplers are interactive methods such as MCMC and SVGD. We should also compare with these kinds of approaches. Sequential Monte-Carlo is currently the gold standard in the field and some such method should be used to give an indication of how well this approach performs to the best out there. Besides this there are a number of alternative neural sampling approaches such as the Path Integral Sampler which are beginning to gain traction. It would also be useful to provide one such comparison. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. I had no issue following the math, the ideas, and the logic. The approach is relatively novel (a highlight of the paper) and if it performed well, would be a very interesting direction for further work and improvement. The paper as it is is not particularly reproducible. The paper lacks experimental details explaining the optimizers used, step sizes, batch sizes, training iterations, etc. Without these, I believe a reader would have a difficult time reproducing this work. ",
            "summary_of_the_review": "This paper proposes the Annealed Fisher Implicit Sampler which is a neural implicit sampling method training to minimize a novel objective  which approximates the fisher divergence between the target distribution and an implicit neural sampler. The method is interesting but the empirical evaluation is severely lacking. No baselines are presented besides methods proposed in this paper. As well, the method is not sufficiently explored and its problems (such as bias from score approximation) are not considered. \n\nThis method is interesting, but I believe a more thorough evaluation is necessary to understand how this approach will scale and how it compares to traditional sampling techniques and competing neural approaches. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4340/Reviewer_gNTU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4340/Reviewer_gNTU"
        ]
    },
    {
        "id": "wjAAy7kq87",
        "original": null,
        "number": 2,
        "cdate": 1666534513424,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666534513424,
        "tmdate": 1666534513424,
        "tddate": null,
        "forum": "eLgK35G3A5d",
        "replyto": "eLgK35G3A5d",
        "invitation": "ICLR.cc/2023/Conference/Paper4340/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a method to sample from a distribution with a known un-normalized posterior, named Annealed Fisher Implicit Sampler.\nMore specifically, they introduce the loss with the same gradient as the Fisher Divergence, provided a perfect estimate of the sampler's score function.\nThey train the sampler via alternatively estimating the sampler's score function and updating sampler parameters via the loss.\nThey employ annealing technique to alleviate sampler's training, with the rationale that the sampler is trained easily on easier target distributions.\nThey show the annealed sampler performance on a variety of benchmarks. They provide proofs of the propositions and a theorem, as well as experiment details in the appendix.",
            "strength_and_weaknesses": "*Strengths*:\n- The proposed method is theoretically well-backed. The loss enjoys the same gradient, as (intractable) Fisher divergence, provided a perfect estimate of the sampler's score function.\n- The empirical study of the method's performance show that it indeed can capture target densities.\n\n*Weaknesses*:\n- The experimental results do not compare to any other methods.\n- The clarity of the paper could use some improvements (concrete examples are given in the Clarity section).",
            "clarity,_quality,_novelty_and_reproducibility": "Generally, the paper is relatively clear. Notation-wise, the paper is very sloppy at times. E.g., on page 4, section 3.1, the authors introduce $s_d(x)$ without defining it, and the reader has to guess its meaning from the context. In section 2.2, $q_\\phi$ is an approximation to $p$, whereas in other sections $p$ is sampler's approximation to the target $q$. The frequent change of notation $\\log p(x)$ to $s_\\theta(x)$ (being the same thing) and $s_\\phi(x)$ (being approximation thereof) is also a bit confusing. \n\nIt is not very clear what MCMC correction step means in practice. How are they performed?\n\nAfter the Proposition 2, it is mentioned that minimizing S2D loss is the same as Fisher Divergence. It is true only for a perfect score function estimation. In practice, it is approximate, so a mention of that is worthwhile in the bold text.\n\nTypos:\n- p. 13, in the definition of $l(x, f, \\nabla f)$ -- an unnecesary integration sign.\n- p. 14, second line of the second equation block: forgotten $dx$\n- p. 16: messing up $p(\\tilde{x} | x)$, $q(\\tilde{x} | x)$\n\nThe authors do not provide any baselines or comparison against other methods besides their own. Even though the reported numbers (true posterior vs their method) are quite low, it would be beneficial to compare against competing methods, especially since the authors do cite them.\n\nThe authors provide experimental details, so reproducibility should not be an issue.",
            "summary_of_the_review": "To sum up, the paper provides a novel method to sample from a distribution with a known unnormalized posterior. The contribution has potential impact, as sampling from such distribution has wide applications. The paper could use some improvement clarity-wise. It would be benefitial to provide comparison against other methods. Initial score: borderline.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4340/Reviewer_mcoE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4340/Reviewer_mcoE"
        ]
    },
    {
        "id": "0CavKwf64Y",
        "original": null,
        "number": 3,
        "cdate": 1666556383704,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556383704,
        "tmdate": 1666556383704,
        "tddate": null,
        "forum": "eLgK35G3A5d",
        "replyto": "eLgK35G3A5d",
        "invitation": "ICLR.cc/2023/Conference/Paper4340/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed an implicit sampler by minimizing the Fisher divergence and then propose to improve it by using the annealing technique. Specifically, the author proposed an alternative objective such that minimizing it is equivalent to minimizing the Fisher divergence. Later, the author tried to solve the distant mode problem by creating a sequence of annealed distributions from simple Gaussian to target distribution. Theoretically, the author showed the equivalence of minimizing S2D (the proposed one) to Fisher divergence and also shows that the proposed implicit sampler can be used as an initialization for MCMC chains. \n\nEmpirically, the author conduct 1 synthetic experiment and 1 Bayesian regression to demonstrate the effectiveness of the proposed approach.",
            "strength_and_weaknesses": "## Strength\nThe paper is clearly written apart from the proof of Proposition 2. So in general, the paper is easy to read and understand. Although it is well-known that the Langevin sampler can be controlled by KL divergence and Fisher divergence is stronger (i.e. upper bounding) than other discrepancies, it is still good the formalize it in theorem 1. The proof strategy of theorem 1 is relatively easy to understand. \n\n## Weakness\nAfter reading the paper, I have several concerns: (1) the usefulness of the proposed approach; (2) the clarity of proposition 2, and (3) the empirical evaluation. \n\nFirst, theoretically speaking, Fisher divergence is stronger than many of the well-known divergences and should provide better control over the convergence. However, in practice, it has been found that it is not the case. There have been many works investigating the theoretical properties of using Fisher divergence as the estimator (see [1], [2]). Especially in [2], it has been found that score matching may fail even under the simple student-t distribution. So, I wonder is your proposed approach works under the following unimodal setting: \n1. Implicit sampler is a student t distribution with mean $\\theta$\n2. target distribution is another student t distribution with mean $\\theta_0$, and $\\theta$ and $\\theta_0$ are far away from each other. \nBy minimizing the Fisher divergence, $\\theta$ should approach $\\theta_0$, is this true in practice (without using the MCMC step afterward)?\n\n\nFor proposition 2, I am not sure I understand it correctly. In the end of the proof, the equivalence is established with the $sg(\\cdot)$ operator but in algorithm 1, the $\\theta$ is back-propagated through the score estimator and target score. So why there isn't a $sg(\\cdot)$ in this case?\n\nFor the empirical evaluation, the tasks are too simple to demonstrate the usefulness of the proposed approach. In the introduction, you mentioned the importance of the neural sampler, but I fail to see this based on the current empirical evaluation. For example, sampling from EBM is an interesting experiment since it is well-known MCMC sampling is too slow to train EBM. Another possible experiment is to train VAE where the encoder is the neural sampler. More baseline can also be considered, at least with the neural Stein sampler. \n\n\n\n\n[1] Barp, A., Briol, F. X., Duncan, A., Girolami, M., & Mackey, L. (2019). Minimum stein discrepancy estimators. Advances in Neural Information Processing Systems, 32.\n\n[2] Gong, W., & Li, Y. (2021). Interpreting diffusion score matching using normalizing flow. arXiv preprint arXiv:2107.10072.",
            "clarity,_quality,_novelty_and_reproducibility": "Apart from Proposition 2, the paper is clearly written and easy to follow. However, the empirical evaluation is a bit weak to support the claim made by the paper. In terms of novelty, the proposed S2D seems to be novel to my knowledge. ",
            "summary_of_the_review": "This paper provides an implicit sampler by minimizing Fisher divergence. My major concern is its usefulness due to the property of Fisher divergence and clarity of Proposition 2. Empirically, I think more large experiments should be conducted to see its effectiveness to support the claim made in the introduction. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4340/Reviewer_G8ZU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4340/Reviewer_G8ZU"
        ]
    },
    {
        "id": "JWxIC8wPNG",
        "original": null,
        "number": 4,
        "cdate": 1667056834473,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667056834473,
        "tmdate": 1667056834473,
        "tddate": null,
        "forum": "eLgK35G3A5d",
        "replyto": "eLgK35G3A5d",
        "invitation": "ICLR.cc/2023/Conference/Paper4340/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed a method to design a neural sampler that minimizes the fisher score discrepancy.",
            "strength_and_weaknesses": "\nWeaknesses: \n-- Some aspects of the paper are not clear to me. It seems these things are accepted in the community though. For example, why the need for a separate scoring network and the score estimation step in general ? If I am modeling p_\\theta anyways, why can I not just use it directly? The authors jump between using s() and log p as the score function, which makes sense but the two notations and statements like \"if we asynchronously estimate the sampler\u2019s score function perfectly.\" tells me there is more to this that is not clear from the way it is written.\n\n-- \n\n-- Please explain \"asynchronous\" estimation. When I hear \"asynchronous\", I tend to think of it in context of parallel or distributed optimization. \n\n-- The derivative using s_d (x) below Figure 2 is not clear to me. What is s_d (x) ? Did I miss where it is defined ? \n\n-- Minor: There are some typos and clarifications required, please proof-read .e.g. \n\"seems work\" --> \"seems to work\". \n\"More specially\"\nWhat is \\lambda in Prop 1 ?\nPlease mention what FSD stands for /before/ using it.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not clear to me. Even the motivation about using Fisher discrepancy, and the various steps to correct it including annealing and then corrections. \n\nI am unable to see the relevance to the community and the novelty.\n\ni do not doubt the reproducibility",
            "summary_of_the_review": "There are several aspects which are unclear to me, and hence I can not recommend acceptance. Further, I am worried about the relevance to the iclr community. I will reserve my judgement till rebuttals.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4340/Reviewer_enF9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4340/Reviewer_enF9"
        ]
    }
]