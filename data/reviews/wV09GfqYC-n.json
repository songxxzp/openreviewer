[
    {
        "id": "A4bMocrd0lZ",
        "original": null,
        "number": 1,
        "cdate": 1666305448390,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666305448390,
        "tmdate": 1669647496142,
        "tddate": null,
        "forum": "wV09GfqYC-n",
        "replyto": "wV09GfqYC-n",
        "invitation": "ICLR.cc/2023/Conference/Paper1346/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "**Update after rebuttal** I think the other reviewers have raised some valid criticism, and have definitely helped to make the manuscript more accessible and improve readability. Having read the authors' response I think that all main issues have been addressed in a satisfactory manner (certainly true for the issues raised by me), and therefore remain in favor of accepting the paper.\n\n\nThis paper extends the Reward Machine framework by allowing for hierarchical compositions of reward machines. The basic idea for the construction is that transitions in one reward machine are captured by whole separate reward machines - a higher-level reward machine can call a lower-level reward machine, which allows for compositionality and re-use of reward machines. The paper also shows how to use hierarchical RL to learn policies from hierarchical reward machines, and how to synthesize hierarchical reward machines from data. Evaluations on simple environments prove that the concept works, and show some favorable results compared to flat reward machines. ",
            "strength_and_weaknesses": "**Main contributions, Impact**\n\n1) Hierarchical extension of the Reward Machines framework. Though not entirely unexpected, the construction presented is solid and well executed. I would give this a medium to high impact since it puts a lot of potential follow-up work on solid footing and addresses important shortcomings of basic reward machines.\n2) Illustrative algorithm / proof of concept for learning policies with hierarchical reward machines. This certainly strengthens the paper since it allows for empirical results. I do not feel confident judging the sophistication of the algorithm, and suspect that there might be improvements in the near future inspired by this work. Impact: medium.\n3) Proof of concept algorithm for learning hierarchical reward machines. Another great point to strengthen the paper since it addresses the criticism of having to manually construct reward machines. Quite a number of choices went into the algorithm, and it currently also comes with quite a number of restrictions / assumptions. While it would be nice to see some alternatives explored and compared to, I think this is beyond the scope of this paper (and certainly beyond the page limit), and similarly to 2) I would expect more follow-up work along these lines to appear in the near future. Impact: medium to potentially high (e.g. for learning complex reward functions from human data).\n\n**Strengths**\n\n* Interesting and natural open problem in the Reward Machines framework addressed in a theoretically sound and solid fashion\n* Very well written intro and theory (up to Sec. 4)\n* Adding important bits for empirical evaluation: (i) adapting/suggesting a hierarchical RL algorithm that can learn with HRMs, (ii) suggesting an algorithm to learn HRMs from data. \n\n**Weaknesses**\n\n* Paper is very dense, this comes at the expense of clarity and sufficient detail in sections 4, 5 and 6. Both 4 and 5 (including some empirical results form 6) could be turned into separate full publications. While I appreciate having 4, 5, and 6 in the paper, and would not recommend removing them, the strict page limitations of a conference make some trade-offs necessary. Additionally, the paper comes already with an excessive appendix (which contains interesting parts).\n* Sections 4 and 5 propose two algorithms - both algorithms could have been designed differently, and there are some (more or less) obvious alternatives and ablations that would be nice to see; but the current paper simply does not have space for them (and I do not recommend filling up the appendix even more).\n\n**Improvements**\n1. There is not too much that can be done about a dense paper with an already long appendix, other than aiming for a journal paper with less severe page restrictions. While I think this would improve the paper, I also think that the current version is clearly above the threshold for publication at the conference, and I also think that publishing these results should not be delayed any longer. So I don\u2019t expect to see an improvement here; maybe as a suggestion, the related work section could potentially be moved to the appendix to give 4 and 5 a bit more space.\n\n2. I think it is worth giving Sections 4 and 5 another pass for clarity - it currently seems that all the information is in place, but not presented in the most easily understandable way. Some overview/summary/illustration of the actual scheme proposed and the algorithms implemented for the empirical evaluation would be helpful. Having said that, the two sections do currently pass my threshold for acceptance, but I think could be improved for an excellent paper.\n\n3. Discussion of limitations of (L)HRMs is very short (determinism, having to define labeling functions). It would be nice to expand on this a little bit more, particularly compared to RMs.\n\n**Minor comments**\n\nA) Theorem 1: It would be nice to be more precise what \u201cequivalent\u201d means in the main paper (e.g. equivalent: an automaton that accepts the same language (language = set of all possible label traces \\lambda that respect the MDP transition dynamics)).\n\nB) Learning HRMs - why select tasks and instances with lower returns first? Why not choose highest returns first and switch once the increase in returns starts to saturate (i.e. learn the easiest tasks first, then switch to the next hardest ones once there is no progress on the easy ones anymore).\n\nC) Learning HRMs - how exactly are learned options re-used? \u201cAn option is selected from a\npool of options appearing in lower height RMs\u201d; how is this selection done, probabilistically or by exhaustive evaluation and some deterministic selection criterion?\n\nD) Not sure if the experiments are optimal to show the advantages of non-flat HRMs. I would imagine non-flat HRMs to perform even better in cases where a simple task (such as gathering paper) must be repeated multiple times (e.g. to create a composite object of \u2018book\u2019 and \u2018map\u2019, \u2018paper\u2019 needs to be gathered twice). The current tasks (at first glance) seem to be chosen to collect a series of objects once. I might have missed that the same advantage would play out in flat HRMs (which are different from plain RMs?), so please correct me if this is wrong.\n",
            "clarity,_quality,_novelty_and_reproducibility": "For both clarity and quality the whole paper passes my personal threshold for publication. Having said that, I think the first half of the paper is very well written and of high quality (many important theor. questions and relations answered), whereas 4, 5, and 6 could be slightly improved for an excellent paper. Readers that are completely unfamiliar with the Reward Machines framework might have a harder time with the first part of the paper, but I do not see an easy way to fix that (the paper does not have space for a general RM intro).\n\nThe method and algorithms in the paper are to the best of my knowledge novel and original. The extensive and detailed appendix helps greatly with reproducibility.\n",
            "summary_of_the_review": "The paper constructs and analyzes an interesting and sensible hierarchical extension to the Reward Machines framework, which addresses important shortcomings and puts follow-up work on a solid theoretical footing. It also introduces two important and nontrivial algorithms for learning from HRMs and learning HRMs themselves from data. Overall I think this is a well executed and interesting extension to the Reward Machines framework, which has seen good adoption lately; the paper will be relevant to researchers in hierarchical RL and modeling of complex reward functions / specifications (which itself is a growing area of interest). The main idea of the extension is sensible and straightforward; the execution in the paper is very good and the precise theoretical construction presented is sound and interesting - this is a nontrivial contribution. My main criticism is that the paper is very dense and packs a lot of material into the page limit - while the first sections are very well written and can be well understood (with a background in basic Reward Machines), Section 4, 5, and 6 could be improved in terms of clarity and detail (but adding detail is hard given the already crammed paper). I think this is a high-quality, well-written and well executed paper that is ready for publication, and has good potential to stir follow-up theoretical work and almost certainly more investigations into learning with HRMs and synthesizing HRMs from data, and therefore currently suggest acceptance at the conference. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1346/Reviewer_M97T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1346/Reviewer_M97T"
        ]
    },
    {
        "id": "7KlW5TX3TY",
        "original": null,
        "number": 2,
        "cdate": 1666605802221,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605802221,
        "tmdate": 1666605802221,
        "tddate": null,
        "forum": "wV09GfqYC-n",
        "replyto": "wV09GfqYC-n",
        "invitation": "ICLR.cc/2023/Conference/Paper1346/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper extends the concept of reward machines to hierarchical reward machines (HRM), and presents a method for learning policies for HRMs and for learning HRMs from data (learning is interleaved with policy optimization).  ",
            "strength_and_weaknesses": "Strength:\n- Relevance: the paper tackles an important problem of learning and exploiting hierarchical task specifications.\n- Novelty: The presented formalism (HRM) and the methods for learning policy and HRMs are novel.\n\nWeaknesses:\n- Clarity: The paper is hard to follow (see \"Clarity\" below)\n- Significance: HRMs seem to suffer from too strong assumptions (see \"Quality\" below)",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nI found the paper rather difficult to follow, which is partly due to the fact that I am not familiar with reward machine and the notation used in the paper. However, I also think that the formalism is not well presented:\n\n- Section 2 states that $r(u, u')$ returns the reward for a transition of RM states. However, if I understand correctly, this reward should depend on the actual low-level transition (even with the assumed sparse reward structure, due to discounting). The RM paper (Toro Icarte et al. 2022) instead defines $\\delta_r(u, u')$ which returns a reward *function*, rather than a scalar reward. This deviation is not addressed in current submission.\n\n- I found it difficult to comprehend Figure 1b, in particular the \"not rabbit\" condition in the edge from $u_0^0$ to $u_0^1$, since a corresponding condition does not appear in the edge from $u_0^0$ to $u_0^2$. Only later, I realized that such condition needs to be added to preserve determinism, and that the choice for using the particular edge was arbitrary.\n\n- The paper does not seem to mention whether a tuple is removed from the call stack after a reward machine \"accepts\". If no tuples are removed from the call stack, the statement that \"each RM appears in the stack only once\" would indicate a major limitation, as it would not only imply that RM can not call themself recursively, but also that the same RM can not be used at several stages of a rollout.\n\n- The algorithms for learning policy and HRM are not sufficiently well discussed.  For learning the policies, I found the definition of the reward transition function confusing as it seems to ignore the effects of discounting. Furthermore, I do not see how the algorithm is capable of intra-option learning, given that it uses Semi-MDP Q-Learning and assumes options to terminate for updating the Q-function. I also found it confusing that the descriptions mentions DQN, although the experiments use grid worlds, which I assume used tabular Q functions.\nFor the algorithm for learning HRMs, I find it strange the paper only considers a multi-task setting, where *several* HRMs are learned. Wouldn't it be more sensible to consider learning of a single HRM for one task?  \n \nQuality:\nThe presented formalism of HRMs seems rather limited due to several strong assumptions:\n- The paper only considers sparse reward function, where the reward is 1 for goal states and 0 otherwise.\n- When learning HRMs, dead-end traces must be common across tasks and the depth of each hierarchy must be known a priori.\n- It is not clear to me, whether HRMs can correctly handle discounting, and whether the same RM can be used multiple times during a task (like the \"Navigate\" action in the Tax environment (Dietterich, 2000).  \n\nSeveral aspects of the formalism seem problematic:\n- The requirement that a trace uniquely specifies the option history (\"determinism\") can only be satisfied by introducing arbitrary preferences over subtasks, as in Fig. 1.b.\n- By assuming that every RM contains a single accepting state and a single rejecting state some tasks may require an exponential number of RMs.\n\nNovelty:\nThe formalism of HRMs is novel.\n\nReproducibility:\nThe paper does not provide source code nor sufficient details to reproduce the experiments. ",
            "summary_of_the_review": "The paper makes relevant contributions, which, however, are currently not sufficiently well presented. There are several aspects that need to be clarified, for example, regarding the treatment of discounting and the possibility of executing the same RM several times during a single trajectory.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1346/Reviewer_2jNi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1346/Reviewer_2jNi"
        ]
    },
    {
        "id": "ZYrAhe0nmU",
        "original": null,
        "number": 3,
        "cdate": 1666740827098,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666740827098,
        "tmdate": 1669737941488,
        "tddate": null,
        "forum": "wV09GfqYC-n",
        "replyto": "wV09GfqYC-n",
        "invitation": "ICLR.cc/2023/Conference/Paper1346/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper extends the Reward Machines (RM) formalism to hierarchical RMs (HRMs) by allowing RMs to call other RMs as sub-programs. The paper demonstrates that this allows for more efficient learning of RMs on long-horizon tasks in a number of toy grid-world environments.",
            "strength_and_weaknesses": "# Strengths\n\n- incorporating hierarchy into RMs seems like an important step towards making them more applicable to long-horizon tasks\n\n- the submission rigorously defines all aspects of the problem setup\n\n- the toy experiments demonstrate that the introduced hierarchical RMs learn faster than regular (flat) RMs if a RM hierarchy is pre-defined, they also demonstrate learning of long-horizon tasks with HRMs (not possible with RMs)\n\n\n# Weaknesses\n\n(A) **hard to follow writing**: The writing of the paper is very hard to follow. I attribute this in large part to the very heavy usage of formalism and introduction of terms, which make smooth reading challenging. It seems that the paper could benefit a lot from replacing / augmenting some of the formal definitions with their plain-english equivalents. On other occasions crucial terms are not properly defined. Eg in the background section 2, the terms \u201cpropositions\u201d and \u201clabels\u201d are not properly introduced, no intuition / example is given for what they could refer to. This makes it very hard to understand the paper for readers (like myself) that aren\u2019t already familiar with the RM literature.\n\n(B) **assumptions not clearly mentioned**: compared to other papers that use (deep) RL to learn options & policies the submission seemingly makes a number of assumptions (given subtask hierarchies or at least a priori knowledge about depth of the hierarchy, symbolic observations, access to training tasks for *non-hierarchical* RMs, \u2026). However, these assumptions are only step-by-step introduced throughout the text and can be easily missed. It would be good to add a \u201cproblem formulation\u201d section or similar that explicitly lists the required assumptions for learning hierarchies of HRMs and how they compare to prior works, so that readers can better understand in what use cases they can consider HRMs.\n\n(C) **no comparisons to other option learning approaches**: the only comparison in the experimental section is to flat RMs \u2014 this is a meaningful comparison since the proposed approach is a hierarchical extension of flat RMs. However, since the paper proposes an approach for learning hierarchical options, it should compare to prior works on option learning with deep RL too, if applicable. E.g. the work of Sungryull Sohn could be applicable, since it also addresses hierarchical RL with discrete observational symbols (eg Sohn et al., NeurIPS 2018).\n\n(D) **only tested in toy environments**: the submission tests the proposed approach only in simple 2D grid-world like environments with low-dimensional observation and action spaces. This makes it hard to judge how well the method would scale to more realistic settings, eg. learning robot control in 3D environments.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is rigorous but hard to read. It provides a clear extension of the prior RM work, but significance with respect to other option learning papers is not clearly established.",
            "summary_of_the_review": "Overall, I found the submission quite hard to read. I was not previously familiar with RMs and I found the heavy use of formalism and special terms throughout the submission hampered the reading flow. It also makes it hard for me to properly judge the novelty of the work: my perception is that the submission adds novelty over the previous RM work, but it also seems to make multiple assumptions that can reduce its impact outside the currently rather niche RM topic. The experimental evaluation does not help judge this either, since it is limited to toy environments and does not compare to non-RM works.\n\nI am leaning towards not accepting the submission, but am willing to change my mind if the authors can clearly state their assumptions and how they relate to other option learning papers (ie is there good use cases for their approach that others cannot do, can you compare to any non-RM approach?). Additionally, I am curious to know what other reviewers opinions are, especially for reviewers with a better overview of the RM literature.\n\n=============\nPost-Rebuttal: I updated my score to a weak accept based on the discussion with the other reviewers -- see my reply below.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1346/Reviewer_8bbu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1346/Reviewer_8bbu"
        ]
    }
]