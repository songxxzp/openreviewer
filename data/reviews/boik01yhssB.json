[
    {
        "id": "4GeaOjBFIC6",
        "original": null,
        "number": 1,
        "cdate": 1666673616989,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673616989,
        "tmdate": 1666673616989,
        "tddate": null,
        "forum": "boik01yhssB",
        "replyto": "boik01yhssB",
        "invitation": "ICLR.cc/2023/Conference/Paper2101/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a method to stabilize decision tree learning by randomizing the decision rule for selecting the split. The authors prove several theorems that quantify the accuracy / stability tradeoff, and perform experiments on several datasets of varying sizes to demonstrate the tradeoff in practice.",
            "strength_and_weaknesses": "Strengths\nClear, well-written, principled approach to an important problem. The theoretical results clearly and cleanly demonstrate the performance tradeoffs. The analysis provides stronger bounds than those available from standard DP technique.\n\nWeaknesses\nThe claim in the abstract - \u201cThe experimental results on real-world datasets demonstrate that the proposed algorithm achieves a low average sensitivity with an insignificant decrease in accuracy\u201d - seems not justified. If I understand figure 4 correctly, there is in fact a significant decrease in accuracy for low sensitivity approaches, on many of the datasets.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is presented very clearly, and the theory and experiments exhibit high quality work. The use of randomized selection rules is very common in statistical literature on tree-based models. They occur in random forests, and extremely randomized trees (Guerts et al. 2006). I believe a connection could be drawn, since randomization is used in those cases to stabilize the resulting ensembled model (in the statistical sense, i.e. variance reduction).",
            "summary_of_the_review": "A simple modification to standard tree-building procedures provides some nice guarantees about model stability with respect to ablation of small amounts of training data.  The paper is well-written and technically sound.  The main contribution seems to be the theoretical analysis that precisely quantifies the accuracy and sensitivity bounds. The experimental results do show a tradeoff occurs in practice, somewhat in contradiction to the abstract.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2101/Reviewer_sr4W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2101/Reviewer_sr4W"
        ]
    },
    {
        "id": "y96rXvVDEE",
        "original": null,
        "number": 2,
        "cdate": 1666683764721,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683764721,
        "tmdate": 1669190721911,
        "tddate": null,
        "forum": "boik01yhssB",
        "replyto": "boik01yhssB",
        "invitation": "ICLR.cc/2023/Conference/Paper2101/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an analysis of the sensitivity of a learned decision tree when perturbing the learning set. Authors define perturbation and design a training algorithm which is stable with respect to such perturbation, providing a theoretical quantification of it.",
            "strength_and_weaknesses": "Positive points\n- The topic is definitely interesting\n- The idea, according to my knowledge, is novel and well explained\n- The structure of the manuscript is clear\n- Experiments and findings are interesting\n\nNegative points\n1. I\u2019m not completely convinced about the significance of the proposed ideas. More in detail, I understand that it is interesting to evaluate the sensitivity of tree learning for small changes in the training data, but maybe this is of limited help. Actually Random Forests (or in general ensemble tree methods) start from this diversity and exploit this behavior to enrich single trees, why do we need to correct this? Why do we would prefer to have less sensitive decision trees? Or, in a provocative perspective, why should we work with single trees? I think this is an issue that deserves a discussion. \nA second comment: the theorem is focused on error on the training set, which is not as interesting as the error on the testing set (the generalization error). Other analyses focus on the generalization error.\n\n\n2. I have some concerns on the experiments.\n- The choice of the datasets is not well motivated. Moreover, it is not clear why authors used different sets of problems for different parts of the experimental evaluation: in section 7.2 they used breast cancer and diabetes, in section 7.3 breast, and is section 7.4 all the large datasets. \n- From table 1 it seems that trees for 3 of the 8 datasets have a very low depth, with two having depth 1 (which, according to me, can not be considered as a tree anymore). Why not using a standard depth, such as log(n)? Comparing results from so differently deep trees may lead to biases in the presented results: did authors consider the tree depth in the analysis?\n- Authors repeated the whole procedure 10 times, meaning that the variability across different repetitions can be very high (considering that strong subsampling has been done to create a training set, especially for large datasets). Results only display averaged accuracies, what about standard deviations? Are differences in the plot statistically significant? Without a statistical analysis strong conclusions can not be derived.\n\n3. A minor concern: authors defined \u201cperturbation\u201d as the removal of a single (or more than one) element from the training set. This is not the only possibility, authors should discuss relation with other definitions, like adding noise, or resampling like in bootstrap, just to cite a few.",
            "clarity,_quality,_novelty_and_reproducibility": "According to my knowledge the work is novel and original.\n\nSome comments to improve the paper:\n- Authors formulate their theory only on the basis of a classification problem. Clearly this is very important, but what about other tasks? I\u2019m thinking to regression, but also clustering, density estimation or anomaly detection\u2026 Any idea of how to extend this framework to such cases? For regression it should not be so complicated, whereas for others it can be, especially because we don\u2019t have the concept of \u201coptimal model\u201d.\n\n- Please provide a text explanation of the intuition behind the algorithm proposed in Alg 3.\n\n- Please add around the paper the reference to the material presented in the supplementary part (for example, after presenting the theorems, please add a quick sentence saying that the proof is in the appendix)\n\n- In section 7.3 authors derive some conclusive claims on their methods, but the experiment is based on a single run, with a fixed parametrization of epsilon and m and on a single dataset. Of course this may be interesting for illustrative purposes, but please do not derive conclusive statements on the method.\n\n- In table 1 there is reported the size of the training set, even if only 1000 samples are selected in each run from the second group of datasets (and 80% from the first). Maybe this should be made clearer in the table. \n\n",
            "summary_of_the_review": "Interesting paper with a nice idea. Experiments not completely convincing, some doubts on the significance.\n\nUPDATE AFTER REBUTTAL. I carefully read the responses, the clarifications, and the additional material, and I thank the authors for the significant efforts made in clarifying my doubts. After all, I still maintain my opinion on the paper: this represents an interesting work, with few limitations on experiments and significance (even if I acknowledge the efforts made by the authors in improving these aspects in the rebuttal)",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2101/Reviewer_hkCt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2101/Reviewer_hkCt"
        ]
    },
    {
        "id": "PAgYI9lfXM",
        "original": null,
        "number": 3,
        "cdate": 1667134368457,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667134368457,
        "tmdate": 1667134368457,
        "tddate": null,
        "forum": "boik01yhssB",
        "replyto": "boik01yhssB",
        "invitation": "ICLR.cc/2023/Conference/Paper2101/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper describes a mean to lower the high sensibility of decision trees using randomization of algorithm construction. Some theorems are given and experiments are done showing that in general, trees obtained by the randomization process are less sensitive. ",
            "strength_and_weaknesses": "+: an interesting problem treated in an original way, that I did not know of\n\n-: some unclarity regarding experiments and methodology that I give here, that authors could maybe clarify:\n\n* If I understand Algorithm 3 well, each potential cut or optimal split of the tree is now a probabilistic decision (the cut is made or not). I honestly do not understand how this result in more stability of the tree? In particular, one would expect randomized cuts to put more diversity in built trees? What is the general value of the probability of accepting a cut (it is hard to grasp intuitively at first sight without being familiar with the approach). For example, in Figure 3.b, how does randomization help to have more systematic obtention of the original tree in the second line? \n \n* It would be good to have some idea of the variance values over some of the graphs. For example, in Figure 2.c, one would expect the high variations to be due to variance and not general trends, especially as those looks very small and edgy?\n\n* In figure 4, it seems that the evolution of all the curves in terms of training accuracy is the same for all the settings (for each curve, the sequence of ordinate values are exactly the same. Given the fact that this are randomized splits over different data sets, how is this possible? \n\n-: some unclarity about the reasonableness of the made assumptions, the considered distance and the obtained results:\n\n* I would disagree to some extent that a distance based on the maximal common sub-tree is a natural distance between learned decision trees. In particular, it seems the distance will consider that a split made in the same variable but on a possibly slightly different value will lead to different trees, which is highly questionable interpretability-wise, as two trees with all the same variables but different yet close cut-points would roughly be the same for an expert, yet would be maximally different in this paper. Hence, I would like authors to clarify how much they think this distance is natural, and how much is it a convenient way to get theoretical results? \n \n* Maybe some intuition about the theorems would be good. For example, from the text alone, it appears that theorems 4.1. and 5.1. are exactly the same (at least I could not detect any textual/formal difference). Similarly, it is not clear at all whether Theorem 6.1. is in general useful, as the optimal stump making only one cut may actually have a very bad accuracy (especially at startup)? \n\n* From a non-expert view, it is very difficult to know whether the obtained theoretical results are easy derivation from previous results in the same vein, or are genuinely difficult to obtain. In connection with the above remark, it is also quite difficult to see how strongly connected those theoretical results are to the actual practical implementation/methodology, and helps in predicting the behaviours of those? Maybe a bit more intuition would be needed to fully appreciate this.  ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is well-written, and I think could be quite readable for people familiar with the involved techniques. \n\nQuality: the paper quality is generally fine\n\nNovelty: appear to be quite ok\n\nReproducibility: I could download the package but not install all dependencies, but there is a clear effort to make the code available for basic testing. ",
            "summary_of_the_review": "The paper requires one to know quite well the particular approaches used in the paper (derived by Yoshida and colleagues, for the most part of it), that seems to be well-known in combinatorial optimisation, maybe a bit less in machine learning. As I had to make an emergency review, I could just skim over those and my familiarity with them is too poor to make a strong judgment about the used techniques.\n\nAll in all, the approach looks interesting, but it would require a bit of clarifications for non-expert readers to better follow the paper. In its current form, intuition is sometimes hard to grasp, but the methodology is described with sufficient details to be reproducible. There are some aspects in the experiments that I could not interpret given my understanding, however.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2101/Reviewer_tBhR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2101/Reviewer_tBhR"
        ]
    },
    {
        "id": "UNd0Vjk8yv",
        "original": null,
        "number": 4,
        "cdate": 1667239741851,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667239741851,
        "tmdate": 1670862143872,
        "tddate": null,
        "forum": "boik01yhssB",
        "replyto": "boik01yhssB",
        "invitation": "ICLR.cc/2023/Conference/Paper2101/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "[Varma et al 2021] recently proposed the notion of average sensitivity to measure the stability of solutions produced by graph algorithms. The authors of the paper under review propose to use the AS as a measure of the stability of algorithms learning decision trees w.r.t. random permutations of the training examples. The authors further propose an algorithm whose average sensitivity can be decreased at the cost of decreasing the prediction accuracy. The method is empirically evaluated and compared against standard greedy algorithm.",
            "strength_and_weaknesses": "Strengths:\n\nThe notion of stability of the learning algorithm is both practically relevant and theoretically interesting. There are not many existing papers on the topic. This paper provides a way how to study the issue of stability of decision trees formally. \n\nThe authors prove approximation guarantees for the proposed algorithm and the algorithm's ability to decrease the AS.  \n\nThe method is relatively simple modification of the standard greedy algorithm.\n\nWeaknesses:\n\nAlthough AS is exactly defined, it is not clear how relevant this measure is in practical applications. E.g. it would be more appealing to know a probability with which the learned tree is exactly the same after perturbing the data, or a probability that AS will below a specified threshold. On the other hand currently there are not much other alternatives.\n\nThe proven bounds show that AS of the proposed algorithm decreases with increasing number of examples. However, the bounds do not show why the proposed method should outperform the standard greedy algorithm. On the other hand, experimental results suggest that on small data the proposed method is indeed beneficial if compared to the greedy one.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity. Most of the parts of the paper are clearly written. A weak point is a missing intuitive explanation of the proposed method (Algorithm 3). The algorithm differs from the standard greedy method by introducing randomness in the selection of the decision rule, which results in increasing the stability (i.e. decreasing AS). This is quite counter-intuitive on its own. The implementation of the randomized strategy, which was at least for me important to understand how the stability is introduced, is deferred to the appendix. I believe more space in the paper should be dedicated to explaining how stability can emerge from the randomized learning strategy. \n\nTypos:\n- It is not mentioned that Theorem 4.1. applies to Algorithm 3.\n- pp 5: \"eseentially\"\n- pp 5: Typos in the definition of the set $\\Omega$.\n\nNovelty. To my knowledge, the application of the AS as a measure of the stability of the decision tree learning algorithm is novel. \n\nQuality. The paper seems technically sound. Statements are mathematically precise and supported by proofs. \n\nReproducibility. The authors provide code that allows to reproduce the results.",
            "summary_of_the_review": "The paper introduces the notion of average sensitivity as a measure of the stability of decision tree algorithms and proposes a simple randomized strategy to reduce it. Experiments show benefits of the proposed randomized method over standard greedy algorithm on small sample sizes. The paper would benefit from adding an intuitive explanation of the key idea, i.e. increasing stability of the learning algorithm via introducing randomness. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2101/Reviewer_s724"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2101/Reviewer_s724"
        ]
    }
]