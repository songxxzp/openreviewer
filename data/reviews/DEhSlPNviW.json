[
    {
        "id": "1Z4OO4L4ExZ",
        "original": null,
        "number": 1,
        "cdate": 1666595863250,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666595863250,
        "tmdate": 1666595863250,
        "tddate": null,
        "forum": "DEhSlPNviW",
        "replyto": "DEhSlPNviW",
        "invitation": "ICLR.cc/2023/Conference/Paper947/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper presents a set of descent and optimality properties for the gradient hard-thresholding (GHT) operator widely used in sparse optimization algorithms such as iterative hard-thresholding. Regarding the descent property, the authors establish in Theorem 1 a gradient-norm descent property of GHT for restricted strongly smooth objectives. In terms of the optimality property, the authors mainly show in Theorem 2&3 that the HT-stability condition (as defined in Definition 4) is necessary for a global sparse minimizer, and for any accumulation point of an IHT sequence as well. A simulation study on sparsity recovery with quadratic loss is carried out to verify the theoretical predictions.",
            "strength_and_weaknesses": "# Strength:\n\n 1. The main theoretical claims made in this paper are correct and mathematically sound.\n \n2. The paper is in general well organized and the technical details are clearly presented .\n\n\n# Weaknesses\n\n1. The motivation behind this study looks not clear enough. The theoretical properties proved in the current work are fairly standard, and as admitted by the authors are well-addressed in the previous analysis of iterative hard-thresholding. I cannot see any particular reason why the present study about proving these prior results would be of any benefit for advancing the related lines of research. \n\n2. The technical novelty of the main results is quite limited. More specifically, the gradient-norm descent property as summarized in Theorem 1 has already been developed in the proof of Jain et al. (2014, Theorem 1). With regard to the stability of hard-thresholding, some results identical to Theorem 2&3 have been established by Beck and Eldar (2013), and Yuan et al. (2014, Lemma 21)\n\n\n# References:\n\nP. Jain, A. Tewari, and P. Kar. On iterative hard thresholding methods for high-dimensional m-estimation. NIPS, 2014.\n\nA. Beck and Y. C Eldar. Sparsity constrained nonlinear optimization: Optimality conditions and algorithms. SIOPT, 2013.\n\nX. Yuan, P. Li and T. Zhang, Gradient Hard Thresholding Pursuit, JMLR, 2014.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The technical details are presented in a clear and clean way. The novelty of contribution, however, is not significant with respect to the prior best-known results. The overall quality of this work is below expectation. ",
            "summary_of_the_review": "Based on the above comments, it is assessed that this paper in the current form is not strong enough to be published. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper947/Reviewer_BYNJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper947/Reviewer_BYNJ"
        ]
    },
    {
        "id": "SJhnD4Y6dxF",
        "original": null,
        "number": 2,
        "cdate": 1666603707872,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603707872,
        "tmdate": 1666603707872,
        "tddate": null,
        "forum": "DEhSlPNviW",
        "replyto": "DEhSlPNviW",
        "invitation": "ICLR.cc/2023/Conference/Paper947/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors study the extensively-studied iterative hard thresholding (IHT) algorithm. They establish a new and critical gradient descent property of the hard thresholding (HT) operator, which is related to the distance between sparse points. In addition, they introduce the notion of HT-stable/unstable stationary points, and establish the escapability property of HT-unstable stationary points and the local reachability property of strictly HT-stable stationary points. Moreover, they show that the IHT sequence converges globally under the assumption that the function values at HT-stable stationary points are distinct. Such an assumption is novel and has not been found in the literature. Finally, some simple simulation results are provided for synthetic data.",
            "strength_and_weaknesses": "Strength:\n\nThis paper seems to be technically sound.\n\nWeaknesses:\n\nThis seems to be a pure optimation paper. For example, almost all the closely relevant papers listed in Table 1 are published in optimization journals. In addition, the simulation results are weak and rather limited, and are not related to any practical machine learning/deep learning tasks. \n\nThe authors mention in the abstract that \"the IHT sequence converges if the function values at HT-stable stationary points are distinct, where the last condition is a new assumption that has not been found in the literature\". But from this paper, I cannot see how impactful/meaningful will this new assumption be. \n\nSome minor comments:\n\nIn the abstract, using latex notation of the Lipschitz constant $L$.\n\nWrite down the full name of RSS when it first appears on page 3.\n\nClaim 1 may be better written in ordinary text as it is a very simple and widely-used property, instead of highlighting it as a separate claim.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is generally well-written and easy to follow. \n\nSome new technical results for the extensively-studied iterative hard thresholding (IHT) algorithm, although it is difficult to see how meaningful these new results are. \n\nThis work is mainly theoretical, and very limited simulation results are provided. Thus there is little concern about the reproducibility.",
            "summary_of_the_review": "It is nice to see some new technical results for the iterative hard thresholding (IHT) algorithm. But it is a pity that from the current submission, I cannot see how meaningful/interesting these new results/assumptions are to the machine learning community. \n\nFrom the current submission, my impression is that this is a pure optimization paper, and is better submitted to an optimization journal, rather than a machine learning conference. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper947/Reviewer_F4td"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper947/Reviewer_F4td"
        ]
    },
    {
        "id": "gF0i4l6MmA",
        "original": null,
        "number": 3,
        "cdate": 1666626878701,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626878701,
        "tmdate": 1666626878701,
        "tddate": null,
        "forum": "DEhSlPNviW",
        "replyto": "DEhSlPNviW",
        "invitation": "ICLR.cc/2023/Conference/Paper947/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the stability properties of the hard thresholding operator in gradient descent framework and introduces several new concepts such as HT-stable and HT-unstable stationary points. Moreover, the paper aims to answer four fundamental questions regarding the local/global minimizers and accumulation points of a sparse recovery problem. In the numerical experiments, the authors intend to use some synthetic data sets to justify the proposed theoretical results. However, the paper has limited novelty and lacks real data experiments from applications. ",
            "strength_and_weaknesses": "Strengths: \n1. The paper has attempted to dig out theoretical properties of a classical hard-thresholding operator with some theoretical results. \n2. The new concepts - HT-stable/HT-unstable stationary points - may be useful for other related discussions or studies.\n3. The simulation results verified the proposed theories. \n\nWeaknesses:\n1. There are some minor language issues, and some statements are confusing.\n2. It is not clear how the new concepts are valid and could be used for other studies.\n3. The simulation section is not clearly organized. Since the data matrix is randomly generated, it would be great to check the average performance after many trials. No real applications are involved which would limit the applicability of theories. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity needs to be improved throughout the paper, and some statements should be more formal. Some examples are listed as follows:\n1. In abstract, \"the IHT sequence leaves it\", \"escape them\". \n2. In introduction, \"the $\\ell_0$-norm case have been\"->\"... has been\". \n3. In p.2 \"$\\mathcal{X}$ is a constraint set\"\n4. In (Q1), \"for a local/global ...\" -> \"for the existence of a local/global ...\"?\n5. The description of Corollary 3 is entirely confusing and non-professional.\n6. In p.8, \"whose elements are standard normal\"? Should that be about the probability distribution?",
            "summary_of_the_review": "Although the paper proposes some new theories for discussing the gradient properties of hard-thresholding operators, the novelty and application outreach are limited. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper947/Reviewer_kzNB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper947/Reviewer_kzNB"
        ]
    },
    {
        "id": "XOhN5we0Sxy",
        "original": null,
        "number": 4,
        "cdate": 1666837819540,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666837819540,
        "tmdate": 1666837819540,
        "tddate": null,
        "forum": "DEhSlPNviW",
        "replyto": "DEhSlPNviW",
        "invitation": "ICLR.cc/2023/Conference/Paper947/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work studies the well known iterative hard thresholding (IHT) algorithm for sparse optimization. The algorithm works as follows: In each iteration of this algorithm, a gradient step is taken, and then the hard thresholding operator is applied, which restricts the solution vector to its top $s$ entries in absolute value. The results of the paper are convergence properties for smooth functions. Specifically, the authors prove that the algorithm converges to a stationary fixed point and characterize the properties of such fixed point. ",
            "strength_and_weaknesses": "Strengths:\n- The theoretical analysis is sound.\n- The results are presented in a clear and intuitive way.\n- A numerical simulation of the IHT algorithm is presented.\n\nWeaknesses:\n- My main concern is novelty, which I will explain more about in the next section.\n- The numerical simulation only consists of one random linear regression instance. It would be better to have an analysis multiple instances in order to account for the variance.",
            "clarity,_quality,_novelty_and_reproducibility": "I will expand on novelty.\n\nI believe that some of the results in Section 4.1 are similar to already known results. For example, a very similar property to what is proved in Theorem 1 and Corollary 2 can be seen in the inequality right before (15) in https://arxiv.org/pdf/2204.08274.pdf. \n\nIn addition, I believe the characterization of HT stationary points (also known as fixed points) in Section 4.2 is known. For example, in page 12 of https://people.maths.ox.ac.uk/thompson/SPARSpres.pdf the conditions are the same as Definition 4, so it is known that HT stationary points are the accumulation points of IHT.\n\nThe authors should explain in detail how these and similar previously known results relate to their work, since they seem to have a significant overlap.",
            "summary_of_the_review": "In summary, the theoretical analysis is sound but I have concerns over its novelty that are not addressed by the manuscript. Therefore I cannot recommend acceptance until the contribution of this paper with respect to previous work is clearer.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper947/Reviewer_raL8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper947/Reviewer_raL8"
        ]
    }
]