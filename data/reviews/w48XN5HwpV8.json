[
    {
        "id": "4NixO2T-Pxd",
        "original": null,
        "number": 1,
        "cdate": 1666008799070,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666008799070,
        "tmdate": 1666008799070,
        "tddate": null,
        "forum": "w48XN5HwpV8",
        "replyto": "w48XN5HwpV8",
        "invitation": "ICLR.cc/2023/Conference/Paper6198/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the concept of Trojaned Twin Model, which provides an interesting view w.r.t. the clean model and its trojaned counterpart. In practice, a universal poisoning perturbation is craft on an ensemble of adversarially trained models, and imposed onto the training set for poisoning. Empirical evaluation are done on CIFAR-10, GTSRB and ImageNet.",
            "strength_and_weaknesses": "Strength:\n- The idea of Trojaned Twin Model is interesting, and Definition 2 makes sense to me.\n\n- The writing is clear and the formulation used in this paper is in the appropriate format.\n\n- The empirical evaluation is done on different datasets, while several baselines are compared.\n\nWeaknesses:\n- As to Theorem 1, there is $|1-f^*(x_{bad}+v)-f^*(x_{bad})|<\\epsilon$. For a practical distribution $\\mu(x)$, if $\\epsilon$ is small (satisfying the definition of Trojan Twin), then $x_{bad}$ is usually located at low-probability regions. So consequently, $\\mu(\\mathcal{B}(x_{bad},\\epsilon/(4\\beta)))$, i.e., the ratio of successful trojan $\\rho$ would be quite small. Besides, practical model family $\\mathcal{F}$ usually has a large value of Lipschitz constant $\\beta$, especially for deep networks.\n\n- $|1-f(x+v)-f(x)|<\\epsilon$ is not a good criterion for poisoning, for example, $f(x+v)=f(x)=0.5$ corresponds to $\\epsilon=0$, but this case is not considered as a successful poisoning.\n\n- In Section 3.4, actually, I do not see any close connection between the concept of Trojan Twin and the proposed Algorithm 1. For me, Algorithm 1 is just constructing universal poisoning perturbation on an ensemble of adversarially trained models, which is nothing new beyond [1]+[2].\n\n- Why the poisoning trigger in Algorithm 1 is preferred to be universal, rather than input-dependent? Besides, do the baselines in experiments exploit the same knowledge from adversarially trained models as done in the proposed method?\n\n\nRefs:\n\n[1] Universal Adversarial Perturbations. CVPR 2017\n\n[2] Ensemble Adversarial Training: Attacks and Defenses. ICLR 2018",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of this paper are good, and I admire the novelty of the idea of Trojaned Twin Model. The reproducibility is not check because no code is provided.",
            "summary_of_the_review": "I like the idea of Trojaned Twin Model, which is interesting and formally defined. However, the bound in Theorem 1 seems impractical, and the connection between the theoretical analyses and the proposed Algorithm 1 is loose.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety"
            ],
            "details_of_ethics_concerns": "This paper proposes a universal poisoning attack, which may result in some ethic problem.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6198/Reviewer_YzEQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6198/Reviewer_YzEQ"
        ]
    },
    {
        "id": "2tmuIPzpUn",
        "original": null,
        "number": 2,
        "cdate": 1666580037778,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580037778,
        "tmdate": 1666580037778,
        "tddate": null,
        "forum": "w48XN5HwpV8",
        "replyto": "w48XN5HwpV8",
        "invitation": "ICLR.cc/2023/Conference/Paper6198/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method to backdoor a neural network so that it classifies certain samples containing a trigger as the attacker intended. The proposed approach assumes to have a set of surrogate models to generate a universal perturbation as done in an evasion attack, and take the universal perturbation as a backdoor. The approach is empirically compared to baselines.",
            "strength_and_weaknesses": "Strengths.\n- S1: The paper tries to prove the existence of a backdoored model.\n- S2: The proposed approach is compared to 5 baselines on three datasets.\n\nWeaknesses.\n- W1: The empirical performance doesn't show significant improvement. Actually, the performance is similar or worse than some of the existing approaches.\n- W2: Neural networks are known to be a universal function approximator, and thus the existence of such a model doesn't seem to add much value, especially with adversarial examples with universal perturbation.\n- W3: The approach is already known in both backdoor and evasion scenarios. Also, this type of optimization is commonly used in many attacks, and the approach is a direct adoption of universal perturbation, which has been also used for poisoning. Among many, a quick search shows \"Detecting Backdoor in Deep Neural Networks via Intentional Adversarial Perturbations,\" Xue et al., 2021; \"Hidden Trigger Backdoor Attacks,\" Saha et al., 2020; \"Invisible and Efficient Backdoor Attacks for Compressed Deep Neural Networks,\" Phan et al., 2022; \"AdvDoor: Adversarial Backdoor Attack of Deep Learning System,\" Zhang et al., 2021.\n- W4: The experimental setup is unclear, and there are some modifications to common tasks which makes it harder to compare. For example, the proposed attack uses a set of clean models which is not specified, and can be used as an unfair advantage of the proposed attack, especially if the model has the same architecture or is trained on the same dataset as the victim model.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is in general easy to read for a person with a background on the ML backdoor problem, but there are some typos (e.g., \"is very closed to\"), and missing information (e.g., clean model set, data normalization to understand \"L2 distance less than 10\"). Due to the missing information, reproducibility is low despite the simple algorithm.",
            "summary_of_the_review": "This paper revisits a known property of the backdoor problem. The empirical evaluation is unclear, and the improvement over the existing approaches is not significant. The model set is not explained which is very important in understanding the advantage and limitations of the proposed approach. The related work considers some of the existing optimization-based approaches model-based (the attacker has complete control of the training pipeline and deliver the poisoned model), which I believe is an oversimplification, as many optimization-based approaches use a surrogate model similar to the clean model set used in this paper to generate a poisoned training dataset. In summary, this paper does not seem to show a clear advantage of the proposed approach, has an unclear experimental setup, and misrepresent existing approaches.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6198/Reviewer_8fMD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6198/Reviewer_8fMD"
        ]
    },
    {
        "id": "kH0Xnmwi6v2",
        "original": null,
        "number": 3,
        "cdate": 1666667331878,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667331878,
        "tmdate": 1666667331878,
        "tddate": null,
        "forum": "w48XN5HwpV8",
        "replyto": "w48XN5HwpV8",
        "invitation": "ICLR.cc/2023/Conference/Paper6198/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigate the existence of trojan model for backdoor attack --- a model that is in theory similar to the Bayes optimal yet performs poorly under some universal backdoor trigger. The paper formalizes its notion, proves its conditional existence and show an attack algorithm that generates universal backdoor triggers for real-world tasks.  ",
            "strength_and_weaknesses": "**Strength**\n\nThe paper is technically sound. It provides both theoretically and empirical insights to the problem of backdoor attack.\n\n**Weakness**\n\nI have some questions regarding the technical details of the paper.\n\n1) The theoretical bounds in Theorem 1 and Proposition 1 are somewhat disjoint from the experiment. The experiment checks the existence of universal backdoor trigger, but does not directly check the coefficient of these bounds. While finding the coefficient may be hard for real data, could you provide a toy example for, say, two Gaussian distributions and give the audience a sense of how big the coefficients are?\n\n2) In Figure 2, the horizontal axis is the filter pruning ratio. Could you provide some references on how the pruned filters are selected? Does the curve shape in Fig 2 still hold if a different set of filters are pruned?\n\n3) In Table 3, the mean AIV of your method is >2 against ABS. Why do you use a t-test to show the evasiveness instead of removing all training examples with AIV>2 and then retrain the model on the remaining samples?",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear in writing. The results are novel as far as I know. Relation with previous work is also adequately explained.",
            "summary_of_the_review": "Overall, this paper provides solid theoretical and empirical results for investigating the existence of trojan twin model and universal backdoor trigger. I'm giving a weak accept for now, and willing to raise once my questions are addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6198/Reviewer_HyKs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6198/Reviewer_HyKs"
        ]
    },
    {
        "id": "RqG_QZLh4FX",
        "original": null,
        "number": 4,
        "cdate": 1666795383599,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666795383599,
        "tmdate": 1666795383599,
        "tddate": null,
        "forum": "w48XN5HwpV8",
        "replyto": "w48XN5HwpV8",
        "invitation": "ICLR.cc/2023/Conference/Paper6198/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the backdoor adversarial attack. It theoretically discuss the existence of an attacking counterpart model and corresponding design an attacking algorithm. The proposed method is evaluated on benchmark datasets.",
            "strength_and_weaknesses": "# Strength\n1. Investigating backdoor attacks under adversarial training is relatively new.\n2. The theoretical results seems interesting.\n\n\n# Weakness\n1. The claim that \"existing backdoor attacks cannot resolve adversarial training\" is questionable. The empirical evidences provided by the author do not fully support this claim.\n2. The improvement of the proposed method is relatively marginal.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is a little bit hard to follow considering the usage of many informal expressions. The quality and novelty are relatively good. I did not check the reproducibility.",
            "summary_of_the_review": "As in Strength vs Weakness",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6198/Reviewer_DxEo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6198/Reviewer_DxEo"
        ]
    }
]