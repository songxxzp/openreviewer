[
    {
        "id": "hajs8peZC7",
        "original": null,
        "number": 1,
        "cdate": 1666576785547,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666576785547,
        "tmdate": 1666742018237,
        "tddate": null,
        "forum": "N7ts-GTfuy",
        "replyto": "N7ts-GTfuy",
        "invitation": "ICLR.cc/2023/Conference/Paper1419/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose 4D generative adversarial networks (GANs) to learn the unconditional generation of 3D-aware videos in neural implicit representations. The experimental results show that the proposed method learns a rich embedding of decomposable 3D structures and motions whose quality is comparable to that of existing 3D or video GANs.",
            "strength_and_weaknesses": "**Strengths**\n\n- The paper is clearly written.\n- It is the first paper to generate a 3D-aware video in neural implicit representations.\n\n**Weaknesses*\n\n- The technical contribution should be clearly demonstrated at the end of the Introduction.\n- According to the results in the supplementary material, the generated videos are quite short and the background images are not consistent among frames.\n- The proposed method should be compared to the NeRF-based methods for dynamic scenes, such as D-NeRF.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clearly written.\n- The contributions are only marginally significant or novel.\n- The authors have not provided the code and enough details for reproduction yet.",
            "summary_of_the_review": "Please refer to the strengths and weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1419/Reviewer_rreh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1419/Reviewer_rreh"
        ]
    },
    {
        "id": "OTEWQX_hO6B",
        "original": null,
        "number": 2,
        "cdate": 1666727610900,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666727610900,
        "tmdate": 1666727610900,
        "tddate": null,
        "forum": "N7ts-GTfuy",
        "replyto": "N7ts-GTfuy",
        "invitation": "ICLR.cc/2023/Conference/Paper1419/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a system to generate multi-view consistent videos. We have only seen 3D GANs on image domain so far and this paper extends it to video domain. There are two core components: a time-conditioned 4D generator and a time-aware video discriminator. Experiments are conducted on 3 datasets ranging from talking faces to moving people. The results are promising and encouraging.",
            "strength_and_weaknesses": "**Strength**\n\n- Extending the 3D-aware GAN to video domain is an interesting direction and this paper proposes a legitimate first solution to this problem.\n- The proposed two modules time-conditioned 4D generator and time-aware video discriminator make a lot of sense and work well in practice.\n- The results are encouraging and the generated videos are view-consistent. \n\n**Weakness**\n\n- Claiming the time-aware video discriminator to be one of two core contributions seems inappropriate. This module is introduced in DIGAN (Yu et al., 2022) and no further changes are made in this work. The original module is also introduced for video GAN.\n- It's a bit strange to me that in Eq.1 the time is used as a multiplier. What's the motivation behind this? When time step is 0, this will make entire feature to be 0-vector. Are the other options such as concatenation or positional encoding tested?\n- The writing is messy and inconsistent. For example,  Eq.1 use $\\psi^3\\circ\\psi^2\\circ\\psi^1$ for MLPs, the mapping network is denoted as $w=MLP(z)$, and Eq.3 use $\\phi_{\\sigma}$ to denote 2-layer MLP. Another example, there is an extra comma in eq.4 after $\\gamma(d)$. Another example, $n$ is defined as a vector above Eq.1 but it's used as a function in eq.1.  These are just few examples. Overall, there is lots of room for writing improvement.\n- In the approach it's mentioned there are two training options (section \"Training on Image and Video Datasets\"). Which performs better? I don't find further discussion or ablation study in sec. 4.3. \n- The visual quality isn't very satisfying. I can see artifacts like extra arms Fig.3. Also, these results are of low-resolution and the local fine details are not recovered well. The quality of the generated faces also falls far behind the image 3D GAN. \n- Even though the evaluation metrics are not perfect for evaluating 3D GANs, the quantitative results are just competitive to 2D methods and sometimes worse (Tab.1).\n- Only one 3D static baseline (StyleNeRF) is tested. It's not enough and also StyleNeRF itself isn't the state-of-the-art method.\n- No geometry is visualized or shown in the paper. It's important to demonstrate the learned 3D for 3D GANs in my opinion. Does the network learn reasonable structure from the input data?\n- I don't understand why Tab.2 and Tab.3 are so incomplete compared to Tab.1. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nAs pointed out in the above section, some of the writing isn't very clear. There remains lots of improvement space especially for the approach section.\n\n**Novelty**\n\nOne of the claimed contribution (time-aware video discriminator) isn't novel. For the other contribution (time-conditioned 4D generator), I have some doubts about the way of time conditioning as stated above.\n\n**Quality**\n\nThe visual quality isn't very satisfying and I think the experiments are incomplete and needs some further efforts. \n\n**Reproducibility**\n\nImplementation details are provided in Appendix but no code is submitted. ",
            "summary_of_the_review": "I have concerns in multiple aspects of this work including writing, experiments, and technical contribution. It's unlikely that these weaknesses can be properly addressed during rebuttal period. Therefore, I recommend rejection. However, this paper explores an interesting yet challenging direction and I do encourage the authors keeping improving the quality of this work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I don't have any ethics concerns. I didn't check the research integrity issues (e.g., plagiarism, dual submission) though.\n",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1419/Reviewer_PyKL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1419/Reviewer_PyKL"
        ]
    },
    {
        "id": "KyK2fsEtYO",
        "original": null,
        "number": 3,
        "cdate": 1667334502426,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667334502426,
        "tmdate": 1667335120242,
        "tddate": null,
        "forum": "N7ts-GTfuy",
        "replyto": "N7ts-GTfuy",
        "invitation": "ICLR.cc/2023/Conference/Paper1419/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This works presents approach to multi-view-consistent video generation, whereby the camera viewpoint in a generated video can be adjusted (or the same video can be generated from multiple camera viewpoints/perspectives). Furthermore, it is able to achieve this using 2D image/video streams. By using time-conditioned implicit fields, this work extends methods based on neural implicit respesentation to the video domain, crucially without the requirement for explicit 3D data/supervision. They classify their model as a 4D generative model, i.e. a 3-D aware video generator.",
            "strength_and_weaknesses": "**Strengths**:\n\n- Clear and well written paper, with a well motivated approach to incorporating 3D structure in video generation models and tasks.\n- Great treatise of the capabilities and limitations of the proposed approach.\n- Deep consideration of the ethical challenges and threats of this research (and a strong rejection and condemnation of misuse of such work)\n- Exhaustive consideration of prior and related works.\n- The method appears to be computationally efficient and quite performant when compared to prior art (e.g. Table 1.)\n- Extends the use of INRs to 3D video generation using only 2D image/video data. Whereby the viewpoint in the generated video is controllable parameter.\n\n**Weaknesses**:\n\n- The proposed approach is limited to simple datasets with easy foreground/background separation and limited objects in the foreground. This weakness is acknowledged in the paper and is in general a challenge for the broader field.\n- The approach is limited to generating 16 frames of videos. Again, this weakness is acknowledged in the paper and is in general a challenge for the broader field.\n- Code to reproduce the main experiments is missing.",
            "clarity,_quality,_novelty_and_reproducibility": "- **Quality**: This is a high quality article detailing a concrete and unique contribution to the video generation literation.\n- **Clarity**: This article is well articulated and clear. It exhuastively covers the proposed approach in a manner that makes it straightforward to understand and replicate the contribution.\n- **Originality**: This work is original and unique. It extends the capabilities of current approaches to facilitate a new capability not encountered before in this line of research.",
            "summary_of_the_review": "Overall, this is a great and significant contribution to the video generation literature. It is well-polished and provides for a concrete and unique contribution to the field. I support this papers publication to ICLR. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1419/Reviewer_jBEi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1419/Reviewer_jBEi"
        ]
    },
    {
        "id": "AQRUHJoxWUn",
        "original": null,
        "number": 4,
        "cdate": 1667415318771,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667415318771,
        "tmdate": 1667415318771,
        "tddate": null,
        "forum": "N7ts-GTfuy",
        "replyto": "N7ts-GTfuy",
        "invitation": "ICLR.cc/2023/Conference/Paper1419/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a 3D-Aware Video Generation pipeline. It integrates a GAN model and a time-conditioned implicit field to enable the pipeline to generate 4D videos. The proposed method encodes each video into a 3D content code and a motion code. These two codes, together with the camera pose, determine the video content. In discriminator, the proposed pipeline separately evaluates the quality of each frame and the rationality of the co-occurrence of two given frames in the same video.  ",
            "strength_and_weaknesses": "Strengths:\n\n1. The proposed architecture successfully combines GAN and Implicit field and is the first work to generate 4D videos with GAN.\n\n2. The proposed method outperforms existing works remarkably in terms of ACD and ID. It suggests that this paper significantly improves the temporal consistency, which can also be concluded from qualitative comparison.\n\n3. The paper is clearly written and easy to follow.\n\nWeaknesses:\n\n1. Some tricks are interesting, but the related experiments are insufficient.\n\n    - Image pre-training seems to contribute a lot to the performance. However, there are few ablation studies about this option. In Supplementary Section E, the authors only provide two groups of figures to compare pre-training and joint training. Authors should provide quantitative results about it.\n\n    - Authors claim that one of the key components of the proposed framework is an image-based discriminator. Can we use a deterioration of video-based discriminator (e.g., let $t_k=t_l$) to replace the image discriminator?\n\n    - More qualitative results about the motion generator should be provided. \n\n2. In Table 1, StyleGAN-V outperforms the proposed method in aspects of FVD and FID. Does it mean that the implicit field causes many difficulties for the model to fit the original distribution?\n\n3. Authors should clarify more about the differences between DIGAN and the proposed method. In my opinion, the only significant difference is that DIGAN uses INR directly, while this paper exploits Nerf to equip the model with more 3D information. In the architecture design, the split of FG&BG and the motion generator are the new components compared to DIGAN. However, the former component has been applied in the related works \\[1-3\\]. It is hard to identify the contribution of this paper in its current form.\n\n\n\\[1\\] FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling\n\n\\[2\\] LOLNeRF: Learn from One Look\n\n\\[3\\] NeuralDiff: Segmenting 3D objects that move in egocentric videos",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: well, easy to read and understand\n\nQuality: well\n\nNovelty: not very\n\nReproducibility: looks OK",
            "summary_of_the_review": "An interesting study with some novel designs and extensive experiments. But the novelty should be further justified.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1419/Reviewer_dFag"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1419/Reviewer_dFag"
        ]
    }
]