[
    {
        "id": "XjxYhjJ48w3",
        "original": null,
        "number": 1,
        "cdate": 1666529124905,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666529124905,
        "tmdate": 1666529124905,
        "tddate": null,
        "forum": "wWg_Ee5q_W",
        "replyto": "wWg_Ee5q_W",
        "invitation": "ICLR.cc/2023/Conference/Paper5097/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Iterative NAT improves translation performance with multiple decoding steps, at the cost of sacrificing decoding speed. In response to this problem, this paper proposes to leverage distillation technique to imitate the behaviors of multiple steps with a single decoding step. To this end, the authors maintain two NAT models, namely teacher and student. \n\nThe student model learns to predict the multiple-step output of the teacher with two additional training objectives for the imitation learning: 1) a KL-divergence loss of the probability distributions of target tokens between the teacher and the student; 2) an Euclidean distance between the last layer representations of the two models.\n\nAs the training processes, the performance of student model is improved (for both single- and multiple- decoding steps), which can further enhance the teacher model. Accordingly, the teacher model is updated by the student via a slow-exponential-moving average.\n\nExperimental results show that the proposed approach consistently improves performance over CMLMC (Huang et al., 2022) across language pairs on both distilled and raw data.",
            "strength_and_weaknesses": "Pros:\n1. An promising direction to improve fully NAT, which can maintain the advantage of decoding speed;\n\n2. An interesting way to leverage the performance of multiple-step decoding via distillation.\n\nCons:\n\n1. The approach is relatively complicated. It introduces 3 additional hyper-parameters (decoding steps of teacher model, interpolation weights of KL and Euclidean losses, and ), which potentially threat the robustness of the approach. Are these hyper-parameters the same across models (e.g. CMLMC and Imputer) and datasets (e.g. En-De and En-Ro).\n\n2. The methods to distill multiple steps are practical but not innovative enough. The KL and Euclidean losses are straightforward, and widely-used in imitation learning. \n\n3. The distillation pipeline still relies on an external teacher model (e.g. the pertained NAT model in this work), which prevents the model from learning from scratch (e.g. DAT (Huang et al., ICML 2022)).\n\n\nSuggested Revisions:\n1. The ultimate goal of NAT is  to become an independent translation model that achieves comparable with or even better performance than the AT models. Now the majority of NAT models are base setting (i.e., 6 layers of 512 dimensions) trained on medium-scale data (e.g., 4.5M WMT14 En-De), which still lags behind AT models (e.g., big or deep models trained on 20M WMT17 En-De data). It would be appreciated if the authors can validate the effectiveness of NAT models on big settings and/or larger datasets.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is generally well written and the experiments that have been conducted make sense and are useful for evaluating the proposed methods. ",
            "summary_of_the_review": "This paper focuses on improving the performance of single-step NAT (on raw data), which is a promising and important direction to make NAT be independent model. The proposed Distill Multiple Steps (DiMS) method is effective across datasets and model architectures.\n\nHowever, the method introduces several hyper-parameters, which require a grid-search for individual dataset (e.g. Section 4.5). This method also relies on an pretrained teacher model, which prevents NAT from learning from scratch.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5097/Reviewer_mVeK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5097/Reviewer_mVeK"
        ]
    },
    {
        "id": "6531MdqLpss",
        "original": null,
        "number": 2,
        "cdate": 1666677557435,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677557435,
        "tmdate": 1666677557435,
        "tddate": null,
        "forum": "wWg_Ee5q_W",
        "replyto": "wWg_Ee5q_W",
        "invitation": "ICLR.cc/2023/Conference/Paper5097/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes to use progressive distillation for distilling non-autoregressive models, and update the teacher based on the exponential moving average of the student. The experiments assess the idea on the machine translation task and exhibit some improvement when comparing with single-step results.",
            "strength_and_weaknesses": "Strength: reducing the number of iterations for the non-autoregressive approach is a critical topic for the field, and the use of progressive distillation is a reasonable choice.\nWeakness: the final results are worse than without applying the progressive distillation, and therefore making the contribution limited.",
            "clarity,_quality,_novelty_and_reproducibility": "The uploaded file seems like an image-converted pdf, making it difficult to read.",
            "summary_of_the_review": "While the idea of using progressive distillation for reducing the inference iterations of non-autoregressive models seems appealing, the resulting quality is not convincing and therefore the work provides limited contribution.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5097/Reviewer_oFCe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5097/Reviewer_oFCe"
        ]
    },
    {
        "id": "1-1ZL4SDu1",
        "original": null,
        "number": 3,
        "cdate": 1666718308906,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666718308906,
        "tmdate": 1666718308906,
        "tddate": null,
        "forum": "wWg_Ee5q_W",
        "replyto": "wWg_Ee5q_W",
        "invitation": "ICLR.cc/2023/Conference/Paper5097/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This article proposed a distill multiple steps method to decrease the number of required steps to speed up iterative non-autoregressive transformer.",
            "strength_and_weaknesses": "Strengths:\n\n1. The training of the distilled student model obtained improved training efficiency with a certain translation quality. \n\n2. The proposed approach gained improvement on the CMLM on the WMT14 EN-De and WMT16 Ro-EN benchmarks.\n\nWeaknesses:\n\n1. For many comparison methods in Table 1, the reported results appeared to be lower than the original paper (e.g., the CMLM model).\n\n2. If the training time of the teacher model is also calculated, the efficiency of the final student model may not necessarily be improved.\n\n3. As shown in Figure 3, the BLEU scores of the proposed method were inferior to those of the baseline CMLM and CMLMC models.\n\n4. There were too many hyper-parameters (e.g., n, lambda, and p) in the proposed DiMS.",
            "clarity,_quality,_novelty_and_reproducibility": "The contribution of this study is small in the machine translation community.",
            "summary_of_the_review": "Please see review in the previous sections.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5097/Reviewer_ABHG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5097/Reviewer_ABHG"
        ]
    }
]