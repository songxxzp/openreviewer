[
    {
        "id": "otWKZ5F2Fay",
        "original": null,
        "number": 1,
        "cdate": 1666386342583,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666386342583,
        "tmdate": 1669224520198,
        "tddate": null,
        "forum": "tQG-o3SeipT",
        "replyto": "tQG-o3SeipT",
        "invitation": "ICLR.cc/2023/Conference/Paper2130/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a counterexample to prove that certified cascading robust classifiers does not hold up to its certified robustness claim. Then, they proposed an attack method to show that on real data, models certified for cascading robust classifiers failed to held up to their certification. In other words, there are adversarial examples that makes the certified robust accuracy significantly lower than the certification. Finally, they proposed an alternative ensemble method, weighted voting ensemble, that is provably sound (can correctly provide a certified robust radius).",
            "strength_and_weaknesses": "Strength\n- The paper is straight forward, rigorous, and easy to follow.\n\nWeaknesses\n- I am not confident how important/widespread cascade robust classifiers are. If it is barely used, it may limit the scope of this paper.\n- In Table 4 and 5, is the CRA for cascading ensemble the correct CRA or the overestimated CRA? It seems it is the latter one. If that's the case, I think it is necessary to compare the ERA with the CRA of weighted voting ensemble so that we can understand how effective is the weighted voting ensemble.\n- It also seems that weighted voting have CRA and Acc less than or equal to a single model. So what could be the use case of this ensemble over a single model.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper seems reproducible, clear, and easy to read. It does point out some issue with the robust certification of cascade classifiers that is not previously known. \n\nThe result seems reproducible.\n\n",
            "summary_of_the_review": "I think this is a well-written paper. However, the paper seems to have a limited scope and the proposed method seems not so useful.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2130/Reviewer_YgY8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2130/Reviewer_YgY8"
        ]
    },
    {
        "id": "rjuUMIXf1BC",
        "original": null,
        "number": 2,
        "cdate": 1666628884086,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628884086,
        "tmdate": 1666628884086,
        "tddate": null,
        "forum": "tQG-o3SeipT",
        "replyto": "tQG-o3SeipT",
        "invitation": "ICLR.cc/2023/Conference/Paper2130/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Cascading ensembling of certifiably adversarially robust classifiers, which has been used by previous works, is shown to be unsound, which is demonstrated by theoretical proof as as well as experimental results on real test set data.",
            "strength_and_weaknesses": "Strengths:\n\n- The theoretical introduction and methodology are clearly presented and appear to be sound.\n\n- The experimental evaluations are very strong, as (contrary to other counterexamples against similar certifications) they work with fixed input points and fixed radii. I.e. not finding specific, potentially non-sensical input images but evaluatin the actual test set.\n\nWeaknesses/Issues/Suggestions/Questions:\n\n- The paper is mostly about showing that a previously presented improvement of adversarial robustness certification is invalid.\nI think it is very important to point out crucial issues with previously published and well cited papers, so I think that the submission is a good contribution for ICLR.\nHowever, I am not sure how popular/relevant the cascading methods in [1] and [2] are currently and if they are nominally relevant when applied to ensembles of and compared with state of the art certifyable robusntess techniques. A bit more evidence for them being used would underline the importance of the submission.\n\n- A more concrete overview of the attacked ensemble from [1] would be helpful. Particularly on how the sequence of models has been trained.\n\t- I would assume that if the models were trained independently (e.g. just from different seeds), the attack success rate on the cascading ensemble would be much smaller. Is that correct?\n\n- It would be helpful to emphasize that cascade ensembling over different certification methods for one classifier, as e.g. done in [3], is sound.\n\n- \"We solve the optimization problems on lines 6 and 8 using projected gradient descent (PGD)\" -- Why do you not use a stronger adversarial attack scheme, like AutoAttack [0]? Intuitively, the same considerations as for adversarials should apply for optimizing the objective for $\\tilde{F}(x+\\delta)$, right? Since and attack and not a defense is presented, this is not a major issue since the claims made by the paper could only be improved, but it could lead to improvements and might be the correct thing to do.\n\n- The paragraph on \"Surrogate Objectives\" is very confusing to me and should potentially be rewritten.\n\t - Optimizing the indicator functions does not seem to make sense: Would $argmax_\\delta \\mathbb{1}[\\tilde{F}_{cert}^{(i)}(x+\\delta)]$ not be realized for too many points, often including $\\delta = \\vec{0}$ since the function value is either $0$ or $1$?\n\t - Before, completely black box models were considered, but now access to the logits is assumed; this new assumption should be stated.\n\t - For the sake of demonstrating wrong certifications, it might make sense to use a specific objective for each model that appears in the counterexample. \n\n- There are very small differences between the evaluations in Table 1 and the original numbers from [1]. How can they be explained? Potentially, the explanation should be included in the paper.\n\n- The proposed weighted does not seem to be a convincing contribution.\n\t - Its results are not better than using the single first model (correct?).\n\t - Its construction and its theoretical soundness are not particularly surprising or insightful. \n\t - The case presented in Example 5.4. shows that there are situations where a voting ensemble does better than a single constituent model. However it doesn't seem to be possible to decide if this is the case in a given situation (without knowing the test labels). There are certainly also many situations where it is the other way around, and the voting ensemble is worse than a single model.\n\t - Still, it is a baseline alternative to cascading ensembling, and thus it is interesting to have it discussed and evaluated in this paper, I would just not see it as a recommendable alternative to using a single model, as I understand the results.\n\n[0] Croce and Hein, ICML 2020: \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks\"\n\n[1] Wong et al., NeurIPS 2018: \"Scaling provable adversarial defenses\"\n\n[2] Blum et al. 2022: \"Boosting barely robust learners: A new perspective on adversarial robustness.\"\n\n[3] Gowal et al. 2018: \"On the effectiveness of interval bound propagation for training verifiably robust models.\"",
            "clarity,_quality,_novelty_and_reproducibility": "- The presentation is clear (maybe except for the \"Surrogate Objectives\" paragraph) and easy to follow.\n- The issue with cascading ensembles has not been known previously as far as I am aware, thus the main result is novel.\n- The used models are clearly referenced, and the methods are described well enough that manual re-implementation should not be an issue.\n- The submitted code appears to be well documented and to contain all relevant experiments (though I didn't run it myself yet).",
            "summary_of_the_review": "- The paper points out an important mathematical issue with the method of cascading ensembling of certifiably adversarially robust classifiers and shows with convincing experiments that this issue is relevant in practice.\n- This makes it an interesting contribution in my opinion.\n- The proposed weighted voting ensembling method is to my understanding not convincing by the discussed theoretical properties or experimental results, but it does not really hurt the paper and its efficacy is not overclaimed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2130/Reviewer_VBmo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2130/Reviewer_VBmo"
        ]
    },
    {
        "id": "oLE9cqz3ty",
        "original": null,
        "number": 3,
        "cdate": 1666642110772,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642110772,
        "tmdate": 1667080539180,
        "tddate": null,
        "forum": "tQG-o3SeipT",
        "replyto": "tQG-o3SeipT",
        "invitation": "ICLR.cc/2023/Conference/Paper2130/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper pointed out a fallacy that cascading robust classifier is provably robust to epsilon perturbation. The authors first pointed out a counter example of such method and then provided a systematical algorithm to generate the adversarial examples from a sequence of provably sound classifiers. This paper essentially showed that chaining the epsilon-robust classifiers are by itself not epsilon-robust.",
            "strength_and_weaknesses": "Strengthes:\n- The layout of this paper is good and easy to follow, pleasant to read and the idea is interesting. In fact, I am surprised such mistake in cascading classifiers are pointed out until now. \n- The completeness of this paper is good: the authors first provided a theoretically sound proof; followed by a practical algorithm to find the adversarial examples; and finally intensive experiments are made to show that such algorithm is indeed able to \n\n\nWeaknesses:\n- Does (Blum et al. 2022) also employ the erroneous cascading ensemble? I roughly checked their paper and it seems not the case.\n- It looks like even in (Wong 2018), the cascading ensemble was still claimed as one of the smaller contributions (only mentioned in one paragraph). So I think the negative impact of the unsoundness is limited ==> not sure whether publishing in a workshop is a better option (but I left this for authors and ACs to decide).\n- Another question is -- instead of defining $\\tilde{F}_{\\text{cert}}=1$ \n\nas  $\\tilde{F}_{label}$ to be epsilon-locally robust, can we define \n\n$\\tilde{F}_{cert}=1$ \n\nto be \n$\\tilde{F}_{label}\\text{ is } \\epsilon$-locally robust AND\n\n$\\tilde{F}_{label}$ equals to the ground truth $y$? \n\nBecause if the classifier makes a mistake, then we don\u2019t need to care about its local stability at all. By strengthening the definition, the cascading classifier seems to be correct again.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality, novelty and reproducibility are all good. Overall, this is an interesting paper.",
            "summary_of_the_review": "There aren't a lot to comment for this submission, after all this is a correction of previous misbelief. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2130/Reviewer_XiH9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2130/Reviewer_XiH9"
        ]
    }
]