[
    {
        "id": "x3kxs9sLZLU",
        "original": null,
        "number": 1,
        "cdate": 1665955835386,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665955835386,
        "tmdate": 1665955835386,
        "tddate": null,
        "forum": "54F8woU8vhq",
        "replyto": "54F8woU8vhq",
        "invitation": "ICLR.cc/2023/Conference/Paper2938/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose an algorithm, called CHAOS (Context and History Aware Other-Shaping) with the goal of capturing both learning context and history.  ",
            "strength_and_weaknesses": "Strength: \n- Some empirical results. \n\nWeaknesses:\n- The novelty is unclear.\n- Presentation lacks clarity and precision. The theory background is incomplete and vague, also contains several mistakes/undefined parts. \n- No actual result is presented beyond some empirical evaluation, which is neither conclusive nor sufficient. \n- No formal reasoning or meaningful argument is given. I am not sure what the usefulness of this work would be at all for the ICLR audience.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: poor\n\nQuality: poor\n\nNovelty: marginal ",
            "summary_of_the_review": "N/A\n\n ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2938/Reviewer_PvhL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2938/Reviewer_PvhL"
        ]
    },
    {
        "id": "aBYTfRET-_",
        "original": null,
        "number": 2,
        "cdate": 1666387650993,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666387650993,
        "tmdate": 1668700551839,
        "tddate": null,
        "forum": "54F8woU8vhq",
        "replyto": "54F8woU8vhq",
        "invitation": "ICLR.cc/2023/Conference/Paper2938/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates training an agent that's own learning algorithm attempts to \"shape\" how its' co-players learn. This shaping ideally should cause it so that the co-players' play an equilibrium that is favorable to the agent employing the shaping. This technique was previously studied in the Model-Free Opponent Shaping paper referred to in the abstract. They show that a policy trained through evolutionary strategies is able to employ such a co-player shaping policy. ",
            "strength_and_weaknesses": "**Strengths**\n - The authors demonstrate that memory persistent across episodes does not confound their results, which I suspected during reading the first portion of the paper.\n \n**Weaknesses**\n - This work appears to have significant overlap with the prior work M-FOS to the point where I'm not sure if it is different. Inspection of the Algorithm block from this paper and the previous are almost identical. I believe the CHAOS one also contains a typo and that the phi on line 13 should be theta. Moreover, in Section 4 of the M-FOS paper they directly mention that genetic algorithms (a type of evolutionary strategy) would work well within this framework. \n - The method section is challenging to follow and not technical enough to be confident in how the algorithm actually works. I would encourage the authors to add exact optimization functions and how they fit within the overall algorithm. \n - If it is true that the previous work cannot handle settings where DNN implement policies, a statement I am not convinced of from this manuscript, I would like to have seen a demonstration of this algorithm in such a setting and explanation as to why the previous work could not handle it. ",
            "clarity,_quality,_novelty_and_reproducibility": "- I found the paper surprisingly challenging to follow without having looked at the previous work (M-FOS). \n- The novelty appears to me to be marginal, empirically demonstrating an instance of the prior work. \n- The authors include detailed hyperparameter listings, but insufficient details to implement their algorithm. Without source code being released I would not expect this work to be reproducible. ",
            "summary_of_the_review": "I do not think the paper as it currently exists is ready for publication. I am not convinced that this work is significantly different enough from the previous work to justify publication (it appears to just be allowing a RNN's state to persist across episode). Additionally, I found the prose rather challenging to follow and the claims and method not clear enough to contribute to the scientific discussion of the area.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2938/Reviewer_Ae1h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2938/Reviewer_Ae1h"
        ]
    },
    {
        "id": "SeY_fofGTMk",
        "original": null,
        "number": 3,
        "cdate": 1666650540230,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650540230,
        "tmdate": 1668849704626,
        "tddate": null,
        "forum": "54F8woU8vhq",
        "replyto": "54F8woU8vhq",
        "invitation": "ICLR.cc/2023/Conference/Paper2938/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a meta-learning approach to shaping the policy of other agents in a multi-agent learning setting. The proposed method, called CHAOS, meta-trains an RNN agent policy that utilizes both intra-episode history and inter-episode context information to select actions so that it can successfully shape the policies learned by the other agents to maximize its own rewards. Experiments are performed on 2-player matrix games to show that CHAOS outperforms or matches previously proposed meta-learning based other shaping algorithms that become computationally expensive with increasing action space dimensions or that do not utilize context and history information in their policy. \n",
            "strength_and_weaknesses": "The paper contextualizes the prior work and outlines the proposed improvements compared to the related approaches M-FOS and GS. The following points require further correction / clarification. \n \n1) There are some typos throughout the paper.\n- Sec 3, POSG: reward function is $\\mathcal{R}$ and not $\\mathcal{\\tau}$? \n- Sec 3, POSG: single player case $\\mathcal{I} = \\{1\\}$ or $N = \\{1\\}$?\n- Page 4, Good Shepherd: The gradient is wrt $\\phi^e_{-i}$?\n- Page 4, Good Shepherd, last sentence in the paragraph is incorrectly framed?\n- Sec 4, paragraph 2: Inconsistent notation in policy ($\\phi$, $\\theta$) and distribution ($\\rho$). \n- Sec 5.3: \u201c...1)identifying the \u2026 an co-player\u2019s \u2026\u201d -> \u201c..identify\u2026. a co-player\u2019s\u2026\u201d\n2) Algorithm 1: Line 9 - is the naive learner updated every time step or after T steps in the trial? There is no mention of E episodes. How are $\\theta_m$ and $\\theta_n$ different? Line 13 - update $\\phi$ or update $\\theta_m$ or $\\theta_n$?\n3) It would help to clearly describe the evaluation setting in the meta-learning framework. Are the results reported for 1 trial after the training has converged for both the meta-learner and the opponent? Is the opponent being trained from scratch during the evaluation trial? Is the opponent reset / re-initialized at the beginning of each trial during training as well as evaluation?\n4) It would help to clearly describe the legends in subfigures showing probability of cooperation conditioned by state and the state visitation frequency. Particularly, it was unclear to me, for example in Fig 2b whether CC is the action of the (opponent, meta-learner) or (meta-learner, opponent) in the previous step? What are the corresponding states in Fig 5b / 5e / 5f ?\n5) Sec 6, Coin game: \u201cBoth GS and CHAOS demonstrate shaping \u2026 (see Fig 5).\u201d - I did not understand this, needs further explanation.\n6) Sec 6, Ablations: \u201c.. when the hardstop is triggered (see Fig 6c).\u201d - I don\u2019t think this is shown in Fig 6c since it contradicts the description in the caption for Fig 6c. \n7) Fig 6: Is the single trial evaluated after the training has converged for both CHAOS and PPO? \n8) The x-axis has a different range in Fig 6d compared to Fig 6c and 6e - it does not result in a fair comparison. The plot for \u2018PPO\u2019 should be the same in Fig 6c and Fig 6d and maybe we will see this if the range / scale of the x-axis is fixed. The paper says that in contrast with Fig 6d, Fig 6e shows with sufficient timespans, history can be used to shape in the absence of context  - but the curves are almost identical till 200 generations in Fig 6e as well as Fig 6d. \n9) Why are the curves for PPO different in Fig 6a and Fig 6b? Are they comparable? Can you show the state visitation frequency with CHAOS, GS and PPO for this ablation study?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper lacks sufficient clarity in explaining the proposed method and overlooks several finer details that I have listed above. The proposed method implements RL2 from prior work for meta-learning the agent policy to shape the opponent in a 2-player matrix game. The quality of experimental analysis also needs to be improved and I have listed some of the current shortcomings above.  \n",
            "summary_of_the_review": "I recommend rejection of the paper in its current form. The description of the proposed method and the implementation and evaluation details need to be further elaborated and the experimental results should address the questions I have raised above.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2938/Reviewer_Hqnc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2938/Reviewer_Hqnc"
        ]
    },
    {
        "id": "73K4smtpQY",
        "original": null,
        "number": 4,
        "cdate": 1666681343058,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681343058,
        "tmdate": 1666681343058,
        "tddate": null,
        "forum": "54F8woU8vhq",
        "replyto": "54F8woU8vhq",
        "invitation": "ICLR.cc/2023/Conference/Paper2938/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to capture co-playr learning dynamics in MARL. The authors propose Context and History Aware Other-Shaing (CHAOS) to address this problem. The CHAOS agent is a meta-learner using RNN architecture to learns to shape its co-player. The authors conduct extensive experiments on matrix games.",
            "strength_and_weaknesses": "One of the main conerns for the paper is its novelty. The CHAOS algorithms seems to be a combination of the existing components and methods introduced in the background section. Can the author better clarity the novelty of CHAOs? \n\nAnother concern for the paper is its experimental part. \n- It is claimed that the proposed method (CHAOS) is suitable for high-dimensional games. However, the most complex experimental benchmark in this paper is the Coin Game in a grid-world style. It is far less complex than the commonly used, challenging, and high-dimensional SMAC benchmark. It is therefore worth evaluating the method on more complex benchmark to better support the claim.\n- It is also mentioned that \"Cooperation failures, in which self-interested agents converge to collectively worst-case outcomes, are a common failure mode of MARL methods.\" Is this problem common for widely-used MARL algorithms like QMIX and MAPPO. In addition, how does the method compare against these popular baselines?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing for the paper is not clear, particularly in the method section (Section 4). The algorithm is not motivated and discussed well. In addition, the experiments do not well support some claims in the introduction section.",
            "summary_of_the_review": "My main concern for the paper is its novelty and experimental evaluation. It would be better to clarify more details and motivation for proposing the algorithm in Section 4. It can also be improved by evaluating CHAOS in more commonly-used MARL benchmark such as SMAC by comparing it with recent MARL algorithms. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2938/Reviewer_zKsT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2938/Reviewer_zKsT"
        ]
    }
]