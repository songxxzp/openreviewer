[
    {
        "id": "bcHg8PbgCQR",
        "original": null,
        "number": 1,
        "cdate": 1666591608825,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591608825,
        "tmdate": 1666591608825,
        "tddate": null,
        "forum": "n6H86gW8u0d",
        "replyto": "n6H86gW8u0d",
        "invitation": "ICLR.cc/2023/Conference/Paper4254/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces TANGOS, a novel regularization method which promotes specialization and orthogonalization among the gradient attributions of the latent units of a neural network. TANGOS has benefits for out-of-distribution generalization and can be combined with others. Extensive empirical evaluation with TANGOS is presented. ",
            "strength_and_weaknesses": "Strengths\n\n- Extensive analyses on the working principles of the method\n\n- Empirical results are showing consistent improvements\n\n- Insights on why the method works\n\nWeaknesses\n\n- The contributions are small - specialization and orhthonogalization are straightforward ideas. \n\n- Overall, very small and simple datasets are used in experiments. Real-world tabular data would have more samples with more complex dynamics.\n\n- The improvements are very small - the results are barely better than L2 regularization. \n\n- The impact is not analyzed with multiple commonly used tabular deep learning architectures. \n\n- It is unclear whether the hyperparameters are reoptimized for each method. \n\n- No experimental results on randomness, showing error bars at the main events could be useful. ",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is well written and the core idea is clear. \n\nQuality is mediocre. Experiments section can be improved.\n\nI think Novelty is low. Methodological innovations are relatively straightforward\n\nReproducibility: Some details are left out to fully reproduce. ",
            "summary_of_the_review": "The paper is borderline. In general, the empirical validation is weak and the contributions are not major. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4254/Reviewer_qYko"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4254/Reviewer_qYko"
        ]
    },
    {
        "id": "iWkP3jLdjtI",
        "original": null,
        "number": 2,
        "cdate": 1666806079529,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666806079529,
        "tmdate": 1666806079529,
        "tddate": null,
        "forum": "n6H86gW8u0d",
        "replyto": "n6H86gW8u0d",
        "invitation": "ICLR.cc/2023/Conference/Paper4254/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper propose a new regularization technique for the structured tabular data, namely TANGOS. The method works because intuitively it makes sense to enforce specialization of each unit and orthogonalization between units. The paper has also done extensive experiments on regression and classification to support the effectiveness of the regularization method. ",
            "strength_and_weaknesses": "Strength:\n1. The experiments has been conducted very extensively to demonstrate the effectiveness of the method.\n2. The idea makes intuitively senses.\n\nWeakness:\n1. The idea novelty is limited. The specialization and orthogonalization idea are not that new.\n2. There is no ablation study on which part is more important, specialization or orthogonalization.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper's writing and presentation of the work is clear and the results should be reproducible. As mentioned early, the novelty is the limited aspect.",
            "summary_of_the_review": "Overall, the paper proposes an interesting and effective regularization method. The experiments have been done extensively. Despite the limitation of the novelty, the paper is still a well-written paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4254/Reviewer_Aiai"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4254/Reviewer_Aiai"
        ]
    },
    {
        "id": "7LVQBZ4CC4",
        "original": null,
        "number": 3,
        "cdate": 1667540852416,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667540852416,
        "tmdate": 1669838140631,
        "tddate": null,
        "forum": "n6H86gW8u0d",
        "replyto": "n6H86gW8u0d",
        "invitation": "ICLR.cc/2023/Conference/Paper4254/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper draws upon ideas from the ensemble learning and gradient attribution literatures to produce a neural network based approach to tabular learning.  The main ideas are as follows (1) each penultimate neuron should be sensitive to only a few of the input features (2) each penultimate neuron should be sensitive to non-overlapping sets of features wrt the other penultimate neurons.  The justification for this intuition is that in ensemble learning, ensemble learners benefit from diversity in its weak learners\u2013and we can view a neural network as an ensemble of weak learners where the weak learners are the neurons of the penultimate layer.",
            "strength_and_weaknesses": "\n Strengths\n- I really like the overall direction of drawing upon the ensemble learning literature to help improve the performance of neural networks in tabular learning.  GBDTs are still SOTA in tabular learning, so taking the strengths of GBDTs and incorporating them into deep models is promising.\n- The paper is very clear and the experimental setups/plots are all easy to follow\n- When combined with other regularizers, TANGO appears to produce strong results across datasets\n- The paper makes a strong case that the regularizer is both doing something unique vs other regularizers and that it accomplishes the ideas of sparsifying the diversifying the \u201cweak learners\u201d\n\nWeaknesses\n- The paper lacks comparison to GBDTs.  Given that GBDTs are still SOTA in the tabular setting, it\u2019s important to compare against them.  Otherwise why do deep learning at all?\n- The grid search for TANGO is larger than for the others with 6 choices, while for L1/L2 it\u2019s 3 choices, dropout it\u2019s 2 choices, and input noise it is 2 choices.  Maybe a slightly unfair comparison?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of the empirical evaluation, the clarity are all good.  The work appears reproducible.  The novelty is good, as I don't think any other work has tried to incorporate ensemble learning into neural nets in this way (via gradient attribution).",
            "summary_of_the_review": "Overall, I like the ideas in this paper and they seem like a promising way to make deep learning work in the tabular setting.  However, there is no comparison to GBDTs, which is critical in the tabular setting (see https://deepai.org/publication/why-do-tree-based-models-still-outperform-deep-learning-on-tabular-data).  For now, my rating is a 5.5 pending an evaluation with GBDTs.  Now, TANGO does not necessarily have to outperform GBDTs, but I would like there to be a case made that TANGO is a valuable step towards beating GBDTs.\n\n===========I raise my score to a 6 after rebuttal.  If accepted, I recommend the authors spend some time to rewrite the paper such that the comparison with boosting methods is made more clear (i.e. why would a deep learning method that doesn't have pure generalization outperformance vs boosting be a valuable contribution?).  The rebuttal period helped make this more clear to me and I think the ideas in the paper are worthy of presentation at the conference.  One additional thing I might want to see in this paper is a proof-of-concept of TANGOS in multi-modal/meta/interpretability, though I leave it up to the authors.===========",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4254/Reviewer_xMqZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4254/Reviewer_xMqZ"
        ]
    }
]