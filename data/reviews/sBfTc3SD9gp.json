[
    {
        "id": "BmDo542Wdu",
        "original": null,
        "number": 1,
        "cdate": 1666584561696,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584561696,
        "tmdate": 1666584561696,
        "tddate": null,
        "forum": "sBfTc3SD9gp",
        "replyto": "sBfTc3SD9gp",
        "invitation": "ICLR.cc/2023/Conference/Paper5542/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a class of efficient adaptive bilevel optimization methods based on momentum techniques to solve the nonconvex-strongly-convex bilevel optimization problems. Moreover, it studied the convergence properties of the proposed methods, and provided the solid convergence analysis. It also conducted the empirical experiments on data hyper-cleaning and hyper-representation learning tasks to verify the efficiency of the proposed algorithms.",
            "strength_and_weaknesses": "**Strength:**\n\nThis paper proposed a class of efficient adaptive bilevel optimization methods, which build on the unified adaptive matrices and momentum techniques. Moreover, it provides the solid theoretical analysis for the proposed algorithms, and proved the proposed algorithm reaches the best known sample complexity.\n\n**Weakness:**\n\nThe proposed algorithms have many tuning parameters.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written. The novelty of this paper is that it proposed the adaptive gradient-based methods for nonconvex-strongly-convex bilevel optimization and it also provided solid convergence convergence.",
            "summary_of_the_review": "This paper studied the adaptive bilevel methods for the nonconvex-strongly-convex bilelve optimization. One of the main contribution of this paper the the theoretical analysis for the proposed adaptive gradient-based methods for bilelve optimization. Experimentally, the proposed algorithms outperform various algorithms in data hyper-cleaning and hyper-representation learning tasks. \n\nSome Questions:\n\n1. Why use the momentum steps in the lines 5-6 of Algorithms? \n\n2. Why the norm of adaptive matrix has a lower bound in Assumption 7?\n\n3. Why the authors choose the decreasing parameter $\\eta_t$ ? Could we choose the \nconstant parameter $\\eta_t$ in the proposed algorithms ?\n\n4. How to choose the tuning parameters in the proposed Algorithms ?\n\n5. The convergence analysis does not show the advantage of the adaptive gradient method since \nit does not exploit the specific coordinate-wise adaptive stepsize used by Adagrad? In the best case, the convergence can be as good as the non-adaptive gradient descent ascent.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5542/Reviewer_LYRV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5542/Reviewer_LYRV"
        ]
    },
    {
        "id": "5fZwQ5Oz2S",
        "original": null,
        "number": 2,
        "cdate": 1666589860319,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666589860319,
        "tmdate": 1666589860319,
        "tddate": null,
        "forum": "sBfTc3SD9gp",
        "replyto": "sBfTc3SD9gp",
        "invitation": "ICLR.cc/2023/Conference/Paper5542/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed efficient adaptive bilevel optimization methods based on the momentum techniques for the nonconvex-strongly-convex bilevel optimization. It provided the solid convergence analysis for the proposed methods, and proved that these methods obtain the best-known complexity. The experimental results on data hyper-cleaning and hyper-representation learning tasks demonstrate the efficiency of the proposed algorithms.",
            "strength_and_weaknesses": "Strength:\n\nThis paper proposed efficient adaptive bilevel optimization methods based on momentum techniques for the nonconvex-strongly-convex bilevel optimization. It provided the solid convergence analysis for the proposed methods, and proved that these methods obtain the best-known complexity.\n\nWeakness:\n\nThe parameters in adaptive matrices maybe depend on the tuning parameter in the proposed algorithms.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of this article is that it studied the adaptive bilevel methods for the nonconvex-strongly-convex bilevel optimization and provided solid theoretical analysis. This article is well-organized and easy to understand.",
            "summary_of_the_review": "This paper proposed efficient adaptive bilevel optimization methods based on momentum techniques for the nonconvex-strongly-convex bilevel optimization. It provided the solid convergence analysis for the proposed methods, and proved that these methods obtain the best-known complexity. The experimental results on data hyper-cleaning and hyper-representation learning tasks demonstrate the efficiency of the proposed algorithms.\n\nSome Comments:\n\n1. It would be great if the authors would write how they can solve the subproblems in lines 5 and 6 of Algorithms 1-2. \n\n2. What is the intuition to use the momentum steps in the lines 5-6 of Algorithms 1-2? \n\n3. These momentum steps improve the sample complexity in the proposed algorithms?\n\n4. In Theorem 6, the term $\\sqrt{\\frac{1}{T}\\sum_{t=1}^T\\mathbb{E}\\|A_t\\|^2}$ is bounded?\n\n5. How to choose the adaptive learning rates of Algorithms 1-2 in the experiments?\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5542/Reviewer_5EBw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5542/Reviewer_5EBw"
        ]
    },
    {
        "id": "8slbziFKWaV",
        "original": null,
        "number": 3,
        "cdate": 1666722727390,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666722727390,
        "tmdate": 1668440320607,
        "tddate": null,
        "forum": "sBfTc3SD9gp",
        "replyto": "sBfTc3SD9gp",
        "invitation": "ICLR.cc/2023/Conference/Paper5542/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Inspired from Adam, authors propose adaptive bilevel algorithms, and a variance reduced variation, coined \"biAdam\" and VR-BiAdam. They show respectively $1/\\epsilon^4$ and $1/\\epsilon^3$ convergence rate. Authors propose experiments on data hypercleaning and hyperrepresentation learning\n",
            "strength_and_weaknesses": "Major concerns:\n- I have doubt about the practical impact of the proposed work, as many bilevel optimization problems, it requires to select $3$ sequences of stepsize $\\eta_t, \\alpha_t, \\beta_t$. To me, it seems very unclear how to select these parameters in practice.\n- I also have doubt about the theoretical impact of the paper \"Our VR-BiAdam algorithm reaches the best known sample complexity of $1/\\epsilon^3$\". A recent paper [1] managed to prove $1/\\epsilon^2$ rate for a single loop algorithm. Could authors comment on this?\n\n[1] Dagr\u00e9ou, M., Ablin, P., Vaiter, S. and Moreau, T., 2022. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. NeurIPS2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "Paper is clear, and the idea seems new",
            "summary_of_the_review": "I have doubts on the practical and theoretical impact of the paper",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5542/Reviewer_7cUX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5542/Reviewer_7cUX"
        ]
    },
    {
        "id": "fvju_7adeOS",
        "original": null,
        "number": 4,
        "cdate": 1666816030245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666816030245,
        "tmdate": 1666816308229,
        "tddate": null,
        "forum": "sBfTc3SD9gp",
        "replyto": "sBfTc3SD9gp",
        "invitation": "ICLR.cc/2023/Conference/Paper5542/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work focus on adaptive frameworks for bilevel optimization. It first introduce BiAdam that achieves $\\tilde{O}(\\epsilon^{-4})$ complexity. Then it introduces BiAdam with variance reduction and achieves $\\tilde{O}(\\epsilon^{-3})$ complexity. Both results matches  state-of-the-art complexities.  ",
            "strength_and_weaknesses": "strengths: \n\n\na) The complexities matches the previous results, and the algorithm does not need a mini-batch\n\n\nb) The algorithm can be used in the setting with constraints.\n\n\nweaknesses:\n\n\na) The novelty in this paper is quite limited. The results are built on the large body of bi-level optimization literature and super-ADAM. Noticeable, there are already many works for bi-level optimization can achieve the same complexities. \n\nb) The adaptive framework is not very meaningful here. Note that all the adaptivity is absorbed in the matrices $A_t$ and $B_t$ in the algorithms. The stepsize can be considered to be inversely related with eigenvalues of matrices. In Assumption 7, it assumes $A_t \\succeq \\rho I_d$ and $B_t=b I_p\\left(b_u \\geq b \\geq b_l>0\\right)$, which basically indicates stepsize for $x$ is upper bounded and stepsize for $y$ is lower and upper bounded. Then by controlling the other parameters in the stepsizes, e.g., $\\lambda$ and $\\gamma$, the stepsizes  can be treated just like non-adaptive stepsizes. Therefore, the analysis does not need too much novelty to accommodate adaptive stepsize, e.g., Lemma 10.  This leads to some drawbacks: 1) upper and lower bounds for adaptive matrices $A_t$ and $B_t$ need to be know in order to pick other hyperparameters, 2) usually, $\\rho$ in Adam is chosen to be very small just for stability. But when $\\rho$ is small, Theorem 1 needs a very small $\\gamma$ which may reduce the gain from Adam stepsize. ",
            "clarity,_quality,_novelty_and_reproducibility": "clarity: typo in the abstract, \"mate learning\" -> \"meta learning. \n\n",
            "summary_of_the_review": "The results in this paper is reasonable, but it may lack novelty. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5542/Reviewer_LXpt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5542/Reviewer_LXpt"
        ]
    }
]