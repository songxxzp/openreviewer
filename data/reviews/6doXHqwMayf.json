[
    {
        "id": "hKCLIDzGed",
        "original": null,
        "number": 1,
        "cdate": 1666011507876,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666011507876,
        "tmdate": 1666011507876,
        "tddate": null,
        "forum": "6doXHqwMayf",
        "replyto": "6doXHqwMayf",
        "invitation": "ICLR.cc/2023/Conference/Paper5809/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider learning from a two-layer network, using a) a student with matching architecture trained with a two-phase Langevin+GD procedure b) linear methods. They provide upper (resp. lower) bounds on excess test error with the number of samples, and show that the student net always display faster rates than linear method. Their discussion also provides some insight about the role of convexity (or lack thereof) to rationalize these findings.",
            "strength_and_weaknesses": "Strengths:\n\nThe paper is overall well-written, clear and easy to follow. The related works are very satisfyingly discussed.  The question asked (when, and why do neural net outperform linear methods) is definitely an important one, and the setting the authors study is very natural. I have not read the proofs.\n\nWeaknesses:\n\nA plot with the learning curves of the student network, along with those of some kernel methods (RF. NTK, or usual kernels such as RBF), with the relevant upper/lower rates of decay, would be extremely helpful in visualizing (and bolstering) the theoretical results.\n\nQuestions:\n\nTo my understanding, the bound for the student network seems to hold after a threshold of the order of exp(m**2) (theorem 4.5), which can be extremely large already for widths 50, thereby raising the question of whether the regime described can be observed at all unless the network is very narrow. More discussion in this sense would be helpful (or, in the case I am misunderstanding, a clarification from the authors). Note that this is more a clarification question and in any case the results remain of interest.\n\nFor small m (e.g. m=2), does the student net perform clearly better than the linear methods right after the threshold in n predicted in Theorem 4.5? For large dimensions d, I would naively expect from the results of Aubin et al. (The committee machine....., 2019) that there exists a large range of n where this is not the case.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear. All parameters necessary for Fig.1 seem to be specified. Although I'm not extremely familiar with the literature in the field, the results seem of solid interest.",
            "summary_of_the_review": "Although I have not reviewed the proof, the paper comes across as clear, well-written, and seems to my understanding to bring solid results, I am therefore giving a good score, but with average-low confidence.\n\nThe paper does not display to me a clear weakness, my only comment being that a plot which illustrates the main point of the paper would to my mind improve on the clarity and impact.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5809/Reviewer_51MF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5809/Reviewer_51MF"
        ]
    },
    {
        "id": "aTPbeaxcQ-",
        "original": null,
        "number": 2,
        "cdate": 1666431257457,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666431257457,
        "tmdate": 1670868997428,
        "tddate": null,
        "forum": "6doXHqwMayf",
        "replyto": "6doXHqwMayf",
        "invitation": "ICLR.cc/2023/Conference/Paper5809/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the excess risk of two-layer ReLU neural networks training by two-stage gradient based algorithms with spherical data iid spread on the unit sphere. Under such neural networks training, the authors demonstrate that, two layer neural networks are able to achieve the minimax $O(1/n)$ rate in a certain norm-based space, when compared to kernel methods with $O(n^{-1/2})$ rate (slightly faster  than this rate via the convex argument technique).",
            "strength_and_weaknesses": "**Pros:** \n\n1. propose a two-stage gradient descent algorithm combining the gradient Langevin dynamics and standard gradient descent over the first layer for neural networks training\n\n2. Study the separation between kernel methods and neural networks in terms of excess risk\n\n\n**Cons:**\n\n1. The authors claimed that their results do not require high over-parameterization conditions. However, according to Theorem 4.5, the convergence rate is $O(m^5/n)$, as well as the minimum singular value $\\sigma_{min}$ heavily depending on $m$. \nThat means, their results are only valid to $m \\ll n$ rather than over-parameterization.\nFurthermore, I don\u2019t agree with the authors\u2019 current claimed $O(1/n)$ rate for neural networks, because this is a width-dependent result though the authors set $m < d$ in this work. \nInstead, kernel methods obtain a width-independent $O(n^{-1/2})$ convergence rate, e.g., (Bach, JMLR 2017, Gal et al. NeurIPS 2022). Hence, in this case, it is not enough to show the separation between kernel methods and neural networks.\n\n[1] Vardi, Gal, Ohad Shamir, and Nathan Srebro. \"The Sample Complexity of One-Hidden-Layer Neural Networks.\" NeurIPS 2022.\n\n2. If we put the above key issue aside, a small $m$ (as well as the experiments) makes sense for the teacher network with few activations.  But for the student network, a few $m$ significantly restrict the model expressivity. I'm wondering the size of function space (in my next question). Besides, this also effects the convergence of phase II in Proposition 4.4 with $O(1/m^3)$. The convergence results require to work in a large $m$ setting, falling into a self-contradiction suitation.\n\n3. The considered function space in Eq. (1) is a norm-based function space, and can be regarded as a subspace of variational space for two-layer neural networks via signed measure (or similar to the path-norm based space), see (Bach, JMLR2017). The separation between kernel methods and two-layer neural networks has been studied in this larger space [2] as kernel methods suffer from the curse of dimensionality $\\Omega(n^{-1/d})$. \n\n[2] Parhi, Rahul, and Robert D. Nowak. \"Near-minimax optimal estimation with shallow ReLU neural networks.\" IEEE Transactions on Information Theory (2022).\n\nWhile this work focuses on a smaller function space and kernel methods are close to the $O(n^{-1/2})$ rate in high dimensional settings. I understand the authors aim to show the separation between kernel methods and neural networks on the convergence rate: $O(n^{-1/2})$ vs. $O(1/n)$. However, strictly speaking, this is not the claimed \u201ccurse of dimensionality\u201d in the approximation theory view.\n\n4. Proposition 4.3 appears vacuous due to the exponential order of $m$ and $\\beta$. Regarding $\\beta$, the authors suggest using \u201ca sufficiently large $\\beta$\u201d in phase I to ensure a near-optimal result. However, it makes the first term exponentially large in the first term of Proposition 4.4, resulting in a vacuous result.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written, and has good quality, but the obtained results and claims appear a little specious. Apart from the above drawback, what\u2019s the difference between the current problem setting and the mean field regime in terms of the function space and the obtained gradient based algorithm?\n",
            "summary_of_the_review": "This work presents the separation between kernel methods and neural networks in terms of excess risk, but the problem setting with a few $m$, sufficiently large $\\beta$ in Proposition 4.3, and the function space require more discussion and refinement for publication. I suggest 1) the authors center around a few $m$ for the teacher network, but the student network allows for more neurons;\n2) fix the over-parameterization issue in the excess risk;\n3) avoid an exponential order of a sufficiently large $\\beta$ in Proposition 4.3.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5809/Reviewer_xk6b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5809/Reviewer_xk6b"
        ]
    },
    {
        "id": "mFF7k3d0tWv",
        "original": null,
        "number": 3,
        "cdate": 1666646463444,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646463444,
        "tmdate": 1666647318299,
        "tddate": null,
        "forum": "6doXHqwMayf",
        "replyto": "6doXHqwMayf",
        "invitation": "ICLR.cc/2023/Conference/Paper5809/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper consider the optimization and the statistical guarantee of two-layer ReLU neural networks under the teacher-student settings, demonstrating that we can obtain a fast rate on the excess risk with a computational-efficient two phase optimization algorithm, that can be shown to beat any linear estimators on the statistical efficiency.",
            "strength_and_weaknesses": "### Strength:\n* The conclusion on the superiority of the neural networks is interesting.\n\n### Weakness:\n* The presentation of the paper, especially Section 4.2 should be improved. The two-phase algorithms are not presented in a unified way, which leads to some ambiguities for me.\n* The results are not a substantial improvement over [1]. The only new result for this paper is showing that when the teacher and the student have the same width, we can have a computationally efficient algorithm to perform the estimation, using the idea from [2].\n\n[1] Suzuki, Taiji, and Shunta Akiyama. \"Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods.\" In International Conference on Learning Representations. 2021.\n\n[2] Zhang, Xiao, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. \"Learning one-hidden-layer relu networks via gradient descent.\" In The 22nd international conference on artificial intelligence and statistics, pp. 1524-1534. PMLR, 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "Other than Section 4.2, the paper is clearly written. However, I need to say the novelty is limited.",
            "summary_of_the_review": "The main concern for me is about the novelty. If I understand correctly, the only new content for this paper is the computational efficient algorithm. However, this part is presented poorly. I do have a specific question on this two-phase algorithm. Following the constants selected in Theorem 4.5, we still need $k^{(1)} = \\exp(\\mathrm{poly}(m))$, which can still be pathological when $m$ is not so small. Is this really satisfactory? Furthermore, \tI\u2019m wondering if Proposition 4.3 is a satisfactory result, as when $\\beta$ goes to infinity, the risk does not go to 0. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5809/Reviewer_Xjf3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5809/Reviewer_Xjf3"
        ]
    },
    {
        "id": "jmBX070IaU",
        "original": null,
        "number": 4,
        "cdate": 1666670116817,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666670116817,
        "tmdate": 1666670116817,
        "tddate": null,
        "forum": "6doXHqwMayf",
        "replyto": "6doXHqwMayf",
        "invitation": "ICLR.cc/2023/Conference/Paper5809/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the excess risk of two-layer ReLU neural networks in a teacher-student regression model. In particular, they showed that the student network could learn the teacher network under certain conditions in two phases: first by noisy gradient descent\nand then by vanilla gradient descent. ",
            "strength_and_weaknesses": "Strength:\n1. The proposed algorithm can recover the teacher network's parameters, which is a hard problem. \n2. This paper is well-written and organized.\n3. This paper has proof and experiments to support their results.\n\nWeakness:\n1. This paper requires the student network has the same width as the teacher network. It is unknown whether such a result can be generalized to the case where we don't know m in advance.\n2. To connect the noisy SGD and vanilla SGD, the algorithm requires reparameterizing the neural network. This kind of trick seems to be impractical in real applications, especially for DNNs.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear and well-written. The idea is quite interesting, although the application may be limited. ",
            "summary_of_the_review": "In general, this paper is well written. My only concern is whether the exact recovery of the parameter is necessary for NN training, if we only care about the excess risk. Instead of reparemetrizing, can we directly decrease the noise level in Noise GD to zero?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5809/Reviewer_vbHH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5809/Reviewer_vbHH"
        ]
    },
    {
        "id": "ULfgnZ5rev-",
        "original": null,
        "number": 5,
        "cdate": 1666693042459,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666693042459,
        "tmdate": 1666693042459,
        "tddate": null,
        "forum": "6doXHqwMayf",
        "replyto": "6doXHqwMayf",
        "invitation": "ICLR.cc/2023/Conference/Paper5809/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the excess risk of learning neural networks in the teacher-student setting, as well as the theoretical superiority of neural networks over other linear (or kernel) methods. It provides an excess risk bound of $\\frac{\\log n}{n}$ convergence rate (with polynomial dependency on other parameters) for the neural networks, and also a minimax lower bound of $n^{-\\frac{d+2}{2d+2}-o(1)}$ for all kernel methods to learn the same target function class.",
            "strength_and_weaknesses": "Strengths:\n1. This paper is clearly and rigorously written. All the notations are well explained and most of the technical difficulties are introduced naturally. It is very delightful to read as the paper managed to convey these highly technical statements with such clarity and fine elaboration.\n2. The story in this paper is complete, in that this paper almost resolves most of the questions in its setting. They have the convergence guarantee with well-explained optimization algorithms and analysis, and also a generalization guarantee as excess risk upper bound. For the lower bound, they proved the LB for all linear classes (kernels) that their generalization over the teacher class is poor compared to student neural nets. \n3. The problem this paper deals with is a long-pursued one in the deep learning theory community. The results in this paper go beyond the previous works and are proven for a setting with almost minimal assumptions over the target function class (for example, it goes beyond the orthogonality condition on which many previous results are based, and it goes beyond the local convergence result of Zhou et al. 2021). This is highly non-trivial progress in this direction. The results of this paper also provide some solid technical contributions.\n\nWeaknesses:\n1. The two-stage optimization process, which uses two different optimization algorithms, seems to be the remaining problem to solve. It would be better if the analysis can be applied to settings without this technical workaround.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is written clearly and with high quality. The novelty of its results lies in that it improved the previous analysis to settings where assumptions are more natural.",
            "summary_of_the_review": "This paper presents a solid contribution to the area of deep learning theory, although it has not resolved all the problems under its setting, it still manages to improve the previous analysis and give an almost complete story of learning in the student-teacher setting beyond kernel methods. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5809/Reviewer_VLSM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5809/Reviewer_VLSM"
        ]
    }
]