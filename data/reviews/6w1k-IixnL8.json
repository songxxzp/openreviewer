[
    {
        "id": "kskbO28k9o",
        "original": null,
        "number": 1,
        "cdate": 1666332719270,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666332719270,
        "tmdate": 1666332719270,
        "tddate": null,
        "forum": "6w1k-IixnL8",
        "replyto": "6w1k-IixnL8",
        "invitation": "ICLR.cc/2023/Conference/Paper1924/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors offer a method of estimating the grouping loss, which fills in some shortcomings of common measures of network calibration. They then use their new metric to measure the grouping loss of a variety of different architectures and settings. ",
            "strength_and_weaknesses": "## Strengths\n\n- The method is rigorously defined\n- The results are interesting and highlight an exact problem and provide a possible solution to the shortcomings of ECE\n\n\n## Weaknesses\n\n- I am confused about the actual application of Proposition 4.3. $\\text{GL}_{\\text{explained}}$ will have an entry for every bin and every class, right? Then how are the figures in Figure 6 calculated on a multiclass problem like ImageNet-R? If only the top level prediction is considered, as is stated, then doesn't this add more uncertainty into the whole grouping since each class has a differing amount of irreducible uncertainty will all other classes? I understand the argument about small numbers of test instances in that particular dataset, but would it be possible to compare top-level grouping loss with per class grouping loss on a smaller dataset which has more samples, like CIFAR-10? I am curious how GL will behave when inter-class relationships are considered.\n\n- To clarify the application of proposition 4.3 to a multiclass problem, would it be correct to characterize the sum of $\\hat{\\mu}_j$ in a multiclass problem as \"count the number of times the actual class of region $j$ is the predicted class from the classifier?\"\n\n- Figure 7 does not quantify what happens to the grouping loss before and after isotonic recalibration. I would be very interested to see the information in figure 7 in the form of a table to see the actual differences in the numbers.\n\n- Isotonic calibration performance is reported, but the most popular form of recalibration (temperature scaling) is not reported. Does temperature scaling perform any different than isotonic calibration?\n\n- Do the authors have any intuition as to why the NLP tasks have a drastically lower grouping loss as compared to the vision tasks? \n\n## Minor\n\n- The end of the counterexample paragraph on page 2 refers to the grouping loss without defining what it is, which leads to confusion. \n\n- NLP Section: \"However, we found no evidence in the in-distribution one.\" This sentence is not clear. No evidence of what? What is the in-distribution 'one?'\n\n- NLP Section: \"This suggests again that our of distribution settings lead to stronger heterogeneity, and thus grouping loss.\" I find this sentence confusing. Why 'again?' was this brought up before in the text? Why would stronger heterogeneity cause larger grouping loss? It might seem intuitive that this is the case, but what is the reason that heterogeneity would cause higher grouping loss? \n\n- Conclusion: \"captures well the grouping loss.\" --> captures the grouping loss well.",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n\nThe text is very clear aside from a few small issues highlighted above.\n\n## Quality\n\nThe quality is high.\n\n## Novelty\n\nTo my knowledge, the work is sufficiently novel.",
            "summary_of_the_review": "Overall, I think this work highlights and analyzes a known problem with ECE and recalibration methods in general. This is an important topic, and I think the knowledge here can be applied in a number of future works. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1924/Reviewer_sCnp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1924/Reviewer_sCnp"
        ]
    },
    {
        "id": "VYMPO8lwCQ",
        "original": null,
        "number": 2,
        "cdate": 1666522186513,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666522186513,
        "tmdate": 1669204607058,
        "tddate": null,
        "forum": "6w1k-IixnL8",
        "replyto": "6w1k-IixnL8",
        "invitation": "ICLR.cc/2023/Conference/Paper1924/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper focuses on a decomposition of strictly proper scoring rules that reveals three terms: calibration loss, group loss, and an irreducible term. Intuitively, the group loss term (GL) deviates from the aggregate measures used in calibration; e.g. if a model is systematically over-confident on a subgroup and under-confident on another, the overall calibration can be small but GL is large. The authors argue that evaluating GL is important and they present an estimator that provides a lower bound to GL. They conduct experiments on vision and language tasks, which suggest that models continue to have a large group loss even after post-hoc calibration.\n",
            "strength_and_weaknesses": "The paper brings insight into calibration and supports its arguments theoretically. The authors conduct experiments on several architecture families in both language and vision in real-world datasets and also present experiments on synthetic data. \n\nBut, the primary weakness in my opinion lies in the motivation behind this work and I would appreciate it if the authors clarify the following point in their rebuttal. First, using a proper scoring rule, such as the log-loss and the Brier score, would automatically capture both calibration and group loss at the same time and they can be estimated easily. Is there a reason for practitioners to report a calibration error and a group loss, when they can report a proper score. My main takeaway from the paper is that one should not rely on calibration alone, such as using ECE, but rather use proper scoring rules because they capture group loss as well, which seems to be a simple and accurate message to take away. But, I don\u2019t see the value of trying to calculate a lower bound on the group-loss term, especially given that the results on the simulated data in Figure 4 are not encouraging. \n\nSecond, I haven\u2019t yet figured out how the regions or partitions are identified using decision trees in Section 4. What is the input to the decision tree? For example, in vision, are those raw images? I think it would be really useful to provide examples of images for each group. \n\n\nSome minor comments:\nIn Lemma 4.1, \u201cnegative entropy\u201d may not be the right term since the score rule can be something else.\nIn Figure 4, why is $GL_{LB}$ not a lower bound when the number of bins is small?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written. I think it offers an interesting insight into calibration and the limitations of using ECE alone, even when also reporting accuracy (as shown in Figure 1). In terms of reproducibility, the paper should include more details about how decision trees are used to partition the space since this is a crucial part of the experiments. ",
            "summary_of_the_review": "The paper is well-written and offers an interesting insight into calibration. All arguments are supported by experiments and proofs. I find the experimental results quite interesting. My only concerns are: (1) the practical implications are unclear yet since one can simply use proper scoring rules, and (2) more details should be provided about the experiments as I mention above. \n\n\n======  Post rebuttal ======\n\nThank you for confirming my understanding and for the clarification.\n\nI see now your argument of why quantifying the group loss can be useful. Since a proper scoring rule includes the irreducible term, it might be worthwhile instead to look into the sum of calibration and group loss. As you mention in the revised draft,  a classifier with a Brier score of 0.15 could have optimal probabilities (irreducible loss close to 0.15) or poor ones (irreducible loss close to 0).\n\nI think this is a useful message and the proposed method using decision trees is simple and easy to apply. I have updated my scores.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1924/Reviewer_LCym"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1924/Reviewer_LCym"
        ]
    },
    {
        "id": "kcakrcZ6KH",
        "original": null,
        "number": 3,
        "cdate": 1666657335871,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657335871,
        "tmdate": 1666657335871,
        "tddate": null,
        "forum": "6w1k-IixnL8",
        "replyto": "6w1k-IixnL8",
        "invitation": "ICLR.cc/2023/Conference/Paper1924/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors of the paper derive an estimator of a term that lower bounds the grouping loss term of a proper scoring rule. They show that this estimator is tighter than baselines and use it to analyze existing models. The derivations seem novel and correct; however, it is unclear to me why having this estimator is interesting or useful in practice.",
            "strength_and_weaknesses": "STRENGTHS:\n* The paper provides a novel estimator for the lower bound of the group loss. This term was previously hard to measure.\n* The methods used are novel, technically sound, and non-trivial. The level of technical depth required to derive the estimator is significant.\n* The experiments evaluate a lot of realistic state-of-the-art models.\n\nWEAKNESSES:\n* I am not convinced from reading this paper that measuring the grouping loss is something I would like to do in practice and why this is interesting.\n* It is not clear to me how the experiments support the main claims of the paper.\n* Some of the claims seem off off to me (see below).",
            "clarity,_quality,_novelty_and_reproducibility": "DETAILS ABOUT MAIN CONCERN:\n\n1. I am not convinced from reading this paper that measuring the grouping loss is interesting. \n\n1.a. It would be helpful to highlight specific examples of why that's the case. The experiments report the group loss of different models. Why is this information interesting? How is it actionable? Also, why do we even care about making the group loss small? If the model is initially over-confident, the group loss will increase after calibration, but that's a good thing, it's just correcting for overconfidence in the model. A discussion of this would be helpful. One way in which I can imagine why estimating the group loss may be useful is that we may potentially optimize this loss directly (not sure if this will lead to anything interesting, but it's an idea).\n\n1.b. In particular, we know that a proper score decomposes as: proper_score = calibration + group_loss + irreducible_term. If we can measure the proper_score and calibration, then we can compute (group_loss + irreducible_term), where irreducible_term is constant across models. Why is this not sufficient? In the experiments section, the authors compare multiple models in terms of their group loss. It seems like we should be able to get a relative ranking by simply using (proper_score - calibration) = (group_loss + irreducible_term), which should be simpler to compute and wouldn't require complex math.\n\nDETAILS ABOUT OTHER CONCERNS:\n\n2. The experiments seem like they could be improved. There is only one relatively limited simulation that explores whether the bound is sufficiently tight. If I were to trust this, I would probably need more experiments. Also, the experiments on NLP simply report the group loss, but it's unclear to me how this connects to the main estimator. Also, there are not baselines, and I'm not sure what point that experiment is trying to make (except that the group loss is non-zero).\n\n3. Some of the claims sound off. In particular, the claim that \"a common confusion is to mistake confidence scores of a calibrated classifier with true posterior probabilities and think that a calibrated classifier outputs true posterior probabilities\" seems off to me. I've been working in this area for a while, and I can't imagine researchers believing this is true. The authors provide some quotes, but these seem either misinterpreted (i.e., the author was imprecise and his claim can be interpreted in multiple ways, e.g., [Q1]) and/or come from outside the mainstream machine learning community (top ML conferences and stats journals). This is more of a minor point, but I wouldn't claim that debunking this misunderstanding is a major contribution of the paper.\n\n\n[Q1]: \"This is exactly the purpose of calibration techniques, which aim to map the predicted probabilities to the true ones in order to reduce the probability distribution error of the model\"",
            "summary_of_the_review": "I am not convinced the problem studied by this paper is sufficiently significant.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1924/Reviewer_rpCw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1924/Reviewer_rpCw"
        ]
    }
]