[
    {
        "id": "4FJ1Hpt84dq",
        "original": null,
        "number": 1,
        "cdate": 1666578199338,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578199338,
        "tmdate": 1666578199338,
        "tddate": null,
        "forum": "6f47WT-HtuH",
        "replyto": "6f47WT-HtuH",
        "invitation": "ICLR.cc/2023/Conference/Paper2515/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This manuscript introduces a framework to model data generation where the many bias-inducing factors are allowed for an exploration of the bias inheritance mechanism. Through the parametric framework, the authors analyze the data imbalance problem, investigate the various sources of bias, and propose a novel mitigation strategy based on matched inference approach, consisting of the introduction of coupled learning models. Experiments show that the coupled strategy can strike superior fairness-accuracy trade-offs.",
            "strength_and_weaknesses": "Strength: \n\n+. The framework of modeling data generation is simple and intuitive. The analytical results illustrate sources of bias.\n+. The coupled neural networks method is interesting. The authors identified the classification bias in the T-M is due to the mismatch between the generative model of data and the learning model. To enhance the learning model to account for the presence of multiple sub-populations and labeling rules, a new strategy is developed where multiple neural networks are trained with the exchange of penalty.\n+. Extensive experiments are conducted.\n\nWeakness: \n\n-. The proposed data generation framework is too simple. The framework is linear and without any uncertainty. \n-. This paper presents many analytical and experimental results which affect the bias in the classification tasks. It would be more convincing to discuss how to identify these sources of bias from a real dataset, i.e., define metrics for those sources of bias.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is not easy to read. There are too many results, some are consistent with existing works and some are new. The authors are encouraged to re-organize the results and conclude the results in each section.",
            "summary_of_the_review": "This paper presents a generation prototype where the key factors of bias are analyzed theoretically and experimentally. Then a mitigation approach using coupled networks is proposed.\n\nThe framework illustrates the factors of bias but lacks quantitative measurement. The concepts of those factors are helpful for understanding bias in the data generation process but have some limitations to measuring bias in real data whose generation process is unavailable. In addition, the prototype is too simple compared with a real dataset.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2515/Reviewer_NozC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2515/Reviewer_NozC"
        ]
    },
    {
        "id": "KEfaIOjRVI",
        "original": null,
        "number": 2,
        "cdate": 1666723996290,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666723996290,
        "tmdate": 1666723996290,
        "tddate": null,
        "forum": "6f47WT-HtuH",
        "replyto": "6f47WT-HtuH",
        "invitation": "ICLR.cc/2023/Conference/Paper2515/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a high-dimension model by combining the Gaussian Mixture and Teacher-Student setups to understand the unfairness issue of the real dataset. Through the tools of statistical physics, analytical characterizations of the typical properties of learning models trained in this synthetic \u201cT-M\u201d framework are provided. Despite the simplicity of the data model, the proposed framework is able to reflect some unfair behaviors observed on real-world datasets. Moreover, a basic loss-reweighing scheme is re-examined in this framework, which allows for an implicit minimization of different unfairness metrics and quantifies the incompatibilities between some existing fairness criteria.",
            "strength_and_weaknesses": "Strength: \nIt is nice to have a synthetic framework where all the parameters are under our control to help us understand the unfairness issue in real data. Also, it is good to see that the results are obtained in the over-parametrized regime, where $n,d \\to \\infty$, and $n/d<1$.\n\nWeaknesses:\nHowever, the insights obtained from this model seem to be very vague. It is unclear to me how the figures and remarks provided in section 3 can be useful in practice. A better way to present the result might be to focus on the trade-off between the model performance and fairness measure and show how the parameter of the T-M model will influence the trade-off.\n\nMoreover, the proposed coupled networks are only discussed on the last page using one figure. If the authors want to emphasize such a contribution in designing a new algorithm by studying the proposed T-M model, more detailed discussions are needed, and significant rewriting is recommended.\n\nFor Figure 2, I cannot understand why a smaller value of $m_{+/-}$ leads to better fair performance. To my understanding, the smaller $m_{+/-}$ is, the more correlation between the label and the group, which should lead to worse fairness. \n\nFigure 4, it is said in the caption that the first plot is about the accuracy gap, but the y axile of the plot is Disparate impact.\n\nFigure 5, third column, what is \"predicted parity 10\"?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is not well-written. All indices for appendices are missing. \n\nAlso, a lot of notations are used without definition. \n\nThe definition of Disparate impact (DI)  should be added to make the paper more self-contained. \n\nIn equation (2), $T_{c^\\mu}$ is not defined, maybe it should be $W_T$. \n\nSome notations are with \\tilde, and some are not, which is hard for me to understand their differences. \n\nIn equation (3), $\\mathbf{W}$ is used without definition. Are they corresponding to the weights $w$ trained using (2)? What makes it even more confusing, $w$ is reused in equation (8) as the optimizing variable.\n\nAlso, how to interpret $\\delta q$ in equation (4)?\n",
            "summary_of_the_review": "There are some interesting aspects of the paper, but it cannot be published in its current form.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2515/Reviewer_jtpj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2515/Reviewer_jtpj"
        ]
    },
    {
        "id": "yDM3Qs5W4mJ",
        "original": null,
        "number": 3,
        "cdate": 1666992563907,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666992563907,
        "tmdate": 1670335676103,
        "tddate": null,
        "forum": "6f47WT-HtuH",
        "replyto": "6f47WT-HtuH",
        "invitation": "ICLR.cc/2023/Conference/Paper2515/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes to investigate ML bias through the \"data geometry\". In particular, the authors introduce a Teacher-Mixture (T-M) model, which is a combination of Gaussian Model and Teacher-Student model, and consider potential biases under this specific type of data modeling. Analytical and empirical results are provided.",
            "strength_and_weaknesses": "## Strength\n\nThe strength of the paper lies in the effort to consider the problem of ML bias under certain data modeling. The paper focuses on \"data geometry\" and presents analytical results accordingly.\n\n## Weakness\n\nThe weakness of the paper comes from the difficulty to parse the results and therefore, to understand the goal and contribution.\n\n### 1. what exactly does the word \"bias\" mean\n\nWhile I understand the fact that \"bias\" is a heavily overloaded term in the literature, I think it would be better if the paper can make the term more precise and informative. The definition of \"bias\" is not presented until Page 4, and the biases are described in the form of a remark. I understand that (please correct me if I were wrong) the authors may want to have some generality in the discussion, but I do think it is necessary to make sure readers do not get lost, especially when the technical setup (the T-M model, the notations, the loss functions, etc.) is presented in full detail before readers even know what kind of bias the paper is trying to deal with.\n\n### 2. the bias of interest\n\nAs a follow-up on 1., it seems that Sections 3 and 4 further narrow down the kind of bias to _disparate impact_, and consider a subset of ones in Remark 1. This way, I am not sure how to parse Remark 1, and the analytical results in Section 2. On one hand, the paper presents one specific type of data modeling assumption (which, may not corresponds to reality, as pointed out by authors), and the listed biases are specifically tailored to such data modeling (e.g., in Remark 1, all listed biases can only be instantiated after laying out the T-M modeling), and so are the two analytical results in Section 2. On the other, the analyses on source investigation (Section 3) and mitigation strategy (Section 4) are conducted after \"choos[ing] a metric of fairness\" (here is DI). This inconsistency of fairness notions is very confusing. What is the intended takeaway message? Further clarifications would be very helpful.\n\n### 3. question on the claimed contribution\n\nIn Section 1, the paper claims that the work \"allow an exact quantification of the intrinsic trade-offs between [previous group-level fairness notions]\". I am having some difficulties connecting the presented analyses to this goal. In particular, in light of the concerns in 1. and 2., it would be very helpful if authors can share some insights on how tools of statistical physics and data geometry can shed light on bias quantification and mitigation. For example, analytical result 1 is a \"non-rigorous yet exact replica method from statistical physics\", what should be the implication of this result?\n\n### 4. the referenced literature\n\nIn Section 1, to the best of my knowledge, the fairness notion presented by Dwork et al. (2012) is not group-level fairness.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I personally find the paper not easy to follow. There are multiple terms that appear in the paper as a key word (although come with references occasionally), e.g., geometrical properties, ML fairness phenomenology, statistical physics, structural feature, multi-task learning, etc. I think it would be better if authors can clarify how those terms/tools fit in the analysis, what is the goal, and what are the takeaway messages.",
            "summary_of_the_review": "The paper proposes to investigate ML bias through the data geometry. The authors introduce T-M model, and investigate bias under this specific type of data modeling. There are worries about the clarity of the paper and the intended theoretical and technical contribution.\n\n====Post Rebuttal====\n\nI acknowledge that I have read reviewers' comments, authors' responses, and have incorporated them in evaluation.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2515/Reviewer_9w79"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2515/Reviewer_9w79"
        ]
    },
    {
        "id": "GazU4bpv7jZ",
        "original": null,
        "number": 4,
        "cdate": 1667066253809,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667066253809,
        "tmdate": 1667066253809,
        "tddate": null,
        "forum": "6f47WT-HtuH",
        "replyto": "6f47WT-HtuH",
        "invitation": "ICLR.cc/2023/Conference/Paper2515/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper analyzes common fairness metrics achieved by empirical risk minimization on a synthetic data generation model. The work leverages existing physics results to fully characterize solutions for two-group, binary label scenarios where the group-conditional covariate distribution follows a simple Gaussian distribution, and the group-conditional labels are determined by a simple hyperplane over the covariates. \n\nThe analytical solution to this simple model enables the authors to characterize (in particular) disparate impact and positive transfer as a function of key model parameters",
            "strength_and_weaknesses": "Strength:\n\nThe paper is well written, the data model is simple and understandable, and the results are clearly expressed in terms of key parameters such as covariate variance, relative group ratios, label frequencies, feature-to-sample ratios, and similarities in the labeling rule. \n\nSome of these parameters lend themselves to quality discussions on existing approaches such as group re-weighting techniques, and on positive data transfer as a function of data scarcity and label rule similarity.\n\nWeaknesses:\n\nThough the approach is novel, I do not think the results highlight any truly novel aspect of bias. The label rebalancing literature already has results on the analytical solution of, for example, minimax cross-entropy solutions to the reweighted problem that similarly highlight better transfer for similar posterior prediction distributions p(Y|X, G) without specific assumptions on the data model (Although here the results are more concisely grounded in terms of data scarcity as well, which is a nice addition). The discussion of Coupled Networks is interesting, but seems to be only briefly mentioned and not entirely grounded in the theory and results shown in the paper (e.g., why elastic penalties? how well does this work in practice?, how does it compare to existing methods that have train-time access to group membership?)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and clearly presented. The analytical solution to this syntethic data generation problem is, I believe, novel",
            "summary_of_the_review": "The paper uses techniques from statistical physics to analytically compute the ERM solution to a binary classification problem with two demographic groups. The analytical solution is used to discuss aspects of transfer learning and sample reweighting. Though the model and results themselves seem novel, I don't see particularly impactful insights that arise from the paper's analysis beyond the scope of the particular data model.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2515/Reviewer_CwVZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2515/Reviewer_CwVZ"
        ]
    }
]