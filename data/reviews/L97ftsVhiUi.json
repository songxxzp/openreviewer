[
    {
        "id": "tRvul8d5iC",
        "original": null,
        "number": 1,
        "cdate": 1666688186288,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688186288,
        "tmdate": 1666688186288,
        "tddate": null,
        "forum": "L97ftsVhiUi",
        "replyto": "L97ftsVhiUi",
        "invitation": "ICLR.cc/2023/Conference/Paper4139/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a causal framework to formalize SSDA, and theoretically explain what characteristics should a robust domain adaptation model have. They also discuss the maximal training data utilization and present a collaboratively debiasing learning framework to make use of the training data.",
            "strength_and_weaknesses": "Pros:\n* This paper understands the semi-supervised DA from the perspective of causality, which is novel and significant.\n\nCons:\n* This paper always tells the readers what they have done, but not their reasons. For example, in Intro, they adopt the IPW to develop the equitable pseudo-label propagator, and claim that the negative impact caused by label distribution shift can be mitigated. I think there exists a obvious gap between this technique and the label distribution shift. This type of gaps should be further clarified in the revised version.\n\n* Some previous works (e.g., [1] [2]) have studied the DA problem through the lens of causality. However, I do not find the necessary discussions about the similarities and differences between CAKE and previous works.\n\n* They use the CycleGAN to realize the cross-domain causal generation. However, the related analysis about why the images generated by CycleGAN are under causal interventions is missing. Actually, they use the CycleGAN to implicitly make the disentanglement and generation, and they do not learn the generative factors explicitly. Thus, clarifying this mechanism is very important.\n\n\nQuestions:\n* Intuitively, I cannot distinguish the cross-domain style variables and intra-domain style variables in an image. If possible, please give a specific example to explain these two types of variables.\n\n* They claim that the two types of style variables serve as the confounders. Based on the definition of confounder, it is the common causes of $x$ and $y$. However, in Figure 2, $S_I$ and $S_C$ are only the cause of $x$. Besides, they state that there exists spurious correlation between $S_I$, $S_C$ and $C$ for several times, but I cannot find this in Figure 2. I am still confused about this issue.\n\n* Some typos. For example, in Eq.1 and Eq. 2, $P(Y|do(x),D=D_T)$ and $P(Y|do(x);D=D_T)$ appear simultaneously.\n\n* In Theorem 1, I think $D=D_T$ indicates that the value of $D$ has been fixed. Why does this term, $\\sum_{D\\in\\{D_S,D_T\\}}$ , appear in the decomposition of $P(Y|do(x),D=D_T)$?\n\n[1] Kun Zhang et al. Domain adaptation under target and conditional shift. ICML, 2013.\n\n[2] Mingming Gong et al. Domain Adaptation with Conditional Transferable Components. ICML, 2016.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality and symbol system need to be further improved.\nThe originality is fine.\nThe clarity is not good enough.",
            "summary_of_the_review": "Please refer to Strength And Weaknesses.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4139/Reviewer_HFEB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4139/Reviewer_HFEB"
        ]
    },
    {
        "id": "S6_or_JZq86",
        "original": null,
        "number": 2,
        "cdate": 1666703175910,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666703175910,
        "tmdate": 1666703175910,
        "tddate": null,
        "forum": "L97ftsVhiUi",
        "replyto": "L97ftsVhiUi",
        "invitation": "ICLR.cc/2023/Conference/Paper4139/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Focusing on semi-supervised domain adaptation, this paper considered two key points: robust domain adaptation learning and maximal cross-domain data utilization. Based on this,  a collaborative debiasing learning framework is proposed, which utilizes two complementary semi-supervised learning classifiers to mutually exchange their unbiased knowledge.",
            "strength_and_weaknesses": "Strength:\n- The causal view of SSL DA sounds good.\n- The proposed method is novel.\n\nWeaknesses:\n- Some typos in the paper.\n- The ICT module is in an unsupervised manner, which is not very matched to the semi-supervised setting.\n- The descriptions of the proposed method are not very clear. More details should be added in the supplementary.",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty is well explored, yet the paper is not presented clearly, especially for the detail of the method.",
            "summary_of_the_review": "The general idea of this paper is easy to understand and some theoretic and experimental analyses could support their claims. However, whether is it really applicable or acceptable for research is hard to estimate.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4139/Reviewer_dEun"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4139/Reviewer_dEun"
        ]
    },
    {
        "id": "J8m40tl5YM",
        "original": null,
        "number": 3,
        "cdate": 1666745998883,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666745998883,
        "tmdate": 1666745998883,
        "tddate": null,
        "forum": "L97ftsVhiUi",
        "replyto": "L97ftsVhiUi",
        "invitation": "ICLR.cc/2023/Conference/Paper4139/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper addresses the SSDA problem from the perspective of causality learning. It involves two step for learning, one using GAN and data augmentation to generate intermediate data and second using two semi-supervised learner to do cross-supervision for the aim of debiasing. The effectiveness of the proposed method is demonstrated on two datasets.  ",
            "strength_and_weaknesses": "## Strength\n1. The proposed method is technically sound. \n2. Experiments verify the effectiveness of the proposed method. \n3. The paper is well written. It presents the proposed method in an appealing and professional way with nice figures, formulas, proofs, and proper  language.   \n\n\n## Weakness\n1. The novelty is limited. While the paper tactically presents the proposed method in an appealing way, by linking to the causality learning theory and using propositions, theorems, and proofs, the techniques in essence are quite common in this field. The so-called invariant causal factor generator is just using a GAN model to transfer labeled source image to the style of the target domain for the cross-domain causal factor, and for intra-domain causal factor using stochastic data augmentation which was used by several existing SSDA methods [1,2,3]. The collaboratively debiasing technique, i.e., using one-classier's output to supervised another classifier was also proposed before to address different problems, even for domain adaptation [4] (which is the only one I can recall at this moment, but there should be more). Linking DA to causality learning theory looks new to this problem, but it more like reiterating a well-acknowledged view in a different way that a domain can be decoupled into a domain-shared factors and domain-invariant factors. Besides, after using almost one-page trying to link DA to the causality theory, the paper concludes with the remarks that \"it is non-trivial to personally determine ...\" and \"we employ a compromise solution\", which make it doubtful for the motivation of introducing the causality stuff. In addition, SSDA is highly practical problem; it extends from UDA by adding some label supervision from the target domain to boost generalization performance. Personally I do not think many of the theorems and proofs presented in this paper are of big meaning for this practical problem. At last, the meaning of SSDA as an individual research problem is how to utilize the few labeled target samples, otherwise, SSDA can be simply reduced to a speicial case of UDA with an enlarged labeled sample set. The only technique in the proposed method that is specifically for SSDA is using two semi-supervised learners to do cross-supervision; using semi-supervised learner to learn from a mixture of labeled and unlabeled data brings little insights to help address the SSDA problem.  \n\n2. The performance is not convincing. The authors claim they \"**extensively** evaluate their method\" and \"**significantly** outperforms SOTA methods\", which I do not agree. First of all, previous SSDA methods [1,2,3 and other cited methods] usually conduct extensive experiments to verify the effectiveness and advantages over existing methods. The evaluation in this paper is far less than the previous methods, in terms of both datasets employed and backbone types. Second, previous SSDA methods usually train in one-step and can then can be deployed. The proposed method requires one extra step to train an external GAN model to generate intermediate data. The training process is more complicated, which might be ok if the results are significantly better. But as can be seen in the tables that the proposed method is only slightly better, which makes it questionable for the practical value. Last, there are other published works (e.g., [1,2]) which report better results on the employed benchmark than what are shown in the table; these works are not cited, discussed and        compared. \n\n\n- [1] Cross-domain adaptive clustering for semi-supervised domain adaptation, CVPR'21\n- [2]  Semi-Supervised Domain Adaptation with Prototypical Alignment and Consistency Learning, ICCV'21.\n- [3]  Clda: Contrastive learning for semi-supervised domain adaptation, NeurIPS'21.\n- [4] CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation, ICLR'22",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear. \nThere is a link to the code.\nThe novelty is limited (see the weakness above).",
            "summary_of_the_review": "This paper wraps some common techniques in some professional, appealing and smart way. The technical contribution is not enough and the evaluation is not sufficient and convincing.  I would vote for a clear rejection. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4139/Reviewer_sxkD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4139/Reviewer_sxkD"
        ]
    }
]