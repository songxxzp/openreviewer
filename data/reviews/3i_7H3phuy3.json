[
    {
        "id": "O_0S2wxFfdx",
        "original": null,
        "number": 1,
        "cdate": 1665848527893,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665848527893,
        "tmdate": 1665848527893,
        "tddate": null,
        "forum": "3i_7H3phuy3",
        "replyto": "3i_7H3phuy3",
        "invitation": "ICLR.cc/2023/Conference/Paper6175/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates the case when deterministic policies are learned in a GAN framework. The authors describe occuring instabilities and attribute them to exploding gradients. ",
            "strength_and_weaknesses": "# Strenghts\n\n* The authors try to thorougly understand existing algorithms and investigate their weaknesses, which is an interesting and value contribution.\n* The authors try to support their insights using experiments and toy models, which is helpful.\n\n# Weaknesses\n\nIn my opinion, the biggest weakness of the paper is insufficient clarity in writing. At times, wrong grammar or word-use make reading more difficult, but the intended meaning can still be inferred. Other times however, I found myself unable to follow the writing. Furthermore, the authors sometimes introduce concepts in quotations marks without ever clearly explaining their meaning (e.g. \"aggressive interval\", \"non-confidence\" or \"invalidity\"). \n\nMy other big concern is with respect to Theorem 1 in which they show the probability of exploding gradients. As far as I understand T1, the argument is that the gradient of a JSD divergence w.r.t a deterministic policy can explode. However, I'm not srue this can explain the experimental results, as this JSD is not the optimization target - instead an RL loss is being used. \n\nLastly, more experimental results would be needed.",
            "clarity,_quality,_novelty_and_reproducibility": "As discussed, in its current form, I found it very difficult to follow the writing of the paper. ",
            "summary_of_the_review": "I'm not convinced of the validity of the key results of the paper (T1). However, more importantly, I found the writing at times too difficult to follow, so I am unable to appropriately evaluate the contributions of the paper. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6175/Reviewer_rV27"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6175/Reviewer_rV27"
        ]
    },
    {
        "id": "a7LMFmwob8a",
        "original": null,
        "number": 2,
        "cdate": 1665993008157,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665993008157,
        "tmdate": 1665994137822,
        "tddate": null,
        "forum": "3i_7H3phuy3",
        "replyto": "3i_7H3phuy3",
        "invitation": "ICLR.cc/2023/Conference/Paper6175/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper appears to show that deterministic policies may cause exploding gradient when used with adversarial imitation learning (AIL), despite their improved sample efficiency. At the same time, the paper appears to show that stochastic policies do not suffer from exploding gradients. This led to the conclusion that deterministic policies are \"incompatible with AIL\".",
            "strength_and_weaknesses": "Strength\n1. The paper raises an interesting observation that deterministic policies may cause exploding gradient with AIL. If properly proven/demonstrated, the result would have significant impact on future method design.\n\nWeakness\n1. The paper is poorly structured and unclear. Many aspects of the proof are not sufficiently motivated or connected to the overall conclusion. For instance, it is unclear to me how Prop. 1 is related to Theorem 1. In addition, the notion of \"non-confident\" reward for ARIL has very little explanation (e.g. how it is related to Prop. 2).\n\n2. The primary proof for the main claims is confusing and skip too many steps. Crucially, it is unclear how the exploding gradient is tied to $Pr(||\\Sigma^{-1}(a_t - h(s_t))||_2 \\ge C)$. In addition, it appears that $||\\Sigma^{-1}(a_t - h(s_t))||_2 > 0$ since $L_2$ norm is positive with non-zero $\\Sigma$. Therefore it is confusing why $Pr(||\\Sigma^{-1}(a_t - h(s_t))||_2 \\ge C)=0$ (Remark 2) for stochastic policies.\n\nIn addition, the proof relies on the assumption that for every gradient step, the discriminator is optimal. This is an unrealistic assumption and undermines the strength of the claim.\n\n3. The empirical analysis only shows that deterministic policies fail for certain seed. If exploding gradient is indeed an issue as the theoretical results suggested, the authors should explicitly detect exploding gradients rather than just showing learning failures. On the other hand, a large number of runs must be repeated for stochastic polices (much more than 9 seeds, and many runs for each seed) to corroborate that stochastic policies have no exploding gradients, since it's impossible to empirically prove the negative (i.e. stochastic policies have no exploding gradient).",
            "clarity,_quality,_novelty_and_reproducibility": "As discussed, the paper is difficult to follow in the current form, given the relatively poor writing quality.",
            "summary_of_the_review": "The paper is difficult to follow . While the observation/claim appears interesting, both the theoretical and empirical justification falls short. The manuscript may require significant revision before acceptance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6175/Reviewer_F1Ws"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6175/Reviewer_F1Ws"
        ]
    },
    {
        "id": "zQuposEUQRD",
        "original": null,
        "number": 3,
        "cdate": 1666786220824,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666786220824,
        "tmdate": 1666786220824,
        "tddate": null,
        "forum": "3i_7H3phuy3",
        "replyto": "3i_7H3phuy3",
        "invitation": "ICLR.cc/2023/Conference/Paper6175/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "* This paper explores the mechanism behind the generative adversarial imitation learning, which provide an important conclusion that instability is caused by deterministic policies, rather than GANs.\n\n* It provides some existing methods relieve exploding gradients, but at the expense of \u201cnon-confidence\u201d, and ST-GAIL has advantages in mitigating instability.\n",
            "strength_and_weaknesses": "Pros:\n* I believe that there is well backed motivation for work based off of the plentiful literature review. \n*This article provides a theoretical basis for strengthening the study of learning stability and provides a plan for the design of future GAIL work.\n\nCons:\n*This paper is only analyzed under the toy data set but does not test it under the actual scene data set.\n* The composition of this article does not demonstrate carefully, and the writing is very obscure and difficult to understand.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* The writing is very obscure and difficult to understand.\n\n*  It is interesting this paper explores the mechanism behind the generative adversarial imitation learning, which provide an important conclusion that instability is caused by deterministic policies, rather than GANs.",
            "summary_of_the_review": "I would tend to accept this paper as it is novel enough and supported by theoretical analysis.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6175/Reviewer_59jo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6175/Reviewer_59jo"
        ]
    },
    {
        "id": "YhXjlscIH3n",
        "original": null,
        "number": 4,
        "cdate": 1666858152943,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666858152943,
        "tmdate": 1666858152943,
        "tddate": null,
        "forum": "3i_7H3phuy3",
        "replyto": "3i_7H3phuy3",
        "invitation": "ICLR.cc/2023/Conference/Paper6175/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to show that using deterministic policy in a generative adversarial imitation learning algorithm is not proper, and results in a significant instability. This paper does the following:\n- empirical comparisons between SD3-GAIL, TD3-GAIL, TSSG\n- show a toy domain and prove that PLR approaches 0 as the size of state space increases\n- variance of the gradient of GAIL objective explodes in mismatched cases if we use deterministic policy\n- if we use CR instead of PLR, such a problem is addressed with some cost.\n",
            "strength_and_weaknesses": "Strength:\n- this paper addresses a less explored problem of theoretically analyzing the \"instability\" of previous GAIL variants.\n\nWeaknesses:\n- bad writing quality:\n\nThere are a large number of grammatical errors and word misusages. The paper also refers to a number of other algorithms without any detailed explanation - e.g. TD3-GAIL, SD3-GAIL, and combination reward function. How they are defined is quite important in this analysis paper, so I think it had to be described in the paper for completeness. There are many theoretical results, but their implications are not well explained. These issues are combined to make the paper extremely hard to read.\n\n- not convincing analyses:\n\n1. The paper starts by arguing that some algorithms show pathological behavior (of high instability) when PLR and deterministic policy is combined. However, the paper simply makes the comparisons between SD3-GAIL, TD3-GAIL, and TSSG; by comparing different algorithms, there are so many different factors other than PLR and deterministic policy that affects the performance, and it is not accurate to make the hypothesis out of these comparisons. I believe the authors should have compared the consequence of different reward functions and policy choices in the same algorithm. Furthermore, it would have been better if other metrics other than simple returns are compared, e.g. gradient variance, minimum learned reward, etc, to show the validity of further technical analyses.\n\n2. In proposition 1, the authors argue that PLR is bad because, in this toy domain, the reward can approach 0 as the number of states grows to infinity. Why is that a problem? It will be fine unless we have $r=0$ for all state actions, and for a certain number of states $|\\mathcal{S}|$ we will always have positive rewards for some state actions. And it seems to be natural to have a small reward if we have such long expert trajectories.\n\n3. In theorem 1, the authors argue that the variance of gradient explodes as a variance of a policy approaches 0. However, it is not a new analysis and it is widely known if we have a low-variance stochastic policy, the gradient variance of usual policy gradient algorithms will explode, e.g. in REINFORCE algorithm. And it is also well known that DPG does not suffer from such a problem since it has defined the policy as deterministic in the first place. Therefore, for me, the author's argument on theorem 1 of saying GAIL with deterministic policies is unstable is not convincing unless any empirical evidence is provided, e.g. the actual variance of gradient during training.\n\n- the result is not very significant:\n\nThe paper does not propose any new algorithm based on the analysis. Since most of the findings in this paper do not seem to be very significant to me, I cannot highly value the contribution of the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "While the arguments made in the paper are quite original, I believe that the quality and clarity of the paper are not good enough.",
            "summary_of_the_review": "Due to the weaknesses I listed above: low writing quality, not convincing analysis, and lack of algorithm as a result of the analysis, I recommend rejection of the paper.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6175/Reviewer_EU3W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6175/Reviewer_EU3W"
        ]
    },
    {
        "id": "R1cl9EuFN7",
        "original": null,
        "number": 5,
        "cdate": 1667507351690,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667507351690,
        "tmdate": 1667509421905,
        "tddate": null,
        "forum": "3i_7H3phuy3",
        "replyto": "3i_7H3phuy3",
        "invitation": "ICLR.cc/2023/Conference/Paper6175/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper is about the instability in training when a deterministic learned policy is used in generative adversarial imitation learning (DE-GAIL) and how to alleviate the problem. The authors first show that the training of DE-GAIL is empirically unstable in Mujoco environments that have continuous action and state spaces. Then, they theoretically show that the reward of DE-GAIL becomes zero when the deterministic policy is used in finite MDP and when the cardinality of the state space goes to infinity in Proposition 1. Also, they argue that the instability is mainly due to the reward becoming zero. For MDPs with continuous action spaces, they theoretically show that DE-GAIL suffers the problem of exploding gradients due to a mismatch between actions from the expert and the learned policy. Furthermore, they use Lemma 1 to show that having a mismatch between the actions from the expert and the learned policy is equivalent to having an optimal discriminator function value of one which is also equivalent to having an infinite reward. Lastly, in Proposition 2, they prove that using CR instead of PLR can alleviate the problem. This is done by showing that PLR has a value larger than or equal to that of CR when their discriminator values are the same.",
            "strength_and_weaknesses": "**Strength**\n1. The authors theoretically show that the deterministic learned policy brings instability in training.\n\n2. The authors theoretically show that the problem can be alleviated by using the modified reward function of AIRL.\n\n**Weaknesses and Questions**\n\n1. There are no experiments that validate or support Proposition 1. The experimental results in Fig 1~3 are made from environments with continuous state and action spaces while Proposition 1 is about environments with finite action space. If the authors could empirically validate Proposition 1, that would strengthen the paper.\n\n2. There are no experiments that validate Proposition 2. The empirical results shown in the paper are only about PLR-DE-GAIL. The results of CR-DE-GAIL should be compared with that of PLR-DE-GAIL.\n\n3. Need more strong empirical validation for Theorem 1. Fig 2 and 3 may show instability in training DE-GAIL, but they don\u2019t empirically show that the exploding gradients problem actually happened and that the problem is caused by the instability. If the authors could show that the low-performing seeds in Figure 3 indeed suffered from the exploding gradients while the other seeds did not, that would validate Theorem 1 strongly.\n\n4. The statement In pg 6: \u201cFor any state $s_t$, the action $a_{t \\rightarrow t+1}$ occurs 3 times while others occur at most twice in the expert demonstration.\u201d seems to be wrong. If you follow the pseudo-code in Figure 4, when the agent is in $s_{\\frac{3g}{4}+1}, s_{\\frac{3g}{4}+2}, \\cdots, s_{g-1}$ the agent never executes the action $a_{t \\rightarrow t+1}, \\forall t\\in\\{ \\frac{3g}{4}+1, \\frac{3g}{4}+2, \\cdots, g-1  \\}$.\n\n5. On pg 7, authors mentioned that \u201cThe learned policy is exposure to be transferred to the mismatching case due to the limited area in the sub-optimal matching compared with the optimal; see seed 1 in Fig. 3(a) for instance\u201d. But I think you cannot guarantee that the result from seed 1 in Fig.3 (a) is such a case. Could you show that the seed 1 experiment shows results similar to Fig 6?\n\n6. Proposition 1: The derivation of the \u201clength of the expert trajectory\u201d seems to be wrong. In Appendix A.1 in the proof of Proposition 1, the authors derived this equation :  $N=2\\left(\\frac{g}{4}+\\left(\\frac{g}{4}+1\\right)+\\cdots+\\left(\\frac{g}{4}+\\frac{g}{4}-1\\right)\\right)+2\\left(\\frac{g}{2}+1\\right) \\frac{g}{2}-1+\\frac{3 g}{4}=\\frac{11 g^2+24 g-16}{16}$. I may be wrong, but the derivation is different from mine. For the first for loop in the pseudo-code of Figure 4, the sub-trajectory length is $2\\left(\\frac{g}{4}+\\left(\\frac{g}{4}+1\\right)+\\cdots+\\left(\\frac{g}{4}+\\frac{g}{4}\\right)\\right)+\\frac{3 g}{4}$ where the first term comes from executing actions $a_{i\\rightarrow j}, a_{j\\rightarrow i}$, and the last term comes from executing the action $a_{i\\rightarrow i+1}$. For the second for loop, since the agent only traverses between $s_{\\frac{3g}{4}}$ and $s_{\\frac{g}{2}}, s_{\\frac{g}{2}+1}, \\cdots, s_{g-1}$ except $s_{3g/4}$, the length of sub-trajectory due to the second for loop should be $2\\left( (g-1)-g/2 \\right)$. For the last line, the sub-trajectory of length 1 comes from executing the action  $a_{3g/4 \\rightarrow g}$. By summing them up, I got $(g^2 + 36g)/16$.\n\n7. According to \u201cDefinition 1\u201d on pg 7,   state-action pairs are in a mismatch when $|| \\Sigma ||_2 \\rightarrow 0$ and $a_t \\neq h(s_t)$. And the pairs match when $a_t = h(s_t)$. But Fig 6 draws actions in the neighborhood of a_t1 to show the matched case. And the paragraph under Fig 6 mentions that the matched case is made by having actions in the neighborhood of a_t1.\n\n8. In Appendix A.2 (proof of Theorem 1), the authors derived $\\nabla_h \\pi_h(a \\mid s)=\\pi_h(a \\mid s) \\kappa(s, \\cdot) \\boldsymbol{\\Sigma}^{-1}(a-h(s))$. Why is there $\\kappa$ when you define $\\pi_h$ as in the equation at the bottom of pg 6?\n\n9. In Remark 2, since $|| \\Sigma ||^{-1}_2 ||a_t-h(s_t)||_2 \\ge || \\Sigma^{-1} (a_t-h(s_t))||_2$, shouldn't be the inequality between probabilities (probability mentioned in Theorem 1 and $\\operatorname{Pr}(\\Xi)$) the other way around?\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: poor\n\nBelow are the questions related to the clarity of the paper:\n\n1. In Figure 6, adding the legend for the red line and also adding x-axis and y-axis labels will make the clarity of the paper higher. And rather than writing $a_{t1}$ and $a_{t2}$ in the figure, putting markers and adding legends showing that they are optimal and suboptimal actions would be more informative to the readers.\n\n2. On pg 7, I found this sentence hard to understand: \u201cThe learned policy is exposure to be transferred to the mismatching case due to the limited area in the sub-optimal matching compared with the optimal\u201d. I\u2019d appreciate elaboration on the sentence.\n\n3. I was unable to find information on how Fig 6 was drawn. Was it drawn as an illustrative example? or, is it an empirical result?\n\n4. On pg 7, the authors mentioned that \u201cThe threshold of matching is set as 0.035.\u201d. Could you elaborate on how 0.035 was computed? Also, the \u201cthreshold\u201d was never mentioned before. What is the \u201cthreshold\u201d and where is it applied?\n\n5. In Appendix A.2 (proof of Theorem 1), the authors cited (Guan et al., 2021a) for the gradient of the JS divergence with respect to h, but I was not able to find the contents related to the gradients of JS divergence in the paper. Is the paper cited for policy gradient? If so, elaboration on the derivation of $\\hat{\\nabla}_h   D_J   (\\rho^{\\pi_h}, \\rho^{\\pi_E} )$ ($D_J$: JS divergence) would help the readers to understand.\n\n6. Is the probabilistic lower bound related to the lower bound in Remark 2? If not, where is it mentioned in the paper?\n\n**Reproducibility** : good\nThe details of the experiments for Fig 2 and 3 are mentioned in the paper\n\n**Quality** : poor\nDue to the lack of details in Fig 6 and hard-to-understand sentences.\n\n**Novelty** : good\n\n\n",
            "summary_of_the_review": "The paper presents a novel theoretical analysis of the instability in the training of DE-GAIL and also theoretically shows how the modified reward function of AIRL can alleviate the problem. However, there seem to be some cases where statements or derivations are wrong, Furthermore, the paper lacks experiments that support propositions and theorems.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6175/Reviewer_Qudi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6175/Reviewer_Qudi"
        ]
    }
]