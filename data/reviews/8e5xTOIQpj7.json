[
    {
        "id": "5ITpyXu66C",
        "original": null,
        "number": 1,
        "cdate": 1666310713942,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666310713942,
        "tmdate": 1666310713942,
        "tddate": null,
        "forum": "8e5xTOIQpj7",
        "replyto": "8e5xTOIQpj7",
        "invitation": "ICLR.cc/2023/Conference/Paper2767/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper aims to address the fact that deep networks may condition on spurious features that are correlated with the target class without being causal. They attempt to leverage the front door criterion in order to avoid the need to explicitly condition on confounders. Their approach works by treating the hidden representation as a mediator (this is incorrect) and then attempting to control for confound via a front door-like update (except they represent interventions as MAML-style gradient perturbed parameters). ",
            "strength_and_weaknesses": "This papers is built on a series of serious misunderstandings of the relationship between a model and a data generating process in causal inference. The front door criterion says that if we have access to data of the form X -> Z -> Y, where Z mediates the relationship between X and Y, then we can control for any confounding between X and Y (including unobserved confounding) by the front door adjustment formula. But not - this is a statement about the true data generating process --- i.e. Z is really some variable in the real world that we observe --- it is not a statement about our model of the data generating process, as this paper assumes. Their key idea is to treat the hidden representation as a mediator, which they attempt to justify in appendix A.3, but this justification is just wrong: the hidden representation z = g(X) is *not* on the **causal** path from X to Y. It is just an intermediate step in the computation of model's estimate,  \\hat{Y}. We can see this by noting that if we were to intervene on z = g(X) (e.g. by setting it to zero), nothing would change in the true Y (because our model's estimate is not on the causal path of the true data generating process!), but \\hat{Y} would change. \n\nSimilarly, the section on the gradient-based instantiation of FEIE seriously misunderstanding what an interventional distribution is (again, it's a property of the true data generating process, not the model), so nothing that's written there makes sense from a causal perspective.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has very serious flaws; I didn't check reproducibility because I couldn't get past all the errors in the method. The writeup could also use a serious proof read - there are too many typos and ambiguous statements to list here.",
            "summary_of_the_review": "This paper has very serious technical flaws and is not ready for publication/",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2767/Reviewer_K8gB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2767/Reviewer_K8gB"
        ]
    },
    {
        "id": "G1M2e0alLy6",
        "original": null,
        "number": 2,
        "cdate": 1666613830934,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613830934,
        "tmdate": 1666613830934,
        "tddate": null,
        "forum": "8e5xTOIQpj7",
        "replyto": "8e5xTOIQpj7",
        "invitation": "ICLR.cc/2023/Conference/Paper2767/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "To eliminate domain effect in deep learning, existing approach relies on identifying explicitly the confounder and remove confounder effect via backdoor criterion.  The proposed method implements front door criterion via gradient. \n\nThe author proposed to use a neural network to represent $\\sum_{x}Pr(Y|Z=z, x)$, and shed light on the connection with Model Agnostic Meta Learning, which is, similar to MAML, use a \u201clook-ahead\u201d gradient operation to represent the conditional distribution. \n\nTo achieve this look ahead, the author proposed to sample a small portion of the whole sample which represent the whole distribution. \nTo achieve a sampling strategy which could mimic the all-observation distribution, the author use K-means algorithm to cluster the data into several clusters and take a small portion from each of the cluster. \n",
            "strength_and_weaknesses": "Strength:\nI like the way that the author try to use different subfield to address the domain generalization problem with meta learning and causal inference.\n\nWeakness:\nPlease see the questions: \n\n\n\nQuestion:\nThe original front door criteria is actually a concatenation of two time back-door criterion, which is concatenating two times do operation. The first do operation is p(Z=z|do(X=x)), which means if one cut the parent of node X, what would be the conditional distribution of Pr_{modified graph}(Z=z|X=x), usually, this z is observed and believed to be a direct cause of target variable Y and x is a direct cause of z.\n \n\nWhy p(Z=z|X=x)= 1? If this is true, then $z$ should be a deterministic map of X.\n\nFor the second backdoor part $\\sum_x Pr(Y|Z, x) p(x)$, which is essentially treating x as an indirect confounder, due to the deterministic map z=h(x), the summation over z is not needed, Pr(Y|Z, X) only depend on X. As the author has observed, directly concatenating $z=h(x)$ would result in $z$ being ignored. But the author introduced using a \u201clook-ahead\u201d (name I give) gradient to represent this probability, i did not see much explanation why? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:   As stated in the question, some parts of the paper is not well explained. \n\nQuality: OK.\n\nNovelty: Good\n\nReproducibility: Not known. ",
            "summary_of_the_review": "I think the paper is very innovative but still lacks some detailed explanation to the crucial part. Especially the way that the author use look-ahead gradient step to approximate the conditional distribution and using a neural network to extract a deterministic feature from image X to z, which is not how front door criterior works initially. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2767/Reviewer_pVYZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2767/Reviewer_pVYZ"
        ]
    },
    {
        "id": "s5Eah-Oh8U",
        "original": null,
        "number": 3,
        "cdate": 1666616577836,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616577836,
        "tmdate": 1666616577836,
        "tddate": null,
        "forum": "8e5xTOIQpj7",
        "replyto": "8e5xTOIQpj7",
        "invitation": "ICLR.cc/2023/Conference/Paper2767/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a novel method (CICF) that is based on the idea of front-door adjustment for hidden confounders. They additionally demonstrate the relation between CICF and the popular meta-learning strategy MAML (Finn et al., 2017). The authors provide a theoretical interpretation of why MAML works.",
            "strength_and_weaknesses": "**Strengths:**\n\nThe paper introduces a novel idea to use the front-door criterion instead of the back-door for covariate adjustment. They theoretically demonstrate how to incorporate front-door criteria ideas into the learning algorithm. They additionally demonstrate that front-door criteria could be used to theoretically explain the previously proposed meta-learning algorithm (MAML).\n\n**Weaknesses and questions:**\n\n- Why the approach proposed by eq. (4) won\u2019t also lead to a trivial solution? Won\u2019t the gradient become close to zero when the model converges? Won\u2019t be the information on x tilda also ignored?\n- It is not clear to me why results for previously published models don\u2019t agree with the DomainBed framework. The experiments would look more convincing if they were implemented and compared within DomainBed framework and good reproducibility practices.\n- Grad-CAM maps from supplementary figure 9 look less optimistic than the ones in figure 4. For example, it shows that the model still attention to the bone to classify the dog. Also, such visualizations would be more interesting to compare not to models learned with ERM strategy, but with any causal learning method, e.g. IRM.\n- Also for synthetic examples in order to show the effect of the confounding variable was removed, it would be more interesting to see results on colored MNIST.\n- A more explicit demonstration of the removed confounding effect would be very helpful.\n- The results in Table 1,3,4 are better on average, but not across all environments. If the model was truly correcting confounding, why would be there such inconsistencies?\n- Figure 9 is also not very convincing that the learned features are somehow significantly different. The plots look actually quite similar up to random seed or some t-SNE hyperparameters.\n- Confidence intervals in Tables 1,3,4 would be quite helpful.\n- How did the authors choose the best set of hyper-parameters for each dataset?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is novel and clearly written. The authors provide training details in the appendix, but the code is not provided as far as I understood.",
            "summary_of_the_review": "I find the proposed method and theoretical part of the part quite interesting, but I am not convinced by the experiments (described above my questions and concerns).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2767/Reviewer_wmrP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2767/Reviewer_wmrP"
        ]
    },
    {
        "id": "Pcdmx0MTcso",
        "original": null,
        "number": 4,
        "cdate": 1666633642818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666633642818,
        "tmdate": 1666633642818,
        "tddate": null,
        "forum": "8e5xTOIQpj7",
        "replyto": "8e5xTOIQpj7",
        "invitation": "ICLR.cc/2023/Conference/Paper2767/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a novel method of learning robust DNNs through the use of a network containing two si models, one that learns an intermediate variable Z and then combines it with the front door criterion to learn the interventional distribution p(Y|do(X = x)). This interventional distribution has invariance properties, and that is why it is a common learning strategy used when it comes to building robust models. Various strategies to learn certain intractable models are described, and state of the art performance is shown on certain datasets. Additionally, this provides a theoretical justification for an existing approach, which is appreciated. ",
            "strength_and_weaknesses": "Strength: Uses a front-door graph instead of a back door graph, which has novelty. Describes how to fit intractable models and achieves state of the art results on  datasets.\n\nWeakness: A big challenge with causal inference is being able to correctly fit the nuisance models when we are interested in learning the target parameter. Here, I\u2019d like to see some more assurance around the proposed training strategy being able to fit the nuisance models correctly and consequently correct learn the interventional distributions in a variety of situations. ",
            "clarity,_quality,_novelty_and_reproducibility": "The approach is presented clearly, as well as the various learning strategies involved. There is a thorough description of the algorithm as well as various choices for subnetworks and feature visualization as well.",
            "summary_of_the_review": "This paper uses a two stage network to induce a front door causal graph and learn the interventional distribution p(Y | do(X+x)). since this interventional distribution is invariant, it is robust to distribution shift and domain generalization. This allows the authors to learn a prediction model that is robust and outperforms state of the art methods. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2767/Reviewer_jBAD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2767/Reviewer_jBAD"
        ]
    }
]