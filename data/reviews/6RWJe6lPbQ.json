[
    {
        "id": "8rpIX45ECe",
        "original": null,
        "number": 1,
        "cdate": 1666356789969,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666356789969,
        "tmdate": 1666356789969,
        "tddate": null,
        "forum": "6RWJe6lPbQ",
        "replyto": "6RWJe6lPbQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2385/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors aim to solve the measurement problem of graph-level clustering, and proposes an end-to-end method to jointly optimize graph representation and clustering, and the clustering goal can guide the learning of the entire graph, which is more effective than two stages.",
            "strength_and_weaknesses": "Strength: An end-to-end method is proposed to jointly optimize the two modules of graph representation and clustering. \nWeaknesses: \n(1) The motivation of this paper is that the similarity of the graph level is difficult to measure, which is a very common question that most people have asked. The proposed measurement method is not novel, and the only thing is to reduce the dimensionality of the feature.\n(2) As far as I know, there are many works on graph clustering before, which are not mentioned in the related works of this article.\n(3) The graph representation module of the proposed method is the previous work, and the clustering module is also the previous DEC clustering method. It's more like putting together previous work, which is not innovative enough. \n(4) In the experimental part, there is no graph-level clustering work for comparison. Some of the methods compared in this paper are graph representation and k-means or SC, so whether other graph representation methods plus the clustering method proposed in this paper (that is, DEC) are better than the effect of this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of the work is poor because of a lack of innovation in motivation and method. The idea is relatively clear, the two modules of graph representation and clustering are combined to optimize each other to form an end-to-end framework, the originality is poor, the graph representation method and the clustering method are all previous work, and the changes are not large, so I think the originality is lacking.",
            "summary_of_the_review": "The motivation for this paper is that graph-level similarity is difficult to measure, and the solution is to map to a low-dimensional space (represented by a graph) and then measure similarity through an MLP layer, which lacks innovation. In addition, the proposed graph clustering method lacks novelty, and the related work does not summarize the previous graph clustering work, and the experiment is relatively insufficient.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2385/Reviewer_jD5G"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2385/Reviewer_jD5G"
        ]
    },
    {
        "id": "gjk9wwABZtb",
        "original": null,
        "number": 2,
        "cdate": 1666550009470,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666550009470,
        "tmdate": 1666550009470,
        "tddate": null,
        "forum": "6RWJe6lPbQ",
        "replyto": "6RWJe6lPbQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2385/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper mainly tackles the graph-level clustering problem by proposing a novel method called Deep Graph-Level Clustering (DGLC) to learn graph-level representations and maximize the mutual information between the representations of entire graphs and substructures. Specifically, a clustering objective is proposed to guide the representation learning for the graph via pseudo labels. In addition, the authors analyze the effectiveness of graph kernel-based methods and unsupervised graph representation learning methods.",
            "strength_and_weaknesses": "Strength:\n1.\tThe experimental results show the effectiveness of the proposed method over SOTA methods.\n2.\tThe paper is well-organized. The presentation of this paper is good, and most parts of the paper are easy to follow.\n3.\tThe datasets are publicly available, and the proposed method is easy to implement.\nWeakness:\n1.\tThe novelty of this paper is limited. The proposed method simply combines DEC[1] and InfoGraph [2].\n2.\tSome grammar mistakes and typos should be corrected, such as in the first paragraph on page 6, \u201cwhich is also can also be regarded as a self-training strategy\u201d and the missing conjunction \u2018and\u2019 in the sentence below equation 2.\n3.\tThe motivation of the proposed method is not strong enough.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of this paper is limited as the proposed method simply combines DEC and InfoGraph without any modification in the graph-level clustering problem. Despite some grammar mistakes and typos, the paper is easy to follow and thus the overall presentation of this paper is good. As for reproducibility, the data sets and code are publicly available.",
            "summary_of_the_review": "This paper mainly tackles the graph-level clustering problem by proposing a novel method called Deep Graph-Level Clustering (DGLC). Despite a few grammar mistakes and typos, the presentation of this paper is good as most parts of the paper are easy to follow. The novelty of this paper is limited. The proposed method simply combines DEC[1] and InfoGraph [2]. In addition, the motivation of the proposed method is not strong enough. The authors mainly focus on discussing the challenges of the graph kernel-based methods and unsupervised learning-based methods on graph-level clustering problems, while the other existing methods, such as InfoGraph, have already addressed most of these challenges (it\u2019s difficult and ineffective to measure the similarity between graphs for graph kernel-based methods and some existing methods do not utilize the label information to better represent graphs in latent space). In the experiment, the results show the effectiveness of the proposed method over SOTA methods, and the data sets and the code are publicly available.\nIn the experiment, the authors categorize InfoGraph as an unsupervised method, but InfoGraph consists of a supervised loss term considering the label information. It is a little bit unfair to measure the performance of InfoGraph in the unsupervised setting by simply removing the supervised loss term. In addition, the proposed method is similar to InfoGraph, and the main difference is that DGLC benefits from the joint training strategy, while InfoGraph doesn\u2019t. In Table 7, the authors show the running time of the proposed method and SOTA methods. Why does InfoGraph+KM take more time than DGLC? DGLC and InfoGraph both share the same graph-level representation learning strategy, while InfoGraph does not have the clustering module during the training (if KM or SC is only used after the training stage is over) and DGLC consists of a clustering module at the training stage. It seems that the running time of InfoGraph should be smaller than DGLC. \nIn the experiment, the numbers of graphs in all data sets are too small. The authors should experiment on some large graph data sets, such as RDT-M5K data set or QM9 data set.\nOne minor question: How do you get the negative sample s' in equation 6?\n\n\n[1] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In Proceedings of the International Conference on Machine Learning, pp. 478\u2013487. PMLR, 2016.\n[2] Fan-Yun Sun, Jordon Hoffman, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. In Proceedings of the International Conference on Learning Representations, 2020.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2385/Reviewer_RqLm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2385/Reviewer_RqLm"
        ]
    },
    {
        "id": "3rPMQRf7QB",
        "original": null,
        "number": 3,
        "cdate": 1666683245834,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683245834,
        "tmdate": 1666683245834,
        "tddate": null,
        "forum": "6RWJe6lPbQ",
        "replyto": "6RWJe6lPbQ",
        "invitation": "ICLR.cc/2023/Conference/Paper2385/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new end-to-end deep learning method for graph clustering. The goal is to partition a set of graphs according to the similarities of their structures, while learning a graph-level representations. Mutual information theory is exploited in order to maximize within cluster similarities, while penalizing the between clusters similarity\t",
            "strength_and_weaknesses": "The method is well supported by a solid theoretical foundation. While not being an entirely novel idea, the method efficiently combines different elements resulting in an innovative approach. I think the experiments could be more comprehensive. Some general comments are given below.\n\t\n- The authors could better elucidate the contribution of the node label type in their approach. While they mention the label information is not used in the clustering, it would be useful to compare the impact of categorical vs continuous node labels. This is relevant both on the theoretical and experimental side. From the experimental perspective, they chose kernels which cannot handle the continuous node labels, making the comparison not fully fair with the deep learning approaches. \n\n- Only using datasets with 2 classes is quite limiting. Those data are typically used for classification, and I believe 2 classes are not enough to assess the potential of clustering based methods, since the granularity and differences among the classes itself are limited\n\n- Some benchmark datasets as well as methods are missing. On the graph kernels side, NCI1 and NCI109 datasets should a least be included. Furthermore, optimal assignment based kernels could also be integrated in the comparison. Additionally, many graph kernels also provide a feature representation, therefore one could also use k-means on these  for a fair comparison. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. Some steps in the theoretical description could be better elucidated, although the paper is overall well structured. Experiments are well presented to allow for reproducibility. ",
            "summary_of_the_review": "Overall, I don't see major flows in the paper. The proposed method brings an innovative component, while the experiments could benefit from improvements. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2385/Reviewer_vbKc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2385/Reviewer_vbKc"
        ]
    }
]