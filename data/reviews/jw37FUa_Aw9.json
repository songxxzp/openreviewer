[
    {
        "id": "cZ1-GRur-F",
        "original": null,
        "number": 1,
        "cdate": 1666351854653,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666351854653,
        "tmdate": 1666351854653,
        "tddate": null,
        "forum": "jw37FUa_Aw9",
        "replyto": "jw37FUa_Aw9",
        "invitation": "ICLR.cc/2023/Conference/Paper1636/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new Transformer architecture for better holistic explainability. The architecture generates explanations that reflect all the model components instead of only the attention layers. To do so, the authors take explicit designs for each of the Transformer modules so that they can be summarized by a single linear transformation. The authors perform the main and ablative experiments on ImageNet to evaluate the effectiveness of the proposed methods.",
            "strength_and_weaknesses": "Strength:\n\n\u2022\tThe qualitative results look promising. The proposed method generates visualizations with richer details than the baseline methods shown in Fig. 7 and Fig. A1. \n\u2022\tThe quantitative results look impressive. The proposed B-cos Transformer have higher localisation and perturbation scores than the baseline methods under the same model accuracy. Compared to the conventional ViT models of the same depth, B-cos Transformer has better overall performance across different depths. \n\u2022\tThe paper is well-written and easy to follow.\n\nWeakness:\n\n\u2022\tThe authors argue that B-cos Transformer performs at least as well as the baseline ViTs under the same number of backbone layers in Section 5.1. However, the experimental setting is unfair without comparing the computational complexity to the baseline methods, especially when the authors replace the conventional tokenization module with a pre-trained B-cos DenseNet-121. Though deriving more explainable models, the proposed model should be toned down if they are too heavy to deploy. This is the same for Fig. 6, which also lacks comparisons of model complexity vs. Localisation/perturbation. \n\n\u2022\tI am also confused about where the good performance of B-cos Transformer comes from. The authors claim that the positional information design largely improves performance. However, the effectiveness of the positional information cannot be concluded from Fig. 5, with many other components modified. To fully explain the performance gain, comparisons have to be done by attaching the B-cos positional information to the conventional ViTs.\n\n\u2022\tI am also curious about how the third column of Fig. 7 is generated, as the visualizations do not seem to be normalized.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality of this paper is good and the technical part is novel. The authors have not provided the code base in terms of reproducibility.",
            "summary_of_the_review": "There are many confusing points that need to be addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1636/Reviewer_rHTt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1636/Reviewer_rHTt"
        ]
    },
    {
        "id": "d3hPqHeS9j",
        "original": null,
        "number": 2,
        "cdate": 1666573775209,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666573775209,
        "tmdate": 1666574959866,
        "tddate": null,
        "forum": "jw37FUa_Aw9",
        "replyto": "jw37FUa_Aw9",
        "invitation": "ICLR.cc/2023/Conference/Paper1636/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes to replace the modules of transformer with dynamic linear operation so that it is more explainable. It follows the guideline of BCos Networks but adopts transformers on the top of DenseNet features. The method is evaluated on ImageNet with localization and perturbation metric. It claims to have to be more interpretable and perform competitively to ViT.",
            "strength_and_weaknesses": "Strength:\n+ The idea of improving interpretation of transformer is interesting.\n+ The paper is well organized and easy to follow.\n+ The method is well evaluated and the results are well presented.\n\nWeakness:\n- My first concern is the claim of Bcos-ViTs. The authors are using transformers on the top of DenseNet features which is not ViT. A ViT is trained directly on images by considering each patch as a token. The method is more like Bcos Transformer, not Bcos-ViT. \n- The whole explanation principle is exactly the same as the Bcos Networks (CVPR2022) which makes this paper more like an extension from deep network to transformer. What is the new challenge here for explaining DenseNet+transformer using Bcos idea as compared to explaining a DenseNet?\n- The method itself cannot be adopted to explain existing ViT models like [Chefer et al. (2021)], but has to be re-trained to gain interpretation ability. But this paper does not show how the ViT would performance if all the transformers are replaced with the proposed block. It is very misleading to claim DenseNet+Transformer as ViT in Fig 5. How would a standard ViT perform if the proposed Bcos method is used?",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of extending the Bcos Networks to transformer is new, but could be incremental. The presentation and writing are good.  The link of the code cannot be opened.",
            "summary_of_the_review": "Overall, I feel that the contribution does not meet the bar of ICLR and some claims are very misleading to readers. I would recommend weak reject for the current version.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1636/Reviewer_NoVr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1636/Reviewer_NoVr"
        ]
    },
    {
        "id": "gvvMNQk1p-",
        "original": null,
        "number": 3,
        "cdate": 1666688131924,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666688131924,
        "tmdate": 1666688131924,
        "tddate": null,
        "forum": "jw37FUa_Aw9",
        "replyto": "jw37FUa_Aw9",
        "invitation": "ICLR.cc/2023/Conference/Paper1636/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a novel approach towards improving the explainability of vision transformers. Specifically, the paper proposes updating each component in the model using the B-cos transform, which is designed to increase the alignment of the inputs and the weights. The paper builds on a previous state-of-the-art work that introduced the B-cos transform on DNN/CNNs and extends it to vision transformers. Comprehensive experiments are conducted comparing the proposed approach with several state-of-the-art explainability methods.",
            "strength_and_weaknesses": "Pros\n\t- The proposed method is simple and seems easy to implement with the provided details.\n\t- Strong benchmark performance improvements across multiple explainability metrics. Further, the attribution maps of the model are highly detailed and have very low noise.\n\nCons\n\t- This paper is a direct extension of \"B-cos Networks: Alignment is All We Need for Interpretability\" by Bohle et al, which proposed the B-cos transform and implemented it on MLPs and CNNs. In this paper, that transform is applied to the vision transformer. However, the main contribution of this paper is incorporating the B-cos transform in the attention layers and the positional encodings. It's incorporation in the MLP/CNN layers is directly taken from the original paper. This makes the architecture seem a bit derivative.\n\t- The paper proposes using max-out network in equation 12. However, that would double the number of parameters in the model. It would be helpful if some comments are added regarding the increased model size. It would also be helpful to know if removing the max-out layer will significantly impact the final results.\nMost of the results compared the model with the state-of-the-art visualization models. Given this model is a significant architectural modification of the original vision transformer, it would be really helpful to compare the results of the model with those of the vision transformer on other benchmark tasks.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was very well written and was very easy to follow. The authors mentioned that the code will be released upon publication. The architecture is an extension of the model proposed in \"B-cos Networks: Alignment is All We Need for Interpretability\" and seemed a bit derivative.",
            "summary_of_the_review": "I don't think it's a strong accept due to the slightly derivative nature of the work, but overall, the paper extends a very novel idea from DNN/CNNs to vision transformers and supports this with strong benchmark results. Due to this, I think its marginally above the acceptance threshold. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1636/Reviewer_oHsW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1636/Reviewer_oHsW"
        ]
    }
]