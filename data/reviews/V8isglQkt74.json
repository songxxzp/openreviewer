[
    {
        "id": "kb3sZmNTpJ",
        "original": null,
        "number": 1,
        "cdate": 1666324985242,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666324985242,
        "tmdate": 1671231619953,
        "tddate": null,
        "forum": "V8isglQkt74",
        "replyto": "V8isglQkt74",
        "invitation": "ICLR.cc/2023/Conference/Paper6068/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the framework \"Implicit Symbolic Concept Learner\" (IS-CL), a transformer-based architecture that is capable of reasoning about videos. The framework follows a pretraining-transfer-learning pipeline. That is, the model is first pretrained on a collection of unlabelled videos (by the MAE objective), and then finetuned on the target task (e.g., predicting the location of a certain object).",
            "strength_and_weaknesses": "The paper presentation is clear and well-connected with existing works such as NS-CL and ALOE (Figure 1). There are a few places where the authors connect their work with Slot Attention, which has greatly helped me to understand the connections and differences between the proposed framework and others. The proposed framework is also relatively straightforward, with clear motivation. Experimental results have shown the success of the proposed framework. Furthermore, the ablation studies seem very adequate.\n\nTalking about weaknesses, I will start with a few conceptual ones.\n\nFirst, the idea of \"concept learning,\" following the notations from Mao et al, primarily refers to the correspondence between linguistic units (words, phrases) and visual representations (e.g., red, cubic, left-of, etc.). This seems to be different from the definition of \"concepts\" or \"implicit symbolic concept\" in the paper, which, in my understanding, refers to \"implicitly defined objects.\"\n\nSecond, when we talk about \"visual concepts,\", they are usually more \"abstract\" than \"pixel reconstruction.\" However, since the overall training paradigm (pretraining time) is to reconstruct the pixels, I don't think there is evidence that the model is capable of discovering \"symbolic concepts,\" e.g., colors, shapes, etc., from the pertaining. With all that, I am wondering if the title/model should be better phrased as \"implicit object learning\" or similar phrases?\n\nHere're a few technical questions.\n\nFirst, I am wondering why the authors have not tried visual question-answering benchmarks such as CLEVRER, especially given that based on the ablation study (Table 2a), CATER does not require an \"implicit symbolic\" representation (slot =1 then basically it's just a per-image representation). It seems that the framework can be easily extended to that, as in ALOE. And I think extending the framework to that can significantly improve the completeness of the paper.\n\nSecond, in Table 4, it seems that the model still underperforms several baselines. Although authors may argue that those better-performing algorithms are \"object-centric\" I think it is still completely reasonable to compare IS-CL with ALOE, because ALOE also does not use any object-detection labels during training. Again, I think adding a new benchmark that showcases the advantage of the proposed method will be ideal.\n\nThird, more of a clarification question, regarding the comparison between VideoMAE and IS-CL. Is the only difference that you are using a \"single-token\" embedding for each frame whereas VideoMAE uses all visual tokens? Because these two models have very similar training objectives and similar architectures.\n\nFourth, is there any way that we can visualize the learned \"implicit slot tokens?\" For example, can you visualize the attention maps?\n\nFinally, you mentioned that you have changed the slot-attention-style encoding with a customized transformer-style encoding. Do you have any ablation studies on that?\n\nClairification questions:\n\n1. Page 5:  allowing the slots to attend not only to raw visual inputs, but also to the encoded patch-level representation. Can you be more specific about this?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well presented and has novelty over existing works.",
            "summary_of_the_review": "Overall I think this paper is a good contribution to the community. However, there are still a few issues that can be addressed by the authors before publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6068/Reviewer_ejDU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6068/Reviewer_ejDU"
        ]
    },
    {
        "id": "129CAPg3MAw",
        "original": null,
        "number": 2,
        "cdate": 1666625636321,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625636321,
        "tmdate": 1666625636321,
        "tddate": null,
        "forum": "V8isglQkt74",
        "replyto": "V8isglQkt74",
        "invitation": "ICLR.cc/2023/Conference/Paper6068/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for compositional visual reasoning in videos, based on recent advances in self-supervised pretraining. The method first trains a spacial-temporal transformer that reconstructs the video frames under the masked autoencoder paradigm, then performs reasoning via transfer learning. The authors claim and demonstrate via extensive experimental study on the CATER and ACRE datasets, that a generalizable compact representation is learnt, superior to other approaches based on object-centric representation. Ablation studies are conducted.",
            "strength_and_weaknesses": "Strength:\n\n1. This paper proposes a powerful end-to-end network for compositional visual reasoning, and achieved nice results on two datasets that were regarded as (and designed to be) challenging for end-to-end systems. \n\n2. The paper also demonstrates self-supervised pre-training can lead to useful representations for visual reasoning, which sheds light to further research.\n\n3. The paper is well written and backed by rich experimental results.\n\nWeakness\n\n1. The paper claims that the model learns \"implicit symbolic representation\" without a clear definition and elaboration. What is the difference between \"symbolic representation\" vs. a regular latent representation learned by other self-supervised model? Moreover, there isn't sufficient study on the representation itself other than transfer learning results to back these claims. How does the same architecture perform without the representation?\n\n2. The experimental results have not established, decisive evidence that end-to-end method is superior to object-centric representations. Specifically, under same supervision, the model does not significantly outperform ALOE and ALOE++. The paper also does not provide evaluation on the CLEVRER dataset, a video reasoning benchmark where object ALOE performs nicely.\n\n3. Minor: The data flow in figures goes upwards instead of downwards, which might cause unnecessary confusions to readers.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with sufficient discussion on related works, even though the presentation of figures can be improved. The authors claim that code will be released upon acceptance of the manuscript.",
            "summary_of_the_review": "This paper is an interesting paper that applies state-of-the-art self-supervised pretraining to compositional visual reasoning. Despite the reasonable results achieved on two challenging datasets, it is unfortunate that the paper does not provide a deeper look into the learned representation in order to back the claimed novelty. There is definitely a lot of room for improvement here. My score is borderline reject.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6068/Reviewer_LKo8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6068/Reviewer_LKo8"
        ]
    },
    {
        "id": "xmT9T3EIROd",
        "original": null,
        "number": 3,
        "cdate": 1667095968835,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667095968835,
        "tmdate": 1671257370079,
        "tddate": null,
        "forum": "V8isglQkt74",
        "replyto": "V8isglQkt74",
        "invitation": "ICLR.cc/2023/Conference/Paper6068/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed an end-to-end implicit symbolic representation learning framework for visual reasoning tasks. It wisely adopts slot tokens for its bottleneck information properties, masked autoencoding objective, and transformers to learn implicit representations in a self-supervised way. The learned implicit representation can later be applied to solve specific visual reasoning tasks with proper head fine-tuning with target data. Such a learning framework has a very advantage: it does not require specific object abstraction (e.g., detection mask) and thus serves more general purposes. Results on two common benchmarks (CATER and ACRE) show consistent improvements achieved by the proposed framework. ",
            "strength_and_weaknesses": "The studied direction is important, and the proposed method took a further step toward a general solution to visual reasoning. Figure 1 clearly demonstrates the difference and advantages between the proposed method and the previous approaches: the proposed framework can perform visual reasoning with neural networks and non-predefined implicit tokens. All the used components: slot token, MAE objective, and transformers (encoder, temporal) are reasonable. Overall the paper is easy to follow.\n\nExtensive experiments are conducted in CATER and ACRE datasets and show the proposed framework consistently outperforms the previous approaches. Ablation studies are conducted thoroughly around masking ratio, total frame numbers, context frame numbers, slot token numbers, slot pool layer, and slot pool methods.\n\nMany analyses and explanations in the current manuscript are intuitive and without supporting empirical evidence. For example, in \"Number of Slot Tokens\" section, experiments found 1 works the best in the CATER benchmark, and the author explained why: \"the model need only maintain an accurate prediction of where the snitch actually or potentially is\". If we add golden snitches up to 2/3/4 (if possible, or in other sims), will the best performance achieve by 2/3/4 implicit slot accordingly? or the best performance is still achieved by 1 slot.\n\nWhat will the performance change if we set the slot token number to 100? will the proposed framework lose the \"representational bottleneck\" properties and lead to drastic performance drops? \n\nIn the transfer learning setting, the multi-class classification formulation for the goal of both CATER and ACRE tasks encodes strong human priors. The tested visual reasoning tasks are actually solved with specific designs for specific tasks in the end, while the studied implicit representations are uniform, and thus, the contribution did not weak a lot.",
            "clarity,_quality,_novelty_and_reproducibility": "Both the clarity and quality are good. The proposed framework may bring some fresh air to the community. The reproducibility is upon the code and pre-trained models released. ",
            "summary_of_the_review": "In general, the reviewer believes the proposed approach takes a step towards visual reasoning with deep learning, while the analysis is intuitive and probably lacks some empirical analysis for strong support.\n\n--------------\nAfter discussing with AC and all other reviewers, we generally believe the current manuscript lacks some thorough analysis of the proposed framework, especially for 1 slot token works pretty well in many scenarios. I encourage the author to take all the points raised in the review in their revision and also keep improving the performance in various benchmarks. Reviewer ejDU lists the key points we discussed in our meeting, and hope them can help contribute to a better revision. \n\nI appreciate the efforts made by the authors. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6068/Reviewer_YXhg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6068/Reviewer_YXhg"
        ]
    },
    {
        "id": "W8C6eaOsDD",
        "original": null,
        "number": 4,
        "cdate": 1667552355694,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667552355694,
        "tmdate": 1667557737040,
        "tddate": null,
        "forum": "V8isglQkt74",
        "replyto": "V8isglQkt74",
        "invitation": "ICLR.cc/2023/Conference/Paper6068/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "A self-supervised training over video is proposed for learning compositional scene representations. The paper explores the approach on the CATER and ACRE datasets (video versions of CLEVR), and show its benefits in the domain of visual reasoning and question answering over these videos.  ",
            "strength_and_weaknesses": "**Strengths**: \n* **Self-supervised**: The approach is self-supervised and so doesn\u2019t require annotations like alternative approaches for building compositional scene representations such as object detectors etc.\n* **dynamics for object-centered representations**: The approach encourages temporal dynamics to be captured in the representations, by using a self-supervised approach over videos. This is an important and still underexplored signal in the domain of self-supervised compositional learning that could greatly help discover objects. \n* **Compositional Generalization**: The approach explores and show good results on compositional and systematic generalization, which gives good indication that the representations learned indeed disentangle the dimensions needed for the downstream reasoning tasks.\n* **Extensive set of experiments on CLEVR-based data**: see quality point in the question below.\n\n**Weaknesses**:\n* **Experiments on synthetic datasets only**: The model is explored on CLEVR-based datasets only. While CLEVR-based data is definitely a great and suitable for exploring the presented approach, I would strongly encourage the authors to explore the approach either on new synthetic multi-object datasets with greater realism and diversity.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Quality**: The paper has concrete motivation (the benefits of self-supervised approaches compared to stronger object-attribute annotations and the potential of temporal dynamics to encourage object discovery) and explores an important domain. While it would be good to experiments on more datasets, the ones presented are extensive, and explore multiple axes of comspositional generalization, performance on downstream reasoning tasks, and sensitivity to variations and ablations. \n\n**Clarity**: The paper is well-written, the idea is clearly presented and it is accompanied with useful diagrams and visualizations. \n\n**Novelty**: \nThe novelty of the idea is a bit limited, as there was a prior paper called ALOE (Attention over learned object embeddings enables complex visual reasoning) that, like the submitted paper, also used a self-supervised approach with a masked loss over frames and a transformer architecture, and it explores it over both on CATER and ACRE as done here as well as CLEVERER. \n\nThere is a difference between the approaches where ALOE uses representations extracted from Monet while the approach here uses ViT, but from prior experience with working on the CLEVR dataset in particular, I think prior models shown with attention they can in a quite straightforward manner manage to pinpoint specific objects from the raw 2d image representation, when coupled with the right losses and overall model and settings, so the new component in the paper doesn\u2019t substantially contribute on addressing an unsolved technical challenge. \n\n**ViT for object-centric on CLEVR vs. real-world**: This is especially straightforward given that the paper explores the model over the CLEVR dataset, since  they use ViT tokens to represent slots, and in CLEVR with choosing the right resolution, they could correspond approximately 1:1 to CLEVR objects \u2013 since they cover local convex regions, compared e.g. to more complicated objects or to non-object real-world regions such as sky etc.  \n\n**ViT vs Monet/sparser approaches**: It\u2019s unclear to me whether ViT would the ideal approach in discovering objects on more general data, or since considering each slot on a dense 2D map to be an \u201cobject\u201d leads to too many false-positive objects, and may struggle to find good correspondences on other datasets. This is a disadvantage compared to Monet used in ALOE, and there are also other object-centered models that discover sparser more compact representations such as Slot Attention, SAVI (extending slots attention to video and explores more diverse video datasets and also a bit of real-world robotic data), and the GroupViT model \u2013 which works very well on diverse real-world images.  \n\n**Claim correctness in introduction**: Furthermore, I find the conceptual distinction the paper tries to make in the introduction between ALOE which uses Monet vs this paper which uses ViT not compelling: the paper says the former is based on pre-processed extracted objects while the latter train the ViT together with the model, but Monet is an unsupervised approach too and one could think of training it together with the frame prediction explored in ALOE. \n\n**Results**: The quantitative results of the paper are in line with prior works but not surpassing them and in particular not surpassing ALOE. Combined together with the novelty issue compared to ALOE that\u2019s a main weakness of the paper. \n",
            "summary_of_the_review": "The paper shows a new approach in a growing relatively under-explored domain. Multiple experiments are presented on two datasets, the paper is easy to follow and has good visualizations, but the main issues are 1) the bit limited novelty of the idea in light of the ALOE paper that explored self-supervised transformer based approaches for both the dataset explored here and an additional one (see details above), and 2) The fact that the paper doesn\u2019t explore other datasets beyond the CLEVR family. I therefore overall recommend weak rejection at this point and recommend exploring how to extend the approach for additional datasets, which may lead also to new modeling ideas that could strengthen the technical contribution of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6068/Reviewer_Cwxs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6068/Reviewer_Cwxs"
        ]
    }
]