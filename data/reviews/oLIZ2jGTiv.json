[
    {
        "id": "npdwGOpjYv-",
        "original": null,
        "number": 1,
        "cdate": 1666343846981,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666343846981,
        "tmdate": 1666595232235,
        "tddate": null,
        "forum": "oLIZ2jGTiv",
        "replyto": "oLIZ2jGTiv",
        "invitation": "ICLR.cc/2023/Conference/Paper3207/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper deal with training a NN with the nonuniform data distribution under the NTK regime. This paper assumes a data-dependent quadrature rule to build bound for non-uniform data. ",
            "strength_and_weaknesses": "Strength:\nThe link between the data-dependent quadrature rule and non-uniform data is super interesting. I also believe this is the right way to conduct the research.\n\nWeakness:\n\nHowever, the paper presents a pretty hard-to-read bond, and I can't understand how different components of the assumptions are involved in the final results. I suggest the authors use a simple model to illustrate how different factors contribute to the bounds. The starting point I suggest is [1-5], which considers kernel gradient descent under Sobolev/L2 losses.\n\nAt the same time, in the abstract and main context, the author claims\n> Small generalization errors of over-parameterized neural networks (NNs) can be partially explained by the frequency biasing phenomenon\nHowever, as I understand, this paper deals with optimization but not the generalization error. I think the author can follow that variance analysis in [1-5] to see how this relates to the generalization properties. I think it would be interesting to see whether the error introduced by the quadrature rule will ruin the generalization error. (Or this can be revealed by the theorem in this version's paper)\n\nMy third concern is about the algorithm, the algorithm is not a gradient descent to original loss but needs to know the P. For a random dataset sampled from an unknown distribution, Is there any way to estimate the P?\n\nAt the same time,  I would like to ask is $H^\\infty P$ diagonalizable? To me this formulation is strange for the matrix here should be the gram matrix, which is symmetry. I guess the matrix should be $\\nabla f^\\top P \\nabla f$.\n\n[1] Pillaud-Vivien L, Rudi A, Bach F. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. Advances in Neural Information Processing Systems, 2018, 31.\n\n[2] Lin J, Rosasco L. Optimal rates for multi-pass stochastic gradient methods[J]. The Journal of Machine Learning Research, 2017, 18(1): 3375-3421.\n\n[3] Nitanda A, Suzuki T. Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime[J]. arXiv preprint arXiv:2006.12297, 2020.\n\n[4] M\u00fccke N, Reiss E. Stochastic gradient descent in Hilbert scales: smoothness, preconditioning and earlier stopping. arXiv preprint arXiv:2006.10840, 2020.\n\n[5] Lu Y, Blanchet J, Ying L. Sobolev Acceleration and Statistical Optimality for Learning Elliptic Equations via Gradient Descent. arXiv preprint arXiv:2205.07331, 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The results is hard to read.  See weakness in the previous section.",
            "summary_of_the_review": "The paper is super interesting, but the results are hard to interpret. Although I have criticized this paper a lot int he weakness part, I still think  the quadrature rule  idea here is super interesting and is worth accepting. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3207/Reviewer_g5Zh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3207/Reviewer_g5Zh"
        ]
    },
    {
        "id": "tcTRd1buS5",
        "original": null,
        "number": 2,
        "cdate": 1666394636076,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666394636076,
        "tmdate": 1666394636076,
        "tddate": null,
        "forum": "oLIZ2jGTiv",
        "replyto": "oLIZ2jGTiv",
        "invitation": "ICLR.cc/2023/Conference/Paper3207/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to use quadratures to approximate the $L_2$ error of NTK training. By doing so, the spectral analysis of NTK training can be extended to non-uniform data. Further, this paper proposes to use a Sobolev norm to modify the loss function so that the convergence under the modified loss function will exhibit different spectral behavior. By doing so the authors were able to tune the spectral bias of NN training.",
            "strength_and_weaknesses": "Strength:\n1. The introduction of the Sobolev-norm loss function is new and interesting. It can explicitly guide NTK training to favor higher frequencies or lower frequencies.\n\nWeakness:\n1. The idea of using quadrature to approximate the loss function seems not well supported. My main concern is that it seems not easy to find good quadrature when given only the training data, namely, identifying proper $c_i$. Theorem 2 bounds the loss by the quadrature approximation error, and Theorem 3 assumes there is a quadrature weight $c_i$ ready to use. None of these indicate that $c_i$ can be determined properly. This heavily decreases the significance of this work.",
            "clarity,_quality,_novelty_and_reproducibility": "The theoretical techniques are presented clearly, but the motivations or reasoning for the assumptions and formulations are a bit lacking. I think the central concern is about the quadrature weights $c_i$. \n\nThe idea of using quadrature and the Sobolev-norm seems novel. Besides that, I believe the NTK analysis along with the frequency bias are from previous works.\n\n\nSome questions I wish the authors can answer:\n1. In particular, I find assuming (17) to hold extremely unrealistic. Can the authors provide a general approach to compute the quadrature weights $c_i$? Can the authors also comment on the difficulty of obtaining accurate quadrature weights, especially the dependence on the input dimension $d$?\n\n2. In Section F.1, the procedure of computing $c_i$ seems very ad hoc. Can the authors provide further explanation on why optimizing over $\\sum c_i^2$ and what does it mean by `the quadrature rule is exact on $\\Pi_{55}^2$', and why $55$ is the meaningful choice?\n\n\n\n",
            "summary_of_the_review": "Given the current state of this paper, I recommend weak reject due to the lack of support on the quadrature weights assumption, Eqn (17).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3207/Reviewer_JXdi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3207/Reviewer_JXdi"
        ]
    },
    {
        "id": "HEatF_szKN",
        "original": null,
        "number": 3,
        "cdate": 1666455711700,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666455711700,
        "tmdate": 1666456017430,
        "tddate": null,
        "forum": "oLIZ2jGTiv",
        "replyto": "oLIZ2jGTiv",
        "invitation": "ICLR.cc/2023/Conference/Paper3207/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Motivated by quadrature rules, this paper studies the convergence of shallow ReLU networks trained on a quadratic loss defined by a symmetric positive definite matrix $P$.  They study the convergence rate along different components of the matrix determined by the NTK and the matrix $P$ for general target functions.  When $P$ is diagonal and the target is bandlimited, the convergence can be understood in terms of the eigenvalues of the NTK integral operator and the spherical harmonic components of the target.  They also demonstrate that by using a Sobolev-based loss function, one can modulate or even reverse the spectral bias by punishing low/high frequencies at different rates.",
            "strength_and_weaknesses": "**Strengths:**\n\nThe paper is well written and offers a new addition to the spectral bias literature by studying general quadratic losses.  By choosing $P$ appropriately one can compensate for bias's of the NTK.  Furthermore the Sobolev-based loss functions allow one to modify or reverse the spectral bias which can be useful when the low-frequency bias is a difficulty such as in value function approximation [1].\n\n**Weaknesses**\n\nThis work shares the weaknesses of other works in this area, such as being limited to the NTK regime, fixing the outer layer weights, specific initialization, limited to shallow feedforward networks, etc.  However since this weakness is present in other works and the community has not figured out yet how to fully overcome these limitations it is reasonable that this work also operates in this regime.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe work is clear and well written.  I do have some comments and questions in the \"Comments and Questions for the authors\" section.\n\n**Quality**\n\nThe results are of high quality.\n\n**Originality**\n\nThis work is original in that it studies general quadratic losses and introduces the Soblev-type loss functions for the modulation of spectral bias.\n\n**Reproducibility**\n\nSince this is primarily a theoretical work with proofs I do not view reproducibility as applicable here.\n",
            "summary_of_the_review": "**Summary**\n\nThis work illustrates how one can modulate the bias arising from the NTK by optimizing a quadratic loss determined by an appropriately chosen matrix $P$.  Furthermore they introduce Sobolev-style loss functions to enhance or reverse the low frequency bias of feedforward networks.  I believe these are contributions that the ICLR community will be interested in, and thus I recommend to accept this paper.\n\n**Comments and Questions for the authors**\n\nNote on page 2 that equation (2) is only true when one is performing gradient descent on the population loss and not the empirical loss.  It is worth emphasizing this to the reader as one in practice only has access to the empirical loss.\n\nOn page 2 you state \u201cUnder the assumptions that the weights do not change much during training, one can consider the NTK in the mean-field regime given the underlying time-independent distribution of $W$, i.e., $K_\\infty(x, x') = \\mathbb{E}_W[K(x, x' ;W)]$ (Du et al., 2018).\u201d \n I think it is more appropriate to call this regime the NTK regime and not the mean-field regime.  The parameterization and dynamics in the NTK setting are distinct from the mean field setting [2].\n\nOn page 4 you state \u201cAs a consequence of Proposition 1, the matrix $H^\\infty P$ has positive eigenvalues, which we denote by $\\lambda_{n - 1} \\geq \\cdots \\geq \\lambda_0 > 0$\u201d.  This is true but it is not obvious that $H^\\infty P$ is diagonalizable.  Worth referencing your discussion in page 1 in the Appendix here.\n\nThe formatting of Theorem 1 seems off.  Can you put the theorem statement in a theorem environment which should put the statement in italics.  Or alternatively, could you introduce a different formatting to make the statement stick out from the surrounding text.\n\nOn page 5 you state \u201cTherefore, our rate of convergence does not vanish as\nthe number of training data points $n \\rightarrow \\infty$, provided that $M_{\\textbf{P}} = \\mathcal{O}(n^{\u22121/2})$\n. This is the case when $P = n^{-1}I$ which corresponds to eq. (1).  So, we overcome the vanishing convergence rate issue that appears in previous analyses of frequency biasing (Arora et al., 2019; Basri et al., 2019). As $\\eta$ decreases, the gradient descent algorithm gets closer to the gradient flow algorithm (Du et al., 2018), which allows us to more accurately quantify the frequency biasing (see section 4).\u201d  This is not a fair comparison because Arora et al. [3]  and Basri et al. [4] work with the unnormalized squared error ($P = I$ without $1/n$ normalization) versus the choice $P = n^{-1} I$ corresponds to the mean squared error.  If you set $P = I$ you still have the same issue with vanishing convergence and learning rates as you send $n \\rightarrow \\infty$.\n\nOn page 6 you state \u201cUnder the reasonable assumption that our quadrature rule satisfies eq. (17), we can bound the quadrature errors appearing in Theorem 2.\u201d  Is there any way to verify this assumption, at least in reasonable settings?\n\nOn page 6 in Theorem 3, may be worth stating that $\\ell$ in the theorem comes from equation (17)\n\nOn page 8 you state that you \u201ccompute the quadrature weights $(c_i)_{i=1}^n$ for the loss function $\\tilde{\\Phi}$ in eq. (3).\u201d Perhaps mention how you compute the quadrature weights.\n\nOn page 9 in the last paragraph you state \u201cHere, we present the results of the autoencoder for image denoising using the MNIST dataset LeCun et al. (2010)\u201d, however the figure is earlier on the page.  Also there is no conclusion section and the article ends abruptly.  This issue should be fixed before the final version.\n\nOn page 1 in the Appendix you state \u201cSince $H^\\infty$ are symmetric positive definite matrices (see Proposition 1), $H^\\infty P$ has positive real eigenvalues.\u201d  I think you meant to say \u201cSince $H^\\infty$, $P$ are symmetric positive definite matrices\u201d\n\nReferences:\n\n[1] Ge Yang, Anurag Ajay, and Pulkit Agrawal. Overcoming the spectral bias of neural value approximation. In International Conference on Learning Representations, 2022. URL https:\n//openreview.net/forum?id=vIC-xLFuM6.\n\n[2] Mei et al. \u201cA mean field view of the landscape of two-layer neural\nnetworks\u201d. In: Proceedings of the National Academy of Sciences (2018).\n\n[3] S. Arora, S. S. Du, W. Hu, Z. Li, and R. Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In Inter. Conf. Mach. Learn., pp. 322\u2013332. PMLR, 2019.\n\n[4] R. Basri, D. Jacobs, Y. Kasten, and S. Kritchman. The convergence rate of neural networks for learned functions of different frequencies. Adv. Neur. Info. Proc. Syst., 32, 2019.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3207/Reviewer_XAwZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3207/Reviewer_XAwZ"
        ]
    },
    {
        "id": "95mByJQddhK",
        "original": null,
        "number": 4,
        "cdate": 1667116328621,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667116328621,
        "tmdate": 1668960412344,
        "tddate": null,
        "forum": "oLIZ2jGTiv",
        "replyto": "oLIZ2jGTiv",
        "invitation": "ICLR.cc/2023/Conference/Paper3207/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Summary:\n\nThe paper considers the case of non-uniform data in a sphere, and gives a way to demonstrate the frequency bias of neural net learning in the NTK regime. \n",
            "strength_and_weaknesses": "Strengths:\n\nThe paper is well-written and the key ideas in the Theorems are reasonably easy to understand.\n\nWeaknesses:\n\n1. The main issue I have with the paper is the objective of training. The paper assumes one has access to training data from a non-uniform distribution, but the final goal is to minimize error according to the uniform distribution (or L2 loss). This looks like a domain adaptation setting. If this is indeed the goal of the paper this should be made clear. Even if this is so, it seems like a very restricted setting of domain adaptation.\n\n2. For example, in Figure 1, what is the loss (and the final/intermediate trained model output) for training with $\\Phi$ and $\\tilde\\Phi$? Looking at how all the three dashed curves are below the solid curves seems to indicate that the model trained with $\\tilde \\Phi$ is strictly better than the model trained with $\\Phi$ at all points, but that seems at odds with the intuition that the model trained with $\\Phi$ would be better in the region of the circle with more data points than average.\n\n\n3. The Sobolev norm frequency biasing doesn't seem to do any meaningful tradeoffs (in Figure 2). i.e. choosing a larger value of s does seem to make the loss on low-frequency components significantly worse, but the loss on high-frequency doesn't seem to reduce nearly as much.  Is the main claim that choosing negative s and emphasising low-frequency components in the optimisation better than not emphasising any particular component by choosing s=0? If so, couldn't this same effect be achieved by NTK from a different initialisation with smaller variance?\n\n4. Similar comments on Figure 3. Choosing s=-1 makes the red and yellow curve higher than s=0, without meaningfully lowering the blue curve. \n\n5. MNIST experiment: There seems to be some confusion between high-frequency label noise components on the dataset and high-frequency noise components on a single image. The low-frequency bias on NTK says nothing about high or low frequency noise on a single image. \n\n\n======================\n\nAfter discussion phase. Having made the domain adaptation setting made clear solves several of the issues I had with the paper. I am raising my score to a marginal accept.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe motivation to study the problem is not clear and requires more explanation. Some sentences and definitions seem incompplete/imprecise and needs more elucidation. \n\ne.g. in the para before Section 4 :  \"our rate of convergence does not vanish ...\" needs more clarification.\ne.g. Equation 15: E_c is defined for single functions over the domain. Does E_c[g_j g_l] stand for E_c of the pointwise product of the two functions g_j, g_l? If so, this needs clarification.\n\n\n\n",
            "summary_of_the_review": "The paper tackles a novel problem whose significance seems questionable. The technical tools developed seem somewhat useful nevertheless. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3207/Reviewer_F3bk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3207/Reviewer_F3bk"
        ]
    }
]